1
00:00:00,080 --> 00:00:05,520
welcome to this Cuda programming course

2
00:00:02,840 --> 00:00:08,200
where you will learn to leverage gpus

3
00:00:05,520 --> 00:00:10,599
for high performance Computing the

4
00:00:08,200 --> 00:00:13,639
course starts with an overview of the

5
00:00:10,599 --> 00:00:16,160
deep learning ecosystem and guides you

6
00:00:13,639 --> 00:00:19,039
through setting up Cuda and reviewing

7
00:00:16,160 --> 00:00:21,439
essential C and C++ Concepts you'll

8
00:00:19,039 --> 00:00:24,279
explore GPU architecture and write your

9
00:00:21,439 --> 00:00:27,359
first Cuda kernels Advanced topics

10
00:00:24,279 --> 00:00:30,439
include optimizing matrix multiplication

11
00:00:27,359 --> 00:00:32,360
and extending pie torch with practical

12
00:00:30,439 --> 00:00:35,680
applications like implementing a

13
00:00:32,360 --> 00:00:39,239
multi-layer perception for the mest data

14
00:00:35,680 --> 00:00:41,920
set Elliot Alid created this course so

15
00:00:39,239 --> 00:00:44,360
what is Cuda or compute unified device

16
00:00:41,920 --> 00:00:46,480
architecture by Nvidia uh my name is

17
00:00:44,360 --> 00:00:48,520
Elliot and I'm an instructor on free

18
00:00:46,480 --> 00:00:50,920
code Camp as well as a student studying

19
00:00:48,520 --> 00:00:54,480
for my computer science degree so in

20
00:00:50,920 --> 00:00:56,480
this course I bring to you Cuda for deep

21
00:00:54,480 --> 00:00:57,760
learning but don't let that repel you if

22
00:00:56,480 --> 00:00:58,719
you're not in deep learning because

23
00:00:57,760 --> 00:01:01,039
there's still a lot that we're going to

24
00:00:58,719 --> 00:01:03,280
be able to cover uh many other fields of

25
00:01:01,039 --> 00:01:05,360
parallel programming so this is more

26
00:01:03,280 --> 00:01:08,240
oriented for deep learning but not

27
00:01:05,360 --> 00:01:10,759
specifically aimed at it um there's

28
00:01:08,240 --> 00:01:12,600
going to be a lot covered here so uh

29
00:01:10,759 --> 00:01:15,400
I'll show what the final prodject what

30
00:01:12,600 --> 00:01:16,759
the final project is first um so that

31
00:01:15,400 --> 00:01:17,840
you can get a feel forward and see kind

32
00:01:16,759 --> 00:01:20,119
of what we're going to end up building

33
00:01:17,840 --> 00:01:21,600
by the end um and then we'll just kind

34
00:01:20,119 --> 00:01:23,200
of go from there so before we get

35
00:01:21,600 --> 00:01:26,280
started with anything crazy I should

36
00:01:23,200 --> 00:01:27,880
include a disclaimer um this course may

37
00:01:26,280 --> 00:01:29,320
not be fully up to date by the time

38
00:01:27,880 --> 00:01:30,680
you're watching this if you're watching

39
00:01:29,320 --> 00:01:32,479
this 10 years years down the line from

40
00:01:30,680 --> 00:01:33,840
when I've released it it might not all

41
00:01:32,479 --> 00:01:36,360
be the same there might be things that

42
00:01:33,840 --> 00:01:38,560
are updated the new uh compute

43
00:01:36,360 --> 00:01:39,680
capabilities might be you know way

44
00:01:38,560 --> 00:01:41,520
better there might be a bunch of

45
00:01:39,680 --> 00:01:43,200
different stuff happening so I'm not too

46
00:01:41,520 --> 00:01:47,240
sure where the ecosystem will be at in

47
00:01:43,200 --> 00:01:49,159
10 years but as of 2024 this is pretty

48
00:01:47,240 --> 00:01:50,960
much the best you're going to get so

49
00:01:49,159 --> 00:01:52,920
just trying to include that and I

50
00:01:50,960 --> 00:01:55,280
thought I'd try to make everything uh

51
00:01:52,920 --> 00:01:57,600
not entirely centered around time so you

52
00:01:55,280 --> 00:01:59,960
can go back into this version uh or or

53
00:01:57,600 --> 00:02:01,719
certain Cuda versions and reproduce all

54
00:01:59,960 --> 00:02:03,399
the same stuff it just might be a little

55
00:02:01,719 --> 00:02:05,240
bit different down the line if you're

56
00:02:03,399 --> 00:02:08,280
watching this later on so why did I

57
00:02:05,240 --> 00:02:10,520
create this course exactly well a lot of

58
00:02:08,280 --> 00:02:12,920
these performance and kernel engineering

59
00:02:10,520 --> 00:02:14,360
jobs require a lot of knowledge they

60
00:02:12,920 --> 00:02:16,560
require a lot of experience in the

61
00:02:14,360 --> 00:02:18,280
industry uh and it's just really hard to

62
00:02:16,560 --> 00:02:20,760
get up to that point where you're able

63
00:02:18,280 --> 00:02:23,040
to compete with the top and the best of

64
00:02:20,760 --> 00:02:24,160
the best performance Engineers so these

65
00:02:23,040 --> 00:02:27,000
are the people that are writing the

66
00:02:24,160 --> 00:02:30,400
training runs for like gbt 4 gbt 5 all

67
00:02:27,000 --> 00:02:32,280
of this um you need a lot of skill to

68
00:02:30,400 --> 00:02:34,840
optimize a massive neural network

69
00:02:32,280 --> 00:02:37,879
training run and inference on a larg

70
00:02:34,840 --> 00:02:40,280
data center or compute cluster so this

71
00:02:37,879 --> 00:02:42,519
aims to prevent some of that manual

72
00:02:40,280 --> 00:02:45,280
weaving on your part still encouraging

73
00:02:42,519 --> 00:02:47,200
you to do so U on your own but prevent

74
00:02:45,280 --> 00:02:49,239
some of that hardcore labor of going

75
00:02:47,200 --> 00:02:51,440
through and really figuring things out

76
00:02:49,239 --> 00:02:53,159
on your own from scratch uh that's one

77
00:02:51,440 --> 00:02:56,080
of the reasons why I created this

78
00:02:53,159 --> 00:02:58,400
another one is like generally speaking

79
00:02:56,080 --> 00:03:00,080
the point of writing GPU kernels or

80
00:02:58,400 --> 00:03:02,480
playing with code at all on the GPU is

81
00:03:00,080 --> 00:03:05,760
to run something faster so if you have a

82
00:03:02,480 --> 00:03:08,879
nested Loop um you know it's like 4 I in

83
00:03:05,760 --> 00:03:11,040
range 4J in range four 4K in range

84
00:03:08,879 --> 00:03:12,400
whatever however many you want to put uh

85
00:03:11,040 --> 00:03:15,599
essentially what parallel programming

86
00:03:12,400 --> 00:03:17,799
and Cuda allow us to do is unroll those

87
00:03:15,599 --> 00:03:19,680
so if you take like for example four ion

88
00:03:17,799 --> 00:03:22,760
range you could take each little thing

89
00:03:19,680 --> 00:03:25,040
in that and run that instruction on a

90
00:03:22,760 --> 00:03:27,599
different CA cor so if you have 10,000

91
00:03:25,040 --> 00:03:29,400
cacor and you have 10,000 different

92
00:03:27,599 --> 00:03:32,519
iterations in your Loop then you can

93
00:03:29,400 --> 00:03:35,840
affect L do each iteration in a single

94
00:03:32,519 --> 00:03:37,480
instruction or a single thread on on the

95
00:03:35,840 --> 00:03:39,879
GPU so this is some of the things that

96
00:03:37,480 --> 00:03:42,280
allows us to do you're going to use your

97
00:03:39,879 --> 00:03:44,000
job of uh you're going to use your your

98
00:03:42,280 --> 00:03:45,519
knowledge of GPU architecture kernel

99
00:03:44,000 --> 00:03:46,879
launch configurations and a bunch of

100
00:03:45,519 --> 00:03:48,879
other cool stuff we end up learning in

101
00:03:46,879 --> 00:03:51,360
this course to make that code run as

102
00:03:48,879 --> 00:03:54,000
fast as possible uh and then the last

103
00:03:51,360 --> 00:03:56,000
one is really there's so much data

104
00:03:54,000 --> 00:03:58,640
nowadays they say we have way too much

105
00:03:56,000 --> 00:04:00,560
data but very little cleaned data I've

106
00:03:58,640 --> 00:04:03,400
taken everything from all the other

107
00:04:00,560 --> 00:04:06,000
video courses everything on the internet

108
00:04:03,400 --> 00:04:08,879
and YouTube uh and I put them in a

109
00:04:06,000 --> 00:04:10,879
single course so I filtered out a bunch

110
00:04:08,879 --> 00:04:12,879
of the nonsense a lot of you know the

111
00:04:10,879 --> 00:04:14,840
old stuff a lot of the new stuff that

112
00:04:12,879 --> 00:04:17,600
maybe isn't covered as well and kind of

113
00:04:14,840 --> 00:04:19,680
just projected into this one Masterpiece

114
00:04:17,600 --> 00:04:21,400
so this includes topics covered by paid

115
00:04:19,680 --> 00:04:23,120
courses as well I haven't actually paid

116
00:04:21,400 --> 00:04:24,360
for them but I kind of just looked at

117
00:04:23,120 --> 00:04:25,800
you know what are the chapters that they

118
00:04:24,360 --> 00:04:28,600
cover and then include some of those

119
00:04:25,800 --> 00:04:30,199
important Concepts in this course um I

120
00:04:28,600 --> 00:04:31,680
do have links for YouTube videos and all

121
00:04:30,199 --> 00:04:33,440
of these resources which I've gone

122
00:04:31,680 --> 00:04:34,759
through only the high quality ones but

123
00:04:33,440 --> 00:04:36,840
I've gone through a lot of these videos

124
00:04:34,759 --> 00:04:39,600
and resources and these are all going to

125
00:04:36,840 --> 00:04:41,240
be uh put in links inside of the um

126
00:04:39,600 --> 00:04:43,080
GitHub Link in the description so

127
00:04:41,240 --> 00:04:45,199
everything you need is going to be there

128
00:04:43,080 --> 00:04:48,560
um and I put a lot of all of those links

129
00:04:45,199 --> 00:04:51,479
in that um in that link so what are some

130
00:04:48,560 --> 00:04:52,759
use cases for Cuda parallel GPU

131
00:04:51,479 --> 00:04:55,160
programming what are some of the use

132
00:04:52,759 --> 00:04:57,120
cases for this well you have graphics

133
00:04:55,160 --> 00:04:58,400
and rate tracing so the computer

134
00:04:57,120 --> 00:05:01,160
Graphics that you're seeing in video

135
00:04:58,400 --> 00:05:03,560
games um you know user interfaces all of

136
00:05:01,160 --> 00:05:06,280
this you have fluid simulation for like

137
00:05:03,560 --> 00:05:09,479
physics um and modeling you know engine

138
00:05:06,280 --> 00:05:10,960
Dynamics you have video editing so the

139
00:05:09,479 --> 00:05:13,800
video that I'm editing for this right

140
00:05:10,960 --> 00:05:16,600
now is using uh parallel Computing to

141
00:05:13,800 --> 00:05:18,280
render uh crypto mining which a lot of

142
00:05:16,600 --> 00:05:20,800
you might be doing already that's going

143
00:05:18,280 --> 00:05:22,759
to be using uh you know your GPU

144
00:05:20,800 --> 00:05:25,080
hardware and some of the advantages of

145
00:05:22,759 --> 00:05:27,280
that to like mine through the the crypto

146
00:05:25,080 --> 00:05:29,800
mining problems and then you have 3D

147
00:05:27,280 --> 00:05:30,800
modeling and software like blender so

148
00:05:29,800 --> 00:05:32,280
when you have a bunch of different

149
00:05:30,800 --> 00:05:33,600
points going on and you have to render

150
00:05:32,280 --> 00:05:36,080
things it's essentially the same as

151
00:05:33,600 --> 00:05:40,080
video editing but just um 3D instead of

152
00:05:36,080 --> 00:05:42,400
2D so the last one which you probably

153
00:05:40,080 --> 00:05:44,440
guessed it already is deep learning so

154
00:05:42,400 --> 00:05:46,240
the number one use case for Cuda right

155
00:05:44,440 --> 00:05:49,080
now is primarily what I'll be covering

156
00:05:46,240 --> 00:05:50,639
in this course which is deep learning so

157
00:05:49,080 --> 00:05:54,120
we're not going to go as as deep into

158
00:05:50,639 --> 00:05:56,120
like say convolutions but uh to kind of

159
00:05:54,120 --> 00:05:58,120
understand how to optimize an algorithm

160
00:05:56,120 --> 00:06:00,520
like matrix multiplication uh we're

161
00:05:58,120 --> 00:06:02,639
going to go quite in depth with that

162
00:06:00,520 --> 00:06:04,560
so now you might ask Elliot what are the

163
00:06:02,639 --> 00:06:06,800
requirements or the prerequisites for

164
00:06:04,560 --> 00:06:08,560
this course so there are some that are

165
00:06:06,800 --> 00:06:11,560
more intellectual and academic and there

166
00:06:08,560 --> 00:06:13,440
are some that aren't so this is strictly

167
00:06:11,560 --> 00:06:15,240
for NVIDIA gpus in case you didn't catch

168
00:06:13,440 --> 00:06:17,479
on to that earlier um if you don't have

169
00:06:15,240 --> 00:06:19,919
one you can always consider renting uh

170
00:06:17,479 --> 00:06:21,639
the cheapest ones in the cloud um I

171
00:06:19,919 --> 00:06:23,759
advise you to look into the pricing

172
00:06:21,639 --> 00:06:26,360
before giving a definite no on the

173
00:06:23,759 --> 00:06:28,039
pricing for some of these Cloud gpus um

174
00:06:26,360 --> 00:06:30,919
at first I was actually surprised how

175
00:06:28,039 --> 00:06:33,639
low the cost was for some cloud

176
00:06:30,919 --> 00:06:35,639
instances um especially the non-comp

177
00:06:33,639 --> 00:06:38,120
compute demanding ones so if you have

178
00:06:35,639 --> 00:06:39,880
like only a CPU or like a ram intensive

179
00:06:38,120 --> 00:06:42,039
machine it might actually cost

180
00:06:39,880 --> 00:06:44,800
significantly less than one with gpus on

181
00:06:42,039 --> 00:06:46,880
it um the gpus one are still very cheap

182
00:06:44,800 --> 00:06:49,039
you can use things like vast AI which

183
00:06:46,880 --> 00:06:50,520
I'll cover a little bit more um you can

184
00:06:49,039 --> 00:06:52,840
use this for actually getting really

185
00:06:50,520 --> 00:06:55,759
cheap uh consumer grade Hardware that

186
00:06:52,840 --> 00:06:57,360
you can SSH into in the cloud um and

187
00:06:55,759 --> 00:07:00,319
then just do all of your experiments and

188
00:06:57,360 --> 00:07:02,360
go through the course on that

189
00:07:00,319 --> 00:07:05,800
you can continue uh you can continue

190
00:07:02,360 --> 00:07:07,960
running with any you know NVIDIA GTX RTX

191
00:07:05,800 --> 00:07:09,599
or data center level gpus so all of the

192
00:07:07,960 --> 00:07:11,520
Nvidia cards are pretty much supported

193
00:07:09,599 --> 00:07:13,759
for this uh maybe like the lower ones

194
00:07:11,520 --> 00:07:16,160
that are like 15 years old those might

195
00:07:13,759 --> 00:07:18,520
not work um but generally if you have

196
00:07:16,160 --> 00:07:21,440
like a GTX like 1660 or something like

197
00:07:18,520 --> 00:07:24,800
that it's like it's going to be fine

198
00:07:21,440 --> 00:07:26,840
um as for course prequisites Python

199
00:07:24,800 --> 00:07:28,080
Programming will help in understanding

200
00:07:26,840 --> 00:07:30,319
while we're implementing in lower

201
00:07:28,080 --> 00:07:32,280
languages so

202
00:07:30,319 --> 00:07:34,720
um just understanding the whole

203
00:07:32,280 --> 00:07:36,960
programming uh Concepts is really what's

204
00:07:34,720 --> 00:07:38,280
going to be needed here again all these

205
00:07:36,960 --> 00:07:42,039
different languages is just like a

206
00:07:38,280 --> 00:07:43,919
change in syntax right so um you know

207
00:07:42,039 --> 00:07:46,319
we're going to use basic differentiation

208
00:07:43,919 --> 00:07:48,800
and Vector calculus uh that'll make

209
00:07:46,319 --> 00:07:50,319
learning easier if you know it already

210
00:07:48,800 --> 00:07:52,639
um it's really only required for

211
00:07:50,319 --> 00:07:53,840
intuition behind back propagation and

212
00:07:52,639 --> 00:07:56,280
some of the stuff we're going to use to

213
00:07:53,840 --> 00:07:58,319
build neural networks from scratch um

214
00:07:56,280 --> 00:08:00,599
linear algebra will definitely make your

215
00:07:58,319 --> 00:08:02,960
life easier by not having to learn

216
00:08:00,599 --> 00:08:05,639
fundamental algorithms from scratch so

217
00:08:02,960 --> 00:08:07,800
like if you're not really intuitively um

218
00:08:05,639 --> 00:08:10,280
you know into matrix multiplication yet

219
00:08:07,800 --> 00:08:12,560
if you haven't really uh you know gone

220
00:08:10,280 --> 00:08:14,520
into that extensively it might be a

221
00:08:12,560 --> 00:08:16,680
little hard for you to catch up uh but

222
00:08:14,520 --> 00:08:18,520
matrix multiplication is very easy it's

223
00:08:16,680 --> 00:08:20,639
quite trivial in retrospect it's very

224
00:08:18,520 --> 00:08:22,720
it's very easy to understand um but just

225
00:08:20,639 --> 00:08:24,000
the intuition there and optimizing it

226
00:08:22,720 --> 00:08:26,720
might be a little hard if you haven't

227
00:08:24,000 --> 00:08:28,879
worked with it a lot already

228
00:08:26,720 --> 00:08:31,159
um then if you really care I would

229
00:08:28,879 --> 00:08:34,240
recommend just reviewing you know Matrix

230
00:08:31,159 --> 00:08:36,680
uh transpose matrix multiplication chain

231
00:08:34,240 --> 00:08:38,959
rule from calculus and then difference

232
00:08:36,680 --> 00:08:40,279
between gradients and derivatives um

233
00:08:38,959 --> 00:08:42,200
there's maybe a few more that I missed

234
00:08:40,279 --> 00:08:43,640
but those are like the general ideas

235
00:08:42,200 --> 00:08:46,720
that you're going to need for going into

236
00:08:43,640 --> 00:08:48,440
this um and then just a heads up uh if

237
00:08:46,720 --> 00:08:50,760
you are in a Windows machine this might

238
00:08:48,440 --> 00:08:54,360
be a little harder for you so I do have

239
00:08:50,760 --> 00:08:56,240
a little setup guide on Windows Hardware

240
00:08:54,360 --> 00:08:58,760
um but I I do everything here on on

241
00:08:56,240 --> 00:09:00,519
Ubuntu Linux so this is what I'm running

242
00:08:58,760 --> 00:09:01,560
uh just on my local machine here and

243
00:09:00,519 --> 00:09:05,160
this is what we're going to go through

244
00:09:01,560 --> 00:09:08,760
the course with um you can always use uh

245
00:09:05,160 --> 00:09:11,800
WSL on Windows to simulate a a Linux

246
00:09:08,760 --> 00:09:13,399
system or you can use uh Docker so

247
00:09:11,800 --> 00:09:15,920
Docker is an awesome tool that'll allow

248
00:09:13,399 --> 00:09:18,399
you to essentially fire up uh a little

249
00:09:15,920 --> 00:09:20,760
simulated Linux machine uh just in your

250
00:09:18,399 --> 00:09:22,920
terminal on Windows and you can just do

251
00:09:20,760 --> 00:09:24,880
everything through that uh I think it

252
00:09:22,920 --> 00:09:26,839
supports Nvidia gpus directly through

253
00:09:24,880 --> 00:09:28,839
Windows I'm not entirely sure yet I

254
00:09:26,839 --> 00:09:30,279
haven't tested that but um if you're on

255
00:09:28,839 --> 00:09:32,800
a Windows machine machine I would

256
00:09:30,279 --> 00:09:35,240
recommend uh WSL or

257
00:09:32,800 --> 00:09:37,720
Docker if you do run into errors or

258
00:09:35,240 --> 00:09:40,120
issues throughout this uh I do suggest

259
00:09:37,720 --> 00:09:43,279
you you check GitHub stack Overflow

260
00:09:40,120 --> 00:09:45,000
Nvidia developer forums pytorch docks uh

261
00:09:43,279 --> 00:09:47,040
if your issu is related to any of this

262
00:09:45,000 --> 00:09:48,760
course material so you know you have a

263
00:09:47,040 --> 00:09:50,120
lot of resources at your disposal if you

264
00:09:48,760 --> 00:09:51,920
need to resolve an error that doesn't

265
00:09:50,120 --> 00:09:53,360
come up in the course material uh you

266
00:09:51,920 --> 00:09:55,160
also have really powerful language

267
00:09:53,360 --> 00:09:56,880
models to use there's a lot of language

268
00:09:55,160 --> 00:09:58,279
models that have been released recently

269
00:09:56,880 --> 00:10:00,320
that are really really good at solving

270
00:09:58,279 --> 00:10:02,880
and addressing coding problem s so I do

271
00:10:00,320 --> 00:10:06,640
suggest you try those out um if all if

272
00:10:02,880 --> 00:10:08,399
all goes wrong right um all the all the

273
00:10:06,640 --> 00:10:10,320
code and notes for this are kept in the

274
00:10:08,399 --> 00:10:11,800
GitHub repo in the description the

275
00:10:10,320 --> 00:10:13,680
ecosystem is going to change all the

276
00:10:11,800 --> 00:10:16,240
time so in case this video isn't up to

277
00:10:13,680 --> 00:10:17,720
date uh the GitHub repo will be because

278
00:10:16,240 --> 00:10:20,240
I'm able to push that and actually make

279
00:10:17,720 --> 00:10:22,279
changes so if something is a little off

280
00:10:20,240 --> 00:10:24,079
in here you might want to go check in

281
00:10:22,279 --> 00:10:25,560
the repo and see like what it actually

282
00:10:24,079 --> 00:10:27,160
looks like so that you can actually

283
00:10:25,560 --> 00:10:28,839
write it properly and maybe there's a

284
00:10:27,160 --> 00:10:32,120
more optimized version thing things will

285
00:10:28,839 --> 00:10:34,720
change but you get the point uh I do

286
00:10:32,120 --> 00:10:36,000
suggest following uh the repo for

287
00:10:34,720 --> 00:10:39,839
maintaining a structured learning

288
00:10:36,000 --> 00:10:41,800
approach I include excal draw diagrams

289
00:10:39,839 --> 00:10:43,320
so this is going to help illustrate like

290
00:10:41,800 --> 00:10:45,639
high level ideas how we're going to

291
00:10:43,320 --> 00:10:47,440
approach things uh as well as how to do

292
00:10:45,639 --> 00:10:50,959
things on the level of Kernel

293
00:10:47,440 --> 00:10:52,399
optimization so all the way top down all

294
00:10:50,959 --> 00:10:53,880
of it excal draw is awesome for

295
00:10:52,399 --> 00:10:55,839
illustrating things and it's completely

296
00:10:53,880 --> 00:10:58,240
free so all the diagrams there will be

297
00:10:55,839 --> 00:11:00,399
included in the in the GitHub repo uh

298
00:10:58,240 --> 00:11:03,000
and in the course too

299
00:11:00,399 --> 00:11:04,399
um you know you can always uh reach out

300
00:11:03,000 --> 00:11:07,000
to me through my Discord server which

301
00:11:04,399 --> 00:11:08,480
will also be in the GitHub repo um and

302
00:11:07,000 --> 00:11:10,040
you can reach out to me through there

303
00:11:08,480 --> 00:11:11,680
and talk with the community there's

304
00:11:10,040 --> 00:11:13,560
going to be a lot of other students

305
00:11:11,680 --> 00:11:16,079
learning there's going to be a dedicated

306
00:11:13,560 --> 00:11:18,079
set of channels for this so in case you

307
00:11:16,079 --> 00:11:20,320
get stuck or wanted to discuss something

308
00:11:18,079 --> 00:11:22,480
or just have a cool chat in the server

309
00:11:20,320 --> 00:11:24,519
uh you can totally join that I do want

310
00:11:22,480 --> 00:11:27,000
to note early on that this course isn't

311
00:11:24,519 --> 00:11:29,320
on Cuda only so there's a few things

312
00:11:27,000 --> 00:11:31,800
that I cover outside of it including

313
00:11:29,320 --> 00:11:34,880
pytorch referencing uh going into like

314
00:11:31,800 --> 00:11:37,480
Triton and c and C++ with like

315
00:11:34,880 --> 00:11:39,720
externally not including Cuda just to

316
00:11:37,480 --> 00:11:41,360
you know help illustrate things on how

317
00:11:39,720 --> 00:11:44,040
how that the naive version of an

318
00:11:41,360 --> 00:11:45,399
algorithm works but uh so there's

319
00:11:44,040 --> 00:11:47,839
there's the code side and then there's

320
00:11:45,399 --> 00:11:49,959
also um I'm going to provide some

321
00:11:47,839 --> 00:11:52,519
prerequisites or not even prerequisites

322
00:11:49,959 --> 00:11:54,639
but rather just uh a good understanding

323
00:11:52,519 --> 00:11:57,399
about the whole deep learning ecosystem

324
00:11:54,639 --> 00:11:59,440
so this is actually what one of the next

325
00:11:57,399 --> 00:12:01,240
chapters is going to be about is how

326
00:11:59,440 --> 00:12:03,200
does the whole ecosystem work and where

327
00:12:01,240 --> 00:12:05,160
can I apply Cuda it would be a little

328
00:12:03,200 --> 00:12:06,880
silly of me to say here's how you

329
00:12:05,160 --> 00:12:09,360
optimize a kernel and make it run really

330
00:12:06,880 --> 00:12:11,519
really fast on your Hardware but not

331
00:12:09,360 --> 00:12:14,000
actually give you some solid use cases

332
00:12:11,519 --> 00:12:15,440
for that so you might already know what

333
00:12:14,000 --> 00:12:16,959
the use case is but in case you're just

334
00:12:15,440 --> 00:12:19,320
trying to learn Cuda and you might look

335
00:12:16,959 --> 00:12:22,399
at some ways that you can apply it I

336
00:12:19,320 --> 00:12:24,199
provide that Resource as well so spoiler

337
00:12:22,399 --> 00:12:26,040
alert but some takeaways you might get

338
00:12:24,199 --> 00:12:28,519
from this course is that through

339
00:12:26,040 --> 00:12:30,120
experiment experimentation and research

340
00:12:28,519 --> 00:12:32,399
you'll learn that the main GPU

341
00:12:30,120 --> 00:12:34,560
performance bottleneck is memory

342
00:12:32,399 --> 00:12:36,920
bandwidth so in deep learning we have

343
00:12:34,560 --> 00:12:39,079
these giant inscrutable matrices that

344
00:12:36,920 --> 00:12:41,760
cannot fit into the onchip memory at

345
00:12:39,079 --> 00:12:44,440
once so think about if you have like a

346
00:12:41,760 --> 00:12:46,560
giant cluster of gpus and each of them

347
00:12:44,440 --> 00:12:48,519
have really really fast tensor cores

348
00:12:46,560 --> 00:12:50,519
these are like super optimized for doing

349
00:12:48,519 --> 00:12:52,959
you know tensor operations in deep

350
00:12:50,519 --> 00:12:55,800
learning um but if you're doing these

351
00:12:52,959 --> 00:12:58,320
across many gpus you really have to

352
00:12:55,800 --> 00:13:00,560
exchange and and and mix and manage

353
00:12:58,320 --> 00:13:03,360
information between them so you end up

354
00:13:00,560 --> 00:13:05,760
sending electrons uh you know from this

355
00:13:03,360 --> 00:13:07,240
node to this node to this node to right

356
00:13:05,760 --> 00:13:10,199
and there's a lot of this communication

357
00:13:07,240 --> 00:13:12,399
that's going on so you really get a ton

358
00:13:10,199 --> 00:13:14,800
of speed from the compute inside of the

359
00:13:12,399 --> 00:13:16,360
chips but when it comes to communicating

360
00:13:14,800 --> 00:13:18,800
there's actually a a pretty big

361
00:13:16,360 --> 00:13:22,160
bottleneck there and that's you know one

362
00:13:18,800 --> 00:13:25,120
thing that you might take away from this

363
00:13:22,160 --> 00:13:27,720
um there's also on chip constraints too

364
00:13:25,120 --> 00:13:31,079
so you have like GPU vram which is going

365
00:13:27,720 --> 00:13:33,279
to be uh you know comparatively slow to

366
00:13:31,079 --> 00:13:35,360
what the on chip stuff is so vram is

367
00:13:33,279 --> 00:13:37,560
like off the actual you know cores and

368
00:13:35,360 --> 00:13:39,720
all this and then it has to communicate

369
00:13:37,560 --> 00:13:41,760
with the cores and all the the the

370
00:13:39,720 --> 00:13:43,839
shared memory on chip and all the

371
00:13:41,760 --> 00:13:45,519
registers and that ends up being a ball

372
00:13:43,839 --> 00:13:49,079
neck too so it's not just the the

373
00:13:45,519 --> 00:13:51,240
massive um the massive matrices

374
00:13:49,079 --> 00:13:53,079
communicating across a lot of gpus it's

375
00:13:51,240 --> 00:13:54,519
actually a lot of the onchip

376
00:13:53,079 --> 00:13:56,759
communication too so there's multiple

377
00:13:54,519 --> 00:13:59,279
bottleneck that that's arise or that

378
00:13:56,759 --> 00:14:01,199
that arise um but these are just things

379
00:13:59,279 --> 00:14:03,399
that you'll end up coming across and and

380
00:14:01,199 --> 00:14:06,519
being able to address later on through

381
00:14:03,399 --> 00:14:07,839
optimizations another key takeaway is

382
00:14:06,519 --> 00:14:10,199
would be to take an existing

383
00:14:07,839 --> 00:14:11,480
implementation and make it faster so a

384
00:14:10,199 --> 00:14:13,480
lot of the times you'll see a new

385
00:14:11,480 --> 00:14:15,160
research paper come out and you'll see a

386
00:14:13,480 --> 00:14:18,079
really cool algorithm but you might not

387
00:14:15,160 --> 00:14:19,480
know exactly how it works and so or or

388
00:14:18,079 --> 00:14:21,519
you maybe know maybe you know how it

389
00:14:19,480 --> 00:14:23,199
works and you just want to make it fast

390
00:14:21,519 --> 00:14:25,079
and you want to integrate it into Pi

391
00:14:23,199 --> 00:14:26,160
torch for example so this is something

392
00:14:25,079 --> 00:14:28,040
we're actually going to do in this

393
00:14:26,160 --> 00:14:29,920
course is we're going to uh we're going

394
00:14:28,040 --> 00:14:31,759
to build up uh an algorithm and we're

395
00:14:29,920 --> 00:14:33,880
going to optimize it and then we're GNA

396
00:14:31,759 --> 00:14:35,560
actually Port it into a pytorch

397
00:14:33,880 --> 00:14:38,920
extension so that you can call it in

398
00:14:35,560 --> 00:14:40,720
Python which is super cool um but just

399
00:14:38,920 --> 00:14:42,519
learning how to integrate your own

400
00:14:40,720 --> 00:14:45,320
research into things to make them faster

401
00:14:42,519 --> 00:14:46,720
to have it operate at production scale

402
00:14:45,320 --> 00:14:48,160
um these are some really important

403
00:14:46,720 --> 00:14:50,399
things that you'll have to do when you

404
00:14:48,160 --> 00:14:55,079
start working you know very deeply with

405
00:14:50,399 --> 00:14:57,000
Cuda um another thing is karpathy LL M.C

406
00:14:55,079 --> 00:14:59,759
a lot of you have probably heard of this

407
00:14:57,000 --> 00:15:04,120
um if you go search up LL M.C

408
00:14:59,759 --> 00:15:06,399
uh LL M.C on uh on you not not on

409
00:15:04,120 --> 00:15:08,120
YouTube on Google um you'll come across

410
00:15:06,399 --> 00:15:12,240
guy named Andre kpoy and he pretty much

411
00:15:08,120 --> 00:15:15,880
built up a giant gbt2 training run in C

412
00:15:12,240 --> 00:15:18,199
from scratch so it uses C and Cuda and

413
00:15:15,880 --> 00:15:20,279
all of it there's a ton of stuff in it

414
00:15:18,199 --> 00:15:22,720
and I really felt like it's hard to

415
00:15:20,279 --> 00:15:24,639
understand that at first um you know as

416
00:15:22,720 --> 00:15:27,079
someone who's not like super super

417
00:15:24,639 --> 00:15:30,160
enriched uh and have having done Cuda

418
00:15:27,079 --> 00:15:31,560
for like 20 years um

419
00:15:30,160 --> 00:15:34,079
it's kind of hard to understand that at

420
00:15:31,560 --> 00:15:35,720
first so having a really nice basis like

421
00:15:34,079 --> 00:15:38,360
this where you can actually understand

422
00:15:35,720 --> 00:15:40,639
how to use Cuda and where the where the

423
00:15:38,360 --> 00:15:42,680
real uh benefits are from it and how to

424
00:15:40,639 --> 00:15:45,480
use it that will allow you to read and

425
00:15:42,680 --> 00:15:46,600
approach kpoe lm. see a little better so

426
00:15:45,480 --> 00:15:48,920
that was one of the reasons why I

427
00:15:46,600 --> 00:15:51,839
actually made this is to make it easier

428
00:15:48,920 --> 00:15:54,759
for people to go into llm Doc and

429
00:15:51,839 --> 00:15:56,920
understand what's going on so in the

430
00:15:54,759 --> 00:15:59,399
GitHub link and the notion document

431
00:15:56,920 --> 00:16:01,800
inside of my GitHub repo uh you will see

432
00:15:59,399 --> 00:16:04,480
this in the intro section so just a

433
00:16:01,800 --> 00:16:06,639
bunch of cool videos on how uh Cuda

434
00:16:04,480 --> 00:16:09,000
Works how Transformers work a bunch of

435
00:16:06,639 --> 00:16:11,600
just really cool fun videos to you know

436
00:16:09,000 --> 00:16:14,079
really get you motivated and upbeat on

437
00:16:11,600 --> 00:16:17,399
uh all of this so got some technical

438
00:16:14,079 --> 00:16:19,440
stuff we got some fun videos by fireship

439
00:16:17,399 --> 00:16:21,800
um but generally speaking these are just

440
00:16:19,440 --> 00:16:23,720
some cool resources you can check out uh

441
00:16:21,800 --> 00:16:25,399
Cuda programming kudam mode is a really

442
00:16:23,720 --> 00:16:27,160
good server actually I highly recommend

443
00:16:25,399 --> 00:16:29,319
you join this it's just a Discord

444
00:16:27,160 --> 00:16:32,040
community of a bunch of people who are

445
00:16:29,319 --> 00:16:34,759
really into Cuda so I believe Andre gpoy

446
00:16:32,040 --> 00:16:36,399
is in here A bunch of really cool uh you

447
00:16:34,759 --> 00:16:38,680
know coders a bunch of Engineers are in

448
00:16:36,399 --> 00:16:40,399
here just to discussing how to uh how to

449
00:16:38,680 --> 00:16:42,720
get certain kernels working and and

450
00:16:40,399 --> 00:16:45,920
generally just Cuda stuff um hence why

451
00:16:42,720 --> 00:16:47,920
it's called cuda mode right so uh really

452
00:16:45,920 --> 00:16:49,720
cool server I highly recommend you join

453
00:16:47,920 --> 00:16:53,759
that as well as my server which is also

454
00:16:49,720 --> 00:16:56,279
in the GitHub repo but that's

455
00:16:53,759 --> 00:16:57,480
that so now we're going to go into a

456
00:16:56,279 --> 00:17:00,279
little bit about the Deep learning

457
00:16:57,480 --> 00:17:01,680
ecosystem right now so obviously this

458
00:17:00,279 --> 00:17:03,959
going this is not going to be up to date

459
00:17:01,680 --> 00:17:06,400
in five years so just you know take this

460
00:17:03,959 --> 00:17:08,959
with a grain of salt this is not uh this

461
00:17:06,400 --> 00:17:10,720
is not everything this is just what I

462
00:17:08,959 --> 00:17:12,039
found interesting to look at and focus

463
00:17:10,720 --> 00:17:13,360
on and and to be aware of in the

464
00:17:12,039 --> 00:17:14,600
ecosystem and how you can sort of

465
00:17:13,360 --> 00:17:17,559
interconnect things and understand

466
00:17:14,600 --> 00:17:19,640
what's going on so this doesn't actually

467
00:17:17,559 --> 00:17:21,760
go over anything highly technical with

468
00:17:19,640 --> 00:17:23,600
Cuda but I thought it's better to show

469
00:17:21,760 --> 00:17:25,360
you the ecosystem rather than just

470
00:17:23,600 --> 00:17:26,679
entering technical details blindly like

471
00:17:25,360 --> 00:17:29,080
if we just dump straight into Cuda

472
00:17:26,679 --> 00:17:33,120
kernels um you won't know how to connect

473
00:17:29,080 --> 00:17:34,320
the dots later on so when we uh when

474
00:17:33,120 --> 00:17:35,840
we're actually building out good

475
00:17:34,320 --> 00:17:37,679
algorithms it's like okay now you have

476
00:17:35,840 --> 00:17:40,200
the skills to do this where do you apply

477
00:17:37,679 --> 00:17:42,919
that so this is this is what that aims

478
00:17:40,200 --> 00:17:44,360
to give you just a bit of background um

479
00:17:42,919 --> 00:17:46,520
understanding the ecosystem will help

480
00:17:44,360 --> 00:17:47,840
you map out everything properly and it

481
00:17:46,520 --> 00:17:49,799
provides that initial motivation to

482
00:17:47,840 --> 00:17:51,160
learn so some parts are going to get

483
00:17:49,799 --> 00:17:53,000
really hard and when you have that

484
00:17:51,160 --> 00:17:54,720
higher level motivation to see like okay

485
00:17:53,000 --> 00:17:56,880
this is what I can actually build once I

486
00:17:54,720 --> 00:17:59,559
learn how to do this instead of just

487
00:17:56,880 --> 00:18:04,000
let's learn Cuda blindly that that seems

488
00:17:59,559 --> 00:18:05,960
a little naive um so going into it with

489
00:18:04,000 --> 00:18:07,919
like understanding what to do later on

490
00:18:05,960 --> 00:18:11,720
or what you can do I think is really

491
00:18:07,919 --> 00:18:15,240
important um again don't feel free uh

492
00:18:11,720 --> 00:18:16,919
don't don't feel binded to just watch uh

493
00:18:15,240 --> 00:18:19,080
watch me talk about a subject for 20

494
00:18:16,919 --> 00:18:21,280
hours um you may limit your learning if

495
00:18:19,080 --> 00:18:23,080
you just force yourself to sit down and

496
00:18:21,280 --> 00:18:25,400
and just just watch and listen to what

497
00:18:23,080 --> 00:18:26,559
I'm saying um I do encourage you to go

498
00:18:25,400 --> 00:18:27,760
down rabbit holes so if you find

499
00:18:26,559 --> 00:18:30,159
something that interests you in this

500
00:18:27,760 --> 00:18:31,120
section or other ones just totally just

501
00:18:30,159 --> 00:18:34,080
go down there that that's where you

502
00:18:31,120 --> 00:18:36,159
learn a ton right um but anyways I've

503
00:18:34,080 --> 00:18:39,200
I've organized this into several

504
00:18:36,159 --> 00:18:42,400
sections so research

505
00:18:39,200 --> 00:18:43,559
production um low level inference for

506
00:18:42,400 --> 00:18:45,640
Edge

507
00:18:43,559 --> 00:18:48,559
Computing ease of

508
00:18:45,640 --> 00:18:51,559
use compilers and

509
00:18:48,559 --> 00:18:54,280
miscellaneous so we start up at the top

510
00:18:51,559 --> 00:18:57,280
here was the easy ones we have pytorch

511
00:18:54,280 --> 00:18:58,960
we have pflow we have Jacks and fireship

512
00:18:57,280 --> 00:19:01,280
has videos on all these These are very

513
00:18:58,960 --> 00:19:02,440
well documented um I'll let you you know

514
00:19:01,280 --> 00:19:03,720
you can kind of just like read through

515
00:19:02,440 --> 00:19:05,240
these I'm not going to go over every

516
00:19:03,720 --> 00:19:09,960
single bullet point cuz it's already

517
00:19:05,240 --> 00:19:12,919
here um but yeah you have you have mlx

518
00:19:09,960 --> 00:19:15,240
developed by Apple for Apple silicon

519
00:19:12,919 --> 00:19:17,679
open source uh for Apple

520
00:19:15,240 --> 00:19:20,000
devices P torch lightning is like P

521
00:19:17,679 --> 00:19:21,280
torch but reduces boiler plate code so

522
00:19:20,000 --> 00:19:23,120
there's a Reddit post here which was

523
00:19:21,280 --> 00:19:25,640
interesting um when you do like when you

524
00:19:23,120 --> 00:19:28,520
set like your tf32 Precision to do

525
00:19:25,640 --> 00:19:30,720
tensor core computations in in pytorch

526
00:19:28,520 --> 00:19:31,720
um like that's boiler plate code so pie

527
00:19:30,720 --> 00:19:32,919
torch lightning is actually going to

528
00:19:31,720 --> 00:19:34,880
reduce that and it's going to remove

529
00:19:32,919 --> 00:19:36,159
that boiler plate so you don't have to

530
00:19:34,880 --> 00:19:38,840
worry about like including all those

531
00:19:36,159 --> 00:19:42,320
little optimizations and and and uh and

532
00:19:38,840 --> 00:19:43,960
hacks so when it comes to production

533
00:19:42,320 --> 00:19:46,000
this is there's typically two things

534
00:19:43,960 --> 00:19:48,039
that fall in here so you have training

535
00:19:46,000 --> 00:19:49,799
and inference and some of these will

536
00:19:48,039 --> 00:19:52,200
support two of them together some of

537
00:19:49,799 --> 00:19:56,280
them will just support one or the other

538
00:19:52,200 --> 00:19:58,799
um so in here we have VM which is quite

539
00:19:56,280 --> 00:20:01,360
interesting um

540
00:19:58,799 --> 00:20:06,440
search a BLM on

541
00:20:01,360 --> 00:20:06,440
GitHub actually go down and we can see

542
00:20:06,520 --> 00:20:13,720
um where did it go yeah LM impr and

543
00:20:10,200 --> 00:20:16,559
serving and then where did it go

544
00:20:13,720 --> 00:20:19,039
performance yeah so performance

545
00:20:16,559 --> 00:20:20,320
Benchmark againsts tensor rtln which is

546
00:20:19,039 --> 00:20:22,280
the next one that I'll actually talk

547
00:20:20,320 --> 00:20:24,600
about here um but they they Implement a

548
00:20:22,280 --> 00:20:26,919
bunch of like very like essentially

549
00:20:24,600 --> 00:20:29,400
Hardware GPU optimizations that we may

550
00:20:26,919 --> 00:20:32,640
talk about later on um

551
00:20:29,400 --> 00:20:36,240
but BLM is great um tensor RT is pretty

552
00:20:32,640 --> 00:20:38,480
much tensor runtime by Nvidia and they

553
00:20:36,240 --> 00:20:40,400
have a tensor RT LM so it's for like

554
00:20:38,480 --> 00:20:42,159
inferencing language models with all of

555
00:20:40,400 --> 00:20:45,679
these you know all these different

556
00:20:42,159 --> 00:20:47,440
optimizations um specifically for llm

557
00:20:45,679 --> 00:20:51,159
inference

558
00:20:47,440 --> 00:20:53,159
now Triton is Triton is something we're

559
00:20:51,159 --> 00:20:55,320
actually going to cover a bit more Tron

560
00:20:53,159 --> 00:20:56,960
was developed by opening eye we go here

561
00:20:55,320 --> 00:20:59,360
you can see

562
00:20:56,960 --> 00:21:01,039
this uh it tells you about like what the

563
00:20:59,360 --> 00:21:03,520
heck Triton is like what the motivation

564
00:21:01,039 --> 00:21:05,320
was where it came from um but if we look

565
00:21:03,520 --> 00:21:08,559
at this paper from Harvard this is

566
00:21:05,320 --> 00:21:09,960
actually where Triton originated from so

567
00:21:08,559 --> 00:21:11,600
try an Intermediate Language and

568
00:21:09,960 --> 00:21:13,480
compiler for child neural net

569
00:21:11,600 --> 00:21:15,400
computations child neural net

570
00:21:13,480 --> 00:21:16,840
computations is the key here this is

571
00:21:15,400 --> 00:21:18,120
where a lot of the performance comes

572
00:21:16,840 --> 00:21:21,000
from and you'll see this later on when

573
00:21:18,120 --> 00:21:23,039
we build fast algorithms tiling is where

574
00:21:21,000 --> 00:21:25,120
you have like a giant problem where

575
00:21:23,039 --> 00:21:27,600
you're you have to do linear algebra

576
00:21:25,120 --> 00:21:31,360
operations like on tensors and you have

577
00:21:27,600 --> 00:21:33,840
to do them fast on parallel uh parallel

578
00:21:31,360 --> 00:21:35,840
processors like gpus and so what you can

579
00:21:33,840 --> 00:21:37,279
do is you can tile The Matrix into a

580
00:21:35,840 --> 00:21:40,520
bunch of little like squares like

581
00:21:37,279 --> 00:21:42,640
subsquares and you can you can multiply

582
00:21:40,520 --> 00:21:44,720
them together so this way you don't have

583
00:21:42,640 --> 00:21:46,400
to do like an entire thing at once and

584
00:21:44,720 --> 00:21:48,000
then reserve it and and worry about all

585
00:21:46,400 --> 00:21:50,760
that stuff you can literally just select

586
00:21:48,000 --> 00:21:53,760
blocks and the parallel processors in

587
00:21:50,760 --> 00:21:55,320
Cuda are extremely good at processing

588
00:21:53,760 --> 00:21:56,720
those blocks because of the Cuda

589
00:21:55,320 --> 00:22:00,120
architecture which we will talk about

590
00:21:56,720 --> 00:22:01,480
later um but but try is interesting this

591
00:22:00,120 --> 00:22:03,400
is a whole paper which I'm not going to

592
00:22:01,480 --> 00:22:07,480
dig into in this course but a lot of

593
00:22:03,400 --> 00:22:09,760
interesting uh both compiler and um you

594
00:22:07,480 --> 00:22:12,120
know speed ups that you get from

595
00:22:09,760 --> 00:22:13,960
approaching things with a with a tiled

596
00:22:12,120 --> 00:22:15,520
um

597
00:22:13,960 --> 00:22:18,120
philosophy

598
00:22:15,520 --> 00:22:20,679
now toor just some other optimizations

599
00:22:18,120 --> 00:22:23,520
we'll get in performance is torch do

600
00:22:20,679 --> 00:22:26,640
compile so you do torch do compile and

601
00:22:23,520 --> 00:22:28,600
then Open Bracket model close bracket

602
00:22:26,640 --> 00:22:31,120
and this will literally just increase

603
00:22:28,600 --> 00:22:32,880
performance 30% out of the box it'll

604
00:22:31,120 --> 00:22:34,960
take that Dynamic graph that P torch

605
00:22:32,880 --> 00:22:36,760
builds and it'll statically it'll snap

606
00:22:34,960 --> 00:22:38,360
it into a static representation for

607
00:22:36,760 --> 00:22:40,480
production because we're using it for

608
00:22:38,360 --> 00:22:43,240
production uh and it'll just apply

609
00:22:40,480 --> 00:22:44,760
optimizations all all around um which we

610
00:22:43,240 --> 00:22:46,559
will dig more into this course like an

611
00:22:44,760 --> 00:22:48,200
example would be like kernel Fusion

612
00:22:46,559 --> 00:22:50,400
where instead of you know doing a

613
00:22:48,200 --> 00:22:51,720
separate function for each for each step

614
00:22:50,400 --> 00:22:53,960
you're like combining two or three

615
00:22:51,720 --> 00:22:55,760
operations into one single function uh

616
00:22:53,960 --> 00:22:58,159
and that like reduces some overhead comp

617
00:22:55,760 --> 00:22:59,440
computation that you have to do there so

618
00:22:58,159 --> 00:23:00,960
uh just a bunch of these little

619
00:22:59,440 --> 00:23:02,640
optimizations that torch talk compile

620
00:23:00,960 --> 00:23:05,159
does uh extremely recommend for

621
00:23:02,640 --> 00:23:07,159
production uh torch script is a little

622
00:23:05,159 --> 00:23:09,679
older but there's an article here on

623
00:23:07,159 --> 00:23:11,799
Torch script

624
00:23:09,679 --> 00:23:15,120
so torch

625
00:23:11,799 --> 00:23:16,559
script um I haven't actually used but

626
00:23:15,120 --> 00:23:18,480
there are some more discussions here

627
00:23:16,559 --> 00:23:20,240
that you can follow um I know it's a

628
00:23:18,480 --> 00:23:22,120
little older so I typically just resort

629
00:23:20,240 --> 00:23:23,880
to Tor shock comp pile for most things

630
00:23:22,120 --> 00:23:26,240
um but it's it's here in case you want

631
00:23:23,880 --> 00:23:27,520
that and then Onyx runtime is also

632
00:23:26,240 --> 00:23:29,640
interesting I should probably should

633
00:23:27,520 --> 00:23:32,400
have put Onyx before Onyx runtime but it

634
00:23:29,640 --> 00:23:35,080
is what it is um Onyx runtime is pretty

635
00:23:32,400 --> 00:23:37,120
much on top of Onyx so you have this

636
00:23:35,080 --> 00:23:39,000
this thing called Onyx which exports a

637
00:23:37,120 --> 00:23:40,279
model from either pytorch or tensor

638
00:23:39,000 --> 00:23:44,520
floor whatever you want down to this

639
00:23:40,279 --> 00:23:44,520
Onyx format that's intercompatibility

640
00:23:58,600 --> 00:24:02,919
uh it's like a Onyx file extension that

641
00:24:00,480 --> 00:24:06,600
you use for storing neural net uh

642
00:24:02,919 --> 00:24:08,799
weights and tensors so uh Onyx runtime

643
00:24:06,600 --> 00:24:12,360
essentially takes that and allows you to

644
00:24:08,799 --> 00:24:14,400
just run it faster so that was built by

645
00:24:12,360 --> 00:24:16,400
Microsoft uh and then a cool little

646
00:24:14,400 --> 00:24:18,960
project I came across and that chat jbt

647
00:24:16,400 --> 00:24:21,679
recommended I put into this course was

648
00:24:18,960 --> 00:24:25,080
detectron 2 so it's uh it's interesting

649
00:24:21,679 --> 00:24:27,159
you might find it useful but um

650
00:24:25,080 --> 00:24:30,159
developed by Facebook and it's

651
00:24:27,159 --> 00:24:33,080
essentially a computer vision library

652
00:24:30,159 --> 00:24:35,520
that uses uh image detection and

653
00:24:33,080 --> 00:24:37,720
segmentation algorithms

654
00:24:35,520 --> 00:24:39,320
so just a bunch of like really cool

655
00:24:37,720 --> 00:24:41,799
computer vision stuff that it has bunch

656
00:24:39,320 --> 00:24:44,559
of different neural net architectures

657
00:24:41,799 --> 00:24:45,640
and hats that it employs and it's just

658
00:24:44,559 --> 00:24:48,679
one of those fun things that you might

659
00:24:45,640 --> 00:24:50,320
want to mess around with um then we go

660
00:24:48,679 --> 00:24:53,480
to low level which is what this course

661
00:24:50,320 --> 00:24:55,080
is based on in case you haven't read the

662
00:24:53,480 --> 00:24:58,039
the the title it's on

663
00:24:55,080 --> 00:25:01,039
Cuda uh Cuda is compute unified device

664
00:24:58,039 --> 00:25:03,159
architect Ure uh programming language uh

665
00:25:01,039 --> 00:25:05,039
programming platform rather for NVIDIA

666
00:25:03,159 --> 00:25:07,600
gpus um and there's a bunch of stuff

667
00:25:05,039 --> 00:25:11,600
which we'll dig into later

668
00:25:07,600 --> 00:25:13,360
um rock M qu equivalent for AMD gpus and

669
00:25:11,600 --> 00:25:17,399
then you have opencl so this is more

670
00:25:13,360 --> 00:25:19,360
General um built for CPUs gpus uh dsps

671
00:25:17,399 --> 00:25:21,399
other types of Hardware so just like a

672
00:25:19,360 --> 00:25:24,960
general purpose Computing language open

673
00:25:21,399 --> 00:25:27,000
open source um and then we have Edge

674
00:25:24,960 --> 00:25:28,919
Computing and embed systems so what the

675
00:25:27,000 --> 00:25:32,159
heck does Edge Computing mean l what is

676
00:25:28,919 --> 00:25:35,240
Edge Computing um think of the Tesla

677
00:25:32,159 --> 00:25:38,080
Fleet that Tesla has so there's a bunch

678
00:25:35,240 --> 00:25:39,399
of cars that are maybe running into

679
00:25:38,080 --> 00:25:42,320
accidents occasionally and so they want

680
00:25:39,399 --> 00:25:43,840
to report this back to the Tesla data

681
00:25:42,320 --> 00:25:46,640
center to train on and improve the

682
00:25:43,840 --> 00:25:48,240
models so you'll have a bunch of these

683
00:25:46,640 --> 00:25:49,640
this essentially this Fleet and the

684
00:25:48,240 --> 00:25:51,480
purpose of edge Computing is to have

685
00:25:49,640 --> 00:25:52,799
them own doing their each of them doing

686
00:25:51,480 --> 00:25:55,000
their own local

687
00:25:52,799 --> 00:25:56,360
computation and then whenever you're do

688
00:25:55,000 --> 00:25:58,480
an update you're just going to send that

689
00:25:56,360 --> 00:26:01,240
back and you're able to have like the

690
00:25:58,480 --> 00:26:02,840
centralized entity that I guess the

691
00:26:01,240 --> 00:26:04,360
centralized data center is our entity

692
00:26:02,840 --> 00:26:06,039
here and it's just going to do some

693
00:26:04,360 --> 00:26:09,200
training on all those on all that new

694
00:26:06,039 --> 00:26:10,799
data and uh that that's pretty much what

695
00:26:09,200 --> 00:26:14,520
it is it's just like a decentralized

696
00:26:10,799 --> 00:26:16,760
Computing if you will um so you know you

697
00:26:14,520 --> 00:26:18,919
have um you have like tensorflow light

698
00:26:16,760 --> 00:26:20,320
which is like a a light version a

699
00:26:18,919 --> 00:26:23,679
lightweight version of tensorflow and

700
00:26:20,320 --> 00:26:25,159
then pytorch mobile is same thing um

701
00:26:23,679 --> 00:26:27,320
what I mean there's always optimizations

702
00:26:25,159 --> 00:26:29,159
you can do in Cuda and like just plain

703
00:26:27,320 --> 00:26:31,440
pie torch that'll just make stuff run

704
00:26:29,159 --> 00:26:34,240
fast either way but there is py mobile

705
00:26:31,440 --> 00:26:37,600
for that um then you have corl which is

706
00:26:34,240 --> 00:26:40,279
for Apple products so like the Mac OS

707
00:26:37,600 --> 00:26:43,679
watch TV all this

708
00:26:40,279 --> 00:26:45,480
um then you have ease of use which isn't

709
00:26:43,679 --> 00:26:46,919
like entirely Cuda related but I thought

710
00:26:45,480 --> 00:26:49,520
I'd still mention this because some of

711
00:26:46,919 --> 00:26:51,559
these are really awesome so you have uh

712
00:26:49,520 --> 00:26:53,799
you have fast AI which I'm not going to

713
00:26:51,559 --> 00:26:56,559
talk about a lot but you can you can

714
00:26:53,799 --> 00:26:59,559
look you can look into this maybe

715
00:26:56,559 --> 00:27:03,960
separately um so they have their own

716
00:26:59,559 --> 00:27:03,960
they have their own thing here but

717
00:27:04,720 --> 00:27:10,080
um yeah I'm not going to I'm not going

718
00:27:06,799 --> 00:27:12,120
to go over fast AI but they uh they have

719
00:27:10,080 --> 00:27:15,080
some interesting

720
00:27:12,120 --> 00:27:17,840
stuff Onyx which we talked about before

721
00:27:15,080 --> 00:27:19,480
stands for open neural network exchange

722
00:27:17,840 --> 00:27:21,720
so the x is capital and that's where the

723
00:27:19,480 --> 00:27:26,880
X comes from um literally you just do

724
00:27:21,720 --> 00:27:28,720
torsa onyx. export model um and then

725
00:27:26,880 --> 00:27:30,399
dummy input and then just whatever the

726
00:27:28,720 --> 00:27:32,399
the file name is so you can look more

727
00:27:30,399 --> 00:27:34,279
into the torch docks and Onyx as to how

728
00:27:32,399 --> 00:27:36,320
to do this on both P torch and

729
00:27:34,279 --> 00:27:38,840
tensorflow and whatever else you want

730
00:27:36,320 --> 00:27:41,159
but this is how you would export an onyx

731
00:27:38,840 --> 00:27:43,440
format

732
00:27:41,159 --> 00:27:45,960
um and then this is the tensor FL

733
00:27:43,440 --> 00:27:47,880
equivalent so this is essentially this

734
00:27:45,960 --> 00:27:49,519
like nice little image that I got where

735
00:27:47,880 --> 00:27:53,159
like it kind of binds with everything so

736
00:27:49,519 --> 00:27:55,360
P toor Tor flow carass um C Cafe which

737
00:27:53,159 --> 00:27:58,559
was which was initially what P torch was

738
00:27:55,360 --> 00:28:01,799
using um Cafe was one of Cafe was one of

739
00:27:58,559 --> 00:28:05,360
those uh original parts in the pytorch

740
00:28:01,799 --> 00:28:06,640
ecosystem um from a while back um so

741
00:28:05,360 --> 00:28:08,559
that that just kind of shows how they

742
00:28:06,640 --> 00:28:11,200
can interconnect together so you like

743
00:28:08,559 --> 00:28:13,240
export in one of these and then you can

744
00:28:11,200 --> 00:28:15,760
import back into any one of these uh

745
00:28:13,240 --> 00:28:17,880
plus Onyx runtime which runs

746
00:28:15,760 --> 00:28:20,720
faster and then you have weights and

747
00:28:17,880 --> 00:28:22,159
biases so I got a little snippet from

748
00:28:20,720 --> 00:28:24,600
the internet as to like what this looks

749
00:28:22,159 --> 00:28:25,799
like but pretty much allows you to track

750
00:28:24,600 --> 00:28:27,519
your training runs and a bunch of

751
00:28:25,799 --> 00:28:30,360
different charts and statistics about

752
00:28:27,519 --> 00:28:32,360
how your models are are performing so uh

753
00:28:30,360 --> 00:28:35,399
when I'm doing like when I want to train

754
00:28:32,360 --> 00:28:37,480
like a clo a clothing uh recognition

755
00:28:35,399 --> 00:28:39,919
model I can literally have all of these

756
00:28:37,480 --> 00:28:42,480
different ones so accuracy on sandals

757
00:28:39,919 --> 00:28:43,840
shirts trousers pullovers boots right

758
00:28:42,480 --> 00:28:47,600
boots is like kind of chaotic and

759
00:28:43,840 --> 00:28:49,279
pullovers just kind of worked fast um

760
00:28:47,600 --> 00:28:50,799
and then this one too so you can kind of

761
00:28:49,279 --> 00:28:52,279
just track a bunch of things and

762
00:28:50,799 --> 00:28:53,679
understand what how your models are

763
00:28:52,279 --> 00:28:55,679
performing and then show that to like

764
00:28:53,679 --> 00:28:57,200
maybe your maybe your uh employer

765
00:28:55,679 --> 00:29:00,159
whatever or whoever is maybe your

766
00:28:57,200 --> 00:29:02,159
manager and just kind of get things done

767
00:29:00,159 --> 00:29:04,159
that way and document things easily

768
00:29:02,159 --> 00:29:06,799
without having to use same matap plot

769
00:29:04,159 --> 00:29:10,279
lib um it's all just kind of tracked and

770
00:29:06,799 --> 00:29:11,880
imported and taken care of for you um

771
00:29:10,279 --> 00:29:13,360
and then Cloud providers these are

772
00:29:11,880 --> 00:29:15,120
actually quite important to know not

773
00:29:13,360 --> 00:29:16,720
necessarily on the lowlevel part of like

774
00:29:15,120 --> 00:29:18,200
Cuda but these are still good to know

775
00:29:16,720 --> 00:29:22,159
because they play a major role in the

776
00:29:18,200 --> 00:29:24,720
ecosystem um you have AWS so AWS is a

777
00:29:22,159 --> 00:29:26,159
major one I personally use aws's

778
00:29:24,720 --> 00:29:28,960
products and prefer them I'm not

779
00:29:26,159 --> 00:29:31,640
endorsing like not sponsoring them but

780
00:29:28,960 --> 00:29:34,840
um not sponsored by them but I do use ad

781
00:29:31,640 --> 00:29:37,799
us products and uh the two main things

782
00:29:34,840 --> 00:29:39,440
here for ML stuff is ec2 instances so

783
00:29:37,799 --> 00:29:41,720
these are like used universally you just

784
00:29:39,440 --> 00:29:43,600
fire up a like a remote machine you can

785
00:29:41,720 --> 00:29:45,240
SSH into it and then do whatever you

786
00:29:43,600 --> 00:29:47,360
want and you can use all the specs like

787
00:29:45,240 --> 00:29:49,480
it's literally uh command line access

788
00:29:47,360 --> 00:29:51,399
and you could do whatever you want um

789
00:29:49,480 --> 00:29:53,679
and then you have Sage maker so it's a

790
00:29:51,399 --> 00:29:55,399
little bit easier and more ml focused

791
00:29:53,679 --> 00:29:57,159
but you can run jupyter notebooks on a

792
00:29:55,399 --> 00:29:58,640
cluster so instead of worrying about a

793
00:29:57,159 --> 00:30:01,760
command line and having having to fire

794
00:29:58,640 --> 00:30:02,519
things up in like um in vs code like VSS

795
00:30:01,760 --> 00:30:05,519
code

796
00:30:02,519 --> 00:30:08,039
SSH you just uh run a jupyter notebook

797
00:30:05,519 --> 00:30:10,600
literally like in the browser or you can

798
00:30:08,039 --> 00:30:14,600
uh just SSH into uh The sagemaker

799
00:30:10,600 --> 00:30:16,960
Notebook I believe um and then you have

800
00:30:14,600 --> 00:30:19,159
the uh the data labeling part which is

801
00:30:16,960 --> 00:30:20,360
very big in the world today so where

802
00:30:19,159 --> 00:30:22,440
does all the data come from that we're

803
00:30:20,360 --> 00:30:25,679
training models on well this is exactly

804
00:30:22,440 --> 00:30:27,159
where it is um if you go AWS sagemaker

805
00:30:25,679 --> 00:30:29,720
and then you find like the the labeling

806
00:30:27,159 --> 00:30:31,919
part or mechanic Turk I believe is

807
00:30:29,720 --> 00:30:34,880
believe is what it's called that's where

808
00:30:31,919 --> 00:30:37,919
all of the labeling on AWS takes place

809
00:30:34,880 --> 00:30:39,440
so uh you know big stuff there uh

810
00:30:37,919 --> 00:30:41,679
typically costs like a decent amount of

811
00:30:39,440 --> 00:30:43,559
money for people to label your stuff but

812
00:30:41,679 --> 00:30:45,640
that's that's where you find it um and

813
00:30:43,559 --> 00:30:47,279
then model training and deployment you

814
00:30:45,640 --> 00:30:48,760
that's that's also supported by Sage

815
00:30:47,279 --> 00:30:50,559
maker so you want to like deploy your

816
00:30:48,760 --> 00:30:53,799
own llama 3 variant it's like go there

817
00:30:50,559 --> 00:30:56,159
you go Sage maker um then Google Cloud I

818
00:30:53,799 --> 00:30:58,679
don't use as much they have vertex Ai

819
00:30:56,159 --> 00:31:01,080
and their VM machines which are like 2

820
00:30:58,679 --> 00:31:02,480
equivalent then you have Microsoft Azure

821
00:31:01,080 --> 00:31:05,039
which I haven't actually used that much

822
00:31:02,480 --> 00:31:06,279
so um it's just like another top three

823
00:31:05,039 --> 00:31:07,919
like these are the top three players in

824
00:31:06,279 --> 00:31:10,799
the ecosystem and then you kind of break

825
00:31:07,919 --> 00:31:12,559
down to open AI fast Ai and Lambda Labs

826
00:31:10,799 --> 00:31:14,200
so open AI provides their own like

827
00:31:12,559 --> 00:31:15,679
fine-tuning services and you can you

828
00:31:14,200 --> 00:31:17,000
know everyone knows open AI you can

829
00:31:15,679 --> 00:31:18,760
literally go on the website and just

830
00:31:17,000 --> 00:31:22,080
navigate around there and figure out

831
00:31:18,760 --> 00:31:24,960
what you want to do with models um fast

832
00:31:22,080 --> 00:31:28,519
AI so I haven't entirely gotten a

833
00:31:24,960 --> 00:31:29,639
picture here yet but if I go to

834
00:31:28,519 --> 00:31:32,080
bass at

835
00:31:29,639 --> 00:31:34,120
AI um I go to the

836
00:31:32,080 --> 00:31:37,440
console hopefully it doesn't expose

837
00:31:34,120 --> 00:31:40,080
anything bad um but like yeah I can

838
00:31:37,440 --> 00:31:42,200
select any of

839
00:31:40,080 --> 00:31:44,159
these it's just like a bunch of rigs

840
00:31:42,200 --> 00:31:46,440
that I can rent for an hourly right get

841
00:31:44,159 --> 00:31:51,159
all the specs on them everything um and

842
00:31:46,440 --> 00:31:53,080
it's great so you know I set RTX 370s

843
00:31:51,159 --> 00:31:55,799
which is like my graphics card and mine

844
00:31:53,080 --> 00:31:58,120
costs about you know 1 cent per hour

845
00:31:55,799 --> 00:32:01,360
which is which is embarrassingly cheap

846
00:31:58,120 --> 00:32:04,279
but uh yeah this one oh this one is more

847
00:32:01,360 --> 00:32:06,519
expensive but yeah so so vastia is

848
00:32:04,279 --> 00:32:07,720
awesome you can use these like any GPU

849
00:32:06,519 --> 00:32:09,559
you can pretty much select it and just

850
00:32:07,720 --> 00:32:10,960
use it on the Fly and it's like hosted

851
00:32:09,559 --> 00:32:14,080
by someone else in the world that you

852
00:32:10,960 --> 00:32:17,240
SSH into and do stuff from

853
00:32:14,080 --> 00:32:19,760
um then you have Lambda Labs which I

854
00:32:17,240 --> 00:32:19,760
sech set

855
00:32:22,200 --> 00:32:28,760
up actually find Lambda here Lambda

856
00:32:25,360 --> 00:32:32,559
Cloud y so

857
00:32:28,760 --> 00:32:35,399
uh data center dgx systems like

858
00:32:32,559 --> 00:32:37,080
literally you have the Blackwell gpus

859
00:32:35,399 --> 00:32:41,399
you have the

860
00:32:37,080 --> 00:32:44,120
h100s um yeah just pretty much GPU

861
00:32:41,399 --> 00:32:46,440
infrastructure specifically um and it's

862
00:32:44,120 --> 00:32:48,960
like I believe a bit cheaper than the

863
00:32:46,440 --> 00:32:51,720
big three providers like AWS Google and

864
00:32:48,960 --> 00:32:53,519
Microsoft so uh Lambda Labs is commonly

865
00:32:51,720 --> 00:32:55,480
used but typically you would rent things

866
00:32:53,519 --> 00:32:57,360
in a cluster so you're paying like

867
00:32:55,480 --> 00:32:59,000
multiple hundreds or thousands or tens

868
00:32:57,360 --> 00:33:00,919
of thousands of dollars per hour for

869
00:32:59,000 --> 00:33:02,360
these so if you're in a company and

870
00:33:00,919 --> 00:33:04,480
you're trying to get like cheap cheap

871
00:33:02,360 --> 00:33:07,480
gpus that are data center quality you

872
00:33:04,480 --> 00:33:10,159
might want to look at Lambda

873
00:33:07,480 --> 00:33:13,559
um and then compilers so I'm not like a

874
00:33:10,159 --> 00:33:15,960
compiler expert but mainly you're going

875
00:33:13,559 --> 00:33:18,639
to have things like xlaa so this is what

876
00:33:15,960 --> 00:33:21,799
is powering Jacks um you're going to

877
00:33:18,639 --> 00:33:25,039
have lvm which I'm not an expert I hav't

878
00:33:21,799 --> 00:33:26,679
build compiler so um I'll let you look

879
00:33:25,039 --> 00:33:28,200
into that there's a ton of resources on

880
00:33:26,679 --> 00:33:31,679
lvm

881
00:33:28,200 --> 00:33:35,240
um it is it stands for low-level virtual

882
00:33:31,679 --> 00:33:35,240
machine I believe

883
00:33:37,720 --> 00:33:43,080
um go to lvm

884
00:33:40,880 --> 00:33:45,519
project

885
00:33:43,080 --> 00:33:47,480
um a toolkit for the construction of

886
00:33:45,519 --> 00:33:50,080
Highly optimized compilers optimizers

887
00:33:47,480 --> 00:33:53,840
and runtime environments

888
00:33:50,080 --> 00:33:56,919
um multiple components um component

889
00:33:53,840 --> 00:34:00,760
compiles C C++ Objective C and objective

890
00:33:56,919 --> 00:34:02,880
C++ code into lvm bit code um and then

891
00:34:00,760 --> 00:34:05,120
into object files so it's essentially

892
00:34:02,880 --> 00:34:07,799
used for developing stuff in cc++ and

893
00:34:05,120 --> 00:34:07,799
compilers and

894
00:34:07,839 --> 00:34:16,119
general then you have ml ml which is

895
00:34:12,040 --> 00:34:16,119
what is ml look at this

896
00:34:17,359 --> 00:34:26,320
again multi-level intermediate

897
00:34:19,480 --> 00:34:28,440
representation so this was uh ML and lvm

898
00:34:26,320 --> 00:34:31,960
were mainly developed by Chris flater

899
00:34:28,440 --> 00:34:33,919
which um I also have a course um on on

900
00:34:31,960 --> 00:34:36,000
the on the programming language that his

901
00:34:33,919 --> 00:34:37,720
company built called modular the

902
00:34:36,000 --> 00:34:39,399
programming language is called Mojo you

903
00:34:37,720 --> 00:34:42,159
can search that up on free code camp and

904
00:34:39,399 --> 00:34:44,440
go learn Mojo too U but it's like a

905
00:34:42,159 --> 00:34:46,839
pretty much just an AI programming

906
00:34:44,440 --> 00:34:48,839
language for doing like Fast tensor

907
00:34:46,839 --> 00:34:53,320
operations

908
00:34:48,839 --> 00:34:56,320
um so this was moved um it's part of the

909
00:34:53,320 --> 00:34:58,520
lvm project and uh there's some

910
00:34:56,320 --> 00:35:01,440
interesting stuff there it's it's

911
00:34:58,520 --> 00:35:04,079
somewhat newer so there's you

912
00:35:01,440 --> 00:35:06,800
know interesting interesting changes

913
00:35:04,079 --> 00:35:10,320
it's not it's not like super ancient

914
00:35:06,800 --> 00:35:12,440
um but uh the main ones that I'll be

915
00:35:10,320 --> 00:35:14,359
able to talk about are like nvcc so

916
00:35:12,440 --> 00:35:17,480
that's like the Auda compiler Nvidia

917
00:35:14,359 --> 00:35:20,079
Cuda compiler

918
00:35:17,480 --> 00:35:21,359
um and you know there's an architecture

919
00:35:20,079 --> 00:35:23,960
here which I haven't like fully

920
00:35:21,359 --> 00:35:25,800
memorized yet but uh the Nvidia Cuda

921
00:35:23,960 --> 00:35:27,800
compiler is what we're what we're going

922
00:35:25,800 --> 00:35:29,839
to be using to essentially compile our

923
00:35:27,800 --> 00:35:31,560
Cuda scripts and kernels and have them

924
00:35:29,839 --> 00:35:34,839
you know into binary so that we can run

925
00:35:31,560 --> 00:35:36,440
them fast so uh you know these are

926
00:35:34,839 --> 00:35:38,160
interesting interesting compiler

927
00:35:36,440 --> 00:35:39,680
infrastructure I'll probably add to this

928
00:35:38,160 --> 00:35:42,079
with some better descriptions on like

929
00:35:39,680 --> 00:35:44,200
what these are but uh this is like the

930
00:35:42,079 --> 00:35:45,960
general overview and then for

931
00:35:44,200 --> 00:35:48,920
miscellaneous I had I could not leave

932
00:35:45,960 --> 00:35:51,560
out hugging face so last but not least

933
00:35:48,920 --> 00:35:53,480
it's like hugging face right um You

934
00:35:51,560 --> 00:35:56,480
probably already know what it is

935
00:35:53,480 --> 00:35:58,400
but I'll look at it up just in case so

936
00:35:56,480 --> 00:36:02,760
on hugging face you have a bunch of

937
00:35:58,400 --> 00:36:04,720
things um you have models data sets uh

938
00:36:02,760 --> 00:36:06,640
and then spaces and that's like pretty

939
00:36:04,720 --> 00:36:08,839
much all you need to know so if you go

940
00:36:06,640 --> 00:36:12,920
to models you

941
00:36:08,839 --> 00:36:15,119
can oh maybe it'll take a second to load

942
00:36:12,920 --> 00:36:18,520
um

943
00:36:15,119 --> 00:36:21,440
here we have multimodal computer vision

944
00:36:18,520 --> 00:36:23,079
MLP audio tabular reinforcement learning

945
00:36:21,440 --> 00:36:24,440
and then graph machine learning so

946
00:36:23,079 --> 00:36:26,079
there's a bunch of cool stuff you can do

947
00:36:24,440 --> 00:36:28,119
here but most of it is language models

948
00:36:26,079 --> 00:36:30,359
right now um I know like recently

949
00:36:28,119 --> 00:36:33,000
released some of

950
00:36:30,359 --> 00:36:35,000
their I believe it's like image Maybe

951
00:36:33,000 --> 00:36:36,520
video Generation stuff I can't remember

952
00:36:35,000 --> 00:36:38,880
specifically but this is where you'll

953
00:36:36,520 --> 00:36:39,880
see all like the new open source models

954
00:36:38,880 --> 00:36:41,839
uh that you can just pretty much

955
00:36:39,880 --> 00:36:43,800
download and run run in P torch like

956
00:36:41,839 --> 00:36:44,960
that uh you just need enough Hardware

957
00:36:43,800 --> 00:36:47,359
you just need good enough Hardware to

958
00:36:44,960 --> 00:36:48,960
run these and it'll just it'll just work

959
00:36:47,359 --> 00:36:50,800
um and then you have the actual data

960
00:36:48,960 --> 00:36:53,960
sets for these models that that you

961
00:36:50,800 --> 00:36:57,200
train them on so um you know you can go

962
00:36:53,960 --> 00:37:01,440
like 3D data sets which is interesting

963
00:36:57,200 --> 00:37:01,440
um a lot of it is just going to be text

964
00:37:01,480 --> 00:37:05,599
so um if I remove

965
00:37:08,119 --> 00:37:13,800
that yeah Vision data awesome Auto math

966
00:37:12,200 --> 00:37:14,920
text so just all these all these data

967
00:37:13,800 --> 00:37:17,599
sets are here that the models are

968
00:37:14,920 --> 00:37:19,440
trained on um and then you have spaces

969
00:37:17,599 --> 00:37:21,319
which is where you can actually use

970
00:37:19,440 --> 00:37:23,520
models um this is where people will like

971
00:37:21,319 --> 00:37:25,640
host things or get sponsors with custom

972
00:37:23,520 --> 00:37:27,800
Hardware setups uh and they'll be able

973
00:37:25,640 --> 00:37:30,480
to just essentially host these models

974
00:37:27,800 --> 00:37:32,440
and you can try them out and use them so

975
00:37:30,480 --> 00:37:34,119
hogging face is awesome it's a major

976
00:37:32,440 --> 00:37:36,119
player in the whole ecosystem and I

977
00:37:34,119 --> 00:37:39,920
could not leave it out

978
00:37:36,119 --> 00:37:41,800
but uh yeah that's that's pretty much it

979
00:37:39,920 --> 00:37:43,800
for the Deep learning ecosystem I'll see

980
00:37:41,800 --> 00:37:46,920
you in the next

981
00:37:43,800 --> 00:37:48,640
part so doing the setup on Windows we

982
00:37:46,920 --> 00:37:50,800
just need to open up our terminal and

983
00:37:48,640 --> 00:37:53,880
run as administrator I'm starting with

984
00:37:50,800 --> 00:37:56,880
Windows uh we just enable permissions

985
00:37:53,880 --> 00:37:58,960
ensure that it's the system 32 directory

986
00:37:56,880 --> 00:38:02,520
going to navigate get over to the turn

987
00:37:58,960 --> 00:38:05,000
Windows features on and off um we're

988
00:38:02,520 --> 00:38:08,359
going to scroll up and look for

989
00:38:05,000 --> 00:38:10,079
hyperv um ensure that box is checked off

990
00:38:08,359 --> 00:38:13,040
and then we're going to look for virtual

991
00:38:10,079 --> 00:38:16,560
machine platform ensure that is checked

992
00:38:13,040 --> 00:38:18,880
off and then or checked on rather and

993
00:38:16,560 --> 00:38:21,680
then you have a Windows subsystem for

994
00:38:18,880 --> 00:38:24,280
Linux make sure that's also checked on

995
00:38:21,680 --> 00:38:26,839
um in order to get this working you will

996
00:38:24,280 --> 00:38:28,359
need uh to enable virtualization on your

997
00:38:26,839 --> 00:38:30,160
machine

998
00:38:28,359 --> 00:38:33,920
so uh you know once the windows

999
00:38:30,160 --> 00:38:35,640
subsystem is on you can do wl. exe and

1000
00:38:33,920 --> 00:38:38,520
you'll see you know a bunch of options

1001
00:38:35,640 --> 00:38:41,520
so install distribution and we see an

1002
00:38:38,520 --> 00:38:46,119
example there WSL install distribution

1003
00:38:41,520 --> 00:38:48,280
Yu we can go ahead and enter enable and

1004
00:38:46,119 --> 00:38:50,319
we'll just wait for that to complete

1005
00:38:48,280 --> 00:38:53,160
I've sped this up a little bit because

1006
00:38:50,319 --> 00:38:54,880
it takes some time uh realistically it

1007
00:38:53,160 --> 00:38:56,720
takes more than you know a few seconds

1008
00:38:54,880 --> 00:38:58,160
to do this so I'll speed some of these

1009
00:38:56,720 --> 00:39:00,800
things up

1010
00:38:58,160 --> 00:39:03,079
um you has been installed

1011
00:39:00,800 --> 00:39:07,640
awesome changes will not be effective

1012
00:39:03,079 --> 00:39:10,240
until system is rebooted um yeah so we

1013
00:39:07,640 --> 00:39:14,240
run it again um we have this command

1014
00:39:10,240 --> 00:39:16,760
that it's asking us to run so WSL exe

1015
00:39:14,240 --> 00:39:18,800
install no distribution that installs

1016
00:39:16,760 --> 00:39:20,599
correctly um and we get the same thing

1017
00:39:18,800 --> 00:39:23,760
again so we just do a system

1018
00:39:20,599 --> 00:39:25,319
restart now after we've restarted you

1019
00:39:23,760 --> 00:39:28,839
might be greeted with this

1020
00:39:25,319 --> 00:39:31,200
terminal uh your BTU and then uh the

1021
00:39:28,839 --> 00:39:32,920
other command prompt so when you're

1022
00:39:31,200 --> 00:39:34,800
greeted with that if you're just greeted

1023
00:39:32,920 --> 00:39:37,119
with the command prompt you do

1024
00:39:34,800 --> 00:39:38,800
WSL uh and you can get into here and

1025
00:39:37,119 --> 00:39:40,440
just enter a username and a password

1026
00:39:38,800 --> 00:39:43,359
that you're going to use now you should

1027
00:39:40,440 --> 00:39:46,560
be logged into your uh little simulated

1028
00:39:43,359 --> 00:39:48,160
uh Linux environment so once we're in

1029
00:39:46,560 --> 00:39:50,760
here there's a few commands we need to

1030
00:39:48,160 --> 00:39:53,160
run so we're going to update and we're

1031
00:39:50,760 --> 00:39:55,040
going to upgrade everything so just type

1032
00:39:53,160 --> 00:39:56,280
in the commands as you see them there's

1033
00:39:55,040 --> 00:39:58,040
some that we're going to be able to copy

1034
00:39:56,280 --> 00:40:00,680
and paste in so just end that password

1035
00:39:58,040 --> 00:40:02,040
you set earlier I've time-elapsed this

1036
00:40:00,680 --> 00:40:04,520
not time-lapsed but I've sped this one

1037
00:40:02,040 --> 00:40:07,480
up again so that was just a bunch of

1038
00:40:04,520 --> 00:40:08,960
things uh updating um if we go and

1039
00:40:07,480 --> 00:40:11,880
install some other packages that we'll

1040
00:40:08,960 --> 00:40:13,560
need later like WG curl and git we'll

1041
00:40:11,880 --> 00:40:15,400
see that those are also installed as a

1042
00:40:13,560 --> 00:40:18,359
part of the update and upgrade

1043
00:40:15,400 --> 00:40:20,839
commands um and then we just install

1044
00:40:18,359 --> 00:40:24,520
Python 3 pip uh this is just going to be

1045
00:40:20,839 --> 00:40:28,400
python essentially for our machine and

1046
00:40:24,520 --> 00:40:30,880
uh that also runs too so that does not

1047
00:40:28,400 --> 00:40:33,280
come by default apparently um so we just

1048
00:40:30,880 --> 00:40:36,119
need to install that manually but that's

1049
00:40:33,280 --> 00:40:37,800
okay we navigate over to Chrome and we

1050
00:40:36,119 --> 00:40:40,000
search Up Cuda toolkit this is what

1051
00:40:37,800 --> 00:40:41,680
we're looking for Cuda toolkit download

1052
00:40:40,000 --> 00:40:43,800
so you just navigate to the latest one

1053
00:40:41,680 --> 00:40:46,560
it might be 12.5 it might be 12.6

1054
00:40:43,800 --> 00:40:49,640
whatever it is for you go to Linux pick

1055
00:40:46,560 --> 00:40:52,160
your architecture and use WSL you to

1056
00:40:49,640 --> 00:40:53,839
remember we're using uh WSL and then

1057
00:40:52,160 --> 00:40:55,319
just do the run file it's the easiest

1058
00:40:53,839 --> 00:40:58,480
one least amount of instructions you

1059
00:40:55,319 --> 00:41:01,400
have to do um so so going to do the

1060
00:40:58,480 --> 00:41:03,520
first one so w get uh you can just right

1061
00:41:01,400 --> 00:41:07,000
click in the terminal if normal pasting

1062
00:41:03,520 --> 00:41:07,000
doesn't work

1063
00:41:07,720 --> 00:41:13,680
um let's maybe highlight the whole

1064
00:41:11,400 --> 00:41:18,040
thing awesome so that's going to take

1065
00:41:13,680 --> 00:41:21,040
some time to upgrade and uh I'll see you

1066
00:41:18,040 --> 00:41:24,119
guys on the other side okay so now we're

1067
00:41:21,040 --> 00:41:25,920
in the little accept part so just only

1068
00:41:24,119 --> 00:41:28,240
check off the to Cuda toolkit there and

1069
00:41:25,920 --> 00:41:31,640
then you should be good to install

1070
00:41:28,240 --> 00:41:34,280
now we've done the runsh file which was

1071
00:41:31,640 --> 00:41:35,880
the second part of the command and it

1072
00:41:34,280 --> 00:41:37,520
tells us in the summary that we need to

1073
00:41:35,880 --> 00:41:40,960
add some things to our path so I've just

1074
00:41:37,520 --> 00:41:42,800
pulled up this here um but this wasn't

1075
00:41:40,960 --> 00:41:45,839
really working too much so I went off

1076
00:41:42,800 --> 00:41:48,960
and generated some other uh you know

1077
00:41:45,839 --> 00:41:51,560
more upto-date commands with chat GPT uh

1078
00:41:48,960 --> 00:41:53,560
and figured out uh the act the the

1079
00:41:51,560 --> 00:41:55,720
proper ones

1080
00:41:53,560 --> 00:41:57,079
so you'll see those in a second here

1081
00:41:55,720 --> 00:41:59,480
once I pulled them up but this is just

1082
00:41:57,079 --> 00:42:02,359
the this is just a reference so this is

1083
00:41:59,480 --> 00:42:05,720
one of the things we have to do so we

1084
00:42:02,359 --> 00:42:07,400
could just Vim into our bash RC file um

1085
00:42:05,720 --> 00:42:09,240
and then I'll just just pretty much type

1086
00:42:07,400 --> 00:42:11,680
along with me here and then we'll we'll

1087
00:42:09,240 --> 00:42:13,680
save this file so feel free to use Nano

1088
00:42:11,680 --> 00:42:16,960
or Vim whatever whatever you feel

1089
00:42:13,680 --> 00:42:19,400
comfortable with I'm using Vim here but

1090
00:42:16,960 --> 00:42:22,760
uh you know Nano isn't too hard either

1091
00:42:19,400 --> 00:42:24,680
so going to set a Cuda home um user

1092
00:42:22,760 --> 00:42:29,680
local Cuda and then we're going to

1093
00:42:24,680 --> 00:42:32,480
export another one uh called path and as

1094
00:42:29,680 --> 00:42:35,160
a part of that path um we're just

1095
00:42:32,480 --> 00:42:37,160
essentially going to include Cuda home

1096
00:42:35,160 --> 00:42:40,760
and then the binary for

1097
00:42:37,160 --> 00:42:43,960
that and then last but not least um

1098
00:42:40,760 --> 00:42:47,359
we're just going to export the LD

1099
00:42:43,960 --> 00:42:47,359
Library like it also said in the

1100
00:42:51,000 --> 00:42:59,319
summary and then lip 64 to end it

1101
00:42:55,040 --> 00:43:02,000
off awesome so now we can just contrl C

1102
00:42:59,319 --> 00:43:08,040
contrl W and contrl

1103
00:43:02,000 --> 00:43:11,200
Q or col w q we exit that um and then we

1104
00:43:08,040 --> 00:43:14,760
can oh I noticed we missed something so

1105
00:43:11,200 --> 00:43:17,000
Cuda 12.5 instead of just Cuda um so we

1106
00:43:14,760 --> 00:43:21,559
can go ahead and go back into this and

1107
00:43:17,000 --> 00:43:21,559
then just find that part and add

1108
00:43:21,800 --> 00:43:25,800
um and just right click and paste that

1109
00:43:24,119 --> 00:43:28,480
back in and then just delete the last

1110
00:43:25,800 --> 00:43:32,559
Cuda part awesome

1111
00:43:28,480 --> 00:43:35,079
C 12.5 sweet now we can just exit that

1112
00:43:32,559 --> 00:43:39,119
again and

1113
00:43:35,079 --> 00:43:41,160
Source then we just do nvcc uh-- version

1114
00:43:39,119 --> 00:43:44,079
which is the Nvidia Cuda compiler so

1115
00:43:41,160 --> 00:43:45,960
that's working and then Nvidia s SMI so

1116
00:43:44,079 --> 00:43:48,000
we can actually track our GPU stats as

1117
00:43:45,960 --> 00:43:49,960
long as these are both working um we've

1118
00:43:48,000 --> 00:43:52,319
done the job correctly so if you're not

1119
00:43:49,960 --> 00:43:53,240
get if you're getting errors with nvcc

1120
00:43:52,319 --> 00:43:55,760
or

1121
00:43:53,240 --> 00:43:58,240
nvmi uh that's that's not good you need

1122
00:43:55,760 --> 00:44:01,640
to figure that out uh I don't of course

1123
00:43:58,240 --> 00:44:04,280
cover all the errors but

1124
00:44:01,640 --> 00:44:06,559
um that aside we're going to go ahead

1125
00:44:04,280 --> 00:44:07,720
and set up a little Cuda test just to

1126
00:44:06,559 --> 00:44:09,880
make sure that everything's working

1127
00:44:07,720 --> 00:44:12,000
properly and that we can execute a docu

1128
00:44:09,880 --> 00:44:15,000
or Cuda script so I just made a

1129
00:44:12,000 --> 00:44:18,480
directory called cuda setup test and

1130
00:44:15,000 --> 00:44:19,960
we're going to just Vim into um that

1131
00:44:18,480 --> 00:44:21,559
that directory and we're going to edit

1132
00:44:19,960 --> 00:44:23,680
and we're going to make a new main. cuu

1133
00:44:21,559 --> 00:44:25,920
file and inside of here I'm just going

1134
00:44:23,680 --> 00:44:28,599
to go ahead and paste uh some functions

1135
00:44:25,920 --> 00:44:30,319
so uh we include the Cuda runtime header

1136
00:44:28,599 --> 00:44:32,880
we include the io stream which is a part

1137
00:44:30,319 --> 00:44:36,800
of C++ that allows us to use things like

1138
00:44:32,880 --> 00:44:39,119
C out we declare the namespace STD for

1139
00:44:36,800 --> 00:44:42,119
for standard and then we see out hello

1140
00:44:39,119 --> 00:44:46,079
world in our in main function so if we

1141
00:44:42,119 --> 00:44:49,079
do nvcc um out main binary and then

1142
00:44:46,079 --> 00:44:51,400
main. cuu we should uh be able to run

1143
00:44:49,079 --> 00:44:53,920
this binary and get Hello World

1144
00:44:51,400 --> 00:44:56,079
awesome so if this works first try for

1145
00:44:53,920 --> 00:44:57,680
you that's awesome if it didn't that's

1146
00:44:56,079 --> 00:44:59,839
not so awesome but but you should be

1147
00:44:57,680 --> 00:45:02,040
able to figure it out just by navigating

1148
00:44:59,839 --> 00:45:04,440
uh forums so like GitHub the stuff I

1149
00:45:02,040 --> 00:45:05,880
recommended before just navigate around

1150
00:45:04,440 --> 00:45:08,680
and figure out how to install the Cuda

1151
00:45:05,880 --> 00:45:10,800
toolkit for Windows um it pretty much

1152
00:45:08,680 --> 00:45:13,440
applies the same to yuntu I'm going to

1153
00:45:10,800 --> 00:45:14,800
go over some brief instructions here but

1154
00:45:13,440 --> 00:45:16,720
I'm going to switch over to Ubuntu

1155
00:45:14,800 --> 00:45:18,520
because that's what my whole thing is

1156
00:45:16,720 --> 00:45:19,960
based on that's where I do all of my

1157
00:45:18,520 --> 00:45:22,240
stuff and where everything is set up and

1158
00:45:19,960 --> 00:45:23,880
optimized for so I'll see you on the

1159
00:45:22,240 --> 00:45:26,240
other side if we can go ahead and open a

1160
00:45:23,880 --> 00:45:27,920
Chrome tab here and just type in Cuda

1161
00:45:26,240 --> 00:45:31,760
toolkit downlo

1162
00:45:27,920 --> 00:45:36,440
mod so we go to this one on your BTU

1163
00:45:31,760 --> 00:45:39,079
same thing we go Linux x64 is mine might

1164
00:45:36,440 --> 00:45:40,760
be different for you um

1165
00:45:39,079 --> 00:45:44,839
YouTu this

1166
00:45:40,760 --> 00:45:48,319
one run file local um you can do this

1167
00:45:44,839 --> 00:45:50,480
you can also do Network or local so for

1168
00:45:48,319 --> 00:45:53,319
me I did network but that was a little

1169
00:45:50,480 --> 00:45:54,920
while back and having to uninstall it

1170
00:45:53,319 --> 00:45:57,720
and then reinstall it again just gives

1171
00:45:54,920 --> 00:45:59,680
me a bunch of weird Graphics uh errors

1172
00:45:57,720 --> 00:46:03,240
so I'm not going to do that and mess

1173
00:45:59,680 --> 00:46:04,559
with my operating system too much but

1174
00:46:03,240 --> 00:46:06,359
you should be able to just plug this

1175
00:46:04,559 --> 00:46:10,359
directly into terminal so you should be

1176
00:46:06,359 --> 00:46:17,000
able to just pop into here and uh plug

1177
00:46:10,359 --> 00:46:18,200
these in W get the uh the Debian file um

1178
00:46:17,000 --> 00:46:20,520
and then just the rest of this and

1179
00:46:18,200 --> 00:46:23,280
install the Cuda toolkit and then just

1180
00:46:20,520 --> 00:46:25,720
get the the Legacy Cuda drivers I just

1181
00:46:23,280 --> 00:46:29,079
did this this Legacy Cuda drivers if

1182
00:46:25,720 --> 00:46:31,079
that doesn't work do this one um and

1183
00:46:29,079 --> 00:46:37,280
then you would of course want to just do

1184
00:46:31,079 --> 00:46:39,400
do um nbcc version and then Nvidia SMI

1185
00:46:37,280 --> 00:46:43,040
and you should see uh some useful stuff

1186
00:46:39,400 --> 00:46:46,079
pop up here so uh if that if these don't

1187
00:46:43,040 --> 00:46:47,559
work for you right away um you know you

1188
00:46:46,079 --> 00:46:49,160
might want to just restart your computer

1189
00:46:47,559 --> 00:46:51,359
that's usually the best option and then

1190
00:46:49,160 --> 00:46:52,559
try something again um if you do already

1191
00:46:51,359 --> 00:46:54,920
have these installed you don't even have

1192
00:46:52,559 --> 00:46:57,079
to worry about it so I would probably

1193
00:46:54,920 --> 00:47:00,440
check these first probably should said

1194
00:46:57,079 --> 00:47:04,319
that first but uh yeah Ure uh the Auda

1195
00:47:00,440 --> 00:47:04,319
compiler works and then Nvidia

1196
00:47:06,760 --> 00:47:13,359
SMI so now we can finally get into some

1197
00:47:09,800 --> 00:47:15,839
coding um in order to really understand

1198
00:47:13,359 --> 00:47:18,720
how to use Cuda you need to First cover

1199
00:47:15,839 --> 00:47:20,880
C and C++ so this course isn't actually

1200
00:47:18,720 --> 00:47:22,880
about C and C++ so I'm just going to

1201
00:47:20,880 --> 00:47:24,359
provide some resources for you guys to

1202
00:47:22,880 --> 00:47:26,440
learn this stuff and then I'll jump into

1203
00:47:24,359 --> 00:47:28,440
more some more advanced topics uh just

1204
00:47:26,440 --> 00:47:30,520
to watch over and and review the

1205
00:47:28,440 --> 00:47:31,960
subjects so for those of you who are new

1206
00:47:30,520 --> 00:47:36,000
to this stuff for those of you who are

1207
00:47:31,960 --> 00:47:37,960
new to lowlevel C C++ Cuda programming

1208
00:47:36,000 --> 00:47:40,280
um I have some resources for you some

1209
00:47:37,960 --> 00:47:42,760
good articles some good uh things to

1210
00:47:40,280 --> 00:47:45,520
manage through and if you are already

1211
00:47:42,760 --> 00:47:48,240
experience uh just pretty much skip this

1212
00:47:45,520 --> 00:47:50,440
part or even even still look at it to

1213
00:47:48,240 --> 00:47:51,960
maybe touch up on the basics and I'll

1214
00:47:50,440 --> 00:47:57,200
cover some more advanced topics right

1215
00:47:51,960 --> 00:47:59,040
after this so learning C and C++ is hard

1216
00:47:57,200 --> 00:48:00,680
and so you really have to Define what

1217
00:47:59,040 --> 00:48:02,200
the best resources are and how to

1218
00:48:00,680 --> 00:48:03,680
actually learn things properly what is

1219
00:48:02,200 --> 00:48:07,040
the best use of your time right this is

1220
00:48:03,680 --> 00:48:08,680
a common dilemma that we have so I came

1221
00:48:07,040 --> 00:48:13,040
across a Reddit article on best

1222
00:48:08,680 --> 00:48:15,760
resources to learn C++ and it it pretty

1223
00:48:13,040 --> 00:48:17,920
much said learn C+ plus.com plus a bunch

1224
00:48:15,760 --> 00:48:20,440
of other links that you might like so

1225
00:48:17,920 --> 00:48:22,000
learn C+ plus.com is good I've never

1226
00:48:20,440 --> 00:48:24,839
used this before so I don't know how

1227
00:48:22,000 --> 00:48:26,559
comparatively good it is um so that

1228
00:48:24,839 --> 00:48:28,760
that's an option of course and then

1229
00:48:26,559 --> 00:48:30,599
there's best way to learn C so looking

1230
00:48:28,760 --> 00:48:35,160
through this I pretty much found that

1231
00:48:30,599 --> 00:48:37,599
Everyone likes the um the modern C this

1232
00:48:35,160 --> 00:48:41,119
one C programming a modern approach so

1233
00:48:37,599 --> 00:48:43,280
it's a it's a newer book um but that's

1234
00:48:41,119 --> 00:48:44,599
how people found best way to learn to C

1235
00:48:43,280 --> 00:48:46,720
um if you are just trying to learn this

1236
00:48:44,599 --> 00:48:48,920
for free and try to you know just go

1237
00:48:46,720 --> 00:48:52,440
through the syntax and understand it as

1238
00:48:48,920 --> 00:48:54,480
quickly as possible um there are some

1239
00:48:52,440 --> 00:48:56,000
resources I would recommend and have

1240
00:48:54,480 --> 00:48:58,880
looked through a little bit so freed

1241
00:48:56,000 --> 00:49:01,200
code camp has some good stuff on this um

1242
00:48:58,880 --> 00:49:03,599
C programming just a bunch of uh blogs

1243
00:49:01,200 --> 00:49:05,480
essentially on how to just pretty much

1244
00:49:03,599 --> 00:49:06,359
just learning the language and then you

1245
00:49:05,480 --> 00:49:08,960
have

1246
00:49:06,359 --> 00:49:12,680
C++ uh just you know maybe some more

1247
00:49:08,960 --> 00:49:16,720
advanced things um you know

1248
00:49:12,680 --> 00:49:18,680
libraries a bunch of modern C++ stuff um

1249
00:49:16,720 --> 00:49:20,160
so free code Camp is a great resource

1250
00:49:18,680 --> 00:49:22,200
and then the one that I've personally

1251
00:49:20,160 --> 00:49:25,599
stuck with for a long time and continue

1252
00:49:22,200 --> 00:49:27,760
to use is W3 schools so it's pretty much

1253
00:49:25,599 --> 00:49:31,040
like a nice easy to read easy on the

1254
00:49:27,760 --> 00:49:34,000
eyes documentation on or just an intro

1255
00:49:31,040 --> 00:49:36,799
rather on how to use C and C++ so I have

1256
00:49:34,000 --> 00:49:40,119
both here um I'd recommend if you're new

1257
00:49:36,799 --> 00:49:42,720
to this just look over each of these and

1258
00:49:40,119 --> 00:49:44,720
do do a bunch of practice questions on

1259
00:49:42,720 --> 00:49:46,640
every single one of these um all these

1260
00:49:44,720 --> 00:49:47,960
are super important there there's some

1261
00:49:46,640 --> 00:49:49,319
of them you might not actually use

1262
00:49:47,960 --> 00:49:51,040
explicitly in the course but it's still

1263
00:49:49,319 --> 00:49:54,319
good to know it regardless just for you

1264
00:49:51,040 --> 00:49:56,480
know having that uh you know lowlevel uh

1265
00:49:54,319 --> 00:49:58,760
brain so that you can dissect problems

1266
00:49:56,480 --> 00:50:00,079
with uh on Cuda applications that we may

1267
00:49:58,760 --> 00:50:02,880
not cover in this

1268
00:50:00,079 --> 00:50:04,400
course so I'd recommend just like

1269
00:50:02,880 --> 00:50:08,000
looking through all of these go down all

1270
00:50:04,400 --> 00:50:10,440
the way to like these examples um like

1271
00:50:08,000 --> 00:50:12,200
everything and then same with C++ as

1272
00:50:10,440 --> 00:50:15,160
well so all all your Basics your

1273
00:50:12,200 --> 00:50:19,359
functions your classes um and then down

1274
00:50:15,160 --> 00:50:21,359
to examples as well so uh that's that's

1275
00:50:19,359 --> 00:50:24,040
pretty much all I have for the basics of

1276
00:50:21,359 --> 00:50:27,119
CN C++ now we're going to go ahead and

1277
00:50:24,040 --> 00:50:28,640
touch on pointers

1278
00:50:27,119 --> 00:50:32,000
we're going to start off with pointers

1279
00:50:28,640 --> 00:50:32,000
if you go to github.com

1280
00:50:32,119 --> 00:50:37,480
inosi Cuda course um and then pop over

1281
00:50:35,920 --> 00:50:38,760
to well you're not going to pop over to

1282
00:50:37,480 --> 00:50:40,079
Dev this is going to be all pushed up

1283
00:50:38,760 --> 00:50:42,440
and ready once you once you're seeing

1284
00:50:40,079 --> 00:50:44,559
this but uh essentially you're just

1285
00:50:42,440 --> 00:50:46,319
going to get C this into a directory of

1286
00:50:44,559 --> 00:50:49,240
your choice uh and then we can go and

1287
00:50:46,319 --> 00:50:51,280
get started with the c and C++ review so

1288
00:50:49,240 --> 00:50:52,559
I have this all in my vs code here and

1289
00:50:51,280 --> 00:50:55,480
it's all zoomed in and nice for you to

1290
00:50:52,559 --> 00:50:57,240
see but um we we'll start off with a

1291
00:50:55,480 --> 00:51:01,559
symbol pointer example

1292
00:50:57,240 --> 00:51:04,200
so uh we initialize an integer integer X

1293
00:51:01,559 --> 00:51:06,960
to 10 this a data this is the data part

1294
00:51:04,200 --> 00:51:09,960
of X um and then we initialize a pointer

1295
00:51:06,960 --> 00:51:12,480
type so this asteris uh that means we're

1296
00:51:09,960 --> 00:51:13,799
doing a pointer to an integer uh and

1297
00:51:12,480 --> 00:51:17,240
we're setting that we're setting the

1298
00:51:13,799 --> 00:51:19,720
name equal to pointer and then this uh

1299
00:51:17,240 --> 00:51:22,559
Ampersand is saying we're going to get

1300
00:51:19,720 --> 00:51:25,319
the memory address of X so we have X

1301
00:51:22,559 --> 00:51:26,920
here which is 10 and the ENT says uh

1302
00:51:25,319 --> 00:51:29,200
we're going to get the memory address of

1303
00:51:26,920 --> 00:51:31,480
of X which is um which is going to be

1304
00:51:29,200 --> 00:51:34,319
the pointer to 10

1305
00:51:31,480 --> 00:51:38,960
um and then we can we can just print

1306
00:51:34,319 --> 00:51:43,040
this out so if I go GCC and we do

1307
00:51:38,960 --> 00:51:45,440
Z 01 and then run that you'll see that

1308
00:51:43,040 --> 00:51:47,920
we get an address so I have the pointer

1309
00:51:45,440 --> 00:51:49,280
I have the pointer type here there's

1310
00:51:47,920 --> 00:51:53,599
there's an index where you can find

1311
00:51:49,280 --> 00:51:57,359
these um if I pull up a tab here and go

1312
00:51:53,599 --> 00:51:59,599
uh Point uh print f

1313
00:51:57,359 --> 00:52:02,359
uh like in for

1314
00:51:59,599 --> 00:52:04,440
example so you have all these different

1315
00:52:02,359 --> 00:52:06,400
things here on C+ plus.com that you can

1316
00:52:04,440 --> 00:52:09,000
use um and these are just like the

1317
00:52:06,400 --> 00:52:12,599
formats and stuff

1318
00:52:09,000 --> 00:52:14,559
so we have uh we have a

1319
00:52:12,599 --> 00:52:17,240
pointer it's this value that we're

1320
00:52:14,559 --> 00:52:20,680
returning uh we get a memory address to

1321
00:52:17,240 --> 00:52:22,799
the value 10 and to get 10 we're passing

1322
00:52:20,680 --> 00:52:25,960
in pointer and then we do the asteris to

1323
00:52:22,799 --> 00:52:27,240
dreference it so dfference means uh we

1324
00:52:25,960 --> 00:52:29,119
we have this essentially we have this

1325
00:52:27,240 --> 00:52:31,359
data thing which is 10 we have the

1326
00:52:29,119 --> 00:52:33,680
memory address to 10 which is the level

1327
00:52:31,359 --> 00:52:35,559
above and E reference is just going to

1328
00:52:33,680 --> 00:52:36,920
go downwards just going to go back to

1329
00:52:35,559 --> 00:52:39,000
back to that so we have this memory

1330
00:52:36,920 --> 00:52:43,000
address just dreference and go back to

1331
00:52:39,000 --> 00:52:46,040
10 um the next example here is a little

1332
00:52:43,000 --> 00:52:48,520
bit tricky um but it's fine it's it's

1333
00:52:46,040 --> 00:52:50,359
relatively intuitive so I don't expect

1334
00:52:48,520 --> 00:52:52,760
it to be that hard but essentially what

1335
00:52:50,359 --> 00:52:56,680
we're doing is we initialize a value to

1336
00:52:52,760 --> 00:52:59,839
42 and then we make a uh a pointer an

1337
00:52:56,680 --> 00:53:01,559
integer pointer type called pointer one

1338
00:52:59,839 --> 00:53:04,640
and we set that equal to the memory

1339
00:53:01,559 --> 00:53:06,760
address of value so it's ersan memory

1340
00:53:04,640 --> 00:53:08,960
address of value um so it's going to be

1341
00:53:06,760 --> 00:53:11,920
like 42 and then we create pointer 1

1342
00:53:08,960 --> 00:53:14,359
which is a memory address or a pointer 2

1343
00:53:11,920 --> 00:53:16,240
42 and then we do the same thing so we

1344
00:53:14,359 --> 00:53:18,520
make a pointer to a pointer which is

1345
00:53:16,240 --> 00:53:20,920
what the double asteris is for uh and

1346
00:53:18,520 --> 00:53:23,920
then we do again Amber sand of pointer

1347
00:53:20,920 --> 00:53:26,720
one so memory address of this pointer so

1348
00:53:23,920 --> 00:53:29,599
then you have 10 you have 42 and then

1349
00:53:26,720 --> 00:53:32,240
you have memory address which is the the

1350
00:53:29,599 --> 00:53:33,960
you have the pointer to uh 42 then you

1351
00:53:32,240 --> 00:53:37,440
have another one above that which is

1352
00:53:33,960 --> 00:53:39,760
pointer to a pointer to to a value and

1353
00:53:37,440 --> 00:53:41,960
then we just do that another time so

1354
00:53:39,760 --> 00:53:47,480
it's pointer to a pointer to a pointer

1355
00:53:41,960 --> 00:53:50,160
to a value um and uh this logic checks

1356
00:53:47,480 --> 00:53:53,720
out and when we print this out we're

1357
00:53:50,160 --> 00:53:55,440
going to return the integer type so D um

1358
00:53:53,720 --> 00:53:58,200
and then we're just going to Triple D

1359
00:53:55,440 --> 00:54:00,359
reference it so we have these multiple

1360
00:53:58,200 --> 00:54:02,359
different layers so we're going up a

1361
00:54:00,359 --> 00:54:04,760
pointer like level one pointer level two

1362
00:54:02,359 --> 00:54:06,240
pointer level three and D referencing is

1363
00:54:04,760 --> 00:54:07,960
just like going down a level so we go

1364
00:54:06,240 --> 00:54:09,839
down one two three levels and back to

1365
00:54:07,960 --> 00:54:12,400
back to that value of 42 which is an

1366
00:54:09,839 --> 00:54:16,520
integer and we can safely return that so

1367
00:54:12,400 --> 00:54:20,000
if I just go GCC and then compile

1368
00:54:16,520 --> 00:54:23,839
02 we go and run that and we get a safe

1369
00:54:20,000 --> 00:54:26,480
output value 42 awesome so now I pop

1370
00:54:23,839 --> 00:54:28,640
over to number three which is where

1371
00:54:26,480 --> 00:54:29,960
things start to get a little bit weird

1372
00:54:28,640 --> 00:54:32,119
uh and initially this was kind of a

1373
00:54:29,960 --> 00:54:34,440
funky topic for me as well but this is

1374
00:54:32,119 --> 00:54:35,480
this is void pointers so void pointers

1375
00:54:34,440 --> 00:54:38,160
are a little funny and they actually

1376
00:54:35,480 --> 00:54:40,480
allow us to do a lot of tricks that uh

1377
00:54:38,160 --> 00:54:42,720
allow for things like polymorphism and

1378
00:54:40,480 --> 00:54:46,240
stuff um but we're not going to go over

1379
00:54:42,720 --> 00:54:49,319
that a ton that's that's like other uh

1380
00:54:46,240 --> 00:54:51,720
that's not covered in this review uh so

1381
00:54:49,319 --> 00:54:54,240
we initialize an integer called num to

1382
00:54:51,720 --> 00:54:55,200
10 we initialize a float called f num

1383
00:54:54,240 --> 00:54:58,240
equal to

1384
00:54:55,200 --> 00:55:01,000
3.14 um and then we have this this void

1385
00:54:58,240 --> 00:55:03,040
pointer so what this means is like if

1386
00:55:01,000 --> 00:55:04,720
you had an integer and then an asteris

1387
00:55:03,040 --> 00:55:08,359
that would mean it's a pointer to an

1388
00:55:04,720 --> 00:55:11,640
integer but void is no type so it's like

1389
00:55:08,359 --> 00:55:13,480
a pointer to no type and that means we

1390
00:55:11,640 --> 00:55:16,000
can actually change which type it is

1391
00:55:13,480 --> 00:55:19,359
pointed to which is a cool little

1392
00:55:16,000 --> 00:55:22,440
feature that you can do in C um so we

1393
00:55:19,359 --> 00:55:25,599
say um void pointer is going to equal

1394
00:55:22,440 --> 00:55:29,280
the memory address of num which is this

1395
00:55:25,599 --> 00:55:32,359
right um and then what we can do here in

1396
00:55:29,280 --> 00:55:36,039
this in this print part we essentially

1397
00:55:32,359 --> 00:55:37,920
uh we take this we cast it to an INT

1398
00:55:36,039 --> 00:55:40,039
pointer type that's what this part is

1399
00:55:37,920 --> 00:55:41,920
for these brackets and then inside the

1400
00:55:40,039 --> 00:55:45,960
int and then after the Asis the the

1401
00:55:41,920 --> 00:55:50,000
pointer cast and then we dreference that

1402
00:55:45,960 --> 00:55:51,079
so we have a void pointer which we cast

1403
00:55:50,000 --> 00:55:53,599
to an

1404
00:55:51,079 --> 00:55:55,559
integer we cast to an integer type it's

1405
00:55:53,599 --> 00:55:57,400
originally holding the memory address of

1406
00:55:55,559 --> 00:55:59,640
int

1407
00:55:57,400 --> 00:56:01,640
um and then we dreference that after

1408
00:55:59,640 --> 00:56:03,039
it's casted so it's going to go up to

1409
00:56:01,640 --> 00:56:05,359
this memory address then it's going to

1410
00:56:03,039 --> 00:56:08,359
go back to 10 which is the value of

1411
00:56:05,359 --> 00:56:13,440
num and then we essentially just do the

1412
00:56:08,359 --> 00:56:14,799
same thing for this F num here um I I

1413
00:56:13,440 --> 00:56:17,039
have nice little descriptions here that

1414
00:56:14,799 --> 00:56:20,400
you can read on your own uh and and then

1415
00:56:17,039 --> 00:56:22,880
a fun little fact so Malik actually

1416
00:56:20,400 --> 00:56:25,240
returns a void pointer but we see it

1417
00:56:22,880 --> 00:56:27,079
point to a specific data type after the

1418
00:56:25,240 --> 00:56:29,440
cast so

1419
00:56:27,079 --> 00:56:32,119
what you typically see Malik as like

1420
00:56:29,440 --> 00:56:33,640
these these opening brackets uh these

1421
00:56:32,119 --> 00:56:35,720
this brackets and then you have the

1422
00:56:33,640 --> 00:56:38,079
actual cast inside of it so what we did

1423
00:56:35,720 --> 00:56:39,920
over here is what you see in Malik so

1424
00:56:38,079 --> 00:56:41,640
it's actually returning a void pointer

1425
00:56:39,920 --> 00:56:43,760
and then you cast that to a specific

1426
00:56:41,640 --> 00:56:46,480
like integer or a floating Point uh

1427
00:56:43,760 --> 00:56:50,319
pointer um and and then you can use that

1428
00:56:46,480 --> 00:56:55,839
for something like like an array um so

1429
00:56:50,319 --> 00:56:55,839
if we go ahead and just GCC compile this

1430
00:56:58,400 --> 00:57:04,920
and then run we get our integer which is

1431
00:57:02,480 --> 00:57:08,680
uh integer type of course and then we

1432
00:57:04,920 --> 00:57:11,839
get our our float 3.14 which is a uh

1433
00:57:08,680 --> 00:57:13,440
which is a float type so void pointers

1434
00:57:11,839 --> 00:57:14,880
are not not void pointers sorry null

1435
00:57:13,440 --> 00:57:16,680
pointers are really interesting and you

1436
00:57:14,880 --> 00:57:19,200
probably found void pointers interesting

1437
00:57:16,680 --> 00:57:22,520
as well um but these are a little bit

1438
00:57:19,200 --> 00:57:25,160
different so null pointers can actually

1439
00:57:22,520 --> 00:57:28,319
make our code more robust through if

1440
00:57:25,160 --> 00:57:31,559
statements um going remove those binary

1441
00:57:28,319 --> 00:57:34,359
files for now to clean some space up um

1442
00:57:31,559 --> 00:57:39,079
but we we initialize a pointer to null

1443
00:57:34,359 --> 00:57:40,799
um if we try to print this out um it's

1444
00:57:39,079 --> 00:57:43,160
it's going to essentially return like

1445
00:57:40,799 --> 00:57:46,359
there's nothing right there's like no

1446
00:57:43,160 --> 00:57:48,520
space actually here there's nothing that

1447
00:57:46,359 --> 00:57:49,839
you can't you can't use that pointer for

1448
00:57:48,520 --> 00:57:52,480
anything because it's null that's the

1449
00:57:49,839 --> 00:57:54,640
whole idea here so what we can do is we

1450
00:57:52,480 --> 00:57:57,760
can check if the pointer is equal to

1451
00:57:54,640 --> 00:57:59,400
n um and then we just essentially just

1452
00:57:57,760 --> 00:58:02,160
report that maybe we throw an error

1453
00:57:59,400 --> 00:58:04,319
message or we we we put a warning up uh

1454
00:58:02,160 --> 00:58:06,280
cannot dreference this right if the

1455
00:58:04,319 --> 00:58:09,079
pointer doesn't have anything you can't

1456
00:58:06,280 --> 00:58:13,319
dreference nothing to something you

1457
00:58:09,079 --> 00:58:16,680
can't do that so uh we actually change

1458
00:58:13,319 --> 00:58:19,359
that up and we we allocate memory uh to

1459
00:58:16,680 --> 00:58:23,240
pointer so to this pointer so that we

1460
00:58:19,359 --> 00:58:24,839
can use it safely later on so uh I'm

1461
00:58:23,240 --> 00:58:27,559
just going to actually compile this so

1462
00:58:24,839 --> 00:58:31,319
you can so you can see what it's

1463
00:58:27,559 --> 00:58:33,599
doing um initial pointer value is nil so

1464
00:58:31,319 --> 00:58:36,039
we have the pointer type we cast to a

1465
00:58:33,599 --> 00:58:39,119
void pointer which is going to be n null

1466
00:58:36,039 --> 00:58:41,920
of course so n that checks out pointer

1467
00:58:39,119 --> 00:58:43,640
is null cannot D reference good so this

1468
00:58:41,920 --> 00:58:46,400
this was

1469
00:58:43,640 --> 00:58:49,799
true after allocation so this is where

1470
00:58:46,400 --> 00:58:56,280
we get into this part Malik is going to

1471
00:58:49,799 --> 00:58:57,640
return uh a void pointer the size of int

1472
00:58:56,280 --> 00:58:59,640
so that means there is actually

1473
00:58:57,640 --> 00:59:00,760
something there now there there is

1474
00:58:59,640 --> 00:59:03,280
something there it doesn't have an

1475
00:59:00,760 --> 00:59:06,720
explicit data type but we have something

1476
00:59:03,280 --> 00:59:10,079
there that's like in I think 32 bits so

1477
00:59:06,720 --> 00:59:13,520
four bytes um and then we check if

1478
00:59:10,079 --> 00:59:15,599
pointer is equal to n again um and it

1479
00:59:13,520 --> 00:59:18,319
doesn't print memory allocation failed

1480
00:59:15,599 --> 00:59:20,359
so that's that's good and then we get to

1481
00:59:18,319 --> 00:59:24,280
number four after allocation pointer

1482
00:59:20,359 --> 00:59:27,079
value is this so we cast this to a uh

1483
00:59:24,280 --> 00:59:29,200
void pointer uh and we can actually see

1484
00:59:27,079 --> 00:59:31,839
this this this memory address we can

1485
00:59:29,200 --> 00:59:34,240
actually see that it works um and then

1486
00:59:31,839 --> 00:59:36,119
we can you know we we know that this

1487
00:59:34,240 --> 00:59:39,640
exists now so we can use it for

1488
00:59:36,119 --> 00:59:41,960
something so um we we essentially check

1489
00:59:39,640 --> 00:59:45,760
it for null safe to use now uh and then

1490
00:59:41,960 --> 00:59:48,079
we have this uh we have this dreference

1491
00:59:45,760 --> 00:59:50,400
pointer so you dreference that memory

1492
00:59:48,079 --> 00:59:53,240
address back to the data part and we set

1493
00:59:50,400 --> 00:59:56,599
that equal to 42 so now you have this uh

1494
00:59:53,240 --> 00:59:57,839
safe to use um you have this you safe to

1495
00:59:56,599 --> 01:00:00,400
use memory address and the data

1496
00:59:57,839 --> 01:00:03,160
associated with that memory address and

1497
01:00:00,400 --> 01:00:05,480
then we can free that pointer um set to

1498
01:00:03,160 --> 01:00:10,599
n after freeing uh and then we see that

1499
01:00:05,480 --> 01:00:13,839
it's it's it's null again so uh yeah if

1500
01:00:10,599 --> 01:00:17,520
pointer is null so we we know that it is

1501
01:00:13,839 --> 01:00:20,400
null safely avoided use after free

1502
01:00:17,520 --> 01:00:23,839
awesome so the the whole point here is

1503
01:00:20,400 --> 01:00:25,520
that we can use no pointers to uh do

1504
01:00:23,839 --> 01:00:26,599
little tricks and we can make our code

1505
01:00:25,520 --> 01:00:29,440
Mar

1506
01:00:26,599 --> 01:00:31,559
by checking if it is null we can avoid

1507
01:00:29,440 --> 01:00:33,079
running into unexpected errors like seg

1508
01:00:31,559 --> 01:00:35,079
faults and other weird things that are

1509
01:00:33,079 --> 01:00:36,400
hard to trace back right sometimes you

1510
01:00:35,079 --> 01:00:37,960
don't want to have to go through all of

1511
01:00:36,400 --> 01:00:40,319
that just to figure out an error so it's

1512
01:00:37,960 --> 01:00:41,799
better to just uh write more robust code

1513
01:00:40,319 --> 01:00:44,559
in the first place and ensure that it

1514
01:00:41,799 --> 01:00:46,319
works properly so in this example

1515
01:00:44,559 --> 01:00:48,000
there's quite a few things going on here

1516
01:00:46,319 --> 01:00:51,520
but I'll try to I'll try to explain this

1517
01:00:48,000 --> 01:00:53,240
as best as possible so we have an array

1518
01:00:51,520 --> 01:00:56,160
uh we just declare an integer array five

1519
01:00:53,240 --> 01:00:59,440
numbers and then we have an uh pointer

1520
01:00:56,160 --> 01:01:01,359
an integer pointer equal to array so in

1521
01:00:59,440 --> 01:01:03,400
C because we're just leaving it as AR

1522
01:01:01,359 --> 01:01:05,720
Ray alone it's going to point to the

1523
01:01:03,400 --> 01:01:07,680
first element uh because that's that's

1524
01:01:05,720 --> 01:01:09,839
how memory is lined up right it's going

1525
01:01:07,680 --> 01:01:11,839
to point to essentially where does this

1526
01:01:09,839 --> 01:01:13,880
thing start uh and that's going to be

1527
01:01:11,839 --> 01:01:16,760
that's going to be 12 of course but we

1528
01:01:13,880 --> 01:01:19,119
know that an array is a pointer on its

1529
01:01:16,760 --> 01:01:21,480
own if we don't dreference or if we

1530
01:01:19,119 --> 01:01:23,920
don't index that array um it's just

1531
01:01:21,480 --> 01:01:26,799
going to be a pointer alone right so if

1532
01:01:23,920 --> 01:01:30,920
I if I print f

1533
01:01:26,799 --> 01:01:33,799
uh and then we go we just go

1534
01:01:30,920 --> 01:01:35,160
array sure I'll let get up co-pilot

1535
01:01:33,799 --> 01:01:39,520
complete

1536
01:01:35,160 --> 01:01:39,520
that not not correct

1537
01:01:41,359 --> 01:01:47,640
yeah we'll see that this array if we

1538
01:01:45,280 --> 01:01:49,160
don't dreference it uh just just on its

1539
01:01:47,640 --> 01:01:52,880
own it is a memory address it's a

1540
01:01:49,160 --> 01:01:56,760
pointer to this array um so we set We

1541
01:01:52,880 --> 01:02:00,319
Set uh let me just delete that we set

1542
01:01:56,760 --> 01:02:01,839
uh an an integer pointer equal to that

1543
01:02:00,319 --> 01:02:05,079
so that's a that's a memory address that

1544
01:02:01,839 --> 01:02:07,319
we have um awesome now we look at our

1545
01:02:05,079 --> 01:02:09,760
position one which I printed out that's

1546
01:02:07,319 --> 01:02:13,319
going to be 12 so we have that that

1547
01:02:09,760 --> 01:02:15,760
start of the array in memory uh and then

1548
01:02:13,319 --> 01:02:16,880
we just dreference that number so it

1549
01:02:15,760 --> 01:02:19,400
goes back to

1550
01:02:16,880 --> 01:02:21,880
12 that's what this asteris is for and

1551
01:02:19,400 --> 01:02:24,440
we print as the the integer

1552
01:02:21,880 --> 01:02:26,119
type awesome now we have a for Loop

1553
01:02:24,440 --> 01:02:29,880
going on here so what the heck is this

1554
01:02:26,119 --> 01:02:33,000
doing we have we have I starts at zero

1555
01:02:29,880 --> 01:02:34,920
we want to stop it uh uh when whenever

1556
01:02:33,000 --> 01:02:36,960
it go whenever it equals 5 we stop it

1557
01:02:34,920 --> 01:02:39,319
and then we increment by one each time

1558
01:02:36,960 --> 01:02:41,720
so inside of here we have an integer

1559
01:02:39,319 --> 01:02:44,599
type and that's going to be the D

1560
01:02:41,720 --> 01:02:46,200
referenced pointer so whatever pointer

1561
01:02:44,599 --> 01:02:49,160
is we're going to dreference that back

1562
01:02:46,200 --> 01:02:50,839
to the original value that it was um and

1563
01:02:49,160 --> 01:02:52,440
then in here we're going to do the

1564
01:02:50,839 --> 01:02:53,960
pointer type of the actual pointer

1565
01:02:52,440 --> 01:02:56,440
itself so this is the memory address

1566
01:02:53,960 --> 01:02:58,400
that we're seeing um and then we're just

1567
01:02:56,440 --> 01:02:59,760
going to increment this each time each

1568
01:02:58,400 --> 01:03:01,039
time this for Loop goes we're going to

1569
01:02:59,760 --> 01:03:04,440
iterate it once and we're going to

1570
01:03:01,039 --> 01:03:06,559
increase we're going to increase uh

1571
01:03:04,440 --> 01:03:09,440
pointer which is the memory address

1572
01:03:06,559 --> 01:03:12,160
we're going to increment that

1573
01:03:09,440 --> 01:03:14,359
so uh I had a little example here that I

1574
01:03:12,160 --> 01:03:16,480
wrote out um these obviously won't be

1575
01:03:14,359 --> 01:03:19,200
like the same examples every time uh

1576
01:03:16,480 --> 01:03:23,039
they're going to be different but uh

1577
01:03:19,200 --> 01:03:27,160
notice how this pointer is incremented

1578
01:03:23,039 --> 01:03:31,520
by four bytes right so this is is not in

1579
01:03:27,160 --> 01:03:35,079
uh bits this is in bytes so if we do

1580
01:03:31,520 --> 01:03:38,200
eight bits in a bite times 4 bytes we

1581
01:03:35,079 --> 01:03:39,799
get 32 bits which is the integer 32 type

1582
01:03:38,200 --> 01:03:41,799
the classical integer

1583
01:03:39,799 --> 01:03:43,839
32 so hopefully it makes a little bit

1584
01:03:41,799 --> 01:03:47,839
more sense now about how memory is laid

1585
01:03:43,839 --> 01:03:49,400
out uh at least on the CPU so yeah we

1586
01:03:47,839 --> 01:03:53,200
just have these essentially skipped by

1587
01:03:49,400 --> 01:03:56,240
four bytes every time uh 4 bytes time 8

1588
01:03:53,200 --> 01:04:00,480
bits per BTE is 32 bits and we get our

1589
01:03:56,240 --> 01:04:03,440
n32 from that I also make a point that

1590
01:04:00,480 --> 01:04:06,039
uh well right now pointers are not in 32

1591
01:04:03,440 --> 01:04:08,760
bits in size but we'll see why having

1592
01:04:06,039 --> 01:04:13,240
them as in32 right now can be a major

1593
01:04:08,760 --> 01:04:17,359
issue so if you actually do um if we go

1594
01:04:13,240 --> 01:04:19,920
Python and we do 2 to the 32 we'll get

1595
01:04:17,359 --> 01:04:26,640
this number so look how big this is this

1596
01:04:19,920 --> 01:04:27,880
is uh 1 2 3 4 5 6 7 8 uh 9 so that's

1597
01:04:26,640 --> 01:04:32,240
that's about

1598
01:04:27,880 --> 01:04:36,359
4.2 that's about 4 GB um in po of 2 of

1599
01:04:32,240 --> 01:04:38,680
course that's 4 GB so if you have say 8

1600
01:04:36,359 --> 01:04:41,359
gab of memory which isn't actually that

1601
01:04:38,680 --> 01:04:45,400
much on this machine I actually have 64

1602
01:04:41,359 --> 01:04:47,079
so I have 64 GB of memory taken up by

1603
01:04:45,400 --> 01:04:48,119
like a single array I mean that

1604
01:04:47,079 --> 01:04:49,960
obviously wouldn't happen that's a

1605
01:04:48,119 --> 01:04:52,319
really large array but let's just say I

1606
01:04:49,960 --> 01:04:54,000
have like one that's like you know 64 GB

1607
01:04:52,319 --> 01:04:56,680
long well we're actually going to get

1608
01:04:54,000 --> 01:04:58,880
overflow when we try to index that way

1609
01:04:56,680 --> 01:05:01,359
so you're going to see later on that

1610
01:04:58,880 --> 01:05:04,440
it's more useful to use uh a certain

1611
01:05:01,359 --> 01:05:08,000
type for these pointers so if we do 2 to

1612
01:05:04,440 --> 01:05:10,839
the 64 so u a double Precision integer

1613
01:05:08,000 --> 01:05:13,559
so N64 you'll see that this is like

1614
01:05:10,839 --> 01:05:15,920
extremely massive um this is

1615
01:05:13,559 --> 01:05:19,039
like I don't know somewhere in the

1616
01:05:15,920 --> 01:05:21,520
exabytes it's like ridiculously high

1617
01:05:19,039 --> 01:05:24,200
um yeah just we we're we're going to

1618
01:05:21,520 --> 01:05:27,200
deal with this later but right now this

1619
01:05:24,200 --> 01:05:29,160
is the general intuition for how uh how

1620
01:05:27,200 --> 01:05:31,000
these pointers are printed out so if I

1621
01:05:29,160 --> 01:05:33,440
scroll up a little bit you'll just see

1622
01:05:31,000 --> 01:05:36,119
uh we print out the dereferenced pointer

1623
01:05:33,440 --> 01:05:38,279
um at that uh essentially take that

1624
01:05:36,119 --> 01:05:39,839
memory address and then we dreference it

1625
01:05:38,279 --> 01:05:41,680
based on the index that it currently is

1626
01:05:39,839 --> 01:05:44,119
so we just bump up the index one we we

1627
01:05:41,680 --> 01:05:46,599
jump ahead 32 bits or four bytes and

1628
01:05:44,119 --> 01:05:48,359
then we print that that value out uh

1629
01:05:46,599 --> 01:05:52,480
each time and you see that vertically

1630
01:05:48,359 --> 01:05:57,480
here um I just

1631
01:05:52,480 --> 01:05:58,680
maybe run that again after it's cleared

1632
01:05:57,480 --> 01:06:00,520
you'll see that it's it's just like

1633
01:05:58,680 --> 01:06:03,200
vertically just skips ahead as we'd

1634
01:06:00,520 --> 01:06:05,760
expect and then this pointer uh which is

1635
01:06:03,200 --> 01:06:07,640
just the that memory address and then I

1636
01:06:05,760 --> 01:06:09,440
just put this out for testing sake you

1637
01:06:07,640 --> 01:06:11,880
don't need to worry about that um but

1638
01:06:09,440 --> 01:06:13,240
yeah this is the general intuition on uh

1639
01:06:11,880 --> 01:06:15,599
how how these things are laid out in

1640
01:06:13,240 --> 01:06:18,440
memory so number six is an interesting

1641
01:06:15,599 --> 01:06:21,119
one it kind of goes back to example

1642
01:06:18,440 --> 01:06:23,119
number two where we have this uh we have

1643
01:06:21,119 --> 01:06:25,039
this value and then pointer pointer

1644
01:06:23,119 --> 01:06:27,920
pointer just Stacks up uh that that's

1645
01:06:25,039 --> 01:06:30,880
essentially what sixes so we have these

1646
01:06:27,920 --> 01:06:32,640
two arrays array 1 and two and they're

1647
01:06:30,880 --> 01:06:35,319
essentially just these vectors or these

1648
01:06:32,640 --> 01:06:39,079
these these arrays 1 2 3 4 and then we

1649
01:06:35,319 --> 01:06:41,559
have an array to uh 5 6 7 8 uh and then

1650
01:06:39,079 --> 01:06:44,480
we just have an integer pointer uh to

1651
01:06:41,559 --> 01:06:46,359
those arrays um and then we store

1652
01:06:44,480 --> 01:06:49,240
another you could say array I just name

1653
01:06:46,359 --> 01:06:51,359
it Matrix to differentiate and we store

1654
01:06:49,240 --> 01:06:53,480
those pointers uh essentially on top of

1655
01:06:51,359 --> 01:06:55,799
each other so what it looks like is we

1656
01:06:53,480 --> 01:06:59,039
have this we have this Matrix so it's

1657
01:06:55,799 --> 01:07:02,880
pointer one and pointer two pointer one

1658
01:06:59,039 --> 01:07:04,760
is the uh it's the it's the pointer to

1659
01:07:02,880 --> 01:07:06,760
that array the array one and then

1660
01:07:04,760 --> 01:07:09,079
pointer two is the is the memory address

1661
01:07:06,760 --> 01:07:11,640
for for array two so essentially what we

1662
01:07:09,079 --> 01:07:14,920
have it is if we if we look at this uh

1663
01:07:11,640 --> 01:07:17,279
this array of of like this Matrix it has

1664
01:07:14,920 --> 01:07:19,799
pointer one pointer two if we flip it so

1665
01:07:17,279 --> 01:07:21,279
instead of like this and this we we

1666
01:07:19,799 --> 01:07:23,319
stack them on top of each other so it's

1667
01:07:21,279 --> 01:07:25,920
pointer one then pointer two it actually

1668
01:07:23,319 --> 01:07:29,079
looks like a matrix so you'll have your

1669
01:07:25,920 --> 01:07:31,520
array one which is 1 two 3 4 and then

1670
01:07:29,079 --> 01:07:32,920
one underneath which is five six seven8

1671
01:07:31,520 --> 01:07:35,799
and so it actually is like a it's it's

1672
01:07:32,920 --> 01:07:38,799
like a grid right uh and if we iterate

1673
01:07:35,799 --> 01:07:38,799
through this

1674
01:07:41,960 --> 01:07:44,960
um

1675
01:07:45,000 --> 01:07:52,079
oh you'll actually see um we just

1676
01:07:48,400 --> 01:07:55,720
essentially iterate through these so uh

1677
01:07:52,079 --> 01:07:58,079
we we we do J and four uh and we we

1678
01:07:55,720 --> 01:08:00,720
print out the D reference Matrix at

1679
01:07:58,079 --> 01:08:04,799
position I so that position I is going

1680
01:08:00,720 --> 01:08:08,920
to give a memory address for

1681
01:08:04,799 --> 01:08:14,400
uh it's going to be you know number two

1682
01:08:08,920 --> 01:08:16,480
uh which is going to be these two arrays

1683
01:08:14,400 --> 01:08:17,960
um and then we're going to dreference

1684
01:08:16,480 --> 01:08:20,000
that which is going to give us our

1685
01:08:17,960 --> 01:08:21,520
actual values and then we're going to

1686
01:08:20,000 --> 01:08:24,159
iterate this each time of course as the

1687
01:08:21,520 --> 01:08:26,239
for Loop goes on um and then we just do

1688
01:08:24,159 --> 01:08:29,159
a next line so that it looks nice nice

1689
01:08:26,239 --> 01:08:30,799
so that's uh that's pretty much all I

1690
01:08:29,159 --> 01:08:32,920
have for pointers we're going to jump

1691
01:08:30,799 --> 01:08:35,000
into custom types now which is more what

1692
01:08:32,920 --> 01:08:37,400
I was talking about for this for these

1693
01:08:35,000 --> 01:08:39,880
weird pointer sizes um we're going to

1694
01:08:37,400 --> 01:08:39,880
dive into

1695
01:08:40,560 --> 01:08:45,759
this so now I'm going to show you pretty

1696
01:08:42,960 --> 01:08:47,279
much the equivalent of the torch dot

1697
01:08:45,759 --> 01:08:49,040
long type so you might have seen this

1698
01:08:47,279 --> 01:08:50,400
before if you're you know I assume you

1699
01:08:49,040 --> 01:08:52,359
have some knowledge of python and

1700
01:08:50,400 --> 01:08:54,799
pytorch so I figured that's the best way

1701
01:08:52,359 --> 01:08:57,080
to illustrate this we have this size T

1702
01:08:54,799 --> 01:08:59,719
this size _ T this is typically how you

1703
01:08:57,080 --> 01:09:01,759
you write this out in C is you'll have

1704
01:08:59,719 --> 01:09:03,679
uh whatever the type is and then you'll

1705
01:09:01,759 --> 01:09:05,759
have underscore T to say that this is

1706
01:09:03,679 --> 01:09:10,359
like a custom type that you made T is

1707
01:09:05,759 --> 01:09:13,480
for type um and this is specifically for

1708
01:09:10,359 --> 01:09:16,000
U like big big numbers right so the idea

1709
01:09:13,480 --> 01:09:17,719
here is this is going to be an unsigned

1710
01:09:16,000 --> 01:09:20,839
long so

1711
01:09:17,719 --> 01:09:25,839
uint uh long

1712
01:09:20,839 --> 01:09:29,199
so int it's going to be a uint 64 that's

1713
01:09:25,839 --> 01:09:31,679
what it's going to be um if we do this

1714
01:09:29,199 --> 01:09:36,040
in in torch you'll

1715
01:09:31,679 --> 01:09:37,239
see I python import torch and then we

1716
01:09:36,040 --> 01:09:42,080
can make

1717
01:09:37,239 --> 01:09:45,400
X this just an array of integers uh and

1718
01:09:42,080 --> 01:09:49,040
then we go x. dtype we'll see a torch.

1719
01:09:45,400 --> 01:09:51,600
N64 so it's just like that 64 uh bit

1720
01:09:49,040 --> 01:09:54,199
Precision that we want to store really

1721
01:09:51,600 --> 01:09:57,199
big really big matrices right so

1722
01:09:54,199 --> 01:09:59,840
especially in Cuda when you have uh like

1723
01:09:57,199 --> 01:10:02,440
really really big tensors that are

1724
01:09:59,840 --> 01:10:04,480
occupying like multiple gigabytes um

1725
01:10:02,440 --> 01:10:06,280
like on my GPU I have 8 gigabyt of

1726
01:10:04,480 --> 01:10:09,360
storage so if we're using

1727
01:10:06,280 --> 01:10:11,719
in32 uh that you might get some overflow

1728
01:10:09,360 --> 01:10:14,239
errors or you might get some just some

1729
01:10:11,719 --> 01:10:15,880
some unexpected bad behavior that is

1730
01:10:14,239 --> 01:10:17,520
going to like mess with things so you

1731
01:10:15,880 --> 01:10:20,400
don't necessarily want that and that's

1732
01:10:17,520 --> 01:10:21,719
the whole point of the size type um I

1733
01:10:20,400 --> 01:10:25,679
kind of wanted to show you that this

1734
01:10:21,719 --> 01:10:27,719
isn't so bad after all so you know just

1735
01:10:25,679 --> 01:10:29,640
to step through this we have we have the

1736
01:10:27,719 --> 01:10:33,199
same array that we went over last time

1737
01:10:29,640 --> 01:10:37,000
we do this size T type um we use size

1738
01:10:33,199 --> 01:10:39,880
and then size of array divided by uh

1739
01:10:37,000 --> 01:10:42,840
divided by the size of an individual

1740
01:10:39,880 --> 01:10:47,360
integer so it's like the total size of

1741
01:10:42,840 --> 01:10:49,199
this uh entire thing divided by the uh

1742
01:10:47,360 --> 01:10:53,480
the size of each individual thing in it

1743
01:10:49,199 --> 01:10:57,040
so you get the total length of the array

1744
01:10:53,480 --> 01:10:59,320
um uh and so if we go 01 I already

1745
01:10:57,040 --> 01:11:01,199
compiled this uh you'll see we get that

1746
01:10:59,320 --> 01:11:05,480
five right so there's five elements in

1747
01:11:01,199 --> 01:11:07,760
here uh we print this size out um I'll

1748
01:11:05,480 --> 01:11:11,560
go over this this Zu part in a second

1749
01:11:07,760 --> 01:11:15,800
here um but we get that output five and

1750
01:11:11,560 --> 01:11:19,520
then this eight um we print out the size

1751
01:11:15,800 --> 01:11:24,120
of this so that's in uh that's in bytes

1752
01:11:19,520 --> 01:11:28,280
by the way we we when we do a like size

1753
01:11:24,120 --> 01:11:33,040
of int we if we do like let's see print

1754
01:11:28,280 --> 01:11:35,880
F uh we'll just say int

1755
01:11:33,040 --> 01:11:40,480
size uh and then we'll

1756
01:11:35,880 --> 01:11:43,040
go uh sure in in bites we'll do that uh

1757
01:11:40,480 --> 01:11:45,719
if I just compile this

1758
01:11:43,040 --> 01:11:47,560
again we'll see int size and bytes so

1759
01:11:45,719 --> 01:11:50,920
this is an

1760
01:11:47,560 --> 01:11:54,280
int32 this just just an in32 and it's

1761
01:11:50,920 --> 01:11:56,719
four bytes or 32 bits so when we have

1762
01:11:54,280 --> 01:11:59,800
this that means it's 64 bits just to put

1763
01:11:56,719 --> 01:12:02,400
that in perspective there so uh when we

1764
01:11:59,800 --> 01:12:04,960
go to this size T we'll see it's an

1765
01:12:02,400 --> 01:12:08,000
unsigned type def so we do a type

1766
01:12:04,960 --> 01:12:09,080
definition unsign so it's a it's only

1767
01:12:08,000 --> 01:12:10,639
going to be positive because we're

1768
01:12:09,080 --> 01:12:12,840
storing like a you know you can't have

1769
01:12:10,639 --> 01:12:14,600
like negative size that's the logic and

1770
01:12:12,840 --> 01:12:17,000
then we have this long which means it's

1771
01:12:14,600 --> 01:12:19,000
going to be uh it's going to Lally tell

1772
01:12:17,000 --> 01:12:22,199
the operating system where we we want 64

1773
01:12:19,000 --> 01:12:24,719
bits not 32 we want it to be long um and

1774
01:12:22,199 --> 01:12:27,360
then just the the size T type so we can

1775
01:12:24,719 --> 01:12:29,760
actually go into this and we can see oh

1776
01:12:27,360 --> 01:12:31,520
type def size type and then we make this

1777
01:12:29,760 --> 01:12:34,199
we declare this thing where does this

1778
01:12:31,520 --> 01:12:37,040
come from oh right here long unsigned

1779
01:12:34,199 --> 01:12:38,880
integer boom just like that super easy

1780
01:12:37,040 --> 01:12:42,719
right

1781
01:12:38,880 --> 01:12:44,679
um and uh I guess to sort of clarify

1782
01:12:42,719 --> 01:12:47,040
like what the whole deal is here we

1783
01:12:44,679 --> 01:12:50,000
could just pop back to this link if I

1784
01:12:47,040 --> 01:12:53,920
just open it on my second screen here

1785
01:12:50,000 --> 01:12:53,920
and pop it over

1786
01:12:54,000 --> 01:13:00,560
oh uh we can search for

1787
01:12:57,159 --> 01:13:03,520
the I'll make this a bit bigger I don't

1788
01:13:00,560 --> 01:13:08,320
know can I make it bigger there we go so

1789
01:13:03,520 --> 01:13:11,440
if I go up uh we'll see that we have

1790
01:13:08,320 --> 01:13:15,120
this uh we have this we have this Z and

1791
01:13:11,440 --> 01:13:19,440
then we have the U so Z is

1792
01:13:15,120 --> 01:13:22,560
uh Z is just going to be uh I can't

1793
01:13:19,440 --> 01:13:24,120
remember exactly what this was for but

1794
01:13:22,560 --> 01:13:26,719
uh then we have this U which is

1795
01:13:24,120 --> 01:13:29,320
essentially just the unsigned in right

1796
01:13:26,719 --> 01:13:31,520
so it's U is unsigned um and then we

1797
01:13:29,320 --> 01:13:34,440
have this size T type which is what the

1798
01:13:31,520 --> 01:13:37,199
which is what the Z is for um and then

1799
01:13:34,440 --> 01:13:39,920
we just we just map that out so uh you

1800
01:13:37,199 --> 01:13:42,280
know if we had a uh if we had just a

1801
01:13:39,920 --> 01:13:44,800
regular integer it would still be size T

1802
01:13:42,280 --> 01:13:46,960
because you can have you know it's still

1803
01:13:44,800 --> 01:13:50,920
an integer you can do stuff with it but

1804
01:13:46,960 --> 01:13:52,520
when we have a u uh then you know it's

1805
01:13:50,920 --> 01:13:54,480
just kind of explicitly you're going to

1806
01:13:52,520 --> 01:13:56,719
have this size T type so that that's

1807
01:13:54,480 --> 01:14:00,360
kind of how we M things there and we can

1808
01:13:56,719 --> 01:14:02,800
use the Z followed by the U uh to

1809
01:14:00,360 --> 01:14:05,080
properly print that out to print out the

1810
01:14:02,800 --> 01:14:07,080
type of that so you know you have all

1811
01:14:05,080 --> 01:14:09,080
these other ones too like uh pointer

1812
01:14:07,080 --> 01:14:11,440
diff

1813
01:14:09,080 --> 01:14:13,960
type uh but that's that's generally the

1814
01:14:11,440 --> 01:14:13,960
intuition

1815
01:14:14,000 --> 01:14:19,199
there and then next up I just wanted to

1816
01:14:16,639 --> 01:14:21,159
cover uh declaring your own custom types

1817
01:14:19,199 --> 01:14:24,800
so you know we saw one in the in the

1818
01:14:21,159 --> 01:14:26,960
standard uh C library the standard iio

1819
01:14:24,800 --> 01:14:28,360
uh uh C library the standard definition

1820
01:14:26,960 --> 01:14:32,199
Library whatever you want to call it the

1821
01:14:28,360 --> 01:14:34,159
headers um and it's also important that

1822
01:14:32,199 --> 01:14:35,800
we can declare our own because we might

1823
01:14:34,159 --> 01:14:37,360
even we might need to use these and we

1824
01:14:35,800 --> 01:14:39,960
actually will use these later on in the

1825
01:14:37,360 --> 01:14:41,880
course so typically how it goes is you

1826
01:14:39,960 --> 01:14:44,440
do this thing called a type def uh which

1827
01:14:41,880 --> 01:14:45,719
is a type definition and for let's just

1828
01:14:44,440 --> 01:14:47,320
say we're going to make a point for

1829
01:14:45,719 --> 01:14:49,480
example it has an X and A Y and their

1830
01:14:47,320 --> 01:14:51,120
floating Point values so we do this

1831
01:14:49,480 --> 01:14:52,679
struct which is going to have some

1832
01:14:51,120 --> 01:14:54,600
elements inside of it it's going to have

1833
01:14:52,679 --> 01:14:57,199
some it's going to have some of these I

1834
01:14:54,600 --> 01:14:58,600
guess objects you could say these items

1835
01:14:57,199 --> 01:15:01,480
uh and it's going to have a float X and

1836
01:14:58,600 --> 01:15:03,520
a float Y and then we declare we we say

1837
01:15:01,480 --> 01:15:05,719
this is going to be a type point we do

1838
01:15:03,520 --> 01:15:08,600
type definition uh it's going to be a

1839
01:15:05,719 --> 01:15:10,760
struct and we're going to make that a

1840
01:15:08,600 --> 01:15:12,480
point type essentially and then inside

1841
01:15:10,760 --> 01:15:15,400
of our int main here since this is

1842
01:15:12,480 --> 01:15:17,000
already declared we can say a point type

1843
01:15:15,400 --> 01:15:19,360
like you know you could like you would

1844
01:15:17,000 --> 01:15:21,480
do an INT P or something we're just

1845
01:15:19,360 --> 01:15:23,840
replacing replacing int or whatever that

1846
01:15:21,480 --> 01:15:25,760
type is with point and we're making that

1847
01:15:23,840 --> 01:15:28,560
variable named p and then we're

1848
01:15:25,760 --> 01:15:30,560
populating it with some values here so

1849
01:15:28,560 --> 01:15:35,199
this is going to be our float X and our

1850
01:15:30,560 --> 01:15:38,080
float y um and then if we go uh you know

1851
01:15:35,199 --> 01:15:41,400
size of Point here if I just compile

1852
01:15:38,080 --> 01:15:42,360
this and then run uh you'll see size of

1853
01:15:41,400 --> 01:15:46,840
point is

1854
01:15:42,360 --> 01:15:48,679
eight so 8 bytes is four bytes plus four

1855
01:15:46,840 --> 01:15:50,920
more bytes so each of these is a float

1856
01:15:48,679 --> 01:15:53,320
32 number that occupies four bytes in

1857
01:15:50,920 --> 01:15:55,360
memory so when you add these together it

1858
01:15:53,320 --> 01:15:57,639
occupies a total of eight bytes so this

1859
01:15:55,360 --> 01:16:01,920
point this point uh type is going to

1860
01:15:57,639 --> 01:16:03,960
cover uh 8 bytes or 64 bits in memory um

1861
01:16:01,920 --> 01:16:06,639
and then just this other C++ script is

1862
01:16:03,960 --> 01:16:08,800
literally the identical to this so you

1863
01:16:06,639 --> 01:16:12,639
can you can declare things the exact

1864
01:16:08,800 --> 01:16:14,400
same way in um in C++ except you might

1865
01:16:12,639 --> 01:16:17,719
just want to use like the io stream

1866
01:16:14,400 --> 01:16:20,080
instead of uh like you maybe in C++ you

1867
01:16:17,719 --> 01:16:23,679
would comment this out and you would go

1868
01:16:20,080 --> 01:16:26,360
uh include uh IO

1869
01:16:23,679 --> 01:16:28,639
stream and and then you would use uh

1870
01:16:26,360 --> 01:16:32,040
using namespace

1871
01:16:28,639 --> 01:16:34,159
STD and then you would uh then you would

1872
01:16:32,040 --> 01:16:36,639
see out

1873
01:16:34,159 --> 01:16:39,360
that

1874
01:16:36,639 --> 01:16:41,560
oh identifier SE is

1875
01:16:39,360 --> 01:16:44,159
undefined okay I guess not I don't know

1876
01:16:41,560 --> 01:16:46,840
why that's not working um but but you

1877
01:16:44,159 --> 01:16:50,480
get the point so very minimal changes so

1878
01:16:46,840 --> 01:16:52,639
now if we pop over to type casting uh I

1879
01:16:50,480 --> 01:16:54,520
have two files in here so just a just a

1880
01:16:52,639 --> 01:16:57,800
single C file and then a read me so

1881
01:16:54,520 --> 01:17:01,120
inside here I have uh static cast

1882
01:16:57,800 --> 01:17:03,679
Dynamic cast uh constant casts and

1883
01:17:01,120 --> 01:17:05,400
reinterpret cast so we're only going to

1884
01:17:03,679 --> 01:17:07,040
be covering these static cast because

1885
01:17:05,400 --> 01:17:09,679
these are the safe ones these are mainly

1886
01:17:07,040 --> 01:17:13,239
what you're going to end up using if any

1887
01:17:09,679 --> 01:17:15,440
um so in here it it's it's very simple

1888
01:17:13,239 --> 01:17:16,920
you have just a like say a floating

1889
01:17:15,440 --> 01:17:20,040
Point number

1890
01:17:16,920 --> 01:17:23,000
6969 and then we have we just declare an

1891
01:17:20,040 --> 01:17:25,800
INT and all we do is just have this F

1892
01:17:23,000 --> 01:17:28,239
and then we do brackets int

1893
01:17:25,800 --> 01:17:30,120
we we put these we just literally put

1894
01:17:28,239 --> 01:17:34,199
this right in front of it and that'll

1895
01:17:30,120 --> 01:17:36,480
statically typ cast uh the float 69.6 n

1896
01:17:34,199 --> 01:17:38,560
to an INT and what this will do is it'll

1897
01:17:36,480 --> 01:17:42,199
just truncate that last part so in

1898
01:17:38,560 --> 01:17:45,679
memory in essentially in binary and bits

1899
01:17:42,199 --> 01:17:47,719
it'll be laid out as um it'll have that

1900
01:17:45,679 --> 01:17:49,280
first part so the first uh the first

1901
01:17:47,719 --> 01:17:51,760
integer piece and then it'll have the

1902
01:17:49,280 --> 01:17:53,840
decimal and then it'll have the uh it'll

1903
01:17:51,760 --> 01:17:56,520
have the it'll have the decimal bits

1904
01:17:53,840 --> 01:17:58,800
afterwards so it'll it'll be like that

1905
01:17:56,520 --> 01:18:00,760
decimal and then after so what it'll do

1906
01:17:58,800 --> 01:18:04,000
is when it's when it's typ casting it'll

1907
01:18:00,760 --> 01:18:06,679
just uh truncate this part and then give

1908
01:18:04,000 --> 01:18:11,560
give us more Precision for these uh for

1909
01:18:06,679 --> 01:18:12,480
these essentially int 32 bits so uh it

1910
01:18:11,560 --> 01:18:14,600
that that's essentially all it's going

1911
01:18:12,480 --> 01:18:17,239
to look like it's it's just going to be

1912
01:18:14,600 --> 01:18:19,360
69 uh it's not going to have any decimal

1913
01:18:17,239 --> 01:18:23,440
places it's going to effectively round

1914
01:18:19,360 --> 01:18:26,320
down you could say um so if I just pop

1915
01:18:23,440 --> 01:18:28,840
into

1916
01:18:26,320 --> 01:18:30,400
uh type

1917
01:18:28,840 --> 01:18:33,560
casting

1918
01:18:30,400 --> 01:18:37,120
and run

1919
01:18:33,560 --> 01:18:40,320
this you'll see uh we get this integer

1920
01:18:37,120 --> 01:18:43,440
format so just truncated and then when

1921
01:18:40,320 --> 01:18:47,760
we do a character um this is actually

1922
01:18:43,440 --> 01:18:49,960
going to convert to uh asky so if you

1923
01:18:47,760 --> 01:18:52,400
remember your asky tables I'll go ahead

1924
01:18:49,960 --> 01:18:56,719
and bring this up on the side

1925
01:18:52,400 --> 01:18:56,719
here asky table

1926
01:18:56,880 --> 01:19:01,080
uh oh it's right literally right there

1927
01:18:58,520 --> 01:19:02,960
okay ask.com

1928
01:19:01,080 --> 01:19:06,199
um we can

1929
01:19:02,960 --> 01:19:08,560
see that this one right here is an

1930
01:19:06,199 --> 01:19:11,320
uppercase E

1931
01:19:08,560 --> 01:19:14,120
right

1932
01:19:11,320 --> 01:19:17,639
so uppercase

1933
01:19:14,120 --> 01:19:19,600
E uppercase E look at that how how easy

1934
01:19:17,639 --> 01:19:22,480
was that right it's not not crazy doing

1935
01:19:19,600 --> 01:19:24,159
an integer to a character conversion so

1936
01:19:22,480 --> 01:19:25,159
that's that that's all type casting is

1937
01:19:24,159 --> 01:19:26,920
I'm not going to go over this

1938
01:19:25,159 --> 01:19:28,800
extensively cuz it isn't like a crazy

1939
01:19:26,920 --> 01:19:30,159
piece we end up using in Cuda but just

1940
01:19:28,800 --> 01:19:31,560
to throw it out there and remind you

1941
01:19:30,159 --> 01:19:35,080
guys of how how simple this type of

1942
01:19:31,560 --> 01:19:38,000
thing is another topic I thought was uh

1943
01:19:35,080 --> 01:19:41,080
briefly worth touching on was macros and

1944
01:19:38,000 --> 01:19:43,280
Global variables so we can we

1945
01:19:41,080 --> 01:19:46,520
essentially have these these basic ones

1946
01:19:43,280 --> 01:19:50,719
like uh you know if if defined if not

1947
01:19:46,520 --> 01:19:53,040
defined L if else and then end if uh so

1948
01:19:50,719 --> 01:19:54,880
it it's we're essentially going to use

1949
01:19:53,040 --> 01:19:57,880
these later on to declare

1950
01:19:54,880 --> 01:19:59,800
hyperparameters and uh just different

1951
01:19:57,880 --> 01:20:01,400
Global things that we'll need access to

1952
01:19:59,800 --> 01:20:03,679
that we don't want to just pass in as an

1953
01:20:01,400 --> 01:20:05,199
extra you know bloated function argument

1954
01:20:03,679 --> 01:20:06,960
when you have like 20 arguments in a

1955
01:20:05,199 --> 01:20:08,679
function you might want to reduce that

1956
01:20:06,960 --> 01:20:10,560
and just declare some of those locally

1957
01:20:08,679 --> 01:20:11,920
so you can use them wherever you want as

1958
01:20:10,560 --> 01:20:13,520
long as they're not changing or you're

1959
01:20:11,920 --> 01:20:16,320
not doing anything weird with them

1960
01:20:13,520 --> 01:20:20,159
you're you're okay right so like in this

1961
01:20:16,320 --> 01:20:23,480
example I do uh Pi like uppercase pi and

1962
01:20:20,159 --> 01:20:25,960
then we set that equal to uh a double

1963
01:20:23,480 --> 01:20:29,280
right so

1964
01:20:25,960 --> 01:20:31,040
uh we can do these functions too so it's

1965
01:20:29,280 --> 01:20:33,360
kind of like a a Lambda function if you

1966
01:20:31,040 --> 01:20:35,320
will a Lambda function in Python we do

1967
01:20:33,360 --> 01:20:38,000
this area and then we we pass whatever

1968
01:20:35,320 --> 01:20:41,320
we want in so R is our radius and then

1969
01:20:38,000 --> 01:20:44,440
we do essentially the the radius is piun

1970
01:20:41,320 --> 01:20:46,040
r^ 2 so we just do PI * R * R and we get

1971
01:20:44,440 --> 01:20:49,800
that so it's just a little Lambda

1972
01:20:46,040 --> 01:20:52,159
function you can do as a macro um and

1973
01:20:49,800 --> 01:20:55,159
then we have if not defined radius so

1974
01:20:52,159 --> 01:20:58,320
radius isn't defined here if and DEP if

1975
01:20:55,159 --> 01:21:00,400
not defin um theine radius and we set

1976
01:20:58,320 --> 01:21:04,040
that value to seven so it's going to be

1977
01:21:00,400 --> 01:21:06,960
an integer seven we end the if we end

1978
01:21:04,040 --> 01:21:09,440
this this whole block and radius is a

1979
01:21:06,960 --> 01:21:12,480
now a declared maer equal to

1980
01:21:09,440 --> 01:21:15,520
seven uh and then we have some if logic

1981
01:21:12,480 --> 01:21:17,560
down here so if radius uh you know it's

1982
01:21:15,520 --> 01:21:20,159
bigger than 10 which is not we Define

1983
01:21:17,560 --> 01:21:21,920
this so this this is like grade out um

1984
01:21:20,159 --> 01:21:24,320
it's not smaller than five also grade

1985
01:21:21,920 --> 01:21:25,960
out and then else U so it's just going

1986
01:21:24,320 --> 01:21:28,080
to stay at seven and then it's going to

1987
01:21:25,960 --> 01:21:29,600
end if so we can do if logic in here as

1988
01:21:28,080 --> 01:21:32,000
well

1989
01:21:29,600 --> 01:21:37,199
um and then if I just go ahead and pop

1990
01:21:32,000 --> 01:21:40,480
out of this to uh macros and we go

1991
01:21:37,199 --> 01:21:40,480
GCC like

1992
01:21:41,520 --> 01:21:47,120
this we're going to oh

1993
01:21:48,199 --> 01:21:53,159
double we'll do uh

1994
01:22:05,840 --> 01:22:13,600
perfect so area of circle with radius 7

1995
01:22:11,159 --> 01:22:14,880
um is going to be this much and that's

1996
01:22:13,600 --> 01:22:18,080
that's a floating Point number which we

1997
01:22:14,880 --> 01:22:19,600
have here so uh this radius I I was not

1998
01:22:18,080 --> 01:22:21,320
careful there this radius is actually an

1999
01:22:19,600 --> 01:22:26,000
integer type so we just set that back to

2000
01:22:21,320 --> 01:22:28,600
D um and then it then it works Bel ly so

2001
01:22:26,000 --> 01:22:31,560
uh that's that's how you do uh macros

2002
01:22:28,600 --> 01:22:33,080
pretty easy this part is where my

2003
01:22:31,560 --> 01:22:34,639
understanding gets a little bit fuzzy I

2004
01:22:33,080 --> 01:22:37,199
haven't worked with compilers

2005
01:22:34,639 --> 01:22:39,760
extensively but I have found some really

2006
01:22:37,199 --> 01:22:41,480
good resources on uh the C and C++

2007
01:22:39,760 --> 01:22:44,719
compilers so I've just provided some

2008
01:22:41,480 --> 01:22:47,239
links here for you to go learn um GCC is

2009
01:22:44,719 --> 01:22:52,320
the most popular C compiler so that's

2010
01:22:47,239 --> 01:22:54,920
gnu uh C compiler um and then g++ is the

2011
01:22:52,320 --> 01:22:57,239
same thing but for C++

2012
01:22:54,920 --> 01:22:59,280
so uh you know I have different articles

2013
01:22:57,239 --> 01:23:02,840
on here from free code camp that you can

2014
01:22:59,280 --> 01:23:05,159
go and look at so uh there's there's

2015
01:23:02,840 --> 01:23:06,400
this one and then we have the other one

2016
01:23:05,159 --> 01:23:09,400
as

2017
01:23:06,400 --> 01:23:12,840
well what does a compiler explain for

2018
01:23:09,400 --> 01:23:15,360
beginners um so like an analogy

2019
01:23:12,840 --> 01:23:18,600
essentially just converting uh

2020
01:23:15,360 --> 01:23:21,400
converting your machine code down to uh

2021
01:23:18,600 --> 01:23:23,560
down to assembly and and you have all

2022
01:23:21,400 --> 01:23:25,800
these representations in between that

2023
01:23:23,560 --> 01:23:27,360
that the computer work with to help

2024
01:23:25,800 --> 01:23:30,320
understand things better uh and then

2025
01:23:27,360 --> 01:23:33,440
it'll convert that assembly code down to

2026
01:23:30,320 --> 01:23:36,440
uh essentially the uh CPU instructions

2027
01:23:33,440 --> 01:23:38,520
in uh in bits and bytes so binary ones

2028
01:23:36,440 --> 01:23:40,719
and zeros uh and that'll get sent

2029
01:23:38,520 --> 01:23:42,199
through as as essentially electrons and

2030
01:23:40,719 --> 01:23:44,280
charges through your through your

2031
01:23:42,199 --> 01:23:47,679
circuitry in the computer and that is

2032
01:23:44,280 --> 01:23:50,199
what actually executes this stuff um and

2033
01:23:47,679 --> 01:23:51,840
then just the C C++ compiler is a little

2034
01:23:50,199 --> 01:23:54,159
bit

2035
01:23:51,840 --> 01:23:56,480
uh might be a little bit higher level I

2036
01:23:54,159 --> 01:23:59,320
know C++ is a higher level language than

2037
01:23:56,480 --> 01:24:01,320
c um but this these are just like really

2038
01:23:59,320 --> 01:24:04,000
good explanations that I can't really

2039
01:24:01,320 --> 01:24:06,679
top myself without messing up so I

2040
01:24:04,000 --> 01:24:09,760
provided these links here um hopefully

2041
01:24:06,679 --> 01:24:12,199
this all makes sense but uh we won't

2042
01:24:09,760 --> 01:24:14,960
really need to understand too much about

2043
01:24:12,199 --> 01:24:18,199
compilers Downstream it's good to know

2044
01:24:14,960 --> 01:24:20,760
what they're doing but in order to debug

2045
01:24:18,199 --> 01:24:22,760
code you just need to know kind of the

2046
01:24:20,760 --> 01:24:24,000
architecture of what the compiler is

2047
01:24:22,760 --> 01:24:26,400
what it's doing like where it's where

2048
01:24:24,000 --> 01:24:28,960
it's stuff not necessarily like all the

2049
01:24:26,400 --> 01:24:30,840
math and representations happening um

2050
01:24:28,960 --> 01:24:34,280
it's good to know of course but it's not

2051
01:24:30,840 --> 01:24:35,639
needed to write functioning code so uh

2052
01:24:34,280 --> 01:24:37,040
we'll see that later on like we're just

2053
01:24:35,639 --> 01:24:39,000
going to essentially type in these

2054
01:24:37,040 --> 01:24:40,440
compiler flags and and all this and that

2055
01:24:39,000 --> 01:24:43,719
that's what the next section on actually

2056
01:24:40,440 --> 01:24:45,639
is uh it's on make files so make files

2057
01:24:43,719 --> 01:24:47,840
are really useful they're going to help

2058
01:24:45,639 --> 01:24:51,320
you uh be more efficient about

2059
01:24:47,840 --> 01:24:53,440
developing C C++ and Cuda code so

2060
01:24:51,320 --> 01:24:56,639
instead of going into here and just

2061
01:24:53,440 --> 01:24:58,280
typing uh you know GCC every single time

2062
01:24:56,639 --> 01:25:00,199
and maybe maybe you have autocomplete

2063
01:24:58,280 --> 01:25:01,800
like me but either way you just want

2064
01:25:00,199 --> 01:25:03,679
this to be a faster process and you want

2065
01:25:01,800 --> 01:25:06,040
to be able to automate and and have more

2066
01:25:03,679 --> 01:25:07,960
control over what happens uh and just

2067
01:25:06,040 --> 01:25:11,080
manage things better make files is what

2068
01:25:07,960 --> 01:25:12,719
you want so inside of make files it it

2069
01:25:11,080 --> 01:25:14,040
looks really complicated and it's like

2070
01:25:12,719 --> 01:25:17,280
learning this new language but it's

2071
01:25:14,040 --> 01:25:21,280
really not it's not that bad um you can

2072
01:25:17,280 --> 01:25:24,360
Define variables so like GCC equals GCC

2073
01:25:21,280 --> 01:25:26,360
and in order to use these uh I can just

2074
01:25:24,360 --> 01:25:28,400
do dollar sign and then brackets and

2075
01:25:26,360 --> 01:25:30,320
then put that variable inside the

2076
01:25:28,400 --> 01:25:31,719
variable name inside and it'll just

2077
01:25:30,320 --> 01:25:34,520
pretty much reference this when it's

2078
01:25:31,719 --> 01:25:37,719
called um this is a command by the way

2079
01:25:34,520 --> 01:25:39,320
so you'll we'll see this in a second um

2080
01:25:37,719 --> 01:25:41,600
I'm going to do a little experiment with

2081
01:25:39,320 --> 01:25:44,040
the Cuda script here but we just have

2082
01:25:41,600 --> 01:25:46,719
this nvcc maaps to the Nvidia Cuda

2083
01:25:44,040 --> 01:25:48,360
compiler and then we have Cuda Flags so

2084
01:25:46,719 --> 01:25:50,199
this is just a little thing that's like

2085
01:25:48,360 --> 01:25:51,639
my GPU architecture we're going to see

2086
01:25:50,199 --> 01:25:54,320
this later you don't have to worry about

2087
01:25:51,639 --> 01:25:59,199
this now but this is just like U my GPU

2088
01:25:54,320 --> 01:26:02,520
architecture is a uh 8.6 compute

2089
01:25:59,199 --> 01:26:04,600
capability uh or

2090
01:26:02,520 --> 01:26:07,600
compute compatibility I think it's

2091
01:26:04,600 --> 01:26:09,480
capability um either way this is just we

2092
01:26:07,600 --> 01:26:11,040
could just have flags and we can just

2093
01:26:09,480 --> 01:26:13,480
have more variables that we plug into

2094
01:26:11,040 --> 01:26:14,600
this stuff but if I go back to the read

2095
01:26:13,480 --> 01:26:18,760
me for

2096
01:26:14,600 --> 01:26:20,600
this we have these targets prerequisites

2097
01:26:18,760 --> 01:26:23,080
and then our commands nested inside of

2098
01:26:20,600 --> 01:26:25,360
that so how does this work exactly well

2099
01:26:23,080 --> 01:26:29,520
I'm going to show you this um just by

2100
01:26:25,360 --> 01:26:33,960
pretty much example so um if I

2101
01:26:29,520 --> 01:26:36,920
go if I go I can actually delete this

2102
01:26:33,960 --> 01:26:40,320
line if I just go make

2103
01:26:36,920 --> 01:26:42,719
01 see how it makes a binary here and

2104
01:26:40,320 --> 01:26:45,880
then I can run this binary and it'll say

2105
01:26:42,719 --> 01:26:48,280
boo this this print F just it just

2106
01:26:45,880 --> 01:26:50,920
prints boo like it works um just from

2107
01:26:48,280 --> 01:26:53,119
make 01 so what we're doing is we have

2108
01:26:50,920 --> 01:26:55,400
this make Command which is for make

2109
01:26:53,119 --> 01:26:58,119
files kind of maps the there and then we

2110
01:26:55,400 --> 01:26:59,840
have the 01 part which is the target so

2111
01:26:58,119 --> 01:27:01,239
this this is the target this is left

2112
01:26:59,840 --> 01:27:04,440
side of the colon and then after is

2113
01:27:01,239 --> 01:27:07,639
their prerequisites so notice how I I

2114
01:27:04,440 --> 01:27:11,679
removed that that other part the

2115
01:27:07,639 --> 01:27:15,520
01c so this essentially means we're

2116
01:27:11,679 --> 01:27:17,560
going to uh either confirm that this is

2117
01:27:15,520 --> 01:27:20,440
that this already existed or has been

2118
01:27:17,560 --> 01:27:22,560
done um and if it hasn't we're going to

2119
01:27:20,440 --> 01:27:25,520
do it so you'll see that in these

2120
01:27:22,560 --> 01:27:28,159
examples down here but

2121
01:27:25,520 --> 01:27:30,000
just to fill in the rest so we have a

2122
01:27:28,159 --> 01:27:32,159
bunch of things happening we have this

2123
01:27:30,000 --> 01:27:34,679
we have this variable GCC which is just

2124
01:27:32,159 --> 01:27:38,040
saying you know it's it's essentially

2125
01:27:34,679 --> 01:27:41,119
just going uh you know

2126
01:27:38,040 --> 01:27:43,360
GCC um it's just doing that and then we

2127
01:27:41,119 --> 01:27:46,000
put this at sign in front of

2128
01:27:43,360 --> 01:27:50,119
it which means don't print this out in

2129
01:27:46,000 --> 01:27:50,960
the terminal so if I just uh remove this

2130
01:27:50,119 --> 01:27:54,639
and

2131
01:27:50,960 --> 01:27:58,400
go uh and just remove the at

2132
01:27:54,639 --> 01:28:00,639
and go uh

2133
01:27:58,400 --> 01:28:04,080
clean I'll just I'll just remove this

2134
01:28:00,639 --> 01:28:08,159
for for so that it makes the most

2135
01:28:04,080 --> 01:28:10,679
sense and then I go make uh 01 you'll

2136
01:28:08,159 --> 01:28:15,040
see that it actually uh shows us this in

2137
01:28:10,679 --> 01:28:16,960
the terminal um but if I put an at there

2138
01:28:15,040 --> 01:28:18,320
then it doesn't right so that just

2139
01:28:16,960 --> 01:28:20,080
removes it that just makes things more

2140
01:28:18,320 --> 01:28:22,239
clean and just it's like a best practice

2141
01:28:20,080 --> 01:28:25,000
just easy to see things um you want to

2142
01:28:22,239 --> 01:28:26,840
maybe like look at the ones that uh

2143
01:28:25,000 --> 01:28:28,840
might do weird things and you just want

2144
01:28:26,840 --> 01:28:31,199
to like ensure that all your variables

2145
01:28:28,840 --> 01:28:34,480
are correct um but this is a very simple

2146
01:28:31,199 --> 01:28:37,119
you don't need to print this out um and

2147
01:28:34,480 --> 01:28:39,600
then just jumping down to uh like number

2148
01:28:37,119 --> 01:28:44,360
two here

2149
01:28:39,600 --> 01:28:45,800
so uh number two is uh essentially the

2150
01:28:44,360 --> 01:28:49,159
same thing

2151
01:28:45,800 --> 01:28:51,119
so uh I'm not even going to execute that

2152
01:28:49,159 --> 01:28:54,800
we don't we don't even need to run this

2153
01:28:51,119 --> 01:28:58,000
um and then 03 is just the ca compiler

2154
01:28:54,800 --> 01:28:59,080
so we do nvcc and then the Cuda Flags so

2155
01:28:58,000 --> 01:29:01,440
which essentially what it's going to

2156
01:28:59,080 --> 01:29:05,560
look like is it's going to go

2157
01:29:01,440 --> 01:29:10,159
nvcc Das Arch

2158
01:29:05,560 --> 01:29:11,960
uh get rid of that and then um it's

2159
01:29:10,159 --> 01:29:13,480
going to pass in

2160
01:29:11,960 --> 01:29:16,320
O

2161
01:29:13,480 --> 01:29:19,679
03 CU that that's the binary we want it

2162
01:29:16,320 --> 01:29:22,119
to be and then

2163
01:29:19,679 --> 01:29:24,880
that that's going to run and we should

2164
01:29:22,119 --> 01:29:27,440
be able to go 03 C and it's going to say

2165
01:29:24,880 --> 01:29:29,719
boo from here right so just just the

2166
01:29:27,440 --> 01:29:31,920
same thing um we just include like Cuda

2167
01:29:29,719 --> 01:29:34,400
runtime and all this just to be fancy

2168
01:29:31,920 --> 01:29:35,600
but it just it just outputs the the boot

2169
01:29:34,400 --> 01:29:38,000
the boot

2170
01:29:35,600 --> 01:29:40,320
command

2171
01:29:38,000 --> 01:29:42,920
um yeah this this is a this is very

2172
01:29:40,320 --> 01:29:45,119
simple example so it's essentially just

2173
01:29:42,920 --> 01:29:47,760
converting uh it's converting these

2174
01:29:45,119 --> 01:29:49,440
variable names and not printing anything

2175
01:29:47,760 --> 01:29:52,400
out

2176
01:29:49,440 --> 01:29:53,840
um we just go make 03 and it'll do the

2177
01:29:52,400 --> 01:29:55,280
exact same thing see it takes a little

2178
01:29:53,840 --> 01:29:59,880
while

2179
01:29:55,280 --> 01:30:02,280
same exact thing um we have this clean

2180
01:29:59,880 --> 01:30:04,040
command which is going to actually

2181
01:30:02,280 --> 01:30:07,239
remove all these binaries so when we

2182
01:30:04,040 --> 01:30:08,719
want to clean up everything uh and just

2183
01:30:07,239 --> 01:30:10,400
you know make it nice and presentable

2184
01:30:08,719 --> 01:30:12,000
like we haven't done work like what I'm

2185
01:30:10,400 --> 01:30:14,600
doing with you guys is I'm deleting

2186
01:30:12,000 --> 01:30:16,000
these binaries before doing the lessons

2187
01:30:14,600 --> 01:30:19,840
uh because it looks ugly when I have

2188
01:30:16,000 --> 01:30:23,760
more files so I can just go uh make

2189
01:30:19,840 --> 01:30:26,239
clean gone right um so that that's just

2190
01:30:23,760 --> 01:30:27,400
like a very very simple example and it's

2191
01:30:26,239 --> 01:30:29,880
just it's just like make and then

2192
01:30:27,400 --> 01:30:31,360
whatever the target is uh going back up

2193
01:30:29,880 --> 01:30:34,639
to these

2194
01:30:31,360 --> 01:30:39,679
ones these are a little interesting

2195
01:30:34,639 --> 01:30:41,520
so 01 uncore obj which is for object

2196
01:30:39,679 --> 01:30:45,520
this is going to uh

2197
01:30:41,520 --> 01:30:48,000
GCC uh comp uh it's going to take in

2198
01:30:45,520 --> 01:30:50,440
this C code and it's going to Output uh

2199
01:30:48,000 --> 01:30:52,440
this o or this object file so it's going

2200
01:30:50,440 --> 01:30:53,639
to it's going to essentially this but

2201
01:30:52,440 --> 01:30:58,199
it's just going to do object file

2202
01:30:53,639 --> 01:31:02,280
instead of a binary um and then this 01

2203
01:30:58,199 --> 01:31:05,639
uh obj so this object uh execute

2204
01:31:02,280 --> 01:31:08,560
run that's going to take in this this

2205
01:31:05,639 --> 01:31:10,320
previous one as a prerequisite so that

2206
01:31:08,560 --> 01:31:12,000
means this one has to already be

2207
01:31:10,320 --> 01:31:13,440
complete and if it's not complete we're

2208
01:31:12,000 --> 01:31:18,199
going to run it and make sure it's

2209
01:31:13,440 --> 01:31:19,320
complete in order for this to work so uh

2210
01:31:18,199 --> 01:31:22,760
we're going

2211
01:31:19,320 --> 01:31:24,960
to uh compile this object file into a

2212
01:31:22,760 --> 01:31:27,360
binary so you have the this like the C

2213
01:31:24,960 --> 01:31:31,520
the C representation and then the object

2214
01:31:27,360 --> 01:31:33,719
file which we do uh here and then this

2215
01:31:31,520 --> 01:31:36,119
one takes that new object file and

2216
01:31:33,719 --> 01:31:39,719
converts that into a binary and then we

2217
01:31:36,119 --> 01:31:46,080
execute that binary on this line so if I

2218
01:31:39,719 --> 01:31:47,159
go uh make and then 01 uh obj ex uh

2219
01:31:46,080 --> 01:31:49,960
execute

2220
01:31:47,159 --> 01:31:50,800
run and we don't have any of these files

2221
01:31:49,960 --> 01:31:54,480
in

2222
01:31:50,800 --> 01:31:57,760
here it's going to

2223
01:31:54,480 --> 01:32:00,239
it's literally going to uh ensure that

2224
01:31:57,760 --> 01:32:03,080
this is called first because it hasn't

2225
01:32:00,239 --> 01:32:05,400
yet um so it's going to it's going to

2226
01:32:03,080 --> 01:32:07,119
convert this C file into object and then

2227
01:32:05,400 --> 01:32:08,440
it's going to convert that object into a

2228
01:32:07,119 --> 01:32:11,400
binary and it's going to execute that

2229
01:32:08,440 --> 01:32:13,280
binary so this is just I probably overe

2230
01:32:11,400 --> 01:32:16,159
explain things a little bit but this is

2231
01:32:13,280 --> 01:32:18,800
this is pretty much the idea on how you

2232
01:32:16,159 --> 01:32:21,040
uh how you can automate uh just C

2233
01:32:18,800 --> 01:32:23,119
compilation C++ compilation we're going

2234
01:32:21,040 --> 01:32:24,440
to use this more for uh Cuda scripts

2235
01:32:23,119 --> 01:32:26,719
down the road

2236
01:32:24,440 --> 01:32:28,239
uh but that's that's the general idea

2237
01:32:26,719 --> 01:32:30,199
and then this phony part at the top

2238
01:32:28,239 --> 01:32:32,600
might look a little weird but this

2239
01:32:30,199 --> 01:32:36,719
essentially means uh we're not we're not

2240
01:32:32,600 --> 01:32:39,639
going to uh it's just like a a a way to

2241
01:32:36,719 --> 01:32:41,760
make uh things easier to use make it so

2242
01:32:39,639 --> 01:32:44,719
you don't run into errors I had a decent

2243
01:32:41,760 --> 01:32:47,560
explanation here so um say we had a make

2244
01:32:44,719 --> 01:32:49,360
file make file with a Target named clean

2245
01:32:47,560 --> 01:32:51,880
so in this in this cleanup command that

2246
01:32:49,360 --> 01:32:53,280
that makes everything nice again suppose

2247
01:32:51,880 --> 01:32:55,280
we have a directory named clean in the

2248
01:32:53,280 --> 01:32:57,760
same directory as the make file so here

2249
01:32:55,280 --> 01:33:00,080
if we had something named clean um if we

2250
01:32:57,760 --> 01:33:02,400
run make clean make will not run the

2251
01:33:00,080 --> 01:33:04,159
command um it will not run the command

2252
01:33:02,400 --> 01:33:06,520
in the Target clean because clean

2253
01:33:04,159 --> 01:33:06,520
already

2254
01:33:07,760 --> 01:33:11,360
exists instead it will see that the

2255
01:33:09,600 --> 01:33:13,040
director clean already exists will not

2256
01:33:11,360 --> 01:33:14,639
it'll not run it so in short we

2257
01:33:13,040 --> 01:33:16,560
essentially take a bunch of mappings

2258
01:33:14,639 --> 01:33:19,560
from Target names to

2259
01:33:16,560 --> 01:33:21,159
commands um that's that's where this

2260
01:33:19,560 --> 01:33:22,760
phony thing comes from so it's like a

2261
01:33:21,159 --> 01:33:24,440
it's like a phony if you will I don't

2262
01:33:22,760 --> 01:33:27,800
know what the philos was behind that

2263
01:33:24,440 --> 01:33:29,639
naming but that's that's how it works um

2264
01:33:27,800 --> 01:33:32,280
and then we have some just some other

2265
01:33:29,639 --> 01:33:34,600
stuff in here so I already went it over

2266
01:33:32,280 --> 01:33:38,719
the at symbol and then there's this one

2267
01:33:34,600 --> 01:33:42,840
too so the the colon equals um I don't

2268
01:33:38,719 --> 01:33:45,159
think we use this in here but uh equals

2269
01:33:42,840 --> 01:33:46,840
is used for dividing variables or uh

2270
01:33:45,159 --> 01:33:48,560
it's called a recursive assignment so

2271
01:33:46,840 --> 01:33:50,560
both used for dividing variables both of

2272
01:33:48,560 --> 01:33:54,159
these are this one is a recursive

2273
01:33:50,560 --> 01:33:57,000
assignment so value of the variable is

2274
01:33:54,159 --> 01:33:58,840
reevaluated each time it's used um and

2275
01:33:57,000 --> 01:34:01,159
then this one is a simple assignment or

2276
01:33:58,840 --> 01:34:03,040
immediate one it's evaluated only once

2277
01:34:01,159 --> 01:34:04,600
at the point of definition so this is

2278
01:34:03,040 --> 01:34:06,760
like typically the safer option you want

2279
01:34:04,600 --> 01:34:08,600
to go down if you get really complicated

2280
01:34:06,760 --> 01:34:10,000
make files you might end up running into

2281
01:34:08,600 --> 01:34:12,480
weird things with these recursive

2282
01:34:10,000 --> 01:34:14,520
assignments so generally it's safe to

2283
01:34:12,480 --> 01:34:17,040
use these ones but it it looks it looks

2284
01:34:14,520 --> 01:34:19,480
a little funny so I didn't include it in

2285
01:34:17,040 --> 01:34:22,119
this example um we will we will use it

2286
01:34:19,480 --> 01:34:24,719
down the road though so last but not

2287
01:34:22,119 --> 01:34:28,000
least uh we have debers so debuggers are

2288
01:34:24,719 --> 01:34:30,280
awesome for just uh an alternative to

2289
01:34:28,000 --> 01:34:32,400
just adding print lines print just print

2290
01:34:30,280 --> 01:34:34,840
this print that did we make it here yay

2291
01:34:32,400 --> 01:34:37,000
we made it ah we failed whatever just

2292
01:34:34,840 --> 01:34:38,760
adding those just blows your code up so

2293
01:34:37,000 --> 01:34:40,840
having debuggers makes that a lot easier

2294
01:34:38,760 --> 01:34:43,199
on you uh you can actually go down to

2295
01:34:40,840 --> 01:34:45,960
literal assembly and see where the

2296
01:34:43,199 --> 01:34:47,239
electrons are in your code uh like in

2297
01:34:45,960 --> 01:34:49,800
your in your script like what is

2298
01:34:47,239 --> 01:34:51,639
happening on the hardware so uh

2299
01:34:49,800 --> 01:34:53,440
debuggers are super useful and in

2300
01:34:51,639 --> 01:34:57,239
particular we're going to be talking

2301
01:34:53,440 --> 01:35:00,040
about the GDB debugger for C and C++ so

2302
01:34:57,239 --> 01:35:03,159
you use them for both um I'm not going

2303
01:35:00,040 --> 01:35:04,400
to like explain these super intensely I

2304
01:35:03,159 --> 01:35:05,960
don't feel comfortable in my own

2305
01:35:04,400 --> 01:35:10,080
explanation for these because there's

2306
01:35:05,960 --> 01:35:13,520
just a lot happening um but there are

2307
01:35:10,080 --> 01:35:15,119
some commands that you generally want to

2308
01:35:13,520 --> 01:35:18,560
be familiar with it's mostly just

2309
01:35:15,119 --> 01:35:21,520
commands and knowing uh what to look for

2310
01:35:18,560 --> 01:35:23,199
in your script so I have these in the

2311
01:35:21,520 --> 01:35:24,560
readme file just just with some

2312
01:35:23,199 --> 01:35:27,719
explanation

2313
01:35:24,560 --> 01:35:30,760
um but a really good uh overview is here

2314
01:35:27,719 --> 01:35:35,080
so this is done from lowle learning has

2315
01:35:30,760 --> 01:35:36,880
an advertisement here but uh essentially

2316
01:35:35,080 --> 01:35:40,280
it's just it's just a really good

2317
01:35:36,880 --> 01:35:41,639
overview on uh GDB uh just going into

2318
01:35:40,280 --> 01:35:45,000
assembly and doing a bunch of cool

2319
01:35:41,639 --> 01:35:49,119
tricks and and debuging C code that way

2320
01:35:45,000 --> 01:35:49,119
so I do recommend you watch that

2321
01:35:50,239 --> 01:35:54,320
video okay we can finally take a

2322
01:35:52,360 --> 01:35:56,080
breather now that was a lot

2323
01:35:54,320 --> 01:35:58,360
uh but I really hope that this is going

2324
01:35:56,080 --> 01:35:59,960
to sort of just ease that uh this is

2325
01:35:58,360 --> 01:36:01,600
designed to be a more passive part where

2326
01:35:59,960 --> 01:36:03,639
you can kind of just sit around and

2327
01:36:01,600 --> 01:36:05,600
listen and watch and just kind of enjoy

2328
01:36:03,639 --> 01:36:08,760
it uh there's no coding involved here at

2329
01:36:05,600 --> 01:36:11,560
all um so I thought I'd provide some

2330
01:36:08,760 --> 01:36:13,080
context on different types of Hardware

2331
01:36:11,560 --> 01:36:14,280
what the whole purpose of gpus are I

2332
01:36:13,080 --> 01:36:15,960
mean you probably already know what they

2333
01:36:14,280 --> 01:36:17,400
are but I just want to provide that

2334
01:36:15,960 --> 01:36:19,920
background just so that we're on the

2335
01:36:17,400 --> 01:36:22,159
same page entirely um and just to

2336
01:36:19,920 --> 01:36:24,760
provide some kind of internal I guess

2337
01:36:22,159 --> 01:36:26,600
preparation uh for for the next part

2338
01:36:24,760 --> 01:36:29,040
it's good to have these braks just to

2339
01:36:26,600 --> 01:36:31,679
you know slow your mind a little bit and

2340
01:36:29,040 --> 01:36:36,800
uh give yourself some time

2341
01:36:31,679 --> 01:36:38,400
but first off we have these CPUs okay

2342
01:36:36,800 --> 01:36:40,560
you already know what a CPU is it's

2343
01:36:38,400 --> 01:36:43,360
general purpose High clock speed per

2344
01:36:40,560 --> 01:36:45,880
core very few number of cores the on

2345
01:36:43,360 --> 01:36:48,000
chip memory is very large so maybe you

2346
01:36:45,880 --> 01:36:49,440
didn't know that the the caches on the

2347
01:36:48,000 --> 01:36:52,800
chip are actually quite large compared

2348
01:36:49,440 --> 01:36:56,199
to the GPU um this is because you know

2349
01:36:52,800 --> 01:36:58,480
the memory band with from uh the CPU to

2350
01:36:56,199 --> 01:37:00,360
the the ram slots on your computer those

2351
01:36:58,480 --> 01:37:01,760
are going to be uh like that transfer

2352
01:37:00,360 --> 01:37:03,119
speed is going to be very slow you have

2353
01:37:01,760 --> 01:37:05,199
to move the electrons all the way from

2354
01:37:03,119 --> 01:37:07,320
this part of the motherboard to this one

2355
01:37:05,199 --> 01:37:09,040
and that takes time right you're you're

2356
01:37:07,320 --> 01:37:10,960
just constantly waiting for data to

2357
01:37:09,040 --> 01:37:12,960
arrive and that's like what takes up

2358
01:37:10,960 --> 01:37:15,000
most of your time right so you have

2359
01:37:12,960 --> 01:37:16,400
these big caches for just purposely like

2360
01:37:15,000 --> 01:37:21,159
load pre-loading things on so that

2361
01:37:16,400 --> 01:37:23,360
they're ready to use um you have lower

2362
01:37:21,159 --> 01:37:25,840
latency so the whole idea of a CPU is to

2363
01:37:23,360 --> 01:37:27,280
just just complete this task as quickly

2364
01:37:25,840 --> 01:37:30,119
as possible and just return the value

2365
01:37:27,280 --> 01:37:31,639
just complete it complete it fast um and

2366
01:37:30,119 --> 01:37:34,639
then they have low throughput as well so

2367
01:37:31,639 --> 01:37:38,520
low throughput means um it can't do as

2368
01:37:34,639 --> 01:37:41,560
much comparatively it can't do as uh as

2369
01:37:38,520 --> 01:37:42,719
much operations per second as a GPU can

2370
01:37:41,560 --> 01:37:44,639
if you're talking about simple

2371
01:37:42,719 --> 01:37:46,840
instructions if they're more complex

2372
01:37:44,639 --> 01:37:48,639
ones like managing and loading data and

2373
01:37:46,840 --> 01:37:51,400
doing like file reads and wres like

2374
01:37:48,639 --> 01:37:54,239
that'll be faster but if you're talking

2375
01:37:51,400 --> 01:37:55,920
like math and matrix multiplication

2376
01:37:54,239 --> 01:37:58,280
uh it is going to be significantly

2377
01:37:55,920 --> 01:38:01,040
slower throughput is more talked about

2378
01:37:58,280 --> 01:38:04,000
as like operations per second so if I

2379
01:38:01,040 --> 01:38:05,800
have a bunch of cores running at say uh

2380
01:38:04,000 --> 01:38:08,159
2 billion clocks per second and I have

2381
01:38:05,800 --> 01:38:10,040
6,000 of them versus six cores that are

2382
01:38:08,159 --> 01:38:11,760
running at 5 billion clocks per second

2383
01:38:10,040 --> 01:38:13,119
do the math right how many operations

2384
01:38:11,760 --> 01:38:15,080
are you going to do on this one versus

2385
01:38:13,119 --> 01:38:16,960
that one that's the whole point of a GPU

2386
01:38:15,080 --> 01:38:19,199
way more cores a little bit lower clock

2387
01:38:16,960 --> 01:38:20,760
speed um but way more cores it's is

2388
01:38:19,199 --> 01:38:24,400
completely outnumbered CPU and that's

2389
01:38:20,760 --> 01:38:27,560
why it's faster um and then on

2390
01:38:24,400 --> 01:38:28,880
gpus we have the 90 hasn't been released

2391
01:38:27,560 --> 01:38:35,000
yet but I thought it was funny to put

2392
01:38:28,880 --> 01:38:37,440
there um gpus are very specialized so

2393
01:38:35,000 --> 01:38:39,480
they can accomplish simpler instructions

2394
01:38:37,440 --> 01:38:41,159
easier to handle ones hence why they

2395
01:38:39,480 --> 01:38:43,520
have smaller controllers on them which

2396
01:38:41,159 --> 01:38:46,040
you'll see in a second um they have a

2397
01:38:43,520 --> 01:38:49,239
lower clock speed like I said way more

2398
01:38:46,040 --> 01:38:51,040
cores and a lower cache so because you

2399
01:38:49,239 --> 01:38:52,679
have that on chip memory because you

2400
01:38:51,040 --> 01:38:54,639
actually have this vram that you can

2401
01:38:52,679 --> 01:38:56,320
access that is on on the GPU and that

2402
01:38:54,639 --> 01:38:58,520
this all of the Nvidia Hardware

2403
01:38:56,320 --> 01:39:01,520
Engineers has essentially optimized for

2404
01:38:58,520 --> 01:39:03,480
accessing that um you're able to get a

2405
01:39:01,520 --> 01:39:06,840
lot higher memory bandwidth that way up

2406
01:39:03,480 --> 01:39:09,480
in the like a high 100 gigabyte per

2407
01:39:06,840 --> 01:39:12,400
second range um so it's like it's it's

2408
01:39:09,480 --> 01:39:14,199
in the hundreds for sure um you have

2409
01:39:12,400 --> 01:39:16,360
higher latency on this remember it's not

2410
01:39:14,199 --> 01:39:18,040
for you know minimize the amount of time

2411
01:39:16,360 --> 01:39:20,760
it takes to complete this task and then

2412
01:39:18,040 --> 01:39:22,840
just done right it's it's more optimized

2413
01:39:20,760 --> 01:39:25,520
for throughput so we already talked

2414
01:39:22,840 --> 01:39:28,599
about this but um then you have these

2415
01:39:25,520 --> 01:39:30,719
tpus which came across recently and

2416
01:39:28,599 --> 01:39:35,040
these are for modern deep learning

2417
01:39:30,719 --> 01:39:36,920
applications so tpus are for literally

2418
01:39:35,040 --> 01:39:38,599
just processing tensors like you do fast

2419
01:39:36,920 --> 01:39:40,639
matrix multiplication fast linear

2420
01:39:38,599 --> 01:39:42,719
algebra that's that's what it is tensor

2421
01:39:40,639 --> 01:39:45,440
is linear tensor operations is linear

2422
01:39:42,719 --> 01:39:48,960
algebra um and it's just specialized for

2423
01:39:45,440 --> 01:39:50,639
doing that so tpus are faster but way

2424
01:39:48,960 --> 01:39:52,719
more expensive and specialized in

2425
01:39:50,639 --> 01:39:54,320
typically not consumer grade Hardware so

2426
01:39:52,719 --> 01:39:57,360
that's why we learn how to you know

2427
01:39:54,320 --> 01:39:58,400
build on top of infrastructure with gpus

2428
01:39:57,360 --> 01:40:01,159
cuz it's you know you can actually

2429
01:39:58,400 --> 01:40:04,840
afford it you can have one at your house

2430
01:40:01,159 --> 01:40:07,080
and for fairly low cost um and then you

2431
01:40:04,840 --> 01:40:09,159
have these fbt which I don't expect you

2432
01:40:07,080 --> 01:40:11,199
know what these are but these field

2433
01:40:09,159 --> 01:40:13,000
programmable gate arrays are very

2434
01:40:11,199 --> 01:40:15,880
specialized pieces of Hardware that

2435
01:40:13,000 --> 01:40:18,520
essentially say uh instead of having to

2436
01:40:15,880 --> 01:40:20,679
write like a or build a custom Hardware

2437
01:40:18,520 --> 01:40:22,400
configuration uh like for a certain task

2438
01:40:20,679 --> 01:40:24,040
for making something that you need to

2439
01:40:22,400 --> 01:40:26,119
run really really fast

2440
01:40:24,040 --> 01:40:29,159
um you can just program these you can

2441
01:40:26,119 --> 01:40:32,080
just program the actual chips to do uh

2442
01:40:29,159 --> 01:40:34,880
something more more fine grain on what

2443
01:40:32,080 --> 01:40:36,719
you want so there's more control over it

2444
01:40:34,880 --> 01:40:39,880
very expensive very low latency very

2445
01:40:36,719 --> 01:40:41,480
high throughput uh high power all this

2446
01:40:39,880 --> 01:40:44,239
right they're these are these are more

2447
01:40:41,480 --> 01:40:45,800
expensive but uh they allow for

2448
01:40:44,239 --> 01:40:48,040
modularity if you

2449
01:40:45,800 --> 01:40:50,880
will um and then just for some

2450
01:40:48,040 --> 01:40:54,440
background on GPU history so you know

2451
01:40:50,880 --> 01:40:56,840
back in 1993 when Jensen started Nvidia

2452
01:40:54,440 --> 01:40:59,159
they had the uh you know they the

2453
01:40:56,840 --> 01:41:03,840
GeForce cards all of these I wasn't

2454
01:40:59,159 --> 01:41:06,239
alive during this time um but you have

2455
01:41:03,840 --> 01:41:08,440
uh then you start getting into you know

2456
01:41:06,239 --> 01:41:10,880
after after the g47 you start getting

2457
01:41:08,440 --> 01:41:15,320
into uh these these better ones so like

2458
01:41:10,880 --> 01:41:17,800
the Tesla cards the fery uh the Kepler

2459
01:41:15,320 --> 01:41:19,239
Maxwell Pascal and then volto is when

2460
01:41:17,800 --> 01:41:21,760
things really started taking off the

2461
01:41:19,239 --> 01:41:24,440
Pascal and the Volta cards then you have

2462
01:41:21,760 --> 01:41:27,040
taring and then a which is what my card

2463
01:41:24,440 --> 01:41:29,239
is based on and then you have Hopper

2464
01:41:27,040 --> 01:41:31,360
which uh in case you didn't know Hopper

2465
01:41:29,239 --> 01:41:35,520
cards are really really fast like the

2466
01:41:31,360 --> 01:41:38,360
h100s and the h20s and even the recently

2467
01:41:35,520 --> 01:41:41,280
released Nvidia Blackwell chips yeah

2468
01:41:38,360 --> 01:41:43,719
those are ridiculous um this is this is

2469
01:41:41,280 --> 01:41:46,679
a little bit outdated of course so there

2470
01:41:43,719 --> 01:41:48,599
are actually uh like chips on here uh I

2471
01:41:46,679 --> 01:41:51,000
just got like a an outdated screenshot

2472
01:41:48,599 --> 01:41:52,760
but you get the idea this is how it

2473
01:41:51,000 --> 01:41:56,159
progressed uh and then you have like the

2474
01:41:52,760 --> 01:41:57,920
relative clock speed per core on here so

2475
01:41:56,159 --> 01:41:59,639
you know some of these were like really

2476
01:41:57,920 --> 01:42:02,400
really high but didn't have very many

2477
01:41:59,639 --> 01:42:04,199
cores um and then Nvidia kind of figured

2478
01:42:02,400 --> 01:42:08,000
out like okay we should just put more

2479
01:42:04,199 --> 01:42:11,880
and more cores on these things right um

2480
01:42:08,000 --> 01:42:14,040
and then you get the the overall uh I

2481
01:42:11,880 --> 01:42:17,119
think floating Point performance is what

2482
01:42:14,040 --> 01:42:19,840
this is so

2483
01:42:17,119 --> 01:42:21,800
uh yeah you once you get to Volta it's

2484
01:42:19,840 --> 01:42:24,760
it's starting to get uh it's starting to

2485
01:42:21,800 --> 01:42:27,040
get really high so I think this is uh

2486
01:42:24,760 --> 01:42:30,440
double Precision Giga

2487
01:42:27,040 --> 01:42:33,760
flops so

2488
01:42:30,440 --> 01:42:36,040
uh you know six essentially six Tera

2489
01:42:33,760 --> 01:42:38,239
flops of compute on the FTA architecture

2490
01:42:36,040 --> 01:42:40,360
which is pretty good um and then it gets

2491
01:42:38,239 --> 01:42:43,599
better and better from there on mine I

2492
01:42:40,360 --> 01:42:45,840
think I have right now it runs at a high

2493
01:42:43,599 --> 01:42:50,040
of around

2494
01:42:45,840 --> 01:42:51,679
23 uh 23 Tera flops on kuas so kuas is

2495
01:42:50,040 --> 01:42:53,560
the fast linear algebra library that

2496
01:42:51,679 --> 01:42:55,320
multiplies matrices really quickly and

2497
01:42:53,560 --> 01:42:59,320
I'm on there I'm about to get pretty

2498
01:42:55,320 --> 01:43:02,320
much like 20 uh 20 gig flops of single

2499
01:42:59,320 --> 01:43:04,440
Precision compute um so that's that's

2500
01:43:02,320 --> 01:43:06,159
that's that's quite

2501
01:43:04,440 --> 01:43:08,800
good

2502
01:43:06,159 --> 01:43:10,639
um what makes these things so fast for

2503
01:43:08,800 --> 01:43:15,400
deep learning I didn't actually cover

2504
01:43:10,639 --> 01:43:17,280
this um on the CPU you have very little

2505
01:43:15,400 --> 01:43:18,760
cores you have these big control units

2506
01:43:17,280 --> 01:43:20,239
that are taking up a ton of space you

2507
01:43:18,760 --> 01:43:23,599
have all these caches everywhere that

2508
01:43:20,239 --> 01:43:26,080
are flooding the thing um

2509
01:43:23,599 --> 01:43:28,080
and like there you you're you're not

2510
01:43:26,080 --> 01:43:29,560
giving that much uh leverage to the

2511
01:43:28,080 --> 01:43:31,800
course right like the course can do

2512
01:43:29,560 --> 01:43:33,520
Advanced complex instructions but there

2513
01:43:31,800 --> 01:43:35,760
aren't that many of them so you can only

2514
01:43:33,520 --> 01:43:37,679
do so much whereas if you have this

2515
01:43:35,760 --> 01:43:40,719
other architecture where you have

2516
01:43:37,679 --> 01:43:44,840
simpler instructions simpler controllers

2517
01:43:40,719 --> 01:43:46,800
simpler uh registers smaller ones uh but

2518
01:43:44,840 --> 01:43:48,520
a ton of cores like see how most of this

2519
01:43:46,800 --> 01:43:51,280
is taken up by cores and then just

2520
01:43:48,520 --> 01:43:54,239
caches and RAM um that's that's ideal

2521
01:43:51,280 --> 01:43:57,119
for gpus so on here you're too

2522
01:43:54,239 --> 01:43:58,920
essentially the the IDE the idea here is

2523
01:43:57,119 --> 01:44:00,199
you're trying to put together a puzzle

2524
01:43:58,920 --> 01:44:03,360
you're trying to put together a jigsaw

2525
01:44:00,199 --> 01:44:05,520
puzzle and the point is is it doesn't

2526
01:44:03,360 --> 01:44:07,000
matter which order you do it in so you

2527
01:44:05,520 --> 01:44:09,440
you don't have to do like this row and

2528
01:44:07,000 --> 01:44:11,080
then or you this column this column this

2529
01:44:09,440 --> 01:44:13,239
like it doesn't matter you do this piece

2530
01:44:11,080 --> 01:44:15,000
here this piece there do like a block

2531
01:44:13,239 --> 01:44:16,719
like a chunk there it doesn't matter as

2532
01:44:15,000 --> 01:44:18,080
long as it's all assembled together

2533
01:44:16,719 --> 01:44:19,960
properly in the end that's what you care

2534
01:44:18,080 --> 01:44:23,400
about and that's what the GPU is really

2535
01:44:19,960 --> 01:44:25,440
good at so typically you'll do things as

2536
01:44:23,400 --> 01:44:27,360
like uh you know one like multiple

2537
01:44:25,440 --> 01:44:29,080
puzzle pieces at a time or multiple

2538
01:44:27,360 --> 01:44:30,719
blocks of puzzle pieces at a time so

2539
01:44:29,080 --> 01:44:33,080
you'll have like a 2 by two or like a

2540
01:44:30,719 --> 01:44:35,199
4x4 thing of Jigsaw pieces that you'll

2541
01:44:33,080 --> 01:44:37,840
maybe do at a time that that's like the

2542
01:44:35,199 --> 01:44:40,440
that's the intuition behind uh Cuda and

2543
01:44:37,840 --> 01:44:42,599
how how you program gpus to run fast and

2544
01:44:40,440 --> 01:44:44,520
solve these problems quickly um on the

2545
01:44:42,599 --> 01:44:46,440
CPU you might be able to only do like

2546
01:44:44,520 --> 01:44:48,840
see there's only four cores here so you

2547
01:44:46,440 --> 01:44:50,560
might be able to only do uh four given

2548
01:44:48,840 --> 01:44:53,480
pieces of that puzzle at the same at

2549
01:44:50,560 --> 01:44:56,360
once uh whereas GPU you you know let's

2550
01:44:53,480 --> 01:44:59,040
say you have like 6,000 cores right uh

2551
01:44:56,360 --> 01:45:00,760
if this if this puzzle has like I don't

2552
01:44:59,040 --> 01:45:03,520
know say like

2553
01:45:00,760 --> 01:45:06,080
12,000 um if it has if it has 12,000

2554
01:45:03,520 --> 01:45:08,560
pieces well you can effectively do that

2555
01:45:06,080 --> 01:45:10,880
in two operations because you're able to

2556
01:45:08,560 --> 01:45:12,880
do the first 6,000 in one and then the

2557
01:45:10,880 --> 01:45:14,520
other 6,000 in the second so it's

2558
01:45:12,880 --> 01:45:18,280
effectively two operations that you do

2559
01:45:14,520 --> 01:45:21,119
it in but if you divide 12,000 by 4 four

2560
01:45:18,280 --> 01:45:23,080
meaning number of CPU cores um you

2561
01:45:21,119 --> 01:45:25,080
actually get 3,000 operations see you

2562
01:45:23,080 --> 01:45:28,280
can see how that can be drastically sped

2563
01:45:25,080 --> 01:45:30,760
up um that's that's why gpus are so fast

2564
01:45:28,280 --> 01:45:33,280
because you can because you can do that

2565
01:45:30,760 --> 01:45:34,920
um now there there are some there are

2566
01:45:33,280 --> 01:45:36,960
some common terms that we refer to these

2567
01:45:34,920 --> 01:45:38,360
things through so CPU is the host you're

2568
01:45:36,960 --> 01:45:40,679
going to see this in Cuda once we start

2569
01:45:38,360 --> 01:45:42,960
writing kernels uh the CPU is called the

2570
01:45:40,679 --> 01:45:44,239
host which is pretty obvious and then it

2571
01:45:42,960 --> 01:45:46,159
just kind of just kind of makes sense

2572
01:45:44,239 --> 01:45:48,119
for that to be named that way and then

2573
01:45:46,159 --> 01:45:50,520
the GPU is the device so you have the

2574
01:45:48,119 --> 01:45:56,040
host CPU and then GPU which is the

2575
01:45:50,520 --> 01:45:57,360
device um the CPU is going to uh mainly

2576
01:45:56,040 --> 01:45:59,280
the performance there is going to be

2577
01:45:57,360 --> 01:46:02,440
latency in seconds so you're looking at

2578
01:45:59,280 --> 01:46:05,080
latency how quick can I do a given task

2579
01:46:02,440 --> 01:46:06,599
and the GPU is throughput in tasks per

2580
01:46:05,080 --> 01:46:08,960
second so for example if you're doing a

2581
01:46:06,599 --> 01:46:12,880
rendering task it's like how many pixels

2582
01:46:08,960 --> 01:46:14,960
can I render per second um or how many

2583
01:46:12,880 --> 01:46:17,560
uh I don't know how many pixels can I

2584
01:46:14,960 --> 01:46:21,360
can I yeah sure render per second that's

2585
01:46:17,560 --> 01:46:25,040
fine um in a typical Cuda program you're

2586
01:46:21,360 --> 01:46:26,960
going to allocate at uh some memory on

2587
01:46:25,040 --> 01:46:28,960
CPU memory so it's going to be a

2588
01:46:26,960 --> 01:46:31,880
classical like C Malik that's what

2589
01:46:28,960 --> 01:46:34,280
you're going to do and then once it's

2590
01:46:31,880 --> 01:46:37,760
allocated on CPU or host you're going to

2591
01:46:34,280 --> 01:46:40,960
copy from host to device or or CPU to

2592
01:46:37,760 --> 01:46:43,440
GPU um and then once it's on the GPU you

2593
01:46:40,960 --> 01:46:45,520
can actually launch a kernel which is

2594
01:46:43,440 --> 01:46:47,599
what these parallel functions are

2595
01:46:45,520 --> 01:46:50,199
essentially so on a CPU you have a a

2596
01:46:47,599 --> 01:46:52,119
function then GPU you have a a kernel

2597
01:46:50,199 --> 01:46:54,040
which is a GPU function that can be

2598
01:46:52,119 --> 01:46:56,400
parallelized

2599
01:46:54,040 --> 01:46:58,840
um and that's that's the main intuition

2600
01:46:56,400 --> 01:47:00,960
there is you you start off with CPU move

2601
01:46:58,840 --> 01:47:02,800
everything do everything really fast on

2602
01:47:00,960 --> 01:47:04,280
GPU and then once you're done with the

2603
01:47:02,800 --> 01:47:05,639
results you move them back and then do

2604
01:47:04,280 --> 01:47:07,320
whatever you want with them from there

2605
01:47:05,639 --> 01:47:09,280
you might even continue to just feed it

2606
01:47:07,320 --> 01:47:11,920
more into into more and more kernels

2607
01:47:09,280 --> 01:47:13,360
until the whole thing is done right uh

2608
01:47:11,920 --> 01:47:16,400
but that's like that's the that's the

2609
01:47:13,360 --> 01:47:18,360
ideal workflow is you have CPU and then

2610
01:47:16,400 --> 01:47:20,159
this GPU thing is like an intermediary

2611
01:47:18,360 --> 01:47:24,639
which you have to convert back to CPU to

2612
01:47:20,159 --> 01:47:26,639
do something useful with um

2613
01:47:24,639 --> 01:47:28,040
the kernel looks like a Serial program

2614
01:47:26,639 --> 01:47:29,679
so if you we're going to look at these

2615
01:47:28,040 --> 01:47:32,560
in a second here when we jump into

2616
01:47:29,679 --> 01:47:35,040
actually writing these but uh it's it's

2617
01:47:32,560 --> 01:47:38,639
going to be a very simple

2618
01:47:35,040 --> 01:47:40,920
function and it's going to it's going to

2619
01:47:38,639 --> 01:47:42,480
have a very few lines uh it's going to

2620
01:47:40,920 --> 01:47:44,119
it's going to look like this basic

2621
01:47:42,480 --> 01:47:47,480
serial script except it's going to have

2622
01:47:44,119 --> 01:47:49,480
some key terms in it um these are mainly

2623
01:47:47,480 --> 01:47:50,719
threads blocks and grids don't even

2624
01:47:49,480 --> 01:47:51,800
worry about those terms right now we're

2625
01:47:50,719 --> 01:47:54,119
going to get into those I'm going to

2626
01:47:51,800 --> 01:47:56,520
explain the philosophy behind them uh

2627
01:47:54,119 --> 01:47:58,280
I'm going to explain uh pretty much the

2628
01:47:56,520 --> 01:48:00,000
whole Cuda architecture for you and just

2629
01:47:58,280 --> 01:48:02,520
help you understand what the heck these

2630
01:48:00,000 --> 01:48:04,239
things are for now some common terms to

2631
01:48:02,520 --> 01:48:07,840
remember before we actually start

2632
01:48:04,239 --> 01:48:09,679
jumping into this stuff are uh well

2633
01:48:07,840 --> 01:48:11,840
first of all kernels so kernels is like

2634
01:48:09,679 --> 01:48:15,239
a weird term um you might have thought

2635
01:48:11,840 --> 01:48:16,800
like popcorn kernels like like this is

2636
01:48:15,239 --> 01:48:18,840
what I thought I was like what popcorn

2637
01:48:16,800 --> 01:48:21,639
Kels why are we using those on on

2638
01:48:18,840 --> 01:48:23,400
computers that doesn't make sense um and

2639
01:48:21,639 --> 01:48:25,599
then I jumped over to convolution

2640
01:48:23,400 --> 01:48:27,239
kernels which is like uh when you do

2641
01:48:25,599 --> 01:48:28,960
like a convolution operation you might

2642
01:48:27,239 --> 01:48:30,320
have seen this in like cnns if you've

2643
01:48:28,960 --> 01:48:31,840
done a lot of stuff in like maybe

2644
01:48:30,320 --> 01:48:34,440
pytorch and you've like look through the

2645
01:48:31,840 --> 01:48:36,599
intuition on that it's like a a sliding

2646
01:48:34,440 --> 01:48:39,480
kernel that does a that does like an

2647
01:48:36,599 --> 01:48:42,520
image processing thing on on yeah just

2648
01:48:39,480 --> 01:48:44,199
images um that that the filter that

2649
01:48:42,520 --> 01:48:46,679
slides and does calculations that's

2650
01:48:44,199 --> 01:48:48,639
called a kernel so I was like uh is it

2651
01:48:46,679 --> 01:48:50,960
that no no it's not it's not a

2652
01:48:48,639 --> 01:48:53,480
convolution kernel uh it's also not a

2653
01:48:50,960 --> 01:48:54,920
Linux kernel either but enough so

2654
01:48:53,480 --> 01:48:57,119
there's lots of different kernels we

2655
01:48:54,920 --> 01:48:58,719
have there's actually four kernels

2656
01:48:57,119 --> 01:49:01,360
popcorn kernels convolution kernels

2657
01:48:58,719 --> 01:49:02,760
Linux kernels uh but the best one is GPU

2658
01:49:01,360 --> 01:49:04,800
kernels so that's the ones we're going

2659
01:49:02,760 --> 01:49:06,480
to be working with um there's actually a

2660
01:49:04,800 --> 01:49:09,760
little keyword that you highlight you

2661
01:49:06,480 --> 01:49:12,360
goore Global uncore uncore and that

2662
01:49:09,760 --> 01:49:13,960
defines a a kernel on the GPU so there's

2663
01:49:12,360 --> 01:49:17,199
actually a way we can explicitly say

2664
01:49:13,960 --> 01:49:19,320
that and uh yeah not not an external

2665
01:49:17,199 --> 01:49:21,960
story I thought the same thing too it's

2666
01:49:19,320 --> 01:49:23,400
like which one is it um but yeah so

2667
01:49:21,960 --> 01:49:25,560
we're going to go into two threads

2668
01:49:23,400 --> 01:49:27,920
blocks and grids that's going to be one

2669
01:49:25,560 --> 01:49:30,679
of the main things in the next chapter

2670
01:49:27,920 --> 01:49:34,920
um and then two more like sort of just

2671
01:49:30,679 --> 01:49:37,800
lingo terms are gem so G mm uh this

2672
01:49:34,920 --> 01:49:41,280
stands for General matrix multiplication

2673
01:49:37,800 --> 01:49:44,080
so what this generally means is uh you

2674
01:49:41,280 --> 01:49:47,880
have it's not just multiplying like a

2675
01:49:44,080 --> 01:49:52,199
and a * bals C it's not it's not a mapal

2676
01:49:47,880 --> 01:49:54,480
um entirely you actually do a ml um and

2677
01:49:52,199 --> 01:49:56,360
then you have this Alpha you have this

2678
01:49:54,480 --> 01:49:59,400
Alpha parameter which you scale the

2679
01:49:56,360 --> 01:50:03,000
result of that by uh and then you add it

2680
01:49:59,400 --> 01:50:05,199
to uh this beta scaler times this times

2681
01:50:03,000 --> 01:50:07,320
The Matrix C which is the shape of the

2682
01:50:05,199 --> 01:50:09,000
output Matrix so that's that's like a

2683
01:50:07,320 --> 01:50:11,320
lot of linear algebra which I'm not

2684
01:50:09,000 --> 01:50:12,880
going to cover right now but in case

2685
01:50:11,320 --> 01:50:16,239
that's like in case that makes sense to

2686
01:50:12,880 --> 01:50:19,199
you it's essentially this Alpha time uh

2687
01:50:16,239 --> 01:50:22,159
time mL of A and B and then you add that

2688
01:50:19,199 --> 01:50:26,639
to a scalar B * C which is the shape of

2689
01:50:22,159 --> 01:50:29,440
that Matrix um that that's what a gem is

2690
01:50:26,639 --> 01:50:31,800
so it's it's a mmal but with more uh and

2691
01:50:29,440 --> 01:50:34,199
then you have S gem so that's just

2692
01:50:31,800 --> 01:50:36,000
general MMO but with but with uh single

2693
01:50:34,199 --> 01:50:38,920
Precision so it's explicitly single

2694
01:50:36,000 --> 01:50:40,000
Precision um you can do a a half M Mo so

2695
01:50:38,920 --> 01:50:42,639
like an

2696
01:50:40,000 --> 01:50:45,079
fp6 uh you can do double mol you

2697
01:50:42,639 --> 01:50:46,599
typically don't though it's like FP

2698
01:50:45,079 --> 01:50:52,040
fp64

2699
01:50:46,599 --> 01:50:55,440
um but yeah so generally speaking um gem

2700
01:50:52,040 --> 01:50:58,920
s gem those are important then you have

2701
01:50:55,440 --> 01:51:02,440
the CPU which is also called The Host

2702
01:50:58,920 --> 01:51:05,440
which runs functions versus the GPU

2703
01:51:02,440 --> 01:51:07,280
which is called the device and it runs

2704
01:51:05,440 --> 01:51:10,440
kernels

2705
01:51:07,280 --> 01:51:12,480
um and that that's pretty much it I hope

2706
01:51:10,440 --> 01:51:14,400
I hope that wasn't too hard uh we're

2707
01:51:12,480 --> 01:51:17,040
going to dig into some kernels now this

2708
01:51:14,400 --> 01:51:18,719
this part's going to be uh a little bit

2709
01:51:17,040 --> 01:51:19,840
it's going to be a little bit intensive

2710
01:51:18,719 --> 01:51:22,159
you'll need to pay attention a little

2711
01:51:19,840 --> 01:51:24,000
bit but uh it's going to be fun and I

2712
01:51:22,159 --> 01:51:26,320
promised by the end uh you're going to

2713
01:51:24,000 --> 01:51:28,480
be really enlightened you're going to

2714
01:51:26,320 --> 01:51:29,920
like the first part of Cuda isn't

2715
01:51:28,480 --> 01:51:31,840
actually that hard we're just going to

2716
01:51:29,920 --> 01:51:33,159
cover very very basic kernels like

2717
01:51:31,840 --> 01:51:35,159
vector addition it's not going to be

2718
01:51:33,159 --> 01:51:36,599
that bad at all um but just to introduce

2719
01:51:35,159 --> 01:51:38,880
you to the philosophy and the whole

2720
01:51:36,599 --> 01:51:41,239
design principles of like basic uh Cuda

2721
01:51:38,880 --> 01:51:41,239
seat

2722
01:51:41,560 --> 01:51:45,840
programming okay so now things are going

2723
01:51:44,000 --> 01:51:48,119
to get a little bit more technical but I

2724
01:51:45,840 --> 01:51:50,360
figured we would kind of enter smoothly

2725
01:51:48,119 --> 01:51:52,119
by just doing a fun and useful activity

2726
01:51:50,360 --> 01:51:55,119
so I pulled up a bunch of Wikipedia

2727
01:51:52,119 --> 01:51:56,719
articles on various GPU architectures

2728
01:51:55,119 --> 01:51:58,400
then we're going to dive into what does

2729
01:51:56,719 --> 01:52:02,480
your GPU actually look like like what

2730
01:51:58,400 --> 01:52:04,320
what are the stats of that um so just

2731
01:52:02,480 --> 01:52:05,679
like looking at these in general let's

2732
01:52:04,320 --> 01:52:07,320
you know we're just looking for things

2733
01:52:05,679 --> 01:52:09,800
that are like useful to know some maybe

2734
01:52:07,320 --> 01:52:11,840
some little history kind of like the uh

2735
01:52:09,800 --> 01:52:13,599
intro to GPU section that we previously

2736
01:52:11,840 --> 01:52:16,599
did

2737
01:52:13,599 --> 01:52:18,520
um so like Pascal was an older one that

2738
01:52:16,599 --> 01:52:20,280
we used to have um there's a bunch of

2739
01:52:18,520 --> 01:52:23,560
cool stuff about this so like the 1080

2740
01:52:20,280 --> 01:52:26,159
and the and the and the 10 and the 10 70

2741
01:52:23,560 --> 01:52:27,560
uh we both based off Pascal um you know

2742
01:52:26,159 --> 01:52:29,760
you have a bunch of information about

2743
01:52:27,560 --> 01:52:34,159
you know where it's from all of this um

2744
01:52:29,760 --> 01:52:36,639
but what's really cool is um if we

2745
01:52:34,159 --> 01:52:38,119
scroll down you have all the technical

2746
01:52:36,639 --> 01:52:41,599
details on these things it's crazy how

2747
01:52:38,119 --> 01:52:42,880
much Wikipedia has um but yeah like you

2748
01:52:41,599 --> 01:52:44,760
we have these

2749
01:52:42,880 --> 01:52:47,040
tables that

2750
01:52:44,760 --> 01:52:49,320
will that will essentially tell us like

2751
01:52:47,040 --> 01:52:53,360
which which generation which Generations

2752
01:52:49,320 --> 01:52:56,280
had what so like you know texture cast

2753
01:52:53,360 --> 01:52:59,880
per SM which we'll go into later

2754
01:52:56,280 --> 01:53:02,280
um dedicated shared memory per uh SM or

2755
01:52:59,880 --> 01:53:04,119
streaming multiprocessor L2 cach per

2756
01:53:02,280 --> 01:53:05,679
chip right so you have all these

2757
01:53:04,119 --> 01:53:07,960
statistics and you can sort of compare

2758
01:53:05,679 --> 01:53:10,639
these over time so if we jump up to

2759
01:53:07,960 --> 01:53:14,239
emper which is actually what my GPU my

2760
01:53:10,639 --> 01:53:19,000
RTX 3070 is based off of

2761
01:53:14,239 --> 01:53:23,320
um you scroll down and uh you get the

2762
01:53:19,000 --> 01:53:26,880
same thing right so like L2 cache um and

2763
01:53:23,320 --> 01:53:31,400
then L2 cach so like 512 kilobytes and

2764
01:53:26,880 --> 01:53:34,520
then this one is um 40 megabytes right

2765
01:53:31,400 --> 01:53:37,760
so you get some interesting comparisons

2766
01:53:34,520 --> 01:53:40,719
here but

2767
01:53:37,760 --> 01:53:42,560
um yeah these this is just kind of the

2768
01:53:40,719 --> 01:53:44,599
stuff you want to be looking at uh when

2769
01:53:42,560 --> 01:53:46,520
it comes to like GPU specs especially if

2770
01:53:44,599 --> 01:53:49,719
you don't have one right you're going to

2771
01:53:46,520 --> 01:53:51,679
find a lot of useful information here um

2772
01:53:49,719 --> 01:53:53,360
you know going to Ampere like you have

2773
01:53:51,679 --> 01:53:56,280
these a100 s that have been used to

2774
01:53:53,360 --> 01:53:58,280
train very big models uh and that's it's

2775
01:53:56,280 --> 01:54:00,960
Amper 100 right that's that's what it's

2776
01:53:58,280 --> 01:54:04,159
called um

2777
01:54:00,960 --> 01:54:06,199
so bunch of cool statistics um which

2778
01:54:04,159 --> 01:54:08,679
different precisions and data types do

2779
01:54:06,199 --> 01:54:13,119
they support so

2780
01:54:08,679 --> 01:54:17,239
like the uh like for example

2781
01:54:13,119 --> 01:54:17,239
Volta um Volta

2782
01:54:18,079 --> 01:54:22,920
supports Volta doesn't support brain

2783
01:54:20,440 --> 01:54:24,639
float 16 but a100 does support brain

2784
01:54:22,920 --> 01:54:26,320
flat 16 so interesting stuff like that

2785
01:54:24,639 --> 01:54:29,320
which you can sort of just do a side

2786
01:54:26,320 --> 01:54:31,400
comparison of AD love lace this micro

2787
01:54:29,320 --> 01:54:34,239
architecture is what is used in the 40

2788
01:54:31,400 --> 01:54:38,360
series cards so ere is like the 30

2789
01:54:34,239 --> 01:54:41,199
series um I believe if we actually go to

2790
01:54:38,360 --> 01:54:41,199
Volta

2791
01:54:41,719 --> 01:54:46,360
architecture uh Volta

2792
01:54:44,440 --> 01:54:50,520
microarchitecture um I believe this is

2793
01:54:46,360 --> 01:54:53,040
used in the the 20 series

2794
01:54:50,520 --> 01:54:54,960
cards the 20

2795
01:54:53,040 --> 01:54:57,920
it's going to be

2796
01:54:54,960 --> 01:55:00,920
somewhere okay maybe

2797
01:54:57,920 --> 01:55:02,840
not

2798
01:55:00,920 --> 01:55:04,880
anyways you can find a bunch of cool

2799
01:55:02,840 --> 01:55:06,760
statistics on these a love La is the 40

2800
01:55:04,880 --> 01:55:09,639
series cards you get a bunch of info on

2801
01:55:06,760 --> 01:55:11,480
that like the the L2 cache it's again

2802
01:55:09,639 --> 01:55:14,440
bigger instead of 40 megabytes around

2803
01:55:11,480 --> 01:55:15,960
here it's 96 which is great um L1 cache

2804
01:55:14,440 --> 01:55:19,280
is actually what matters more so you

2805
01:55:15,960 --> 01:55:21,960
know like 18 megabytes and and and so

2806
01:55:19,280 --> 01:55:24,599
forth um then Hopper which is actually

2807
01:55:21,960 --> 01:55:27,560
what the state-of-the-art gpus right now

2808
01:55:24,599 --> 01:55:29,040
or close to state-ofthe-art is well the

2809
01:55:27,560 --> 01:55:31,840
state-ofthe-art is actually the

2810
01:55:29,040 --> 01:55:33,800
Blackwell uh micro architecture but the

2811
01:55:31,840 --> 01:55:35,920
hopper is like also very like second

2812
01:55:33,800 --> 01:55:38,159
most recent one and these are what the

2813
01:55:35,920 --> 01:55:41,239
h100s are based on these are used to

2814
01:55:38,159 --> 01:55:43,360
train models like gbt 4 Etc uh so you

2815
01:55:41,239 --> 01:55:45,599
can you find a bunch of statistics on

2816
01:55:43,360 --> 01:55:49,480
those without actually going and using

2817
01:55:45,599 --> 01:55:52,079
one um but if we actually want to uh

2818
01:55:49,480 --> 01:55:54,800
print some stuff about our GPU I'm just

2819
01:55:52,079 --> 01:55:57,440
going to open a new terminal tab here um

2820
01:55:54,800 --> 01:56:02,440
I'm going to drag this to the side if we

2821
01:55:57,440 --> 01:56:04,360
pop over to um Cuda samples

2822
01:56:02,440 --> 01:56:05,239
GitHub this is going to print some stuff

2823
01:56:04,360 --> 01:56:06,800
about your

2824
01:56:05,239 --> 01:56:11,719
GPU

2825
01:56:06,800 --> 01:56:14,040
so if we uh take this and we just just

2826
01:56:11,719 --> 01:56:16,400
get clone

2827
01:56:14,040 --> 01:56:17,880
it it's going to take a second to do

2828
01:56:16,400 --> 01:56:20,119
that but we can scroll down in the

2829
01:56:17,880 --> 01:56:23,280
meantime and we can see uh if you're on

2830
01:56:20,119 --> 01:56:25,520
Linux you would you would say CD into

2831
01:56:23,280 --> 01:56:28,639
your whatever directory you desire and

2832
01:56:25,520 --> 01:56:30,560
then and then make uh to actually make

2833
01:56:28,639 --> 01:56:32,360
the binaries for running stuff with it

2834
01:56:30,560 --> 01:56:34,560
so if we print this out here let me

2835
01:56:32,360 --> 01:56:37,040
actually make

2836
01:56:34,560 --> 01:56:39,360
this bit

2837
01:56:37,040 --> 01:56:44,520
bigger

2838
01:56:39,360 --> 01:56:48,239
um we can CD into Cuda samples and then

2839
01:56:44,520 --> 01:56:50,400
inside of here we have uh samples so we

2840
01:56:48,239 --> 01:56:53,040
go into our sample directory as seen

2841
01:56:50,400 --> 01:56:55,079
there so CD into samples

2842
01:56:53,040 --> 01:56:57,679
and then we have

2843
01:56:55,079 --> 01:56:59,119
um we have a bunch of different ones so

2844
01:56:57,679 --> 01:57:00,800
there's things you can experiment with

2845
01:56:59,119 --> 01:57:02,440
here I don't know how how easy these are

2846
01:57:00,800 --> 01:57:05,400
to use I haven't played with them yet

2847
01:57:02,440 --> 01:57:06,920
but we're going to CD into utilities and

2848
01:57:05,400 --> 01:57:09,639
notice how we have this device query

2849
01:57:06,920 --> 01:57:12,639
thing here so this is actually going to

2850
01:57:09,639 --> 01:57:17,440
turn into a

2851
01:57:12,639 --> 01:57:17,440
um we can't execute that yet but if we

2852
01:57:17,599 --> 01:57:23,719
make then we actually can and we can see

2853
01:57:20,560 --> 01:57:24,920
a bunch of details about our GPU so I

2854
01:57:23,719 --> 01:57:28,560
recommend you to do this on your own

2855
01:57:24,920 --> 01:57:29,760
system but uh one cuicable device so

2856
01:57:28,560 --> 01:57:33,440
there's one GPU plugged into my

2857
01:57:29,760 --> 01:57:37,040
motherboard that GPU is the GeForce RTX

2858
01:57:33,440 --> 01:57:39,079
370 the Cuda driver version is 12.5 as

2859
01:57:37,040 --> 01:57:41,119
well as the runtime version uh Cuda

2860
01:57:39,079 --> 01:57:43,920
capability so this is actually very

2861
01:57:41,119 --> 01:57:46,159
important this 8.6 here yours is it

2862
01:57:43,920 --> 01:57:49,199
might be different um you might have the

2863
01:57:46,159 --> 01:57:52,400
same GPU you might not but this 8.6 is

2864
01:57:49,199 --> 01:57:54,239
actually very critical in how we uh we

2865
01:57:52,400 --> 01:57:56,880
know like what is supported on our GPU

2866
01:57:54,239 --> 01:57:59,360
so there might be some operations that

2867
01:57:56,880 --> 01:58:01,639
work and there might be some that aren't

2868
01:57:59,360 --> 01:58:03,239
so if I just drag this um over to the

2869
01:58:01,639 --> 01:58:05,480
side here we we don't need to worry

2870
01:58:03,239 --> 01:58:07,280
about the rest of this as of right now

2871
01:58:05,480 --> 01:58:09,840
maybe some of maybe some of this later

2872
01:58:07,280 --> 01:58:12,360
but uh I'll just I'll just give you that

2873
01:58:09,840 --> 01:58:16,639
information

2874
01:58:12,360 --> 01:58:19,040
so if we go to the uh

2875
01:58:16,639 --> 01:58:22,260
Cuda uh

2876
01:58:19,040 --> 01:58:25,199
capability compute capability

2877
01:58:22,260 --> 01:58:28,599
[Music]

2878
01:58:25,199 --> 01:58:28,599
um what's it

2879
01:58:28,760 --> 01:58:37,239
called sure GeForce products um see 8.6

2880
01:58:34,800 --> 01:58:37,239
just like

2881
01:58:38,159 --> 01:58:42,040
that uh I have to go back to the Cuda

2882
01:58:40,400 --> 01:58:43,480
docs to actually get useful stuff about

2883
01:58:42,040 --> 01:58:46,880
this uh

2884
01:58:43,480 --> 01:58:46,880
Cuda Cuda

2885
01:58:48,400 --> 01:58:58,880
docs so if we go to the

2886
01:58:53,840 --> 01:59:03,440
uh Cuda C we'll just sure we'll do Cuda

2887
01:58:58,880 --> 01:59:09,360
C++ um no we're not going to do Cuda

2888
01:59:03,440 --> 01:59:13,760
C++ we're going to go down to uh Cuda C

2889
01:59:09,360 --> 01:59:16,199
Cuda search Cuda C programming guide and

2890
01:59:13,760 --> 01:59:18,880
inside of the cudas C programming guide

2891
01:59:16,199 --> 01:59:22,560
yes

2892
01:59:18,880 --> 01:59:24,320
um capability

2893
01:59:22,560 --> 01:59:27,199
so like this for example this is what

2894
01:59:24,320 --> 01:59:31,440
I'm looking for so thread block clusters

2895
01:59:27,199 --> 01:59:36,320
in in uh two and then you go to

2896
01:59:31,440 --> 01:59:38,840
2.2.1 it's thread block clusters

2897
01:59:36,320 --> 01:59:42,599
um you only get these if you have a

2898
01:59:38,840 --> 01:59:44,400
compute capability 9.0 or higher um so

2899
01:59:42,599 --> 01:59:46,119
the higher the compute capability the

2900
01:59:44,400 --> 01:59:48,599
better uh

2901
01:59:46,119 --> 01:59:50,239
so I cannot actually use thread block

2902
01:59:48,599 --> 01:59:52,239
clusters on mine because the

2903
01:59:50,239 --> 01:59:53,360
architecture doesn't support it these

2904
01:59:52,239 --> 01:59:56,360
are critical things you're going to

2905
01:59:53,360 --> 01:59:57,920
watch out for and you know as you you

2906
01:59:56,360 --> 01:59:59,840
might actually be able to take advantage

2907
01:59:57,920 --> 02:00:01,800
of some features that someone else can't

2908
01:59:59,840 --> 02:00:04,079
like if you have an a100 and someone

2909
02:00:01,800 --> 02:00:05,360
else has a v00 you can actually do

2910
02:00:04,079 --> 02:00:07,159
things that they can't and you can do

2911
02:00:05,360 --> 02:00:08,400
things faster and more efficiently

2912
02:00:07,159 --> 02:00:10,440
because of things that the architecture

2913
02:00:08,400 --> 02:00:13,480
actually supports so these are these are

2914
02:00:10,440 --> 02:00:16,320
things you're going to watch out for um

2915
02:00:13,480 --> 02:00:18,719
but anyways uh with that being said we

2916
02:00:16,320 --> 02:00:21,079
can actually jump into uh some stuff

2917
02:00:18,719 --> 02:00:23,880
about the just essentially how does the

2918
02:00:21,079 --> 02:00:25,760
Cuda architecture work how does how how

2919
02:00:23,880 --> 02:00:27,599
do we write code and and how do how does

2920
02:00:25,760 --> 02:00:30,320
that whole thing fit

2921
02:00:27,599 --> 02:00:32,440
together so now we can actually get

2922
02:00:30,320 --> 02:00:35,000
really into what Cuda is doing and the

2923
02:00:32,440 --> 02:00:37,400
whole hierarchy of that so inside of

2924
02:00:35,000 --> 02:00:40,520
here I've pulled up chapter 5 writing

2925
02:00:37,400 --> 02:00:42,400
your own kernels um and then Cuda Basics

2926
02:00:40,520 --> 02:00:45,840
and then just the readme and the ID

2927
02:00:42,400 --> 02:00:49,119
exing or indexing file so if you do I

2928
02:00:45,840 --> 02:00:52,639
believe control shift V on this

2929
02:00:49,119 --> 02:00:56,040
or control I don't know what keybind it

2930
02:00:52,639 --> 02:00:59,599
is contrl alt V contrl shift V there we

2931
02:00:56,040 --> 02:01:02,280
go okay um and then we pull up this one

2932
02:00:59,599 --> 02:01:04,800
on the side here just uh just for

2933
02:01:02,280 --> 02:01:08,119
reference let's go through this sort of

2934
02:01:04,800 --> 02:01:11,800
hand in hand

2935
02:01:08,119 --> 02:01:11,800
so I'm going to zoom

2936
02:01:11,880 --> 02:01:15,560
in we just printed this out we just

2937
02:01:14,119 --> 02:01:18,480
printed out device query so we don't

2938
02:01:15,560 --> 02:01:21,159
need to really cover that um but when it

2939
02:01:18,480 --> 02:01:22,599
comes to sort of the more easy stuff to

2940
02:01:21,159 --> 02:01:24,119
get a grasp on I mean we already went

2941
02:01:22,599 --> 02:01:26,280
over this so you have the host which is

2942
02:01:24,119 --> 02:01:28,280
the CPU and uses those RAM sticks on

2943
02:01:26,280 --> 02:01:31,119
your motherboard and then the device or

2944
02:01:28,280 --> 02:01:34,239
the GPU uses the onchip vram or video

2945
02:01:31,119 --> 02:01:39,079
memory um for desktop

2946
02:01:34,239 --> 02:01:42,239
PCS the the surface level run time

2947
02:01:39,079 --> 02:01:46,159
typically goes um you C you you define

2948
02:01:42,239 --> 02:01:47,760
some input on the host or the CPU uh

2949
02:01:46,159 --> 02:01:49,560
which you then later want to run on the

2950
02:01:47,760 --> 02:01:52,679
GPU but in first you have to actually

2951
02:01:49,560 --> 02:01:55,320
Define it on the whole system memory

2952
02:01:52,679 --> 02:01:59,560
and then you would copy that over to GPU

2953
02:01:55,320 --> 02:02:02,599
memory um and then you would uh and then

2954
02:01:59,560 --> 02:02:04,119
you would execute using that on GPU

2955
02:02:02,599 --> 02:02:06,079
memory you would execute you you would

2956
02:02:04,119 --> 02:02:09,239
launch a Cuda kernel and that Cuda

2957
02:02:06,079 --> 02:02:11,159
kernel would use that uh GPU memory and

2958
02:02:09,239 --> 02:02:13,520
do stuff with it and maybe do do some

2959
02:02:11,159 --> 02:02:15,920
useful computation and then once that's

2960
02:02:13,520 --> 02:02:17,639
done uh you would ensure everything is

2961
02:02:15,920 --> 02:02:19,119
all synchronized up like nothing is

2962
02:02:17,639 --> 02:02:21,000
nothing is still waiting you would you

2963
02:02:19,119 --> 02:02:23,599
would synchronize everything and then

2964
02:02:21,000 --> 02:02:25,199
you would transfer it back to CPU so

2965
02:02:23,599 --> 02:02:26,639
that or or the host so that you can you

2966
02:02:25,199 --> 02:02:28,639
know print it out or do something useful

2967
02:02:26,639 --> 02:02:30,679
with it this is typically how the the

2968
02:02:28,639 --> 02:02:34,760
runtime goes um and you'll see this in

2969
02:02:30,679 --> 02:02:36,719
our later chapters um the naming scheme

2970
02:02:34,760 --> 02:02:39,760
like how we actually um what what we

2971
02:02:36,719 --> 02:02:41,719
actually name our pieces of data is

2972
02:02:39,760 --> 02:02:45,040
quite critical so typically what you'll

2973
02:02:41,719 --> 02:02:47,040
do is you go host or H and then

2974
02:02:45,040 --> 02:02:50,280
underscore whatever the variable name is

2975
02:02:47,040 --> 02:02:52,159
so if it's like Matrix a you do hore

2976
02:02:50,280 --> 02:02:55,000
Matrix a that means it's defined on the

2977
02:02:52,159 --> 02:02:59,040
host so you're going to do your your

2978
02:02:55,000 --> 02:03:01,320
Malik your your your C Malik um with

2979
02:02:59,040 --> 02:03:03,400
this and then you're going to do a Cuda

2980
02:03:01,320 --> 02:03:06,079
mem copy which we will see later in a

2981
02:03:03,400 --> 02:03:07,400
second um and that that's where you take

2982
02:03:06,079 --> 02:03:10,360
this host and you you essentially

2983
02:03:07,400 --> 02:03:13,960
transfer it over to this other to this

2984
02:03:10,360 --> 02:03:17,239
other variable uh device so device a

2985
02:03:13,960 --> 02:03:19,159
that's the GPU uh version of of that so

2986
02:03:17,239 --> 02:03:22,480
it just exists on two different pieces

2987
02:03:19,159 --> 02:03:25,440
of memory um and this is just for the

2988
02:03:22,480 --> 02:03:28,239
set variable name a now we have this

2989
02:03:25,440 --> 02:03:31,639
Global which you might have seen already

2990
02:03:28,239 --> 02:03:33,840
um this is visible globally and this is

2991
02:03:31,639 --> 02:03:35,880
very broad this is this is typically

2992
02:03:33,840 --> 02:03:39,599
what a kernel is going to look like uh

2993
02:03:35,880 --> 02:03:42,360
unless you are calling uh say a separate

2994
02:03:39,599 --> 02:03:45,679
uh calling a separate kernel inside of

2995
02:03:42,360 --> 02:03:47,440
another one you would use say device um

2996
02:03:45,679 --> 02:03:49,960
but in this example we'll just stick

2997
02:03:47,440 --> 02:03:51,360
with global um you know you can read a

2998
02:03:49,960 --> 02:03:52,400
littleit little bit more into this if

2999
02:03:51,360 --> 02:03:54,159
you want

3000
02:03:52,400 --> 02:03:56,480
but we're going to use Global for the

3001
02:03:54,159 --> 02:03:57,760
most part device we might see that later

3002
02:03:56,480 --> 02:04:01,000
in the

3003
02:03:57,760 --> 02:04:03,719
course and then host uh is only going to

3004
02:04:01,000 --> 02:04:06,360
run on CPU so uh don't don't really

3005
02:04:03,719 --> 02:04:07,960
worry about that um it's it's kind of

3006
02:04:06,360 --> 02:04:10,400
just telling Cuda that you're going to

3007
02:04:07,960 --> 02:04:11,760
run on on the on the CPU but you you may

3008
02:04:10,400 --> 02:04:13,400
not actually need to because you're just

3009
02:04:11,760 --> 02:04:16,880
going to use you know like the void

3010
02:04:13,400 --> 02:04:19,960
instead of instead of global void right

3011
02:04:16,880 --> 02:04:23,360
um now this this Cuda malic

3012
02:04:19,960 --> 02:04:25,280
term memory allocation on the GP vram so

3013
02:04:23,360 --> 02:04:29,079
that's the global memory on the GPU

3014
02:04:25,280 --> 02:04:32,760
itself so in this example you do you

3015
02:04:29,079 --> 02:04:35,840
know Define a bunch of uh a bunch of uh

3016
02:04:32,760 --> 02:04:42,040
essenti essentially arrays so a pointer

3017
02:04:35,840 --> 02:04:44,119
to a device uh a float array which is a

3018
02:04:42,040 --> 02:04:46,639
pointer on the

3019
02:04:44,119 --> 02:04:50,159
device uh and it's for a and then the

3020
02:04:46,639 --> 02:04:53,079
same thing for B and C now you do kudam

3021
02:04:50,159 --> 02:04:55,719
Malik meaning you allocate it on the GPU

3022
02:04:53,079 --> 02:04:57,559
so you do the memory address for that so

3023
02:04:55,719 --> 02:04:59,440
you put in the memory address for for

3024
02:04:57,559 --> 02:05:02,239
this thing

3025
02:04:59,440 --> 02:05:03,960
um and then you go essentially whatever

3026
02:05:02,239 --> 02:05:06,719
the you know let's just say you have a

3027
02:05:03,960 --> 02:05:08,599
size defined above right uh like it's

3028
02:05:06,719 --> 02:05:11,040
maybe it's like a matrix for example and

3029
02:05:08,599 --> 02:05:14,040
it's size it's like a you know Square

3030
02:05:11,040 --> 02:05:15,920
Matrix size n byn and all you're going

3031
02:05:14,040 --> 02:05:18,239
to do is say you know we want to

3032
02:05:15,920 --> 02:05:20,639
allocate this much memory uh this memory

3033
02:05:18,239 --> 02:05:23,280
address and that's just going to be a

3034
02:05:20,639 --> 02:05:26,840
square Matrix of let's say you know 128

3035
02:05:23,280 --> 02:05:30,199
* 128 time the size of whatever a float

3036
02:05:26,840 --> 02:05:33,639
is so in that case I think it'll be four

3037
02:05:30,199 --> 02:05:35,480
because a float is uh four bytes where

3038
02:05:33,639 --> 02:05:37,880
each B is eight bits you have a floating

3039
02:05:35,480 --> 02:05:40,599
Point 32 number do the math and then you

3040
02:05:37,880 --> 02:05:43,000
end up with the total amount of uh bytes

3041
02:05:40,599 --> 02:05:45,920
that you will need to allocate for uh

3042
02:05:43,000 --> 02:05:49,119
this device uh a matrix and so on

3043
02:05:45,920 --> 02:05:52,320
through b and c as well

3044
02:05:49,119 --> 02:05:54,880
um now Cuda M Copy can copy from both

3045
02:05:52,320 --> 02:05:59,119
device to host host to device or device

3046
02:05:54,880 --> 02:06:01,880
to device for edge cases so um you know

3047
02:05:59,119 --> 02:06:05,360
you would you would slide a little term

3048
02:06:01,880 --> 02:06:08,400
in here camem copy host to device or CM

3049
02:06:05,360 --> 02:06:10,239
copy device to host um and that's that's

3050
02:06:08,400 --> 02:06:11,960
how that would go um we're actually

3051
02:06:10,239 --> 02:06:15,599
going to see usage of this in a second

3052
02:06:11,960 --> 02:06:17,079
here but um understanding camm copy is

3053
02:06:15,599 --> 02:06:19,199
just going to just going to copy things

3054
02:06:17,079 --> 02:06:20,360
around and then Cuda free is obviously

3055
02:06:19,199 --> 02:06:22,079
just going to free memory up on the

3056
02:06:20,360 --> 02:06:23,880
device so when you you're done with

3057
02:06:22,079 --> 02:06:26,159
something or you don't need it anymore

3058
02:06:23,880 --> 02:06:28,000
just you know free that up if it's a big

3059
02:06:26,159 --> 02:06:30,199
if it's a big uh you know if it's just

3060
02:06:28,000 --> 02:06:33,159
like an integer or whatever like just a

3061
02:06:30,199 --> 02:06:34,320
float um like a float a equals 1 or

3062
02:06:33,159 --> 02:06:36,239
something it's like you don't need to

3063
02:06:34,320 --> 02:06:37,400
free that um but if it's a big array

3064
02:06:36,239 --> 02:06:41,000
like this you're going to need to free

3065
02:06:37,400 --> 02:06:44,679
that um

3066
02:06:41,000 --> 02:06:46,400
now the nvcc compiler is something we'll

3067
02:06:44,679 --> 02:06:49,119
dig into maybe a little more in the

3068
02:06:46,400 --> 02:06:53,719
future um but this is all you really

3069
02:06:49,119 --> 02:06:58,239
need to know so the host code is uh

3070
02:06:53,719 --> 02:07:01,400
essentially the these nvcc will compile

3071
02:06:58,239 --> 02:07:04,480
all of this down into uh something that

3072
02:07:01,400 --> 02:07:08,199
the GPU can actually execute but the CPU

3073
02:07:04,480 --> 02:07:09,960
is going to run it so uh CPU is going to

3074
02:07:08,199 --> 02:07:12,239
interpret what that is saying and it's

3075
02:07:09,960 --> 02:07:14,159
going to launch things and and tell the

3076
02:07:12,239 --> 02:07:18,960
GPU to do things it's not just going to

3077
02:07:14,159 --> 02:07:20,920
compile directly down to GPU right so uh

3078
02:07:18,960 --> 02:07:22,559
when it needs to run on GPU when when it

3079
02:07:20,920 --> 02:07:24,400
actually needs those instructions as to

3080
02:07:22,559 --> 02:07:26,840
what to do it's going to get compiled

3081
02:07:24,400 --> 02:07:28,520
down to PTX which is parallel thread

3082
02:07:26,840 --> 02:07:32,679
execution instructions so that's like

3083
02:07:28,520 --> 02:07:36,679
the GPU equivalent of x86 or assembly uh

3084
02:07:32,679 --> 02:07:38,520
you know as it is for um CPU or host um

3085
02:07:36,679 --> 02:07:40,440
and and then it's further going to

3086
02:07:38,520 --> 02:07:43,239
compile that down to Shader assembly

3087
02:07:40,440 --> 02:07:44,880
which we're not going to worry about um

3088
02:07:43,239 --> 02:07:47,199
and this is just and this is stable

3089
02:07:44,880 --> 02:07:49,559
across all of the different Nvidia gpus

3090
02:07:47,199 --> 02:07:51,119
so you don't need to worry about that um

3091
02:07:49,559 --> 02:07:52,199
and then just in time is just a type of

3092
02:07:51,119 --> 02:07:55,199
compil

3093
02:07:52,199 --> 02:07:59,920
so

3094
02:07:55,199 --> 02:08:01,119
um Cuda hierarchy yes this is this is

3095
02:07:59,920 --> 02:08:03,000
where things start to get a little bit

3096
02:08:01,119 --> 02:08:05,920
intuitive

3097
02:08:03,000 --> 02:08:09,719
so imagine you have like a imagine you

3098
02:08:05,920 --> 02:08:12,480
have this giant 3D like a cubic volume

3099
02:08:09,719 --> 02:08:14,400
uh and and this volume is called a grid

3100
02:08:12,480 --> 02:08:16,639
right inside of this grid you're going

3101
02:08:14,400 --> 02:08:19,800
to have a bunch of these smaller Cube

3102
02:08:16,639 --> 02:08:22,159
cubic volumes those are called blocks uh

3103
02:08:19,800 --> 02:08:24,639
and those those blocks are organized

3104
02:08:22,159 --> 02:08:27,000
uh you know you can make them whatever

3105
02:08:24,639 --> 02:08:29,079
size you want it's just like essentially

3106
02:08:27,000 --> 02:08:31,159
uh can think of it like a like a like a

3107
02:08:29,079 --> 02:08:32,440
prism or something uh and you have a

3108
02:08:31,159 --> 02:08:34,840
bunch of these organized in this giant

3109
02:08:32,440 --> 02:08:36,639
3D Volume which is the grid and those

3110
02:08:34,840 --> 02:08:38,360
individual blocks have things inside of

3111
02:08:36,639 --> 02:08:40,040
them called threads and those threads

3112
02:08:38,360 --> 02:08:42,400
are going to do your math operations for

3113
02:08:40,040 --> 02:08:46,440
you so there's there's a lot to unpack

3114
02:08:42,400 --> 02:08:48,360
here but the the individual threads um

3115
02:08:46,440 --> 02:08:49,599
can communicate inside of these blocks

3116
02:08:48,360 --> 02:08:52,000
and that's an important part to remember

3117
02:08:49,599 --> 02:08:53,760
for later when we're optimizing stuff

3118
02:08:52,000 --> 02:08:55,760
but essentially the reason why we have

3119
02:08:53,760 --> 02:08:58,280
all these different pieces inside of

3120
02:08:55,760 --> 02:09:01,199
this massive grid is so that we can get

3121
02:08:58,280 --> 02:09:03,199
the parallelism of gpus so when you have

3122
02:09:01,199 --> 02:09:04,599
this block doing this you know doing

3123
02:09:03,199 --> 02:09:05,920
this piece of the puzzle and then this

3124
02:09:04,599 --> 02:09:08,040
doing another piece of the puzzle and

3125
02:09:05,920 --> 02:09:09,880
they all kind of do their part and at

3126
02:09:08,040 --> 02:09:10,960
the end if they all do their part

3127
02:09:09,880 --> 02:09:12,480
successfully and they're all like

3128
02:09:10,960 --> 02:09:15,440
synchronized and you make sure that

3129
02:09:12,480 --> 02:09:17,440
everything works correctly um you know

3130
02:09:15,440 --> 02:09:20,119
it's it's better than having a single

3131
02:09:17,440 --> 02:09:21,920
CPU thread going through each individual

3132
02:09:20,119 --> 02:09:24,040
thing in that problem and doing it one

3133
02:09:21,920 --> 02:09:25,840
by one oras you have like a bunch of

3134
02:09:24,040 --> 02:09:28,639
these blocks or these threads inside of

3135
02:09:25,840 --> 02:09:30,400
blocks which are uh doing you know

3136
02:09:28,639 --> 02:09:32,280
little independent operations it's doing

3137
02:09:30,400 --> 02:09:35,880
a smaller number of operations and they

3138
02:09:32,280 --> 02:09:37,000
have a lower clock speed uh but they're

3139
02:09:35,880 --> 02:09:38,880
it's going to solve the problem much

3140
02:09:37,000 --> 02:09:41,480
quicker because it's in

3141
02:09:38,880 --> 02:09:43,880
parallel so that that's the whole idea

3142
02:09:41,480 --> 02:09:45,639
here is you have this this 3D Volume

3143
02:09:43,880 --> 02:09:48,079
called a grid inside of it you have

3144
02:09:45,639 --> 02:09:50,800
these other uh you have these other 3D

3145
02:09:48,079 --> 02:09:52,800
sort of Cubes or rectangles whatever um

3146
02:09:50,800 --> 02:09:54,239
and inside of those you have threads and

3147
02:09:52,800 --> 02:09:59,639
those threads are going to also do

3148
02:09:54,239 --> 02:10:02,159
things so I need to breathe but we'll go

3149
02:09:59,639 --> 02:10:04,040
into uh some more technical terms in a

3150
02:10:02,159 --> 02:10:07,079
second here so going to these technical

3151
02:10:04,040 --> 02:10:09,840
terms we can see this grid dim exists

3152
02:10:07,079 --> 02:10:13,480
here in our kernel in our Global uh

3153
02:10:09,840 --> 02:10:17,599
kernel we have a block idx which is you

3154
02:10:13,480 --> 02:10:22,400
know these these three um

3155
02:10:17,599 --> 02:10:24,719
and block dim exists uh here and here

3156
02:10:22,400 --> 02:10:28,360
and here and here and here right you

3157
02:10:24,719 --> 02:10:31,679
have all these uh and then thread idx so

3158
02:10:28,360 --> 02:10:35,440
this grid dim is the number of blocks in

3159
02:10:31,679 --> 02:10:39,679
the Grid at you know say like grid dim

3160
02:10:35,440 --> 02:10:43,280
dot X is going to be uh in this in this

3161
02:10:39,679 --> 02:10:44,719
volumetric uh grid what is the X

3162
02:10:43,280 --> 02:10:46,760
dimension of that so like what is the

3163
02:10:44,719 --> 02:10:48,159
length and then grid dim doy is going to

3164
02:10:46,760 --> 02:10:50,440
be like what is the height and then

3165
02:10:48,159 --> 02:10:53,400
maybe grid dim. Z is going to be the

3166
02:10:50,440 --> 02:10:56,960
depth of that right um and then the

3167
02:10:53,400 --> 02:10:59,480
block idx is going to it's not actually

3168
02:10:56,960 --> 02:11:02,960
uh about the block itself but like where

3169
02:10:59,480 --> 02:11:06,119
is it where is the block within the grid

3170
02:11:02,960 --> 02:11:08,719
so the the grid dim is like how long is

3171
02:11:06,119 --> 02:11:10,520
it how what is the what is like the size

3172
02:11:08,719 --> 02:11:14,280
of the grid itself that is run on the

3173
02:11:10,520 --> 02:11:16,199
GPU and then the block idx is where uh

3174
02:11:14,280 --> 02:11:19,880
each individual block is so a block will

3175
02:11:16,199 --> 02:11:22,800
have a block idx in both the uh maybe x

3176
02:11:19,880 --> 02:11:24,480
y and Zed dimension and that'll be

3177
02:11:22,800 --> 02:11:28,599
essentially its coordinates within that

3178
02:11:24,480 --> 02:11:32,040
grid uh and then the block dim is how

3179
02:11:28,599 --> 02:11:34,119
big that block is so grid fits a bunch

3180
02:11:32,040 --> 02:11:36,559
of blocks into it a block fits a bunch

3181
02:11:34,119 --> 02:11:39,119
of threads into it so the block dim is

3182
02:11:36,559 --> 02:11:42,239
like how how big is this like smaller

3183
02:11:39,119 --> 02:11:45,320
Cube or this or this rectangular prism

3184
02:11:42,239 --> 02:11:47,480
um and then the thread idx is like which

3185
02:11:45,320 --> 02:11:48,800
which thread is it within that block so

3186
02:11:47,480 --> 02:11:51,320
you can see how this like spatial

3187
02:11:48,800 --> 02:11:52,960
hierarchy goes down you have this 3D in

3188
02:11:51,320 --> 02:11:54,880
the grid and then this 3D in the block

3189
02:11:52,960 --> 02:11:57,000
so it's like kind of 6D if you think

3190
02:11:54,880 --> 02:11:58,040
about it that way um I don't want to I

3191
02:11:57,000 --> 02:11:59,639
don't want that to be intimidating

3192
02:11:58,040 --> 02:12:00,840
though like six dimensional I don't want

3193
02:11:59,639 --> 02:12:03,079
that to be intimidating it's just kind

3194
02:12:00,840 --> 02:12:05,360
of how the it's it's it's an efficient

3195
02:12:03,079 --> 02:12:06,960
way of of running things in parallel and

3196
02:12:05,360 --> 02:12:09,599
a way of visualizing it as like a

3197
02:12:06,960 --> 02:12:12,239
software abstraction right uh that's

3198
02:12:09,599 --> 02:12:14,280
that's the idea there

3199
02:12:12,239 --> 02:12:17,800
um and

3200
02:12:14,280 --> 02:12:19,760
then threads like I mean I I I assume

3201
02:12:17,800 --> 02:12:21,559
that this kind of this this spatial idea

3202
02:12:19,760 --> 02:12:23,880
sort of makes sense now so now we can go

3203
02:12:21,559 --> 02:12:26,360
into like why this works um and sort of

3204
02:12:23,880 --> 02:12:29,559
like the more nitty-gritty of that so

3205
02:12:26,360 --> 02:12:32,480
each thread um itself has local memory

3206
02:12:29,559 --> 02:12:34,719
on it so registers which are very fast

3207
02:12:32,480 --> 02:12:38,079
and is private to that individual thread

3208
02:12:34,719 --> 02:12:40,800
so for example if you wanted to add um A

3209
02:12:38,079 --> 02:12:43,079
and B where it's like 1 2 3 and then all

3210
02:12:40,800 --> 02:12:45,159
the way up to n which is like the length

3211
02:12:43,079 --> 02:12:46,760
and then 2 4 six so like counting by

3212
02:12:45,159 --> 02:12:49,679
ones and then count B is counting by

3213
02:12:46,760 --> 02:12:53,480
twos um each thread would do a single

3214
02:12:49,679 --> 02:12:57,119
add so like thread at index say zero

3215
02:12:53,480 --> 02:13:00,199
would be like a at the at the thread

3216
02:12:57,119 --> 02:13:03,880
Index right and so the thread index

3217
02:13:00,199 --> 02:13:05,639
itself tells you how to index into data

3218
02:13:03,880 --> 02:13:08,040
and then you can use that element that

3219
02:13:05,639 --> 02:13:10,440
it gets like from its own index from the

3220
02:13:08,040 --> 02:13:12,040
thread index in that whole space and you

3221
02:13:10,440 --> 02:13:14,599
can actually do operations with that so

3222
02:13:12,040 --> 02:13:18,079
it's like a little hack of uh

3223
02:13:14,599 --> 02:13:20,760
essentially both both getting the right

3224
02:13:18,079 --> 02:13:22,800
elements of data uh and adding and doing

3225
02:13:20,760 --> 02:13:27,960
math operations on them at the same time

3226
02:13:22,800 --> 02:13:30,360
so we end up doing you know uh 1 + 2

3227
02:13:27,960 --> 02:13:33,440
right with a single thread and it's in

3228
02:13:30,360 --> 02:13:36,239
and it's accessing this index uh that

3229
02:13:33,440 --> 02:13:37,800
the index the data based on its based on

3230
02:13:36,239 --> 02:13:39,920
the thread's index and it's adding them

3231
02:13:37,800 --> 02:13:43,719
together uh and then same thing for you

3232
02:13:39,920 --> 02:13:46,440
know maybe thread thread two right um so

3233
02:13:43,719 --> 02:13:47,719
that's just like kind of how the whole

3234
02:13:46,440 --> 02:13:49,480
that that's how the whole like uh

3235
02:13:47,719 --> 02:13:52,679
indexing thing pans out that's why it's

3236
02:13:49,480 --> 02:13:55,000
so cool um and then warps and it's kind

3237
02:13:52,679 --> 02:13:57,679
of interesting if you if you look at

3238
02:13:55,000 --> 02:14:01,440
this Wikipedia article it's

3239
02:13:57,679 --> 02:14:04,679
like warps warps and weft right so you

3240
02:14:01,440 --> 02:14:08,559
have um you have these warps that are

3241
02:14:04,679 --> 02:14:08,559
going through so like these these

3242
02:14:08,960 --> 02:14:13,239
uh these these warps that are going

3243
02:14:11,639 --> 02:14:16,079
through it like up and down and then the

3244
02:14:13,239 --> 02:14:17,119
weft is like uh what the the warps are

3245
02:14:16,079 --> 02:14:21,719
weaving through so they're like

3246
02:14:17,119 --> 02:14:25,000
interlocked like this and so uh you you

3247
02:14:21,719 --> 02:14:29,079
you could sort of think of of the uh

3248
02:14:25,000 --> 02:14:30,840
warps as the uh as like what is what is

3249
02:14:29,079 --> 02:14:35,000
going forward so you could say like a

3250
02:14:30,840 --> 02:14:39,079
warp is a group of threads

3251
02:14:35,000 --> 02:14:43,079
um warp and weft uh the vertical warp

3252
02:14:39,079 --> 02:14:46,159
Yarns uh plural are held in stationary

3253
02:14:43,079 --> 02:14:48,679
um and the horizontal we is drawn

3254
02:14:46,159 --> 02:14:50,639
through them so you essentially have all

3255
02:14:48,679 --> 02:14:52,280
these War like a bunch of threads

3256
02:14:50,639 --> 02:14:54,719
essentially it's a bunch of threads that

3257
02:14:52,280 --> 02:14:56,119
are that are going in and out and and

3258
02:14:54,719 --> 02:14:58,280
you can think of these threads as like

3259
02:14:56,119 --> 02:14:59,599
doing their own math operations and you

3260
02:14:58,280 --> 02:15:01,599
have a bunch of them grouped together in

3261
02:14:59,599 --> 02:15:03,400
a warp right that's that's the whole

3262
02:15:01,599 --> 02:15:06,400
idea there

3263
02:15:03,400 --> 02:15:09,320
um so you know like I said in

3264
02:15:06,400 --> 02:15:10,400
the in the in the Wikipedia article warp

3265
02:15:09,320 --> 02:15:14,079
is a set of

3266
02:15:10,400 --> 02:15:15,880
Yarns um set of Yarns or other things

3267
02:15:14,079 --> 02:15:17,400
stretch in place on a loom where the

3268
02:15:15,880 --> 02:15:19,520
weft is introduced during the weaving

3269
02:15:17,400 --> 02:15:21,599
process is regarded as the longitudinal

3270
02:15:19,520 --> 02:15:23,840
set in a in a finished fabric with two

3271
02:15:21,599 --> 02:15:25,280
or more sets of elements I I don't

3272
02:15:23,840 --> 02:15:26,760
expect you to understand that it's just

3273
02:15:25,280 --> 02:15:29,119
like the idea of threads are grouped

3274
02:15:26,760 --> 02:15:31,639
into warps um and then the the whift

3275
02:15:29,119 --> 02:15:33,000
kind of like it's that other uh

3276
02:15:31,639 --> 02:15:38,159
perpendicular

3277
02:15:33,000 --> 02:15:40,679
part um so warps are inside of blocks so

3278
02:15:38,159 --> 02:15:43,400
remember we have the like the the grids

3279
02:15:40,679 --> 02:15:47,159
and then blocks and then threads um

3280
02:15:43,400 --> 02:15:49,320
inside of blocks you have warps which

3281
02:15:47,159 --> 02:15:51,719
take care of threads so the blocks

3282
02:15:49,320 --> 02:15:54,040
themselves aren't entirely handling the

3283
02:15:51,719 --> 02:15:56,360
threads it's actually the warps that are

3284
02:15:54,040 --> 02:15:59,760
doing a lot of that work

3285
02:15:56,360 --> 02:16:02,159
so you typically organize a warp as like

3286
02:15:59,760 --> 02:16:05,679
a group of threads like uh I believe the

3287
02:16:02,159 --> 02:16:07,920
maximum is 32 threads so a warp will

3288
02:16:05,679 --> 02:16:09,440
handle 32 threads at once within a

3289
02:16:07,920 --> 02:16:11,400
block

3290
02:16:09,440 --> 02:16:14,239
um there is no way of getting around

3291
02:16:11,400 --> 02:16:17,520
using warps the warps scheduler makes

3292
02:16:14,239 --> 02:16:20,239
the warps run so uh you could think of

3293
02:16:17,520 --> 02:16:21,480
like maybe the warp scheduler as the as

3294
02:16:20,239 --> 02:16:25,159
like the weft

3295
02:16:21,480 --> 02:16:26,400
right so um it's it's sort of like going

3296
02:16:25,159 --> 02:16:28,480
through and making sure they don't get

3297
02:16:26,400 --> 02:16:29,920
like disentangled and all this and and

3298
02:16:28,480 --> 02:16:31,559
ensuring that everything like works out

3299
02:16:29,920 --> 02:16:34,519
properly you can use whatever analogy

3300
02:16:31,559 --> 02:16:36,399
Works um but the the warp scheduler

3301
02:16:34,519 --> 02:16:39,280
ensures that the warps which are group

3302
02:16:36,399 --> 02:16:41,800
of threads run um and then you would

3303
02:16:39,280 --> 02:16:44,599
have typically four warp schedulers per

3304
02:16:41,800 --> 02:16:47,200
SM and SM is like the smaller like the

3305
02:16:44,599 --> 02:16:48,599
the streaming multiprocessor on chip

3306
02:16:47,200 --> 02:16:51,639
that's what those are and you can have

3307
02:16:48,599 --> 02:16:55,479
four rep schedulers per SM so

3308
02:16:51,639 --> 02:16:59,479
um you know do the math that's 128

3309
02:16:55,479 --> 02:16:59,479
threads per SM

3310
02:17:01,240 --> 02:17:07,880
um then we have blocks so blocks are

3311
02:17:05,040 --> 02:17:09,479
interesting each block has shared memory

3312
02:17:07,880 --> 02:17:11,200
um visible to all threads within that

3313
02:17:09,479 --> 02:17:12,920
thread block so all of these like you

3314
02:17:11,200 --> 02:17:16,120
know thread one can see the same stuff

3315
02:17:12,920 --> 02:17:19,599
that thread 32 can uh they can or or

3316
02:17:16,120 --> 02:17:21,679
even thread like I don't know like 500

3317
02:17:19,599 --> 02:17:24,599
for that matter um

3318
02:17:21,679 --> 02:17:26,359
um they can all see the same data so

3319
02:17:24,599 --> 02:17:28,599
like within a warp they can they can

3320
02:17:26,359 --> 02:17:30,760
kind of see their they can communicate

3321
02:17:28,599 --> 02:17:33,399
faster but within a block they can still

3322
02:17:30,760 --> 02:17:36,519
communicate very fast uh through this

3323
02:17:33,399 --> 02:17:37,920
shared memory which we call the L1 cache

3324
02:17:36,519 --> 02:17:39,840
and I'll dig into that in a second here

3325
02:17:37,920 --> 02:17:43,280
when we go into the next section but the

3326
02:17:39,840 --> 02:17:49,719
L1 cache is very important for Speed and

3327
02:17:43,280 --> 02:17:49,719
optimizing uh kernels so uh

3328
02:17:52,399 --> 02:17:55,960
yeah essentially just the same thing

3329
02:17:53,599 --> 02:18:00,000
what I said uh shared memory shared

3330
02:17:55,960 --> 02:18:02,359
memory is is more efficient um it's

3331
02:18:00,000 --> 02:18:04,200
faster I think the maximum memory

3332
02:18:02,359 --> 02:18:07,359
bandwidth you get with shared memory is

3333
02:18:04,200 --> 02:18:10,519
like on the order of uh like 15

3334
02:18:07,359 --> 02:18:13,760
terabytes per second and then uh Global

3335
02:18:10,519 --> 02:18:15,840
vram so like when I do Nvidia SMI uh

3336
02:18:13,760 --> 02:18:19,200
like

3337
02:18:15,840 --> 02:18:22,719
this you can see that I get

3338
02:18:19,200 --> 02:18:25,559
um like this this this is my this this

3339
02:18:22,719 --> 02:18:28,280
is actually going at like maybe 6 or 700

3340
02:18:25,559 --> 02:18:30,200
gabes a second Shar shared memory is

3341
02:18:28,280 --> 02:18:35,040
like 15 terabytes so it's like really

3342
02:18:30,200 --> 02:18:38,240
fast um and and blocks use uh shared

3343
02:18:35,040 --> 02:18:41,800
memory which is you know uh on an

3344
02:18:38,240 --> 02:18:45,719
individual SM so the SM will handle that

3345
02:18:41,800 --> 02:18:47,639
um and then grids um during the chronal

3346
02:18:45,719 --> 02:18:50,000
execution the threads within the blocks

3347
02:18:47,639 --> 02:18:51,399
within the grid can access Global memory

3348
02:18:50,000 --> 02:18:53,000
um so that's just like universally

3349
02:18:51,399 --> 02:18:55,880
applied

3350
02:18:53,000 --> 02:18:57,760
um you can you can make things you know

3351
02:18:55,880 --> 02:18:59,639
more advanced if you want to use threads

3352
02:18:57,760 --> 02:19:02,559
but it's going to default using the GPU

3353
02:18:59,639 --> 02:19:07,000
V Ram that 8,192 megabytes that you just

3354
02:19:02,559 --> 02:19:11,200
saw um it's going to contain a bunch of

3355
02:19:07,000 --> 02:19:13,479
blocks um and the whole idea here is

3356
02:19:11,200 --> 02:19:15,000
that with with grids and blocks and

3357
02:19:13,479 --> 02:19:16,800
threads is that you you just have to

3358
02:19:15,000 --> 02:19:18,319
worry like conceptually what is it doing

3359
02:19:16,800 --> 02:19:20,280
you don't have to worry about how things

3360
02:19:18,319 --> 02:19:22,399
are handled on the hardware because this

3361
02:19:20,280 --> 02:19:24,840
whole Cuda this whole Cuda hierarchy is

3362
02:19:22,399 --> 02:19:26,599
a software abstraction right so the so

3363
02:19:24,840 --> 02:19:28,000
the hardware doesn't actually look like

3364
02:19:26,599 --> 02:19:30,120
grids and blocks and threads like it

3365
02:19:28,000 --> 02:19:31,880
doesn't objectively look like that it

3366
02:19:30,120 --> 02:19:34,479
looks different and is compiled down to

3367
02:19:31,880 --> 02:19:37,479
Shader assembly which doesn't actually

3368
02:19:34,479 --> 02:19:40,599
look close to

3369
02:19:37,479 --> 02:19:41,920
uh close to what this what this is right

3370
02:19:40,599 --> 02:19:43,639
now and that is actually run on the

3371
02:19:41,920 --> 02:19:45,479
hardware right so there's there's

3372
02:19:43,639 --> 02:19:47,519
various different levels here that it's

3373
02:19:45,479 --> 02:19:49,319
hard to sort of navigate through but

3374
02:19:47,519 --> 02:19:51,240
this is a lot of this is kind of why I'm

3375
02:19:49,319 --> 02:19:54,479
showing you this stuff if to give you a

3376
02:19:51,240 --> 02:19:59,040
better grasp on that

3377
02:19:54,479 --> 02:20:01,560
um so let's dig into what this uh ID

3378
02:19:59,040 --> 02:20:05,200
exing script is actually doing now going

3379
02:20:01,560 --> 02:20:07,280
into the actual Cuda indexing scheme

3380
02:20:05,200 --> 02:20:10,720
like we saw with threads uh except we're

3381
02:20:07,280 --> 02:20:13,200
going on the level of grids blocks uh

3382
02:20:10,720 --> 02:20:14,680
and threads so everything uh this this

3383
02:20:13,200 --> 02:20:16,960
script in particular is designed to

3384
02:20:14,680 --> 02:20:20,120
print things uh useful things out for us

3385
02:20:16,960 --> 02:20:23,760
so as we can see uh ID you know all

3386
02:20:20,120 --> 02:20:26,880
these different block idx um all of

3387
02:20:23,760 --> 02:20:29,319
these and uh oh I'm going to go into

3388
02:20:26,880 --> 02:20:32,920
these in a second here but essentially

3389
02:20:29,319 --> 02:20:34,840
like you you have the um if we go down

3390
02:20:32,920 --> 02:20:37,800
um we we have we have all these terms

3391
02:20:34,840 --> 02:20:39,520
that we Define right

3392
02:20:37,800 --> 02:20:45,479
so

3393
02:20:39,520 --> 02:20:48,080
uh block X block Y block Zed right so B

3394
02:20:45,479 --> 02:20:50,680
and then T is for Threads so what I

3395
02:20:48,080 --> 02:20:53,520
particularly mean here is uh this is the

3396
02:20:50,680 --> 02:20:57,000
this is the block dim so inside of the

3397
02:20:53,520 --> 02:20:59,080
grid you're going to have uh X like the

3398
02:20:57,000 --> 02:21:01,800
length of the X Dimension is going to be

3399
02:20:59,080 --> 02:21:05,160
two and then the height Dimension is

3400
02:21:01,800 --> 02:21:06,800
three and the depth Dimension Z is four

3401
02:21:05,160 --> 02:21:08,960
right so you're going to have this grid

3402
02:21:06,800 --> 02:21:11,720
uh volume and it's going to be of that

3403
02:21:08,960 --> 02:21:14,479
shape and then in in each individual

3404
02:21:11,720 --> 02:21:17,560
block inside of that inside of that grid

3405
02:21:14,479 --> 02:21:20,640
um you're going to have these thread

3406
02:21:17,560 --> 02:21:22,760
dims uh which this is essentially the

3407
02:21:20,640 --> 02:21:24,600
block Dimension this is the um this is

3408
02:21:22,760 --> 02:21:28,319
the grid Dimension and this is the block

3409
02:21:24,600 --> 02:21:29,640
Dimension um so the you're inside of

3410
02:21:28,319 --> 02:21:32,439
each block you're going to have

3411
02:21:29,640 --> 02:21:34,160
essentially four long four high and four

3412
02:21:32,439 --> 02:21:37,720
deep right so it's going to be this this

3413
02:21:34,160 --> 02:21:41,000
perfect Cube essentially um and

3414
02:21:37,720 --> 02:21:43,600
so we go down we calc we can calculate

3415
02:21:41,000 --> 02:21:46,800
the total number of blocks per grid so

3416
02:21:43,600 --> 02:21:48,439
just essentially base time width time

3417
02:21:46,800 --> 02:21:49,920
height your classical formula and same

3418
02:21:48,439 --> 02:21:51,080
for Threads per block and we can get the

3419
02:21:49,920 --> 02:21:52,720
total number

3420
02:21:51,080 --> 02:21:54,960
in each of these and then we can go ah

3421
02:21:52,720 --> 02:21:58,160
and print them out right so blocks per

3422
02:21:54,960 --> 02:22:00,760
grid threads per block um and then total

3423
02:21:58,160 --> 02:22:02,680
number of threads so we have a certain

3424
02:22:00,760 --> 02:22:04,120
number of threads per block and then if

3425
02:22:02,680 --> 02:22:05,680
we times that by the number of blocks we

3426
02:22:04,120 --> 02:22:07,920
get the total number of

3427
02:22:05,680 --> 02:22:10,600
threads now we have this other type down

3428
02:22:07,920 --> 02:22:13,319
here called dim 3 which is specific to

3429
02:22:10,600 --> 02:22:15,200
Cuda but this is essentially just the

3430
02:22:13,319 --> 02:22:18,520
same thing as we saw before so blocks

3431
02:22:15,200 --> 02:22:21,479
per grid so we have this these um these

3432
02:22:18,520 --> 02:22:23,439
these grid Dimensions X x y and Zed and

3433
02:22:21,479 --> 02:22:26,200
then same thing for uh threads within a

3434
02:22:23,439 --> 02:22:29,600
block so the block the block Dimensions

3435
02:22:26,200 --> 02:22:31,760
um meaning x y and Zed and so we plug

3436
02:22:29,600 --> 02:22:34,760
these into our kernel which is called

3437
02:22:31,760 --> 02:22:37,200
who am I we have this Global uh we have

3438
02:22:34,760 --> 02:22:40,960
this Global header

3439
02:22:37,200 --> 02:22:42,200
and we do these three um we do these

3440
02:22:40,960 --> 02:22:43,600
these three symbols I can't remember

3441
02:22:42,200 --> 02:22:45,680
what these are called it's like the less

3442
02:22:43,600 --> 02:22:48,280
than or greater than symbol uh and then

3443
02:22:45,680 --> 02:22:51,160
you put the uh total number of blocks

3444
02:22:48,280 --> 02:22:53,160
per grid um or the grid Dimensions as

3445
02:22:51,160 --> 02:22:54,680
the first parameter and then the threads

3446
02:22:53,160 --> 02:22:56,560
per block as the second and then there's

3447
02:22:54,680 --> 02:22:58,000
some other ones which you can do after

3448
02:22:56,560 --> 02:22:59,120
and you'll see those in a second but

3449
02:22:58,000 --> 02:23:01,560
these are all all you have to worry

3450
02:22:59,120 --> 02:23:02,880
about right now so the grid dimensions

3451
02:23:01,560 --> 02:23:04,960
and then the block

3452
02:23:02,880 --> 02:23:06,240
Dimensions uh and then we c a device

3453
02:23:04,960 --> 02:23:07,720
synchronized to ensure that everything

3454
02:23:06,240 --> 02:23:09,760
is caught up and we can continue with

3455
02:23:07,720 --> 02:23:12,520
whatever else we need to do um that this

3456
02:23:09,760 --> 02:23:14,279
that would be used like practically um

3457
02:23:12,520 --> 02:23:15,560
so now when we actually go up to here

3458
02:23:14,279 --> 02:23:17,399
this is where things get a little bit

3459
02:23:15,560 --> 02:23:18,880
spatially intuitive okay I'm not going

3460
02:23:17,399 --> 02:23:20,680
to lie this part might be like one of

3461
02:23:18,880 --> 02:23:24,920
the hardest to grasp but I'm going to

3462
02:23:20,680 --> 02:23:28,319
try my best to explain so this block ID

3463
02:23:24,920 --> 02:23:30,800
what is this well this is essentially

3464
02:23:28,319 --> 02:23:33,080
you you can think of uh a bunch of

3465
02:23:30,800 --> 02:23:35,600
apartments in an apartment

3466
02:23:33,080 --> 02:23:38,080
complex um a bunch of floors within each

3467
02:23:35,600 --> 02:23:39,800
apartment and then a number like a room

3468
02:23:38,080 --> 02:23:42,960
on that floor right and so we're trying

3469
02:23:39,800 --> 02:23:45,319
to find where we are within that apart

3470
02:23:42,960 --> 02:23:46,880
apartment complex right so you can think

3471
02:23:45,319 --> 02:23:49,880
of it as

3472
02:23:46,880 --> 02:23:51,920
um you know your apartment is like a

3473
02:23:49,880 --> 02:23:56,240
like a paint right it's like a singular

3474
02:23:51,920 --> 02:24:00,279
pain in this in this volume um and so

3475
02:23:56,240 --> 02:24:03,680
you in this one you uh in in the

3476
02:24:00,279 --> 02:24:08,120
apartment complex you essentially do

3477
02:24:03,680 --> 02:24:09,840
um grid dim dox so this this length part

3478
02:24:08,120 --> 02:24:12,600
and then the Y

3479
02:24:09,840 --> 02:24:16,080
component uh and then times whatever Z

3480
02:24:12,600 --> 02:24:18,439
is so the block idx doz is wherever the

3481
02:24:16,080 --> 02:24:20,399
block position is it's not any it's not

3482
02:24:18,439 --> 02:24:22,399
like how big how big something is it's

3483
02:24:20,399 --> 02:24:24,960
ual index or the position so you have

3484
02:24:22,399 --> 02:24:27,399
this pain which is x * Y and then a

3485
02:24:24,960 --> 02:24:29,160
depth which is z so it's like however

3486
02:24:27,399 --> 02:24:30,560
big the pain is and then go that deep so

3487
02:24:29,160 --> 02:24:32,640
it's like these panes that are like

3488
02:24:30,560 --> 02:24:34,359
layered on top going depthwise right

3489
02:24:32,640 --> 02:24:37,640
it's going deep and so you have these

3490
02:24:34,359 --> 02:24:39,399
panes that are like going that way um

3491
02:24:37,640 --> 02:24:44,000
and then you have

3492
02:24:39,399 --> 02:24:46,920
the uh block idx doy time grid dim dox

3493
02:24:44,000 --> 02:24:49,760
so you can think of this as uh like the

3494
02:24:46,920 --> 02:24:51,880
the grid di is is like this like a floor

3495
02:24:49,760 --> 02:24:56,399
right a floor within that apartment uh

3496
02:24:51,880 --> 02:24:58,160
building and the block ID x.y is like

3497
02:24:56,399 --> 02:25:00,960
which Which floor is it so you have to

3498
02:24:58,160 --> 02:25:03,439
go up that number of floors to get there

3499
02:25:00,960 --> 02:25:05,520
um so it's like you essentially start

3500
02:25:03,439 --> 02:25:06,960
from the bottom you go like this many

3501
02:25:05,520 --> 02:25:08,439
and then this many and then this many

3502
02:25:06,960 --> 02:25:09,880
it's like each of those is like a bunch

3503
02:25:08,439 --> 02:25:11,640
of rooms that you go through to get to

3504
02:25:09,880 --> 02:25:13,800
the next floor right you eventually wrap

3505
02:25:11,640 --> 02:25:16,200
up to this one and then you uh and then

3506
02:25:13,800 --> 02:25:19,319
once you get to the actual X position

3507
02:25:16,200 --> 02:25:20,840
which is like the x is the length you

3508
02:25:19,319 --> 02:25:22,840
actually stop there so it's it's like

3509
02:25:20,840 --> 02:25:26,240
you've went through like a number of

3510
02:25:22,840 --> 02:25:28,479
pains like paines deep um rows High

3511
02:25:26,240 --> 02:25:30,040
which is the number of floors um and

3512
02:25:28,479 --> 02:25:31,439
then you end up with like this this

3513
02:25:30,040 --> 02:25:33,359
final part which is like okay well what

3514
02:25:31,439 --> 02:25:35,359
is the offset at this floor which I

3515
02:25:33,359 --> 02:25:38,439
which my apartment is and it's like like

3516
02:25:35,359 --> 02:25:41,760
right here it's like depth and then goes

3517
02:25:38,439 --> 02:25:43,680
up depth goes up number of floors and

3518
02:25:41,760 --> 02:25:45,359
then like this is where I am and that's

3519
02:25:43,680 --> 02:25:49,160
how you uh that's how you find your

3520
02:25:45,359 --> 02:25:50,520
block ID um like which apartment complex

3521
02:25:49,160 --> 02:25:52,040
or which apartment building are you in

3522
02:25:50,520 --> 02:25:54,160
with that that entire city or the

3523
02:25:52,040 --> 02:25:56,960
empowerment complex right in a 3D

3524
02:25:54,160 --> 02:25:59,600
scenario this block offset is

3525
02:25:56,960 --> 02:26:01,399
essentially the number of you you take

3526
02:25:59,600 --> 02:26:03,479
the total threads per block so the block

3527
02:26:01,399 --> 02:26:05,920
Dimensions how many threads is in each

3528
02:26:03,479 --> 02:26:07,240
you know this this many threads in X

3529
02:26:05,920 --> 02:26:09,520
this many threads and Y and as many

3530
02:26:07,240 --> 02:26:10,840
threads Z you you you multiply all those

3531
02:26:09,520 --> 02:26:15,600
together you get the total threads per

3532
02:26:10,840 --> 02:26:17,600
block or say people per apartment um and

3533
02:26:15,600 --> 02:26:20,760
then times our apartment number so it's

3534
02:26:17,600 --> 02:26:22,800
like uh the total number of threads up

3535
02:26:20,760 --> 02:26:26,960
to your like essentially which which

3536
02:26:22,800 --> 02:26:29,520
thread index um does your like how many

3537
02:26:26,960 --> 02:26:32,680
threads are before your UH apartment how

3538
02:26:29,520 --> 02:26:35,000
many people uh are before your apartment

3539
02:26:32,680 --> 02:26:36,359
uh your apartment number that that's

3540
02:26:35,000 --> 02:26:38,960
what we're saying here so we calculated

3541
02:26:36,359 --> 02:26:40,760
this block ID from before and then we we

3542
02:26:38,960 --> 02:26:42,520
just find like that but on the level of

3543
02:26:40,760 --> 02:26:46,439
threads instead and that's the block

3544
02:26:42,520 --> 02:26:47,920
offset for um calculating you know which

3545
02:26:46,439 --> 02:26:49,880
thread we're at and then we can continue

3546
02:26:47,920 --> 02:26:51,880
that and use the same analogy that we

3547
02:26:49,880 --> 02:26:54,319
used in block ID except for thread

3548
02:26:51,880 --> 02:26:56,920
offset so you know it's like thread ID

3549
02:26:54,319 --> 02:26:58,920
x.x block ID x.x it's like these are

3550
02:26:56,920 --> 02:27:00,680
just like mapped essentially except it's

3551
02:26:58,920 --> 02:27:03,680
like a lower level in the hierarchy it's

3552
02:27:00,680 --> 02:27:07,800
down to threads instead of

3553
02:27:03,680 --> 02:27:10,040
um instead of uh blocks right and so you

3554
02:27:07,800 --> 02:27:11,680
can you can calculate which person you

3555
02:27:10,040 --> 02:27:14,319
are within that individual apartment

3556
02:27:11,680 --> 02:27:15,920
like if it's like a if it's like a big

3557
02:27:14,319 --> 02:27:17,160
apartment with like multiple floors and

3558
02:27:15,920 --> 02:27:18,960
there's like multiple layers in it you

3559
02:27:17,160 --> 02:27:21,600
could use that but you you get the point

3560
02:27:18,960 --> 02:27:23,920
it's a it's a 3D anal ology um and we

3561
02:27:21,600 --> 02:27:25,520
can find which person we are within that

3562
02:27:23,920 --> 02:27:29,000
or which which thread essentially in

3563
02:27:25,520 --> 02:27:31,040
that block it is and so when you add the

3564
02:27:29,000 --> 02:27:33,040
block offset so the total number of

3565
02:27:31,040 --> 02:27:36,160
threads leading up to your apartment

3566
02:27:33,040 --> 02:27:37,720
plus which one you are within that uh

3567
02:27:36,160 --> 02:27:39,319
within that apartment number then you

3568
02:27:37,720 --> 02:27:41,240
can actually find which thread you are

3569
02:27:39,319 --> 02:27:43,840
in the entire grid and then you can do

3570
02:27:41,240 --> 02:27:46,800
stuff with that right and that's what we

3571
02:27:43,840 --> 02:27:48,840
say sign the global ID to so Global

3572
02:27:46,800 --> 02:27:51,399
person ID in the entire apartment

3573
02:27:48,840 --> 02:27:53,200
complex um

3574
02:27:51,399 --> 02:27:55,040
and that's that so there's a lot to

3575
02:27:53,200 --> 02:27:57,040
unpack there feel free to rewatch some

3576
02:27:55,040 --> 02:27:59,720
of this or even try to visualize some of

3577
02:27:57,040 --> 02:28:02,160
this on your own maybe write it out um

3578
02:27:59,720 --> 02:28:04,920
but when we actually go

3579
02:28:02,160 --> 02:28:09,120
into when we go into our terminal here

3580
02:28:04,920 --> 02:28:15,920
and go into five um then cud to

3581
02:28:09,120 --> 02:28:21,160
Basics if we go um nvcc d o we go Zer we

3582
02:28:15,920 --> 02:28:24,160
go 01 and then 01 like this

3583
02:28:21,160 --> 02:28:26,160
um it'll compile this binary which we

3584
02:28:24,160 --> 02:28:29,720
see here and we can just go ahead and

3585
02:28:26,160 --> 02:28:31,840
execute that and it'll show us uh

3586
02:28:29,720 --> 02:28:35,319
precisely all of this that we just that

3587
02:28:31,840 --> 02:28:38,720
we just unpacked so I mean I can't I

3588
02:28:35,319 --> 02:28:42,800
cannot put all of this on the screen but

3589
02:28:38,720 --> 02:28:42,800
uh like for example if we look

3590
02:28:42,840 --> 02:28:49,680
at like how it counts upward right

3591
02:28:46,520 --> 02:28:52,080
so um you have you have all of your

3592
02:28:49,680 --> 02:28:56,640
different Dimensions here and you can uh

3593
02:28:52,080 --> 02:29:00,960
at the very end I believe it outputs the

3594
02:28:56,640 --> 02:29:04,200
uh the particular thread offset so we

3595
02:29:00,960 --> 02:29:07,279
notice that it's like 63 and then it

3596
02:29:04,200 --> 02:29:10,240
jumps to 32 right so it's like 32 and

3597
02:29:07,279 --> 02:29:12,439
then it goes for

3598
02:29:10,240 --> 02:29:14,880
um it goes

3599
02:29:12,439 --> 02:29:19,359
for uh

3600
02:29:14,880 --> 02:29:24,000
32 numbers so if we go 30 I mean it's

3601
02:29:19,359 --> 02:29:28,000
it's technically like minus one uh but

3602
02:29:24,000 --> 02:29:32,520
the the best analogy you can use here is

3603
02:29:28,000 --> 02:29:35,399
this is 32 threads right um that is a

3604
02:29:32,520 --> 02:29:37,279
warp so when we talked about 32 threads

3605
02:29:35,399 --> 02:29:39,399
in a warp this is exactly what it looks

3606
02:29:37,279 --> 02:29:42,560
like so go back to here as well you'll

3607
02:29:39,399 --> 02:29:45,680
see it stops at exactly 32 and you'll go

3608
02:29:42,560 --> 02:29:49,200
up from there so it'll be like 0 to 31

3609
02:29:45,680 --> 02:29:51,720
so it's like 0 1 2 3 4 so it's like 1 to

3610
02:29:49,200 --> 02:29:53,600
31 it's 31 elements and then you'll have

3611
02:29:51,720 --> 02:29:55,720
the additional zero which makes it 32

3612
02:29:53,600 --> 02:29:58,319
that's just the indexing scheme right

3613
02:29:55,720 --> 02:30:00,399
and then when you go from 32 to 63 it's

3614
02:29:58,319 --> 02:30:04,640
it's the same idea um because you go

3615
02:30:00,399 --> 02:30:07,319
from 0 to 63 instead of 1 to 64 so you

3616
02:30:04,640 --> 02:30:10,359
do actually have 32 elements uh 32

3617
02:30:07,319 --> 02:30:12,520
threads per warp in there um and then

3618
02:30:10,359 --> 02:30:15,120
you can just see the global thread ID in

3619
02:30:12,520 --> 02:30:17,279
the entire grid so when we when we

3620
02:30:15,120 --> 02:30:21,000
actually multiply these up we have you

3621
02:30:17,279 --> 02:30:23,640
know in the in the grid we have 2 * 3

3622
02:30:21,000 --> 02:30:26,359
which is 6 and then that * 4 is

3623
02:30:23,640 --> 02:30:33,200
24

3624
02:30:26,359 --> 02:30:34,960
um 24 * 4 * 4 so that that is 16 and 16

3625
02:30:33,200 --> 02:30:38,840
* 4 is is

3626
02:30:34,960 --> 02:30:42,640
64 so you can see 1536 in this entire

3627
02:30:38,840 --> 02:30:45,479
thing right and so um if we scroll like

3628
02:30:42,640 --> 02:30:48,080
backwards we can see uh

3629
02:30:45,479 --> 02:30:52,399
1535 it ends right there and that is the

3630
02:30:48,080 --> 02:30:55,240
final one so um like for example block

3631
02:30:52,399 --> 02:30:58,200
um it's this has two elements so it's

3632
02:30:55,240 --> 02:30:59,479
going to be zero and one and then this

3633
02:30:58,200 --> 02:31:04,240
is going to have three elements so it's

3634
02:30:59,479 --> 02:31:06,680
going to be um 0 1 2 and then this is

3635
02:31:04,240 --> 02:31:10,240
four elements so it's going to be 0 1 2

3636
02:31:06,680 --> 02:31:11,680
3 is four right uh and then the threads

3637
02:31:10,240 --> 02:31:15,920
because those each go up to four it's

3638
02:31:11,680 --> 02:31:18,680
going to be 0 1 2 3 0 1 2 3 0 1 2 3 um

3639
02:31:15,920 --> 02:31:20,279
and then you end up with um essentially

3640
02:31:18,680 --> 02:31:21,680
whatever that number is in the end so

3641
02:31:20,279 --> 02:31:23,520
you can see how this kind of all adds up

3642
02:31:21,680 --> 02:31:26,319
and how this indexing scheme works and

3643
02:31:23,520 --> 02:31:28,920
how we can use these to index pieces of

3644
02:31:26,319 --> 02:31:31,680
data um using like the actual thread and

3645
02:31:28,920 --> 02:31:34,160
block indexes and then and then do

3646
02:31:31,680 --> 02:31:35,560
really fast parallel math with that um

3647
02:31:34,160 --> 02:31:37,160
that's the whole idea here let's go

3648
02:31:35,560 --> 02:31:39,240
ahead and jump into kernels

3649
02:31:37,160 --> 02:31:40,760
now okay so now we're going to do a

3650
02:31:39,240 --> 02:31:42,120
little bit of our math and we're

3651
02:31:40,760 --> 02:31:43,240
actually going to you know see what

3652
02:31:42,120 --> 02:31:45,680
these kernels are actually doing and

3653
02:31:43,240 --> 02:31:47,040
seeing how they work under the hood so

3654
02:31:45,680 --> 02:31:49,319
it's actually very simple this is the

3655
02:31:47,040 --> 02:31:50,479
most simple it gets um but essentially

3656
02:31:49,319 --> 02:31:53,359
we're just going to do some vector

3657
02:31:50,479 --> 02:31:56,880
addition as a practice so adding these

3658
02:31:53,359 --> 02:32:00,479
two together element wise 1 + 6 2 + 7 3

3659
02:31:56,880 --> 02:32:03,040
+ 8 Etc and we get all this um very very

3660
02:32:00,479 --> 02:32:05,479
simple and easy to understand we can we

3661
02:32:03,040 --> 02:32:08,439
have a CPU example here which is obvious

3662
02:32:05,479 --> 02:32:12,560
and easy to look at

3663
02:32:08,439 --> 02:32:14,160
um we have a GPU example which is

3664
02:32:12,560 --> 02:32:15,960
actually a little weird it's a it's

3665
02:32:14,160 --> 02:32:18,080
different than this because here we have

3666
02:32:15,960 --> 02:32:20,720
a for Loop and here we have this this it

3667
02:32:18,080 --> 02:32:23,160
term which is ID block idx time Block in

3668
02:32:20,720 --> 02:32:25,279
plus thread IX and I'm going to explain

3669
02:32:23,160 --> 02:32:26,880
this in a second here but this doesn't

3670
02:32:25,279 --> 02:32:28,439
have a for Loop and essentially what

3671
02:32:26,880 --> 02:32:30,800
this is doing like I talked about before

3672
02:32:28,439 --> 02:32:32,319
is it's just unrolling this Loop so you

3673
02:32:30,800 --> 02:32:33,840
know CPU is going to like do this

3674
02:32:32,319 --> 02:32:35,920
iteration and this one and then this one

3675
02:32:33,840 --> 02:32:37,600
and then this one the GPU is going to

3676
02:32:35,920 --> 02:32:39,479
take all these individual iterations and

3677
02:32:37,600 --> 02:32:41,840
distribute it across a bunch of blocks

3678
02:32:39,479 --> 02:32:43,240
or or caor you could say uh and it's

3679
02:32:41,840 --> 02:32:45,640
going to parallelize that operation and

3680
02:32:43,240 --> 02:32:48,479
make it really really fast so instead of

3681
02:32:45,640 --> 02:32:50,240
doing uh separately like 10 million

3682
02:32:48,479 --> 02:32:52,680
different operations like in order order

3683
02:32:50,240 --> 02:32:55,600
it's going to take roughly 10,000 time

3684
02:32:52,680 --> 02:32:58,840
units um say you had you know 10,000

3685
02:32:55,600 --> 02:33:00,720
cicor to split this across it's like

3686
02:32:58,840 --> 02:33:03,000
well that's that that's actually a lot

3687
02:33:00,720 --> 02:33:04,880
less now that's only about a thousand

3688
02:33:03,000 --> 02:33:07,560
times depths you have to do so that it's

3689
02:33:04,880 --> 02:33:08,840
it's sped up uh an insane amount just by

3690
02:33:07,560 --> 02:33:12,200
Distributing it across and that's

3691
02:33:08,840 --> 02:33:13,840
theoretical of course but um you know we

3692
02:33:12,200 --> 02:33:15,080
initialize vectors this should be this

3693
02:33:13,840 --> 02:33:17,160
should be very intuitive if you've

3694
02:33:15,080 --> 02:33:19,040
written like any random stuff in random

3695
02:33:17,160 --> 02:33:20,439
gens in C before it's going to

3696
02:33:19,040 --> 02:33:22,600
essentially take

3697
02:33:20,439 --> 02:33:26,960
uh a random integer between zero and

3698
02:33:22,600 --> 02:33:28,200
Rand Max so Rand Max is this um very

3699
02:33:26,960 --> 02:33:30,920
easy to understand it's going to be a

3700
02:33:28,200 --> 02:33:32,680
floating Point number um and then a

3701
02:33:30,920 --> 02:33:34,640
timing function just to measure

3702
02:33:32,680 --> 02:33:37,520
execution time again in this script we

3703
02:33:34,640 --> 02:33:39,359
are benchmarking so perform War warm-up

3704
02:33:37,520 --> 02:33:42,319
runs get things you know fired up and

3705
02:33:39,359 --> 02:33:44,560
then Benchmark CPU to GPU and see how

3706
02:33:42,319 --> 02:33:46,319
well it does um but this isn't really

3707
02:33:44,560 --> 02:33:50,040
the important part here what I wanted to

3708
02:33:46,319 --> 02:33:52,000
mostly expand on is what's what's what

3709
02:33:50,040 --> 02:33:53,319
things specifically here apply to Cuda

3710
02:33:52,000 --> 02:33:56,680
and what do you really need to

3711
02:33:53,319 --> 02:33:58,960
understand so we have this Cuda Malik

3712
02:33:56,680 --> 02:34:00,800
which is the same as Malik except it's

3713
02:33:58,960 --> 02:34:02,640
on GPU so it's going to do that it's

3714
02:34:00,800 --> 02:34:07,439
going to allocate memory on on the

3715
02:34:02,640 --> 02:34:10,760
global Dam or the vram on the GPU and uh

3716
02:34:07,439 --> 02:34:14,720
all this really has is a device pointer

3717
02:34:10,760 --> 02:34:18,720
and a size so we have this we have this

3718
02:34:14,720 --> 02:34:21,960
device a this device a vector or array

3719
02:34:18,720 --> 02:34:23,840
is uh declared here which is a pointer

3720
02:34:21,960 --> 02:34:25,120
um and then we set the size for that

3721
02:34:23,840 --> 02:34:28,680
right and this is just the memory

3722
02:34:25,120 --> 02:34:31,359
address of that so uh we we allocate

3723
02:34:28,680 --> 02:34:33,040
device memory with Cuda Malik um and

3724
02:34:31,359 --> 02:34:34,560
then when we actually want to move the

3725
02:34:33,040 --> 02:34:36,319
stuff that we've created on the host

3726
02:34:34,560 --> 02:34:39,359
because remember we initialize these

3727
02:34:36,319 --> 02:34:41,680
vectors on a global or or just a just a

3728
02:34:39,359 --> 02:34:43,560
regular void CPU function so we actually

3729
02:34:41,680 --> 02:34:46,200
have to copy these over now and how we

3730
02:34:43,560 --> 02:34:50,760
do that is we just literally look at

3731
02:34:46,200 --> 02:34:52,479
this destination source how big it is

3732
02:34:50,760 --> 02:34:56,319
and what what kind of copy do we want to

3733
02:34:52,479 --> 02:34:57,880
do so destination is device hence the d

3734
02:34:56,319 --> 02:35:02,319
The Source is

3735
02:34:57,880 --> 02:35:05,200
host it's size big um like we declared

3736
02:35:02,319 --> 02:35:07,880
here and then CM copy host to device so

3737
02:35:05,200 --> 02:35:11,399
CPU it's going to move to GPU and that's

3738
02:35:07,880 --> 02:35:11,399
it very simple

3739
02:35:11,560 --> 02:35:17,040
um we Define uh this numb blocks which

3740
02:35:15,240 --> 02:35:19,760
is a little bit different than what we

3741
02:35:17,040 --> 02:35:22,120
did in this indexing thing uh because

3742
02:35:19,760 --> 02:35:26,720
it's not actually this dim 3 type as we

3743
02:35:22,120 --> 02:35:29,800
saw before um it's it still works though

3744
02:35:26,720 --> 02:35:34,120
the whole idea with this is that uh if

3745
02:35:29,800 --> 02:35:36,200
instead of say uh if instead of having

3746
02:35:34,120 --> 02:35:39,600
like 2 three 4 if we just wanted it to

3747
02:35:36,200 --> 02:35:44,279
be like a length of uh what is this this

3748
02:35:39,600 --> 02:35:46,680
is 24 2 * 3 * 4 is 24 so you would you

3749
02:35:44,279 --> 02:35:50,439
could essentially set this to 24 and

3750
02:35:46,680 --> 02:35:54,000
then set these to one and just having uh

3751
02:35:50,439 --> 02:35:56,960
numb blocks and then putting this in in

3752
02:35:54,000 --> 02:35:59,960
that uh in the kernel launch actually

3753
02:35:56,960 --> 02:36:02,279
just converts the integer to like dim

3754
02:35:59,960 --> 02:36:04,319
three and then it's like it's like numb

3755
02:36:02,279 --> 02:36:06,479
blocks and then one and one so it's just

3756
02:36:04,319 --> 02:36:08,240
like a it's just length only and it it's

3757
02:36:06,479 --> 02:36:10,600
still like it still looks like

3758
02:36:08,240 --> 02:36:12,359
volumetric but it's just laid out

3759
02:36:10,600 --> 02:36:14,319
linearly so it ends up looking like a

3760
02:36:12,359 --> 02:36:17,399
line and could interprets it as a line

3761
02:36:14,319 --> 02:36:21,200
in Hardware

3762
02:36:17,399 --> 02:36:23,439
um then you might ask okay well how

3763
02:36:21,200 --> 02:36:25,359
exactly do we calculate num blocks well

3764
02:36:23,439 --> 02:36:27,560
this is very interesting

3765
02:36:25,359 --> 02:36:29,240
so we have a bunch of things going on

3766
02:36:27,560 --> 02:36:33,160
here and this seems a little funny so we

3767
02:36:29,240 --> 02:36:35,160
have n plus block size minus one and I'm

3768
02:36:33,160 --> 02:36:36,880
going to illustrate this out here now

3769
02:36:35,160 --> 02:36:38,600
just to clear up what the heck is this

3770
02:36:36,880 --> 02:36:41,439
numb blocks things means I actually laid

3771
02:36:38,600 --> 02:36:44,920
out some calculations for this so block

3772
02:36:41,439 --> 02:36:46,520
size is the number of threads inside of

3773
02:36:44,920 --> 02:36:48,040
a block it's the size of the block

3774
02:36:46,520 --> 02:36:50,880
itself which threads are going to fit

3775
02:36:48,040 --> 02:36:53,160
into right so so if we have let's just

3776
02:36:50,880 --> 02:36:56,080
say instead of 10 million elements like

3777
02:36:53,160 --> 02:36:58,560
we have up here let's say we have 1,24

3778
02:36:56,080 --> 02:37:03,040
elements right uh if we're trying to fit

3779
02:36:58,560 --> 02:37:04,359
1,24 elements across uh 256 threads per

3780
02:37:03,040 --> 02:37:05,920
block that means we're probably going to

3781
02:37:04,359 --> 02:37:10,920
want four blocks right it'll split it

3782
02:37:05,920 --> 02:37:13,319
evenly because 256 * 4 is 1024 um and so

3783
02:37:10,920 --> 02:37:15,240
we have to actually calculate this

3784
02:37:13,319 --> 02:37:19,200
manually but we have to keep in mind

3785
02:37:15,240 --> 02:37:20,319
that we are doing uh like we there are

3786
02:37:19,200 --> 02:37:22,680
more things things we have to keep track

3787
02:37:20,319 --> 02:37:26,279
of in case say this number ends up being

3788
02:37:22,680 --> 02:37:29,279
like 1025 right so I actually wrote out

3789
02:37:26,279 --> 02:37:32,840
a script that does that does this math

3790
02:37:29,279 --> 02:37:36,640
uh perfectly for us um so let's first

3791
02:37:32,840 --> 02:37:39,600
look at this so you have this uh 1024

3792
02:37:36,640 --> 02:37:42,240
plus 256 that's that's the length of the

3793
02:37:39,600 --> 02:37:43,880
array plus the block size right number

3794
02:37:42,240 --> 02:37:46,600
of threads per block then you're going

3795
02:37:43,880 --> 02:37:49,240
to do minus one and whatever that is

3796
02:37:46,600 --> 02:37:51,080
divide that by the block size uh and

3797
02:37:49,240 --> 02:37:53,720
then

3798
02:37:51,080 --> 02:37:56,600
uh the the compiler is automatically

3799
02:37:53,720 --> 02:37:59,399
going to floor this answer it's going to

3800
02:37:56,600 --> 02:38:02,319
truncate those those decimal places off

3801
02:37:59,399 --> 02:38:03,920
so if you get like 4.99 or whatever it's

3802
02:38:02,319 --> 02:38:05,240
going to take that 0.99 and just

3803
02:38:03,920 --> 02:38:07,520
truncate it off so you're going to end

3804
02:38:05,240 --> 02:38:11,279
up with four so if we were to do for

3805
02:38:07,520 --> 02:38:16,680
example like 1,00

3806
02:38:11,279 --> 02:38:21,520
um 1024 plus 256 well what's that 1 1,00

3807
02:38:16,680 --> 02:38:23,160
+ 200 is 1200 and then 56 + 24 is 80 so

3808
02:38:21,520 --> 02:38:27,160
we get

3809
02:38:23,160 --> 02:38:33,080
1,280 um and if we divide this by

3810
02:38:27,160 --> 02:38:33,080
256 we end up getting around what's the

3811
02:38:34,640 --> 02:38:40,840
answer divided 256 we get

3812
02:38:41,800 --> 02:38:47,920
um we get this number but remember we

3813
02:38:45,960 --> 02:38:49,600
have the one here which I actually

3814
02:38:47,920 --> 02:38:53,520
forgot for a second there we have have

3815
02:38:49,600 --> 02:38:56,000
the one so it technically is uh 79 so

3816
02:38:53,520 --> 02:38:57,960
you end up with this N9 part and it ends

3817
02:38:56,000 --> 02:39:01,439
up just being four because you truncate

3818
02:38:57,960 --> 02:39:03,040
that off however if you end up having

3819
02:39:01,439 --> 02:39:05,279
this as like

3820
02:39:03,040 --> 02:39:08,240
1,25 then this number is actually going

3821
02:39:05,279 --> 02:39:11,920
to end up as 1,280 because you're just

3822
02:39:08,240 --> 02:39:14,319
adding one back to you know 1279 and you

3823
02:39:11,920 --> 02:39:16,359
end up with five so in case you end up

3824
02:39:14,319 --> 02:39:18,720
adding an extra element you want to

3825
02:39:16,359 --> 02:39:20,080
allocate space and resources for that or

3826
02:39:18,720 --> 02:39:22,120
else you will not get the answer that

3827
02:39:20,080 --> 02:39:23,680
you want so that's all this is doing up

3828
02:39:22,120 --> 02:39:25,560
here and we make sure and this is just

3829
02:39:23,680 --> 02:39:30,240
like a careful calculation to make sure

3830
02:39:25,560 --> 02:39:32,920
that everything goes as as we want um

3831
02:39:30,240 --> 02:39:35,399
and so this is just a a little script

3832
02:39:32,920 --> 02:39:37,040
that I wrote up to test this but I don't

3833
02:39:35,399 --> 02:39:40,040
need that

3834
02:39:37,040 --> 02:39:40,040
anymore

3835
02:39:42,800 --> 02:39:50,040
um

3836
02:39:44,520 --> 02:39:52,040
so going further um

3837
02:39:50,040 --> 02:39:56,080
we essentially in this kernel here let

3838
02:39:52,040 --> 02:39:58,200
me slide up so in this kernel we have

3839
02:39:56,080 --> 02:40:00,640
just this this x Dimension laid out

3840
02:39:58,200 --> 02:40:02,840
right so what you're doing is you have

3841
02:40:00,640 --> 02:40:05,439
this blocks block idx which is which

3842
02:40:02,840 --> 02:40:08,600
block it is in that in that line of a

3843
02:40:05,439 --> 02:40:12,160
grid and then you're multiplying that by

3844
02:40:08,600 --> 02:40:14,319
the size of the block so how how many

3845
02:40:12,160 --> 02:40:17,640
threads are there per block times the

3846
02:40:14,319 --> 02:40:19,880
number of blocks uh and then plus

3847
02:40:17,640 --> 02:40:22,840
whichever whatever

3848
02:40:19,880 --> 02:40:26,960
thread we're at right so this gives us

3849
02:40:22,840 --> 02:40:29,279
the thread um in that in that like line

3850
02:40:26,960 --> 02:40:31,439
of a grid right and so we end up with

3851
02:40:29,279 --> 02:40:34,520
whatever place we're at and we use that

3852
02:40:31,439 --> 02:40:37,359
thread index to then access elements in

3853
02:40:34,520 --> 02:40:40,319
A and B and C uh and then we just do an

3854
02:40:37,359 --> 02:40:42,200
an add operation so whichever you know

3855
02:40:40,319 --> 02:40:44,600
it might be in some cases this might be

3856
02:40:42,200 --> 02:40:46,800
like you know 2.5 million and in some

3857
02:40:44,600 --> 02:40:48,920
cases it might be like three uh in which

3858
02:40:46,800 --> 02:40:50,840
case they're going to be the same number

3859
02:40:48,920 --> 02:40:52,000
uh and then they're going to add and

3860
02:40:50,840 --> 02:40:53,479
we're going to get the answer that we

3861
02:40:52,000 --> 02:40:56,359
expect it just might not happen in the

3862
02:40:53,479 --> 02:40:59,439
order that like a a loop might right so

3863
02:40:56,359 --> 02:41:01,120
instead of doing like uh the first the

3864
02:40:59,439 --> 02:41:02,720
uh the first index and the second index

3865
02:41:01,120 --> 02:41:04,040
and the third it's going to like scatter

3866
02:41:02,720 --> 02:41:05,800
and distribute these and it's just going

3867
02:41:04,040 --> 02:41:08,040
to be fast right so that's what the

3868
02:41:05,800 --> 02:41:10,880
whole idea is there

3869
02:41:08,040 --> 02:41:14,279
um and if we go ahead and actually run

3870
02:41:10,880 --> 02:41:14,279
this script

3871
02:41:16,479 --> 02:41:23,439
um go 0 0 and then we could just so 0

3872
02:41:20,040 --> 02:41:26,800
Vector ad and then enter we'll just run

3873
02:41:23,439 --> 02:41:29,200
this file performing oneup runs uh C

3874
02:41:26,800 --> 02:41:33,240
Benchmark and CPU Benchmark and GPU so

3875
02:41:29,200 --> 02:41:35,800
the CPU average time is uh about .14

3876
02:41:33,240 --> 02:41:38,279
milliseconds which is really fast

3877
02:41:35,800 --> 02:41:41,479
however the GPU average time is

3878
02:41:38,279 --> 02:41:44,439
significantly less than that about 143x

3879
02:41:41,479 --> 02:41:47,120
speed up almost 144 uh and the results

3880
02:41:44,439 --> 02:41:49,760
match up when we compare them uh index

3881
02:41:47,120 --> 02:41:52,600
index wise or element wise so so we just

3882
02:41:49,760 --> 02:41:55,080
verify the results here um we ensure

3883
02:41:52,600 --> 02:41:57,040
that the the absolute value of the

3884
02:41:55,080 --> 02:42:00,359
difference between those two is greater

3885
02:41:57,040 --> 02:42:02,560
than uh 1 * 105 which is just a common

3886
02:42:00,359 --> 02:42:04,640
verification thing so you'll see that

3887
02:42:02,560 --> 02:42:07,160
when we're comparing things uh you know

3888
02:42:04,640 --> 02:42:09,520
more like as we go more into Cuda it's

3889
02:42:07,160 --> 02:42:11,560
going to be this idea of you Benchmark

3890
02:42:09,520 --> 02:42:14,600
uh you get like an average time across

3891
02:42:11,560 --> 02:42:16,200
all the runs and then you make sure that

3892
02:42:14,600 --> 02:42:18,000
you're getting the correct results by

3893
02:42:16,200 --> 02:42:20,000
having this tolerance Factor so

3894
02:42:18,000 --> 02:42:23,640
sometimes this might be like super low

3895
02:42:20,000 --> 02:42:25,560
or it might be like super high um but

3896
02:42:23,640 --> 02:42:28,279
that's that's typically how we'll do

3897
02:42:25,560 --> 02:42:31,000
it then we have the second example of

3898
02:42:28,279 --> 02:42:34,000
vector addition which is uh very much

3899
02:42:31,000 --> 02:42:36,760
the same however instead of just having

3900
02:42:34,000 --> 02:42:39,920
this uh one dimensional like x axis

3901
02:42:36,760 --> 02:42:43,520
thing where we have one two three four

3902
02:42:39,920 --> 02:42:45,399
lines um we have a lot more so going

3903
02:42:43,520 --> 02:42:47,840
back to that example from indexing where

3904
02:42:45,399 --> 02:42:49,560
we had you know three dimensions um if

3905
02:42:47,840 --> 02:42:52,439
we actually apply this to Vector Edition

3906
02:42:49,560 --> 02:42:54,000
we get a noticeable slow down so the

3907
02:42:52,439 --> 02:42:56,040
first thing you'll notice is that this

3908
02:42:54,000 --> 02:42:58,040
has way more lines but you're like

3909
02:42:56,040 --> 02:42:59,960
Elliot certainly this is going to be

3910
02:42:58,040 --> 02:43:02,000
faster right it uses up the whole Space

3911
02:42:59,960 --> 02:43:04,200
instead of just uh instead of just a

3912
02:43:02,000 --> 02:43:06,439
little bit right um well the issue with

3913
02:43:04,200 --> 02:43:08,520
this is that it Cuda is not really going

3914
02:43:06,439 --> 02:43:10,120
to struggle with uh scheduling things

3915
02:43:08,520 --> 02:43:11,479
and making them run fast and and

3916
02:43:10,120 --> 02:43:13,840
compiling down to something that's going

3917
02:43:11,479 --> 02:43:15,520
to like really work at speed um it's

3918
02:43:13,840 --> 02:43:17,640
more so like what are the calculations

3919
02:43:15,520 --> 02:43:20,120
that you're actually doing in a single

3920
02:43:17,640 --> 02:43:21,800
uh thread right so this is a this is

3921
02:43:20,120 --> 02:43:26,000
what's going to happen in a thread

3922
02:43:21,800 --> 02:43:28,840
notice how we do 1 2 3 4 5 six

3923
02:43:26,000 --> 02:43:31,080
operations so um three adds three

3924
02:43:28,840 --> 02:43:33,760
multiplies and three stores so equal

3925
02:43:31,080 --> 02:43:35,479
sign as well and then this one it's like

3926
02:43:33,760 --> 02:43:37,520
you have a bunch of these comparisons

3927
02:43:35,479 --> 02:43:40,319
and and it's just like a bunch of math

3928
02:43:37,520 --> 02:43:44,720
you it's it's like hard to read right

3929
02:43:40,319 --> 02:43:49,560
and the point is this does

3930
02:43:44,720 --> 02:43:52,840
one one multiply one store or one

3931
02:43:49,560 --> 02:43:55,319
multiply two stores and two ads

3932
02:43:52,840 --> 02:43:58,840
significantly less than this one so the

3933
02:43:55,319 --> 02:44:00,840
point is um only use the 3D aspect when

3934
02:43:58,840 --> 02:44:02,080
you absolutely need to when it is like

3935
02:44:00,840 --> 02:44:03,960
dependent on your algorithm and you

3936
02:44:02,080 --> 02:44:05,720
don't need to uh when you when you

3937
02:44:03,960 --> 02:44:08,600
actually have something that that's like

3938
02:44:05,720 --> 02:44:09,760
uh spatially 3D then you can use

3939
02:44:08,600 --> 02:44:11,080
something like this because it might

3940
02:44:09,760 --> 02:44:13,520
actually work a bit easier and you w't

3941
02:44:11,080 --> 02:44:15,520
have to do all these calculations to end

3942
02:44:13,520 --> 02:44:17,600
up laying out this 3D space into like

3943
02:44:15,520 --> 02:44:19,240
this onedimensional thing um and you

3944
02:44:17,600 --> 02:44:21,800
have to worry about like things wrapping

3945
02:44:19,240 --> 02:44:23,319
around and and strides and all this um

3946
02:44:21,800 --> 02:44:24,960
so that's like that's mainly the

3947
02:44:23,319 --> 02:44:27,880
bottleneck there and I just really did a

3948
02:44:24,960 --> 02:44:29,960
comparison between the 3D and the 1D

3949
02:44:27,880 --> 02:44:32,560
Vector Edition kernel so we can go ahead

3950
02:44:29,960 --> 02:44:35,560
and actually compile this

3951
02:44:32,560 --> 02:44:35,560
here

3952
02:44:36,800 --> 02:44:43,880
um so we notice that they're both a lot

3953
02:44:39,600 --> 02:44:47,720
faster however um the speed up CPU

3954
02:44:43,880 --> 02:44:49,640
versus GPU 1D like the GPU 1D is 106

3955
02:44:47,720 --> 02:44:52,600
times faster but the 3D is only 102

3956
02:44:49,640 --> 02:44:57,479
times faster so this is actually faster

3957
02:44:52,600 --> 02:45:00,880
than the um than the GPU 3D um not by a

3958
02:44:57,479 --> 02:45:02,880
crazy amount but you know by by like 3

3959
02:45:00,880 --> 02:45:05,120
4% maybe and if you scale up your

3960
02:45:02,880 --> 02:45:08,200
numbers it might grow but you get the

3961
02:45:05,120 --> 02:45:10,279
point um there's a lot of unnecessary

3962
02:45:08,200 --> 02:45:12,279
calculations there um and it's just kind

3963
02:45:10,279 --> 02:45:14,760
of simpler to go down this route with

3964
02:45:12,279 --> 02:45:16,800
the 1D

3965
02:45:14,760 --> 02:45:18,080
kernel now we dive into something a

3966
02:45:16,800 --> 02:45:19,520
little bit more intuitive

3967
02:45:18,080 --> 02:45:21,120
algorithmically called matrix

3968
02:45:19,520 --> 02:45:23,000
multiplication you might have already

3969
02:45:21,120 --> 02:45:24,840
done this in which case you know this

3970
02:45:23,000 --> 02:45:26,160
might just be some simple review you

3971
02:45:24,840 --> 02:45:27,560
might want to skip ahead it's it's up to

3972
02:45:26,160 --> 02:45:30,000
you really but I'm going to go over this

3973
02:45:27,560 --> 02:45:31,760
be no matter what because some people

3974
02:45:30,000 --> 02:45:34,520
may not know and sometimes it's good to

3975
02:45:31,760 --> 02:45:37,080
get a little refresher on that so we're

3976
02:45:34,520 --> 02:45:39,520
essentially going to write the naive

3977
02:45:37,080 --> 02:45:42,520
version the naive version of the matrix

3978
02:45:39,520 --> 02:45:45,880
multiplication Cuda kernel which is the

3979
02:45:42,520 --> 02:45:49,680
slowest one but it's the most basic and

3980
02:45:45,880 --> 02:45:54,399
intuitive to understand um

3981
02:45:49,680 --> 02:45:58,080
so a matrix looks like this you have

3982
02:45:54,399 --> 02:46:00,439
rows and you have columns right um let

3983
02:45:58,080 --> 02:46:04,439
me actually zoom in a little more here

3984
02:46:00,439 --> 02:46:07,640
so rows and columns um for example a is

3985
02:46:04,439 --> 02:46:11,040
a 3 by two because it has three rows and

3986
02:46:07,640 --> 02:46:13,359
two columns right so it's like three

3987
02:46:11,040 --> 02:46:16,160
high and two long it's like width by

3988
02:46:13,359 --> 02:46:18,200
height you could say or height by width

3989
02:46:16,160 --> 02:46:21,000
and then we have B which is a 2x4 so

3990
02:46:18,200 --> 02:46:24,840
it's two row rows and four uh four

3991
02:46:21,000 --> 02:46:27,160
columns right uh and the idea is is that

3992
02:46:24,840 --> 02:46:30,160
as long as these two inner numbers are

3993
02:46:27,160 --> 02:46:31,359
the same then uh then we then it

3994
02:46:30,160 --> 02:46:34,319
actually works we're allowed to do that

3995
02:46:31,359 --> 02:46:36,160
matrix multiplication um and you'll see

3996
02:46:34,319 --> 02:46:37,880
why in a second here and then these

3997
02:46:36,160 --> 02:46:39,680
outer Dimensions these three and four

3998
02:46:37,880 --> 02:46:44,040
would end up being the new size of the

3999
02:46:39,680 --> 02:46:46,880
new output Matrix C uh so we have you

4000
02:46:44,040 --> 02:46:49,880
know 1 2 3 4 5 6 and 7 8 9 10 11 12 13

4001
02:46:46,880 --> 02:46:52,520
14 and what we do here is is it's very

4002
02:46:49,880 --> 02:46:55,279
is it's very simple you essentially go 7

4003
02:46:52,520 --> 02:46:57,720
and 11 you you take this you have this

4004
02:46:55,279 --> 02:47:00,520
uh this B it's like this and then a is

4005
02:46:57,720 --> 02:47:05,200
like this and so you take the seven and

4006
02:47:00,520 --> 02:47:10,520
11 in in B and you rotate it and you do

4007
02:47:05,200 --> 02:47:13,080
a DOT product with uh one and two so you

4008
02:47:10,520 --> 02:47:14,840
take the seven and and the 11 you flip

4009
02:47:13,080 --> 02:47:17,080
it over and so the seven is going to

4010
02:47:14,840 --> 02:47:19,279
multiply with the one and then the 11 is

4011
02:47:17,080 --> 02:47:21,920
going to multiply with the two right so

4012
02:47:19,279 --> 02:47:24,160
you're just like it's like sideways uh

4013
02:47:21,920 --> 02:47:25,560
and then when you multiply one with one

4014
02:47:24,160 --> 02:47:27,760
with the seven you get seven and two

4015
02:47:25,560 --> 02:47:29,479
with uh 11 gets 22 and then you would

4016
02:47:27,760 --> 02:47:32,040
add those together to get

4017
02:47:29,479 --> 02:47:33,080
29 um and we can see that right here as

4018
02:47:32,040 --> 02:47:36,080
the first

4019
02:47:33,080 --> 02:47:38,840
element so notice how it's like the

4020
02:47:36,080 --> 02:47:41,279
First Column and the first row aligned

4021
02:47:38,840 --> 02:47:43,479
together and so it's like they're

4022
02:47:41,279 --> 02:47:45,640
they're like pointing at one spot it's

4023
02:47:43,479 --> 02:47:47,960
like the first it's like the first row

4024
02:47:45,640 --> 02:47:49,840
up here instead of down here first row

4025
02:47:47,960 --> 02:47:51,800
and then the first column column and

4026
02:47:49,840 --> 02:47:55,359
they meet together and you get this top

4027
02:47:51,800 --> 02:47:57,200
left corner thing um and that's and

4028
02:47:55,359 --> 02:47:59,080
that's where we end up with this 29

4029
02:47:57,200 --> 02:48:01,720
value and then you essentially just do

4030
02:47:59,080 --> 02:48:04,439
this for the rest of them so you go uh 8

4031
02:48:01,720 --> 02:48:07,359
and and 12 and then you you flip that

4032
02:48:04,439 --> 02:48:09,880
flip that sideways and it'll multiply

4033
02:48:07,359 --> 02:48:12,359
with the one and the two

4034
02:48:09,880 --> 02:48:15,359
um and then you put that

4035
02:48:12,359 --> 02:48:17,960
here you have the the second column and

4036
02:48:15,359 --> 02:48:20,080
the first row so it's going to it's

4037
02:48:17,960 --> 02:48:23,200
going to meet in the second column and

4038
02:48:20,080 --> 02:48:24,920
the first row right um and then you just

4039
02:48:23,200 --> 02:48:26,920
continue doing this for the rest of them

4040
02:48:24,920 --> 02:48:29,600
until you end up with your final answer

4041
02:48:26,920 --> 02:48:33,200
so you're essentially just like flipping

4042
02:48:29,600 --> 02:48:35,240
the column of B onto a row of a and

4043
02:48:33,200 --> 02:48:37,880
you're doing a do product operation

4044
02:48:35,240 --> 02:48:39,800
where each uh each of like the like

4045
02:48:37,880 --> 02:48:41,640
element wise you're going to multiply

4046
02:48:39,800 --> 02:48:43,200
and then you add all you reduce and you

4047
02:48:41,640 --> 02:48:45,080
add all of them together and you squash

4048
02:48:43,200 --> 02:48:49,080
it and then you end up with this final

4049
02:48:45,080 --> 02:48:53,680
Matrix uh which is of shape uh 3x4 so

4050
02:48:49,080 --> 02:48:56,960
three uh rows three three rows High by

4051
02:48:53,680 --> 02:49:00,479
four columns wide and then and then

4052
02:48:56,960 --> 02:49:02,120
that's how you do a mat mole um so when

4053
02:49:00,479 --> 02:49:04,319
we go into the I mean typically when

4054
02:49:02,120 --> 02:49:05,439
you're writing out you know hard to

4055
02:49:04,319 --> 02:49:06,520
understand algorithms like this when

4056
02:49:05,439 --> 02:49:09,080
you're trying to fit this all in your

4057
02:49:06,520 --> 02:49:11,200
head ideally you want to write it on the

4058
02:49:09,080 --> 02:49:12,960
CPU first if you just jump straight into

4059
02:49:11,200 --> 02:49:13,960
GPU and try to optimize you're probably

4060
02:49:12,960 --> 02:49:15,319
going to mess up you're probably not

4061
02:49:13,960 --> 02:49:17,479
going to get the answers you're looking

4062
02:49:15,319 --> 02:49:19,960
for and things are going to be weird so

4063
02:49:17,479 --> 02:49:21,479
you write out the maybe even go back to

4064
02:49:19,960 --> 02:49:24,040
Python and write this out in Python

4065
02:49:21,479 --> 02:49:25,760
first so you can visualize it um and

4066
02:49:24,040 --> 02:49:28,880
make sure that yours matches like P

4067
02:49:25,760 --> 02:49:30,960
towards or nump or something and then

4068
02:49:28,880 --> 02:49:33,800
and then you write this out in C and you

4069
02:49:30,960 --> 02:49:36,760
say okay well how do we do a a m Mo on

4070
02:49:33,800 --> 02:49:40,040
the CPU here so you have your a and your

4071
02:49:36,760 --> 02:49:47,160
B and your C Matrix um and then your

4072
02:49:40,040 --> 02:49:49,160
shapes m k n so m is uh this this how

4073
02:49:47,160 --> 02:49:53,560
high a is so m in this case was would be

4074
02:49:49,160 --> 02:49:56,880
three k would be two so two and two and

4075
02:49:53,560 --> 02:50:00,439
then n would be four right so you end up

4076
02:49:56,880 --> 02:50:03,479
doing this like

4077
02:50:00,439 --> 02:50:06,560
um M yeah just just like this

4078
02:50:03,479 --> 02:50:08,760
autocomplete M * K and then you multiply

4079
02:50:06,560 --> 02:50:12,000
that with a K byn Matrix and you get an

4080
02:50:08,760 --> 02:50:15,040
M byn Matrix just space this out so it's

4081
02:50:12,000 --> 02:50:15,040
easier to look

4082
02:50:17,359 --> 02:50:23,160
at uh and and that's that so when we

4083
02:50:20,160 --> 02:50:25,760
look at our our nested for Loops here we

4084
02:50:23,160 --> 02:50:27,120
can see that we iterate over M so that's

4085
02:50:25,760 --> 02:50:31,640
the

4086
02:50:27,120 --> 02:50:33,960
uh that is the height of a right that's

4087
02:50:31,640 --> 02:50:35,399
the number of rows we have and then

4088
02:50:33,960 --> 02:50:37,120
we're going to I plus plus that each

4089
02:50:35,399 --> 02:50:38,880
time and keep in mind when this is laid

4090
02:50:37,120 --> 02:50:40,960
out in memory it's not actually going to

4091
02:50:38,880 --> 02:50:44,560
be a matrix it's going to be an array so

4092
02:50:40,960 --> 02:50:47,120
you're going to have like one 2 3 4 5 6

4093
02:50:44,560 --> 02:50:48,399
instead of 1 two like as an array and

4094
02:50:47,120 --> 02:50:49,840
then another array below it and then

4095
02:50:48,399 --> 02:50:52,560
another it's it's not like that it's

4096
02:50:49,840 --> 02:50:55,239
just laid out at once so you have to

4097
02:50:52,560 --> 02:50:56,840
actually manually consider like the

4098
02:50:55,239 --> 02:50:58,239
wrapping over so you have to actually

4099
02:50:56,840 --> 02:51:01,560
keep that in mind when you're writing

4100
02:50:58,239 --> 02:51:04,120
these and that's a tricky part too

4101
02:51:01,560 --> 02:51:11,800
um so then you have J which is going to

4102
02:51:04,120 --> 02:51:15,000
iterate over um n which is uh n is uh

4103
02:51:11,800 --> 02:51:18,880
the number of columns

4104
02:51:15,000 --> 02:51:21,560
here and then we plus plus that

4105
02:51:18,880 --> 02:51:23,600
like each iteration we we make this

4106
02:51:21,560 --> 02:51:25,720
accumulation sum so we're going to

4107
02:51:23,600 --> 02:51:29,720
accumulate into the sum right because

4108
02:51:25,720 --> 02:51:31,720
you're going You're essentially uh

4109
02:51:29,720 --> 02:51:33,399
accumulating things as you're like when

4110
02:51:31,720 --> 02:51:35,319
we do the add operation and we have all

4111
02:51:33,399 --> 02:51:36,720
these multiplies and we fuse and add

4112
02:51:35,319 --> 02:51:39,880
them together that's what this

4113
02:51:36,720 --> 02:51:42,600
accumulation sum is for uh and so when

4114
02:51:39,880 --> 02:51:50,200
we iterate through um when we iterate

4115
02:51:42,600 --> 02:51:52,439
through k um which is K is uh the

4116
02:51:50,200 --> 02:51:56,080
the the X Dimension you could say in a

4117
02:51:52,439 --> 02:51:57,760
or the number of columns and then K in B

4118
02:51:56,080 --> 02:52:00,479
is going to be the height or the number

4119
02:51:57,760 --> 02:52:03,239
of rows right and and so you iterate

4120
02:52:00,479 --> 02:52:05,960
through that and when you do your sum

4121
02:52:03,239 --> 02:52:07,840
you essentially add it and you do um you

4122
02:52:05,960 --> 02:52:10,680
do essentially this is where the dot

4123
02:52:07,840 --> 02:52:13,560
product comes in right you do a so

4124
02:52:10,680 --> 02:52:17,000
that's I where like whatever I is let's

4125
02:52:13,560 --> 02:52:19,160
say I is like um I is zero right so I is

4126
02:52:17,000 --> 02:52:21,399
going to be um

4127
02:52:19,160 --> 02:52:23,479
I is going to be whatever this is right

4128
02:52:21,399 --> 02:52:28,840
it's going to be the first the the first

4129
02:52:23,479 --> 02:52:32,120
one times whatever K is and K uh K in

4130
02:52:28,840 --> 02:52:36,080
this case is going to be well two so

4131
02:52:32,120 --> 02:52:39,319
when you have zero the zeroth

4132
02:52:36,080 --> 02:52:40,479
um when when I is zero and K is whatever

4133
02:52:39,319 --> 02:52:42,479
number it's still going to end up

4134
02:52:40,479 --> 02:52:44,040
equaling zero and so you have L

4135
02:52:42,479 --> 02:52:47,439
afterwards and that's going to be

4136
02:52:44,040 --> 02:52:50,239
whichever spot at what whatever it

4137
02:52:47,439 --> 02:52:51,359
wherever it is through through K that's

4138
02:52:50,239 --> 02:52:55,200
where it's going to end up at so like

4139
02:52:51,359 --> 02:52:56,680
the offset through the row um and it's

4140
02:52:55,200 --> 02:53:00,960
going to multiply the same thing it's

4141
02:52:56,680 --> 02:53:04,680
going to do um l so L is where it's at

4142
02:53:00,960 --> 02:53:06,359
through K which is going to be um the

4143
02:53:04,680 --> 02:53:07,800
going up and down instead of left right

4144
02:53:06,359 --> 02:53:11,560
it's going to be up and

4145
02:53:07,800 --> 02:53:15,960
down and then you have this n term which

4146
02:53:11,560 --> 02:53:17,439
we could say is uh maybe also uh zero if

4147
02:53:15,960 --> 02:53:19,840
you're just doing the first one here

4148
02:53:17,439 --> 02:53:22,200
like the top top left corner and then

4149
02:53:19,840 --> 02:53:24,040
say j in this case is zero so it's just

4150
02:53:22,200 --> 02:53:26,319
going to end up hitting the it's going

4151
02:53:24,040 --> 02:53:28,239
to end up in hitting the same value so

4152
02:53:26,319 --> 02:53:30,720
you end up just getting the first the

4153
02:53:28,239 --> 02:53:32,760
first points uh and then you you

4154
02:53:30,720 --> 02:53:36,399
multiply them together and then you add

4155
02:53:32,760 --> 02:53:38,720
that um you you you multiply them you

4156
02:53:36,399 --> 02:53:41,080
multiply the first the one and the the

4157
02:53:38,720 --> 02:53:42,520
seven together in the first one and then

4158
02:53:41,080 --> 02:53:45,479
you end up hitting the second one which

4159
02:53:42,520 --> 02:53:48,840
is the two and the 11 um and that gets

4160
02:53:45,479 --> 02:53:52,600
summed up together and you're do doing

4161
02:53:48,840 --> 02:53:55,239
this every single time uh this for Loop

4162
02:53:52,600 --> 02:53:56,760
this second uh for Loop triggers right

4163
02:53:55,239 --> 02:54:01,640
so every time this goes through an

4164
02:53:56,760 --> 02:54:06,160
iteration you're hitting n and n is uh

4165
02:54:01,640 --> 02:54:07,960
just n is just whichever value uh

4166
02:54:06,160 --> 02:54:09,399
whichever value is essentially coming

4167
02:54:07,960 --> 02:54:10,800
next right and so you're just getting

4168
02:54:09,399 --> 02:54:12,760
this one and this one and then this one

4169
02:54:10,800 --> 02:54:14,920
and then this one and so on so forth

4170
02:54:12,760 --> 02:54:17,880
until the end and then you end up just

4171
02:54:14,920 --> 02:54:20,000
writing this out so you you essentially

4172
02:54:17,880 --> 02:54:21,760
assign to Value C to whatever that sum

4173
02:54:20,000 --> 02:54:25,720
is so that you can compute the next dot

4174
02:54:21,760 --> 02:54:27,880
product so uh this this is like very uh

4175
02:54:25,720 --> 02:54:29,760
visual I encourage you to I mean if this

4176
02:54:27,880 --> 02:54:32,439
doesn't completely make sense if you

4177
02:54:29,760 --> 02:54:34,720
haven't like taken a introductory linear

4178
02:54:32,439 --> 02:54:36,239
algebra course I completely get it um

4179
02:54:34,720 --> 02:54:38,399
you might want to just pass us through

4180
02:54:36,239 --> 02:54:40,200
you know language models or look at some

4181
02:54:38,399 --> 02:54:41,600
some intuitive videos on the internet

4182
02:54:40,200 --> 02:54:45,399
and just sort of understand what's going

4183
02:54:41,600 --> 02:54:47,439
on here try to understand uh what like

4184
02:54:45,399 --> 02:54:49,960
how things are wrapping around when they

4185
02:54:47,439 --> 02:54:51,680
when they do like a dried or something

4186
02:54:49,960 --> 02:54:53,640
um that's that's very important to pay

4187
02:54:51,680 --> 02:54:57,880
attention to like this like the K when

4188
02:54:53,640 --> 02:55:01,080
the K is wrapping for example K is uh K

4189
02:54:57,880 --> 02:55:02,880
is here and K is essentially this this

4190
02:55:01,080 --> 02:55:05,439
length so it's going to be like

4191
02:55:02,880 --> 02:55:08,080
whichever whichever whichever uh row you

4192
02:55:05,439 --> 02:55:09,479
want you want to wrap around that entire

4193
02:55:08,080 --> 02:55:11,800
row so you want to go to the length of

4194
02:55:09,479 --> 02:55:14,239
it and wrap and then your offset is

4195
02:55:11,800 --> 02:55:16,000
going to be that and then same idea here

4196
02:55:14,239 --> 02:55:19,160
except instead of rows it's going to be

4197
02:55:16,000 --> 02:55:21,399
like columns column offset right

4198
02:55:19,160 --> 02:55:25,920
um and that's that's the whole idea

4199
02:55:21,399 --> 02:55:27,680
there um and then we go into the GPU

4200
02:55:25,920 --> 02:55:30,040
implementation which is a little bit

4201
02:55:27,680 --> 02:55:32,640
different but we're essentially using

4202
02:55:30,040 --> 02:55:36,239
instead of just an i or an ID a single

4203
02:55:32,640 --> 02:55:39,840
idx term we use a rows and columns so in

4204
02:55:36,239 --> 02:55:42,560
this grid we have the block ID x.y *

4205
02:55:39,840 --> 02:55:46,080
block dim doy block idx is you know

4206
02:55:42,560 --> 02:55:48,479
where the where the block is at in um

4207
02:55:46,080 --> 02:55:51,439
where the essentially where the block is

4208
02:55:48,479 --> 02:55:53,640
vertically um and we're just getting

4209
02:55:51,439 --> 02:55:56,200
essentially the the vertical thread like

4210
02:55:53,640 --> 02:55:58,200
which thread are we do we want within uh

4211
02:55:56,200 --> 02:56:00,200
considering like this vertical grid and

4212
02:55:58,200 --> 02:56:02,760
all the blocks that we have right uh

4213
02:56:00,200 --> 02:56:04,880
going back to what I said before and we

4214
02:56:02,760 --> 02:56:07,960
do the same thing with X so we have this

4215
02:56:04,880 --> 02:56:10,359
we have the thread in the uh vertical

4216
02:56:07,960 --> 02:56:13,080
and the horizontal

4217
02:56:10,359 --> 02:56:14,640
Direction uh and then we as we want to

4218
02:56:13,080 --> 02:56:17,920
this is actually very this is actually

4219
02:56:14,640 --> 02:56:19,399
very uh this is required you actually

4220
02:56:17,920 --> 02:56:21,399
have this you need this if statement

4221
02:56:19,399 --> 02:56:23,760
here because if things go off track or

4222
02:56:21,399 --> 02:56:25,600
if you have like too many threads then

4223
02:56:23,760 --> 02:56:27,479
they might go and compute values that

4224
02:56:25,600 --> 02:56:29,040
you don't want like it might go access

4225
02:56:27,479 --> 02:56:30,520
other parts in memory it's not

4226
02:56:29,040 --> 02:56:32,319
restrained right so it's not going to

4227
02:56:30,520 --> 02:56:33,720
stop when you think it should stop you

4228
02:56:32,319 --> 02:56:37,479
actually need to put careful restraints

4229
02:56:33,720 --> 02:56:41,560
on it and say okay well we we want to

4230
02:56:37,479 --> 02:56:43,359
stop it once the row gets to M because

4231
02:56:41,560 --> 02:56:45,840
there's no other values outside of that

4232
02:56:43,359 --> 02:56:49,560
and then same for column as well right

4233
02:56:45,840 --> 02:56:55,479
so um like when we go up here we

4234
02:56:49,560 --> 02:56:57,800
have we have M by n right so m is the

4235
02:56:55,479 --> 02:57:05,040
um m is this

4236
02:56:57,800 --> 02:57:09,600
part which is the uh where did it go

4237
02:57:05,040 --> 02:57:13,439
yes m is row so row is y right this this

4238
02:57:09,600 --> 02:57:16,680
y this height part and then n is is that

4239
02:57:13,439 --> 02:57:18,720
uh the the width the horizontal part X

4240
02:57:16,680 --> 02:57:20,239
and so that's that's columns which to X

4241
02:57:18,720 --> 02:57:21,359
right and so you have to this is just

4242
02:57:20,239 --> 02:57:24,520
the kind of thing that you have to be

4243
02:57:21,359 --> 02:57:26,200
careful with um Cuda handles this very

4244
02:57:24,520 --> 02:57:29,239
well but you just have to include this

4245
02:57:26,200 --> 02:57:31,520
if statement and then you essentially

4246
02:57:29,239 --> 02:57:34,840
for each thread um because this is

4247
02:57:31,520 --> 02:57:38,399
itself in a thread you're going to uh

4248
02:57:34,840 --> 02:57:39,880
just do a essentially a a DOT product

4249
02:57:38,399 --> 02:57:44,399
between elements and you're you're going

4250
02:57:39,880 --> 02:57:47,080
to do like essentially a a row of a row

4251
02:57:44,399 --> 02:57:48,760
of a and a column of of B and this is

4252
02:57:47,080 --> 02:57:50,319
going to be done per thread

4253
02:57:48,760 --> 02:57:52,479
so each different thread is going to

4254
02:57:50,319 --> 02:57:55,279
have a different maybe a different uh

4255
02:57:52,479 --> 02:57:59,279
row and a different column of B to to to

4256
02:57:55,279 --> 02:58:02,080
compute so you have this um you have

4257
02:57:59,279 --> 02:58:03,600
this K term which is from here um and

4258
02:58:02,080 --> 02:58:05,600
you're you're cycling through that and

4259
02:58:03,600 --> 02:58:08,319
you you just essentially apply the same

4260
02:58:05,600 --> 02:58:10,960
wrapping but instead of worrying about

4261
02:58:08,319 --> 02:58:14,040
all of these nested for Loops you worry

4262
02:58:10,960 --> 02:58:16,720
instead about um the rows and columns so

4263
02:58:14,040 --> 02:58:18,960
these are your actual uh these are the

4264
02:58:16,720 --> 02:58:21,560
you know the way we index with threads

4265
02:58:18,960 --> 02:58:24,399
as I was talking about before

4266
02:58:21,560 --> 02:58:26,960
um but yeah this is uh I'm going to dig

4267
02:58:24,399 --> 02:58:28,439
more into sort of the in intuition

4268
02:58:26,960 --> 02:58:30,880
behind this later in the course when we

4269
02:58:28,439 --> 02:58:33,279
end up optimizing matrix multiplication

4270
02:58:30,880 --> 02:58:36,479
this uh this is this is called a naive

4271
02:58:33,279 --> 02:58:38,279
kernel it's it's very it's very limited

4272
02:58:36,479 --> 02:58:42,359
it doesn't have a ton of optimizations

4273
02:58:38,279 --> 02:58:44,560
it's not like it's not fast it is like

4274
02:58:42,359 --> 02:58:46,279
it's like aunds the speed of what

4275
02:58:44,560 --> 02:58:48,359
state-ofthe-art is it's actually quite

4276
02:58:46,279 --> 02:58:50,040
slow comparatively um and we're going to

4277
02:58:48,359 --> 02:58:52,319
optimize this later on and this is going

4278
02:58:50,040 --> 02:58:53,800
to be the most intuitive thing you will

4279
02:58:52,319 --> 02:58:56,520
probably learn in this entire course is

4280
02:58:53,800 --> 02:58:58,279
matrix multiplication in Cuda so don't

4281
02:58:56,520 --> 02:59:01,399
worry if it this doesn't entirely click

4282
02:58:58,279 --> 02:59:03,920
right now um just kind of worry about

4283
02:59:01,399 --> 02:59:05,479
where these threads are how we're

4284
02:59:03,920 --> 02:59:07,439
getting how we're getting the row and

4285
02:59:05,479 --> 02:59:09,319
column values and then this wrapping

4286
02:59:07,439 --> 02:59:13,279
that we have here and then the the

4287
02:59:09,319 --> 02:59:18,120
offset part right wrapping and

4288
02:59:13,279 --> 02:59:19,319
offset offset um and yeah that's that

4289
02:59:18,120 --> 02:59:23,200
that's pretty much all you have to worry

4290
02:59:19,319 --> 02:59:25,640
about for now um and then we just do you

4291
02:59:23,200 --> 02:59:28,279
know the same route perform warm-up runs

4292
02:59:25,640 --> 02:59:31,960
Benchmark it across 20 benchmarks or

4293
02:59:28,279 --> 02:59:34,520
across 20 runs um Benchmark CPU versus

4294
02:59:31,960 --> 02:59:37,560
GPU and then return the average time in

4295
02:59:34,520 --> 02:59:40,960
micros seconds so if I just uh open up a

4296
02:59:37,560 --> 02:59:48,479
terminal here and go

4297
02:59:40,960 --> 02:59:51,640
nbcc out to two and we go and run that

4298
02:59:48,479 --> 02:59:54,160
forming R up

4299
02:59:51,640 --> 02:59:55,640
runs okay this is very I actually made

4300
02:59:54,160 --> 02:59:59,000
very large matricies maybe we should

4301
02:59:55,640 --> 03:00:01,279
shrink these a little bit

4302
02:59:59,000 --> 03:00:05,840
um we can

4303
03:00:01,279 --> 03:00:08,840
go let's see maybe

4304
03:00:05,840 --> 03:00:08,840
256

4305
03:00:10,359 --> 03:00:15,279
512

4306
03:00:13,080 --> 03:00:16,279
256 yeah the CPU is not going to like

4307
03:00:15,279 --> 03:00:19,680
that

4308
03:00:16,279 --> 03:00:22,920
one uh and so it's benchmarking CPU and

4309
03:00:19,680 --> 03:00:25,680
so that takes 89,000 micros and this

4310
03:00:22,920 --> 03:00:28,359
takes 88 microc so we get just like out

4311
03:00:25,680 --> 03:00:34,200
of the box with these small matricies we

4312
03:00:28,359 --> 03:00:38,840
get a 1,000x speed up with using Cuda

4313
03:00:34,200 --> 03:00:40,239
um and that's that so this is uh this is

4314
03:00:38,840 --> 03:00:42,399
this is uh this is kind of how we test

4315
03:00:40,239 --> 03:00:44,120
stuff but yeah now we're going to go

4316
03:00:42,399 --> 03:00:46,439
ahead and jump into like how do you

4317
03:00:44,120 --> 03:00:49,200
profile these um I know we haven't gone

4318
03:00:46,439 --> 03:00:51,680
extensively into um like how Cuda

4319
03:00:49,200 --> 03:00:56,000
actually works under the hood completely

4320
03:00:51,680 --> 03:00:57,000
there's still more to do but um in a

4321
03:00:56,000 --> 03:01:00,000
little bit we're going to hit up

4322
03:00:57,000 --> 03:01:01,560
profiling I would like to cover uh

4323
03:01:00,000 --> 03:01:03,399
actually some more stuff before we do

4324
03:01:01,560 --> 03:01:06,840
that let me close this

4325
03:01:03,399 --> 03:01:12,319
out we pop into the read me here just

4326
03:01:06,840 --> 03:01:17,399
close this close this um going

4327
03:01:12,319 --> 03:01:17,399
to just zoom out a little bit

4328
03:01:18,520 --> 03:01:24,479
sure so again we have these these dim

4329
03:01:22,000 --> 03:01:26,040
these dim three types um which I was

4330
03:01:24,479 --> 03:01:27,399
talking about before these should make

4331
03:01:26,040 --> 03:01:30,439
sense already these these should not be

4332
03:01:27,399 --> 03:01:33,439
like too hard to grasp

4333
03:01:30,439 --> 03:01:36,560
um this is what it normally looks like

4334
03:01:33,439 --> 03:01:39,200
right you put in you put this

4335
03:01:36,560 --> 03:01:42,160
in you put this in like I said before

4336
03:01:39,200 --> 03:01:44,040
it's going to simplify to a dim three so

4337
03:01:42,160 --> 03:01:47,239
this is going to look like a 16 by one

4338
03:01:44,040 --> 03:01:49,200
by one uh tensor you could say and it's

4339
03:01:47,239 --> 03:01:50,760
going to add that to the kernel launch

4340
03:01:49,200 --> 03:01:52,359
configuration this is the kernel launch

4341
03:01:50,760 --> 03:01:55,760
configuration there's more stuff we can

4342
03:01:52,359 --> 03:01:59,880
add to it um already covered this stuff

4343
03:01:55,760 --> 03:02:02,200
already uh and then you have more stuff

4344
03:01:59,880 --> 03:02:06,399
you can add to it um so we have the grid

4345
03:02:02,200 --> 03:02:10,920
dim the grid dims uh in you know 1 to 3D

4346
03:02:06,399 --> 03:02:13,239
block di in 1 to 3D and then this uh NS

4347
03:02:10,920 --> 03:02:16,319
so this is the number of bytes in shared

4348
03:02:13,239 --> 03:02:18,479
memory that is allocated per block for

4349
03:02:16,319 --> 03:02:22,160
this call um so you're going to

4350
03:02:18,479 --> 03:02:24,760
explicitly allocate memory for a block

4351
03:02:22,160 --> 03:02:27,920
um in shared memory which is really fast

4352
03:02:24,760 --> 03:02:31,120
so typically you would omit this uh

4353
03:02:27,920 --> 03:02:32,760
however if you have a specific uh

4354
03:02:31,120 --> 03:02:34,279
production like you're trying to deploy

4355
03:02:32,760 --> 03:02:36,239
a CTIC kernel in production to run

4356
03:02:34,279 --> 03:02:38,720
something really really fast you might

4357
03:02:36,239 --> 03:02:40,080
actually want to capitalize off of this

4358
03:02:38,720 --> 03:02:41,560
because it'll give you more explicit

4359
03:02:40,080 --> 03:02:43,319
control over what happens and you can

4360
03:02:41,560 --> 03:02:45,319
measure performance a bit better and

4361
03:02:43,319 --> 03:02:46,640
you'll get maybe get some some little

4362
03:02:45,319 --> 03:02:49,479
some little performance gains out of

4363
03:02:46,640 --> 03:02:52,560
that and then there's this s term which

4364
03:02:49,479 --> 03:02:54,520
is uh the stream it's in and I'm going

4365
03:02:52,560 --> 03:02:55,920
to cover streams actually number five so

4366
03:02:54,520 --> 03:02:57,319
don't worry about this too much but but

4367
03:02:55,920 --> 03:03:00,120
streams are pretty cool they let you do

4368
03:02:57,319 --> 03:03:02,800
some some interesting stuff

4369
03:03:00,120 --> 03:03:05,840
um and then this I didn't talk about

4370
03:03:02,800 --> 03:03:08,359
this entirely too much Cuda device

4371
03:03:05,840 --> 03:03:10,880
synchronize and sync threads so Cuda

4372
03:03:08,359 --> 03:03:13,960
device synchronize ensures all the

4373
03:03:10,880 --> 03:03:17,439
kernels or all of the uh threads for one

4374
03:03:13,960 --> 03:03:19,200
problem are all of the like all of the

4375
03:03:17,439 --> 03:03:21,120
all the different parallel computations

4376
03:03:19,200 --> 03:03:23,160
for a problem are done before you begin

4377
03:03:21,120 --> 03:03:25,040
the next so when you when you launch a

4378
03:03:23,160 --> 03:03:26,640
kernel it's going to have a bunch of

4379
03:03:25,040 --> 03:03:30,239
blocks in parel and a bunch of threads

4380
03:03:26,640 --> 03:03:31,840
in parallel run this massive problem um

4381
03:03:30,239 --> 03:03:33,800
and they might not all finish at the

4382
03:03:31,840 --> 03:03:36,120
same time like some of them just like do

4383
03:03:33,800 --> 03:03:38,880
to physics they're they might like not

4384
03:03:36,120 --> 03:03:41,399
finish at the exact same time and so you

4385
03:03:38,880 --> 03:03:43,640
have to explicitly synchronize them you

4386
03:03:41,399 --> 03:03:47,399
have to add this little barrier this

4387
03:03:43,640 --> 03:03:48,960
this ume essentially preventing a a race

4388
03:03:47,399 --> 03:03:52,920
condition

4389
03:03:48,960 --> 03:03:54,520
so if you have a bunch of threads um

4390
03:03:52,920 --> 03:03:56,640
like for example in this one when you're

4391
03:03:54,520 --> 03:03:57,840
bit shifting when you're like moving

4392
03:03:56,640 --> 03:03:59,239
this one over here and then this one

4393
03:03:57,840 --> 03:04:01,720
over here and then this one over here

4394
03:03:59,239 --> 03:04:03,399
it's like well ideally you'd want to do

4395
03:04:01,720 --> 03:04:05,200
this in a certain order and not like

4396
03:04:03,399 --> 03:04:08,399
store something before it's not supposed

4397
03:04:05,200 --> 03:04:11,239
to be stored like if um for example if I

4398
03:04:08,399 --> 03:04:13,640
do this one um and then this one is

4399
03:04:11,239 --> 03:04:15,279
supposed to happen after but it ends up

4400
03:04:13,640 --> 03:04:18,840
doing this one first because we didn't

4401
03:04:15,279 --> 03:04:20,640
synchronize properly um you could you'll

4402
03:04:18,840 --> 03:04:22,279
end up with the wrong answer right so

4403
03:04:20,640 --> 03:04:24,680
you have to purposely synchronize the

4404
03:04:22,279 --> 03:04:26,279
thread so that all of them regardless of

4405
03:04:24,680 --> 03:04:27,760
like this one might be like way ahead

4406
03:04:26,279 --> 03:04:29,800
you have to wait for all the other ones

4407
03:04:27,760 --> 03:04:31,840
to catch up in order for them to hit the

4408
03:04:29,800 --> 03:04:33,319
same spot so you say okay this one's

4409
03:04:31,840 --> 03:04:35,239
done but these ones aren't we're going

4410
03:04:33,319 --> 03:04:36,720
to wait for all these to synchronize up

4411
03:04:35,239 --> 03:04:39,319
together and then we can continue the

4412
03:04:36,720 --> 03:04:41,960
next step right that's what uh Cuda

4413
03:04:39,319 --> 03:04:44,279
device synchronize synchronize will do

4414
03:04:41,960 --> 03:04:47,040
uh after you typically put this after

4415
03:04:44,279 --> 03:04:50,040
launching a kernel and then sync threads

4416
03:04:47,040 --> 03:04:52,880
is put with in a kernel um for threat

4417
03:04:50,040 --> 03:04:54,319
execution inside of it so one is like

4418
03:04:52,880 --> 03:04:56,040
out like when you're trying to

4419
03:04:54,319 --> 03:04:57,680
synchronize the whole grid and then one

4420
03:04:56,040 --> 03:05:00,720
is like synchronize all the threads

4421
03:04:57,680 --> 03:05:04,720
within a within like

4422
03:05:00,720 --> 03:05:06,040
a within a warp so as you might have

4423
03:05:04,720 --> 03:05:07,600
been able to tell I was a little bit

4424
03:05:06,040 --> 03:05:09,399
unsure about that last answer so I

4425
03:05:07,600 --> 03:05:12,279
decided to look it up and sync threads

4426
03:05:09,399 --> 03:05:14,640
is actually on the level of uh thread

4427
03:05:12,279 --> 03:05:18,160
blocks instead of warps so you can do

4428
03:05:14,640 --> 03:05:21,600
syn warps instead of sync threads

4429
03:05:18,160 --> 03:05:24,479
actually pop back to here and we go at

4430
03:05:21,600 --> 03:05:30,239
sync the sync threads you can actually

4431
03:05:24,479 --> 03:05:30,239
do um if you want to do warps you can

4432
03:05:30,840 --> 03:05:36,840
do stin

4433
03:05:34,600 --> 03:05:39,359
warps

4434
03:05:36,840 --> 03:05:41,560
um to sync all of the threads within a

4435
03:05:39,359 --> 03:05:43,920
warp and then this one will do that it

4436
03:05:41,560 --> 03:05:46,920
the same thing but a thread block

4437
03:05:43,920 --> 03:05:46,920
instead

4438
03:05:51,720 --> 03:05:58,600
all reds within a

4439
03:05:56,120 --> 03:06:01,080
war

4440
03:05:58,600 --> 03:06:02,279
um and then this is for an entire thread

4441
03:06:01,080 --> 03:06:04,319
block so just just a piece of

4442
03:06:02,279 --> 03:06:08,239
clarification there one other cool thing

4443
03:06:04,319 --> 03:06:11,040
I came across when uh studying Cuda is

4444
03:06:08,239 --> 03:06:12,880
how you can actually add in uh explicit

4445
03:06:11,040 --> 03:06:16,160
flags and you can you can actually

4446
03:06:12,880 --> 03:06:18,200
convert something like a log to log f

4447
03:06:16,160 --> 03:06:19,800
using compiler Flags

4448
03:06:18,200 --> 03:06:23,399
um and I know that there's a little bit

4449
03:06:19,800 --> 03:06:26,520
to unpack there but if I just go back to

4450
03:06:23,399 --> 03:06:28,000
uh this compilation here actually no we

4451
03:06:26,520 --> 03:06:30,279
don't even use we don't even use any of

4452
03:06:28,000 --> 03:06:32,920
those M functions but if I were to say

4453
03:06:30,279 --> 03:06:34,680
do like log inside of a kernel um that

4454
03:06:32,920 --> 03:06:36,920
would go slower than if I were to use

4455
03:06:34,680 --> 03:06:39,439
log F so log f is like a device

4456
03:06:36,920 --> 03:06:42,000
operation and log is a host operation so

4457
03:06:39,439 --> 03:06:45,479
designed to run on CPU on CPU course

4458
03:06:42,000 --> 03:06:48,279
right um so we can actually do do use

4459
03:06:45,479 --> 03:06:51,640
fast math as a part of the compiler

4460
03:06:48,279 --> 03:06:56,160
Flags I can go um

4461
03:06:51,640 --> 03:06:56,160
use use fast math like

4462
03:06:57,720 --> 03:07:01,600
this and of course we won't really see

4463
03:06:59,920 --> 03:07:06,680
any difference

4464
03:07:01,600 --> 03:07:08,800
but um yeah like same 1006 X same thing

4465
03:07:06,680 --> 03:07:10,640
um but this will actually convert this

4466
03:07:08,800 --> 03:07:13,840
to this in case you don't in case you

4467
03:07:10,640 --> 03:07:15,520
haven't done that uh yet on your own so

4468
03:07:13,840 --> 03:07:20,720
this actually comes from the Cuda math

4469
03:07:15,520 --> 03:07:28,359
API reference manual so uh if we look at

4470
03:07:20,720 --> 03:07:32,560
say like some of the single Precision uh

4471
03:07:28,359 --> 03:07:34,080
intrinsics yeah so uh single Precision

4472
03:07:32,560 --> 03:07:36,040
intrinsic functions that are supported

4473
03:07:34,080 --> 03:07:39,279
only in device

4474
03:07:36,040 --> 03:07:44,160
code right notice how it has like Co F

4475
03:07:39,279 --> 03:07:50,000
uh x uh uh exponentiate with base 10 f

4476
03:07:44,160 --> 03:07:52,399
expf um and then like you know F add um

4477
03:07:50,000 --> 03:07:53,840
round toward zero right all of these

4478
03:07:52,399 --> 03:07:57,080
These are these are designed to execute

4479
03:07:53,840 --> 03:08:00,560
on device um and they have F at the end

4480
03:07:57,080 --> 03:08:03,200
but if you were to just do like just

4481
03:08:00,560 --> 03:08:06,279
Coast for example from the math.h librar

4482
03:08:03,200 --> 03:08:08,080
and C that wouldn't that wouldn't run as

4483
03:08:06,279 --> 03:08:09,279
fast so this is another little thing you

4484
03:08:08,080 --> 03:08:12,520
could add to your kernels if you're

4485
03:08:09,279 --> 03:08:14,120
trying to say do like um if you're

4486
03:08:12,520 --> 03:08:16,920
trying to do like soft Max or something

4487
03:08:14,120 --> 03:08:18,880
in a kernel or if you're trying to um

4488
03:08:16,920 --> 03:08:20,760
maybe do like uh like some Digital

4489
03:08:18,880 --> 03:08:24,640
Signal processing right you can add

4490
03:08:20,760 --> 03:08:28,359
these and and get uh some benefits and

4491
03:08:24,640 --> 03:08:30,000
performance- wise out of those um and

4492
03:08:28,359 --> 03:08:32,840
same thing here like if you wanted to do

4493
03:08:30,000 --> 03:08:35,840
a fuse multiply ad um this will like

4494
03:08:32,840 --> 03:08:38,040
tell the actual uh this will

4495
03:08:35,840 --> 03:08:40,840
actually like pour this into the

4496
03:08:38,040 --> 03:08:43,800
instructions where instead of doing like

4497
03:08:40,840 --> 03:08:46,000
separate uh multiply and add operations

4498
03:08:43,800 --> 03:08:47,560
you're fusing them together so you can

4499
03:08:46,000 --> 03:08:51,840
do little tricks like this and just to

4500
03:08:47,560 --> 03:08:54,560
speed things up performance wise but

4501
03:08:51,840 --> 03:08:57,279
uh yeah now we can uh now we can

4502
03:08:54,560 --> 03:08:59,600
actually dive into uh

4503
03:08:57,279 --> 03:09:01,520
profiling I actually forgot to do the

4504
03:08:59,600 --> 03:09:03,200
tiled matrix multiplication by hand so I

4505
03:09:01,520 --> 03:09:05,399
figured I'll just squeeze this in now

4506
03:09:03,200 --> 03:09:06,680
and and let your mind sit on this for a

4507
03:09:05,399 --> 03:09:09,840
little bit before we actually start

4508
03:09:06,680 --> 03:09:12,000
using it and applying it um but before

4509
03:09:09,840 --> 03:09:15,359
we had this this idea of a matrix

4510
03:09:12,000 --> 03:09:17,920
multiplication which was um you have

4511
03:09:15,359 --> 03:09:20,439
like a

4512
03:09:17,920 --> 03:09:25,359
you have like a matrix

4513
03:09:20,439 --> 03:09:28,080
a and can you see that maybe not I'm

4514
03:09:25,359 --> 03:09:28,080
going to move this

4515
03:09:33,359 --> 03:09:41,640
down switch Mark

4516
03:09:36,640 --> 03:09:41,640
here a matrix

4517
03:09:44,160 --> 03:09:49,319
a you with some numbers in it maybe

4518
03:09:51,279 --> 03:09:57,560
and B and the whole idea here is we do

4519
03:09:55,520 --> 03:10:02,680
product this with

4520
03:09:57,560 --> 03:10:05,279
this this with this this with this do

4521
03:10:02,680 --> 03:10:09,479
the same thing and we bring it down

4522
03:10:05,279 --> 03:10:10,760
here right all the way till the end um

4523
03:10:09,479 --> 03:10:13,120
that is one way to do matrix

4524
03:10:10,760 --> 03:10:15,120
multiplication however you can actually

4525
03:10:13,120 --> 03:10:17,600
make this more efficient by using

4526
03:10:15,120 --> 03:10:19,479
something called piling

4527
03:10:17,600 --> 03:10:21,720
so I'll provide some examples on this

4528
03:10:19,479 --> 03:10:24,080
later in the course but this is the idea

4529
03:10:21,720 --> 03:10:27,920
here

4530
03:10:24,080 --> 03:10:31,520
um you have these uh you have these two

4531
03:10:27,920 --> 03:10:33,479
matrices A and B so I'm just going to

4532
03:10:31,520 --> 03:10:35,600
you have to look at this a little bit

4533
03:10:33,479 --> 03:10:38,479
different

4534
03:10:35,600 --> 03:10:41,239
but this is what it looks

4535
03:10:38,479 --> 03:10:45,120
like

4536
03:10:41,239 --> 03:10:48,239
so we have say

4537
03:10:45,120 --> 03:10:52,000
um let's just say this is a and this is

4538
03:10:48,239 --> 03:10:53,920
B okay and then you have this C Matrix

4539
03:10:52,000 --> 03:10:55,359
and how do you compute like the first

4540
03:10:53,920 --> 03:10:57,000
element right well you would you would

4541
03:10:55,359 --> 03:10:58,479
typically take this row and then this

4542
03:10:57,000 --> 03:10:59,840
column and then you would put that there

4543
03:10:58,479 --> 03:11:03,000
because that's where they intersect

4544
03:10:59,840 --> 03:11:05,680
right um but what you can do is you can

4545
03:11:03,000 --> 03:11:08,319
actually take a chunk you can take a

4546
03:11:05,680 --> 03:11:12,000
chunk like an actual square or rectangle

4547
03:11:08,319 --> 03:11:14,040
of a so like maybe this I just put this

4548
03:11:12,000 --> 03:11:16,880
into like separate

4549
03:11:14,040 --> 03:11:18,920
pieces say this is like a like you know

4550
03:11:16,880 --> 03:11:22,880
maybe a a 9

4551
03:11:18,920 --> 03:11:26,439
by9 right and this is also a 9

4552
03:11:22,880 --> 03:11:29,520
by so technically each this each of

4553
03:11:26,439 --> 03:11:31,359
these is technically like a 3X3 tile or

4554
03:11:29,520 --> 03:11:35,279
a matrix on its own right and so we're

4555
03:11:31,359 --> 03:11:35,279
just splitting up splitting this up into

4556
03:11:35,880 --> 03:11:40,960
tiles and so what you can do here

4557
03:11:42,279 --> 03:11:51,880
is you can as I've lined out here you

4558
03:11:46,239 --> 03:11:54,600
can you can go one time you could do a

4559
03:11:51,880 --> 03:11:57,479
Matrix Matrix here times The Matrix

4560
03:11:54,600 --> 03:12:01,479
there um Like A and B respectively like

4561
03:11:57,479 --> 03:12:04,800
you do a * b um and then you add that to

4562
03:12:01,479 --> 03:12:09,239
the Matrix multiply of two and two A and

4563
03:12:04,800 --> 03:12:12,560
B respectively and then three and

4564
03:12:09,239 --> 03:12:14,319
three you start with these and then you

4565
03:12:12,560 --> 03:12:17,680
then you add to these and you add to

4566
03:12:14,319 --> 03:12:20,359
these so it's like

4567
03:12:17,680 --> 03:12:23,200
A1 A1 *

4568
03:12:20,359 --> 03:12:27,359
B1 you multiply those and then you add

4569
03:12:23,200 --> 03:12:30,000
it to A2 * B2 and then add that to A3 *

4570
03:12:27,359 --> 03:12:33,520
B3 and then you end up with this with

4571
03:12:30,000 --> 03:12:34,920
this C1 here and that's the output and

4572
03:12:33,520 --> 03:12:37,200
this is exactly what I've written out in

4573
03:12:34,920 --> 03:12:41,279
a sort of cube format is like you've

4574
03:12:37,200 --> 03:12:43,680
you've laid out some Matrix a right here

4575
03:12:41,279 --> 03:12:47,640
um which is like a a row and then you've

4576
03:12:43,680 --> 03:12:49,000
laid out some M Matrix B here um and

4577
03:12:47,640 --> 03:12:51,760
you're just you're

4578
03:12:49,000 --> 03:12:53,239
doing this times this and then add to

4579
03:12:51,760 --> 03:12:55,800
this times this and then add to this

4580
03:12:53,239 --> 03:12:58,040
times this um and that's and then you

4581
03:12:55,800 --> 03:13:00,560
just end up with C1 and so what you can

4582
03:12:58,040 --> 03:13:03,399
do with like the reason why this is so

4583
03:13:00,560 --> 03:13:05,520
effective is because you can you can

4584
03:13:03,399 --> 03:13:07,640
actually put these tiles and you can pop

4585
03:13:05,520 --> 03:13:09,800
them over to a faster memory like like

4586
03:13:07,640 --> 03:13:11,720
shared memory uh and then they'll end up

4587
03:13:09,800 --> 03:13:14,160
running like ridiculously fast and you

4588
03:13:11,720 --> 03:13:16,520
can end up doing these computations like

4589
03:13:14,160 --> 03:13:17,680
way faster so if you split it into

4590
03:13:16,520 --> 03:13:19,680
little tiles

4591
03:13:17,680 --> 03:13:21,600
and let each little like streaming

4592
03:13:19,680 --> 03:13:23,960
multiprocessor on the on the chip

4593
03:13:21,600 --> 03:13:27,359
actually take care of the individual uh

4594
03:13:23,960 --> 03:13:29,880
tile or multiple tiles um then you can

4595
03:13:27,359 --> 03:13:33,080
actually get a lot more useful uh you

4596
03:13:29,880 --> 03:13:35,399
get a lot more a lot higher uh compute

4597
03:13:33,080 --> 03:13:37,160
throughput you could say um but don't

4598
03:13:35,399 --> 03:13:39,399
worry about this too extensively this is

4599
03:13:37,160 --> 03:13:41,439
just the intuition behind tiling like

4600
03:13:39,399 --> 03:13:42,840
the difference between this and the

4601
03:13:41,439 --> 03:13:45,200
normal version we were doing where we

4602
03:13:42,840 --> 03:13:46,680
like take a whole row and then we take a

4603
03:13:45,200 --> 03:13:47,960
whole column and then we dot product

4604
03:13:46,680 --> 03:13:51,000
them together

4605
03:13:47,960 --> 03:13:52,399
this is different than that so that I

4606
03:13:51,000 --> 03:13:53,960
just wanted to put that in your head for

4607
03:13:52,399 --> 03:13:57,680
later so that it's not a complete

4608
03:13:53,960 --> 03:13:57,680
surprise when we try to make this

4609
03:13:58,600 --> 03:14:04,960
faster now we dig into how can we

4610
03:14:01,760 --> 03:14:07,680
actually profile the performance metrics

4611
03:14:04,960 --> 03:14:09,399
of our own kernels so how do we optimize

4612
03:14:07,680 --> 03:14:12,239
these right and we're going to use

4613
03:14:09,399 --> 03:14:13,760
Nvidia andite compute for this um if

4614
03:14:12,239 --> 03:14:15,000
you're on Windows you might you might

4615
03:14:13,760 --> 03:14:16,439
not have this it might look a bit

4616
03:14:15,000 --> 03:14:18,239
different I haven't tried it on Windows

4617
03:14:16,439 --> 03:14:20,640
yet but this is what we're going to use

4618
03:14:18,239 --> 03:14:22,359
on Linux here so this is kind of what it

4619
03:14:20,640 --> 03:14:24,960
looks like at the end you can see a

4620
03:14:22,359 --> 03:14:27,760
bunch of details about things um it's

4621
03:14:24,960 --> 03:14:29,439
very very interesting but we're going to

4622
03:14:27,760 --> 03:14:32,120
dig into this in a second here just

4623
03:14:29,439 --> 03:14:35,239
going to close these off and uh we'll go

4624
03:14:32,120 --> 03:14:39,000
ahead and get started so let me close

4625
03:14:35,239 --> 03:14:41,560
these here we'll see in this in this uh

4626
03:14:39,000 --> 03:14:43,239
number five kernels chapter in profiling

4627
03:14:41,560 --> 03:14:45,840
we have a bunch of files so we're going

4628
03:14:43,239 --> 03:14:48,800
to start off with this one the mvtx

4629
03:14:45,840 --> 03:14:50,000
matmo so what is what the heck is mvtx

4630
03:14:48,800 --> 03:14:51,319
you guys already know what matrix

4631
03:14:50,000 --> 03:14:56,160
multiplication is I'm not going to go

4632
03:14:51,319 --> 03:15:00,720
over that nvx is like the the custom

4633
03:14:56,160 --> 03:15:02,520
profiler for uh Kudo kernels right and

4634
03:15:00,720 --> 03:15:03,680
what this allows you to do is it's

4635
03:15:02,520 --> 03:15:06,120
actually quite straightforward if you

4636
03:15:03,680 --> 03:15:07,479
look at what's happening here like it's

4637
03:15:06,120 --> 03:15:09,000
it actually makes a lot of sense what's

4638
03:15:07,479 --> 03:15:10,760
happening so we're able to push this

4639
03:15:09,000 --> 03:15:14,399
into a range like essentially the

4640
03:15:10,760 --> 03:15:16,200
timeline um push matrix multiplication

4641
03:15:14,399 --> 03:15:18,359
push memory allocation right so we're

4642
03:15:16,200 --> 03:15:20,920
doing the

4643
03:15:18,359 --> 03:15:23,199
um this is the whole this is the whole

4644
03:15:20,920 --> 03:15:26,600
matrix multiplication thing from start

4645
03:15:23,199 --> 03:15:28,680
to finish um we push things into a range

4646
03:15:26,600 --> 03:15:34,000
so memory allocation and then we pop

4647
03:15:28,680 --> 03:15:36,880
that we pop that out um we copy pop that

4648
03:15:34,000 --> 03:15:38,800
out so it's like start and finish uh and

4649
03:15:36,880 --> 03:15:41,680
then we we do our our dim

4650
03:15:38,800 --> 03:15:43,319
threes we start kernel execution and

4651
03:15:41,680 --> 03:15:44,920
then it's going to stop that once we've

4652
03:15:43,319 --> 03:15:47,439
launched the kernel run it and then

4653
03:15:44,920 --> 03:15:49,160
synchronize all of our um like our

4654
03:15:47,439 --> 03:15:53,319
everything in our

4655
03:15:49,160 --> 03:15:54,880
grid and then copy back to host right uh

4656
03:15:53,319 --> 03:15:57,040
so this is like very straightforward

4657
03:15:54,880 --> 03:15:59,399
literally all you need so I mean keep in

4658
03:15:57,040 --> 03:16:01,080
mind like when we start this one we we

4659
03:15:59,399 --> 03:16:02,800
have another one afterwards so this one

4660
03:16:01,080 --> 03:16:04,239
is only going to Target the recent one

4661
03:16:02,800 --> 03:16:06,279
that was put up right so it's not going

4662
03:16:04,239 --> 03:16:08,399
to jump back to the first one that was

4663
03:16:06,279 --> 03:16:10,120
ever pushed in it's going to be like

4664
03:16:08,399 --> 03:16:12,120
kind of uh like brackets right so you

4665
03:16:10,120 --> 03:16:14,160
have the uh one layer of brackets on the

4666
03:16:12,120 --> 03:16:16,720
outside and then one on the inside it's

4667
03:16:14,160 --> 03:16:20,600
like um it kind of that that's kind of

4668
03:16:16,720 --> 03:16:23,880
the structure of these of this nbtx tool

4669
03:16:20,600 --> 03:16:28,359
um so when we go ahead and

4670
03:16:23,880 --> 03:16:32,160
nbcc um this we want to pass in the uh

4671
03:16:28,359 --> 03:16:34,800
link NV tools extension that's what mvtx

4672
03:16:32,160 --> 03:16:36,680
stands for so Nvidia tools extension

4673
03:16:34,800 --> 03:16:38,560
we're going to compile that we can go

4674
03:16:36,680 --> 03:16:41,439
ahead and you know run this it'll it'll

4675
03:16:38,560 --> 03:16:43,520
run as expected good um and then we can

4676
03:16:41,439 --> 03:16:47,439
actually if we pop back to this read me

4677
03:16:43,520 --> 03:16:51,640
file um we can do NYS profile

4678
03:16:47,439 --> 03:16:56,160
uh stats equals true on the um it's not

4679
03:16:51,640 --> 03:16:56,160
mammal it's 0 but if we go ahead and run

4680
03:16:57,600 --> 03:17:03,680
this we will notice that there's a bunch

4681
03:17:00,720 --> 03:17:06,080
of cool stats that pop up now if you're

4682
03:17:03,680 --> 03:17:07,399
running from a remote machine you could

4683
03:17:06,080 --> 03:17:09,600
you could use this you could just look

4684
03:17:07,399 --> 03:17:13,040
at this directly from the terminal

4685
03:17:09,600 --> 03:17:14,399
however the uh Nvidia ight compute app

4686
03:17:13,040 --> 03:17:17,960
itself is actually a bit more

4687
03:17:14,399 --> 03:17:20,920
informative than this so

4688
03:17:17,960 --> 03:17:24,399
what we can do is is type your Windows

4689
03:17:20,920 --> 03:17:27,520
key and go windows and then type ncu and

4690
03:17:24,399 --> 03:17:29,720
then press enter and it should bring up

4691
03:17:27,520 --> 03:17:32,359
um eni compute and

4692
03:17:29,720 --> 03:17:34,239
it it it popped up on my second monitor

4693
03:17:32,359 --> 03:17:36,720
I just brought it over here but this is

4694
03:17:34,239 --> 03:17:40,880
what it should look like um and what you

4695
03:17:36,720 --> 03:17:42,760
can do from here is um I'm just going to

4696
03:17:40,880 --> 03:17:47,920
put this on the second one and then drag

4697
03:17:42,760 --> 03:17:49,479
the uh report NIS rep file so not the SQ

4698
03:17:47,920 --> 03:17:51,880
light the SQ light is for for a

4699
03:17:49,479 --> 03:17:55,000
different thing but we drag this uh into

4700
03:17:51,880 --> 03:17:55,000
the into the sidebar

4701
03:17:55,640 --> 03:17:59,640
here now it's in we can see it at the

4702
03:17:58,120 --> 03:18:00,920
top and now there's a bunch of

4703
03:17:59,640 --> 03:18:02,600
interesting things in here that we can

4704
03:18:00,920 --> 03:18:03,880
look at so this this text might be a

4705
03:18:02,600 --> 03:18:06,560
little small if you're on a phone but

4706
03:18:03,880 --> 03:18:09,319
just bear with me here so we have bunch

4707
03:18:06,560 --> 03:18:10,399
of stuff on threads um you know nbtx

4708
03:18:09,319 --> 03:18:12,439
what is like what is happening

4709
03:18:10,399 --> 03:18:16,000
sequentially here we can actually zoom

4710
03:18:12,439 --> 03:18:17,560
in and see um you know the memory copy

4711
03:18:16,000 --> 03:18:19,640
kernel execution takes about 2

4712
03:18:17,560 --> 03:18:21,040
milliseconds and we can see everything

4713
03:18:19,640 --> 03:18:22,319
right so all these are actually pushed

4714
03:18:21,040 --> 03:18:25,279
into the range and we can see what's

4715
03:18:22,319 --> 03:18:27,439
happening um and then of course the you

4716
03:18:25,279 --> 03:18:28,840
know the memory allocation takes a while

4717
03:18:27,439 --> 03:18:30,359
uh and then the matrix multiplication

4718
03:18:28,840 --> 03:18:32,640
from start to finish like we highlighted

4719
03:18:30,359 --> 03:18:34,160
in the code um so that's that's how

4720
03:18:32,640 --> 03:18:36,359
that's what mvtx does you can push

4721
03:18:34,160 --> 03:18:38,120
things into a range and you can see how

4722
03:18:36,359 --> 03:18:40,199
long it actually takes you can see like

4723
03:18:38,120 --> 03:18:42,479
when it's happening on the timeline and

4724
03:18:40,199 --> 03:18:45,760
you can look more more in more detail as

4725
03:18:42,479 --> 03:18:50,160
to like what's happening there right so

4726
03:18:45,760 --> 03:18:53,080
um anyways if we go to the Cuda Hardware

4727
03:18:50,160 --> 03:18:55,239
at the top here we can see uh it

4728
03:18:53,080 --> 03:18:58,880
consists of kernels and memory so

4729
03:18:55,239 --> 03:19:01,960
there's like um copying so cud M Copy

4730
03:18:58,880 --> 03:19:04,640
and then there's the Matrix M kernel

4731
03:19:01,960 --> 03:19:07,920
that we can see here and if we click on

4732
03:19:04,640 --> 03:19:10,760
this we go show in events view we can

4733
03:19:07,920 --> 03:19:11,600
click on this down here Zoom to selected

4734
03:19:10,760 --> 03:19:15,000
on

4735
03:19:11,600 --> 03:19:17,239
timeline we can rightclick we can go

4736
03:19:15,000 --> 03:19:18,319
profile kernel

4737
03:19:17,239 --> 03:19:20,000
and there's a bunch of interesting

4738
03:19:18,319 --> 03:19:21,960
things here and this might be a little

4739
03:19:20,000 --> 03:19:25,399
might be a little overwhelming at first

4740
03:19:21,960 --> 03:19:27,600
but there's common filter metrics PM

4741
03:19:25,399 --> 03:19:29,920
sampling warp sampling other so we're

4742
03:19:27,600 --> 03:19:32,040
just going to use PM sampling right now

4743
03:19:29,920 --> 03:19:33,600
um PM sampling is performance metric

4744
03:19:32,040 --> 03:19:36,040
sampling so it's going to give us very

4745
03:19:33,600 --> 03:19:39,080
detailed metrics about things and we'll

4746
03:19:36,040 --> 03:19:41,960
be able to optimize from that so it's

4747
03:19:39,080 --> 03:19:44,479
going to use this binary 0 file that we

4748
03:19:41,960 --> 03:19:46,000
that we made before during compilation

4749
03:19:44,479 --> 03:19:47,840
um and it's going to bring up this new

4750
03:19:46,000 --> 03:19:50,120
menu here which is different than the

4751
03:19:47,840 --> 03:19:53,239
timeline one um so this timeline view

4752
03:19:50,120 --> 03:19:54,600
and then this is different so in here um

4753
03:19:53,239 --> 03:19:56,439
you know we can see all of our Kernels

4754
03:19:54,600 --> 03:19:58,359
at the top so in case we were like maybe

4755
03:19:56,439 --> 03:19:59,840
profiling two different matrix

4756
03:19:58,359 --> 03:20:01,399
multiplication kernels they they might

4757
03:19:59,840 --> 03:20:03,760
both show up here like the the the

4758
03:20:01,399 --> 03:20:05,560
runtime the lifetime of our program that

4759
03:20:03,760 --> 03:20:09,040
that's what would pop up here uh and all

4760
03:20:05,560 --> 03:20:11,000
the kernels inside of that so uh if we

4761
03:20:09,040 --> 03:20:12,239
go to you know say summary there's

4762
03:20:11,000 --> 03:20:13,880
there's some interesting stuff here

4763
03:20:12,239 --> 03:20:17,040
maybe we don't maybe we don't care about

4764
03:20:13,880 --> 03:20:20,399
this too much there's there's details um

4765
03:20:17,040 --> 03:20:21,880
so you have uh throughput so overview of

4766
03:20:20,399 --> 03:20:25,080
throughput for compute and memory

4767
03:20:21,880 --> 03:20:27,960
resources um PM sampling so uh

4768
03:20:25,080 --> 03:20:29,880
performance metrics we can bring this

4769
03:20:27,960 --> 03:20:34,120
down and we can see things like SM

4770
03:20:29,880 --> 03:20:36,120
throughput uh pipe throughput um a bunch

4771
03:20:34,120 --> 03:20:38,800
of the a bunch of metrics I don't even

4772
03:20:36,120 --> 03:20:40,560
understand yet but uh we have things

4773
03:20:38,800 --> 03:20:43,960
like cach hit rate which is really which

4774
03:20:40,560 --> 03:20:46,239
is really useful um but if we go to like

4775
03:20:43,960 --> 03:20:47,560
speed of light throughput for example um

4776
03:20:46,239 --> 03:20:49,960
this is this this is for the memory

4777
03:20:47,560 --> 03:20:54,160
resources we have the compute throughput

4778
03:20:49,960 --> 03:20:56,040
as a percent so that's at about 90 97%

4779
03:20:54,160 --> 03:21:00,640
and then memory is also at about

4780
03:20:56,040 --> 03:21:02,239
97% so um you know we we get we get to

4781
03:21:00,640 --> 03:21:04,600
see cool things like this and and it'll

4782
03:21:02,239 --> 03:21:05,720
make more sense in a second here we go

4783
03:21:04,600 --> 03:21:09,319
down

4784
03:21:05,720 --> 03:21:11,960
to uh memory workload analysis we can

4785
03:21:09,319 --> 03:21:15,920
see memory throughput in it like very

4786
03:21:11,960 --> 03:21:18,880
detailed memory uh I guess memory

4787
03:21:15,920 --> 03:21:20,600
metrics so gigabytes per second how much

4788
03:21:18,880 --> 03:21:24,800
how much are we able to transfer back

4789
03:21:20,600 --> 03:21:27,680
and forth right bytes um Dam bytes per

4790
03:21:24,800 --> 03:21:31,080
second so that GPU vram how how fast are

4791
03:21:27,680 --> 03:21:35,600
we accessing that um and that that speed

4792
03:21:31,080 --> 03:21:38,720
is about 41 gabes per second which um

4793
03:21:35,600 --> 03:21:42,960
which is not super high and then we have

4794
03:21:38,720 --> 03:21:44,840
like uh L1 hit rates L2 hit rates all

4795
03:21:42,960 --> 03:21:45,960
this and we can see a memory chart here

4796
03:21:44,840 --> 03:21:50,800
there's just like a whole bunch of

4797
03:21:45,960 --> 03:21:53,439
metrics that we get access to um and so

4798
03:21:50,800 --> 03:21:56,479
if we can we can we can look at we can

4799
03:21:53,439 --> 03:21:58,399
pay attention to like this number 41 um

4800
03:21:56,479 --> 03:22:02,319
we'll we'll keep this number in our head

4801
03:21:58,399 --> 03:22:03,960
for now um there's also a source too so

4802
03:22:02,319 --> 03:22:06,000
uh you can look at the actual assembly

4803
03:22:03,960 --> 03:22:08,479
instructions and see um you know how

4804
03:22:06,000 --> 03:22:11,600
many registers is it taken up uh a bunch

4805
03:22:08,479 --> 03:22:15,239
of very lowlevel stuff um which I'm not

4806
03:22:11,600 --> 03:22:16,359
going to dig into right now um but yeah

4807
03:22:15,239 --> 03:22:18,120
there there there's so many settings

4808
03:22:16,359 --> 03:22:23,319
that to dig through anyways we're going

4809
03:22:18,120 --> 03:22:23,319
to keep this number um 41 in our

4810
03:22:23,640 --> 03:22:25,800
head

4811
03:22:24,720 --> 03:22:28,840
[Music]

4812
03:22:25,800 --> 03:22:30,920
now we close this out we'll just put on

4813
03:22:28,840 --> 03:22:33,520
the side for

4814
03:22:30,920 --> 03:22:35,920
now we have some other we have some

4815
03:22:33,520 --> 03:22:37,840
other uh scripts as well so I have a

4816
03:22:35,920 --> 03:22:39,960
naive mmal so this is the one that we

4817
03:22:37,840 --> 03:22:42,319
wrote previously this is the exact same

4818
03:22:39,960 --> 03:22:44,239
just our direct copy and paste uh and

4819
03:22:42,319 --> 03:22:45,920
then we have a tiled ml which I'm going

4820
03:22:44,239 --> 03:22:48,800
to cover a little bit later it's a bit

4821
03:22:45,920 --> 03:22:52,439
more advanced against um

4822
03:22:48,800 --> 03:22:54,760
but we're going to compare the

4823
03:22:52,439 --> 03:22:57,040
performance metrics of the naive versus

4824
03:22:54,760 --> 03:23:00,120
the til ml

4825
03:22:57,040 --> 03:23:01,359
so if we go ahead and pop into here we

4826
03:23:00,120 --> 03:23:07,359
go

4827
03:23:01,359 --> 03:23:10,359
nbcc uh 01 and then 01 and we link uh

4828
03:23:07,359 --> 03:23:10,359
Envy

4829
03:23:13,439 --> 03:23:18,720
tools we run successfully and we can go

4830
03:23:16,080 --> 03:23:21,680
n this profile and then put in 01 right

4831
03:23:18,720 --> 03:23:23,600
there and I'm going to go ahead and drag

4832
03:23:21,680 --> 03:23:26,040
this so we pop up another one I'm going

4833
03:23:23,600 --> 03:23:27,680
to go drag this into an Insight

4834
03:23:26,040 --> 03:23:32,239
compute

4835
03:23:27,680 --> 03:23:35,199
and if we check out our uh Cuda Hardware

4836
03:23:32,239 --> 03:23:37,800
go to kernels Matrix multiply this is

4837
03:23:35,199 --> 03:23:40,640
the night version remember uh show an

4838
03:23:37,800 --> 03:23:42,520
events view Zoom to

4839
03:23:40,640 --> 03:23:47,600
selected

4840
03:23:42,520 --> 03:23:49,279
profile uh we run the PM sampling again

4841
03:23:47,600 --> 03:23:52,399
it's going to run that and then we take

4842
03:23:49,279 --> 03:23:53,680
a look at our new stats um you this is

4843
03:23:52,399 --> 03:23:57,920
this is the exact same thing without

4844
03:23:53,680 --> 03:24:00,920
mvtx but just for context

4845
03:23:57,920 --> 03:24:03,399
details memory workload so we get you

4846
03:24:00,920 --> 03:24:05,359
know 30 37 it's it's pretty close to

4847
03:24:03,399 --> 03:24:10,080
what we had before right um maybe a

4848
03:24:05,359 --> 03:24:14,760
little bit lower um but when we when we

4849
03:24:10,080 --> 03:24:14,760
compile the til MML

4850
03:24:24,600 --> 03:24:29,000
it works as expected and say

4851
03:24:27,120 --> 03:24:30,600
profile we're going to get a number

4852
03:24:29,000 --> 03:24:32,720
three

4853
03:24:30,600 --> 03:24:34,880
here I'm going to goad and drag this

4854
03:24:32,720 --> 03:24:37,040
into Insight

4855
03:24:34,880 --> 03:24:40,239
compute we open this

4856
03:24:37,040 --> 03:24:43,080
up pop over to our

4857
03:24:40,239 --> 03:24:44,359
kernels Matrix multiply optimized show

4858
03:24:43,080 --> 03:24:46,880
in

4859
03:24:44,359 --> 03:24:48,439
events assum to select it on timeline so

4860
03:24:46,880 --> 03:24:49,720
we can see the length of this by the way

4861
03:24:48,439 --> 03:24:53,479
this is how long it takes it's going to

4862
03:24:49,720 --> 03:24:58,199
go from you know 430 milliseconds

4863
03:24:53,479 --> 03:24:58,199
43024 millisecs all the way to

4864
03:25:05,240 --> 03:25:08,299
[Music]

4865
03:25:15,319 --> 03:25:20,279
431.073 is significantly higher than it

4866
03:25:17,960 --> 03:25:21,720
was before so these are the types of

4867
03:25:20,279 --> 03:25:23,880
things you want to look out for when you

4868
03:25:21,720 --> 03:25:26,160
see your memory throughput drop after

4869
03:25:23,880 --> 03:25:28,560
you change something it's like uh maybe

4870
03:25:26,160 --> 03:25:30,760
we maybe we shouldn't do that um you

4871
03:25:28,560 --> 03:25:33,680
know from here it went from uh the naive

4872
03:25:30,760 --> 03:25:35,399
it was at 37 and here it's at 60 right

4873
03:25:33,680 --> 03:25:38,520
so that's making a a better use of

4874
03:25:35,399 --> 03:25:40,920
memory um and we'll we'll we'll see more

4875
03:25:38,520 --> 03:25:43,720
optimizations later on especially in

4876
03:25:40,920 --> 03:25:45,800
this in this um faster matal chapter as

4877
03:25:43,720 --> 03:25:47,399
to how we can seriously get this number

4878
03:25:45,800 --> 03:25:49,439
up

4879
03:25:47,399 --> 03:25:51,239
um but yeah this is this is how you

4880
03:25:49,439 --> 03:25:53,279
profile there's a bunch of cool things

4881
03:25:51,239 --> 03:25:54,960
you want to look out for here um it

4882
03:25:53,279 --> 03:25:57,760
really depends on which algorithm you're

4883
03:25:54,960 --> 03:25:59,680
working with with matrix multiplication

4884
03:25:57,760 --> 03:26:02,800
uh there's some more like there's some

4885
03:25:59,680 --> 03:26:05,199
more um fine grain optimizations that

4886
03:26:02,800 --> 03:26:06,880
are just proven to work so we just we

4887
03:26:05,199 --> 03:26:08,920
can just run those and and kind of

4888
03:26:06,880 --> 03:26:10,560
compare the difference uh but the you

4889
03:26:08,920 --> 03:26:12,640
have you have all the resources at your

4890
03:26:10,560 --> 03:26:16,560
hand here there's tons of things that

4891
03:26:12,640 --> 03:26:19,880
you can use and learn from so uh yeah

4892
03:26:16,560 --> 03:26:22,160
this is uh this is how you profile Cuda

4893
03:26:19,880 --> 03:26:24,439
kernels using Nvidia andite

4894
03:26:22,160 --> 03:26:26,960
compute I have a readme file here with

4895
03:26:24,439 --> 03:26:32,520
just pretty much everything we went over

4896
03:26:26,960 --> 03:26:32,520
so um the NS profile command

4897
03:26:33,199 --> 03:26:40,600
um you can profile python as well so NS

4898
03:26:36,720 --> 03:26:42,560
profile and then um you can do you have

4899
03:26:40,600 --> 03:26:46,439
a like an MLP script in Python you can

4900
03:26:42,560 --> 03:26:48,279
profile that funny enough and it'll just

4901
03:26:46,439 --> 03:26:50,600
use the whatever whatever Nvidia

4902
03:26:48,279 --> 03:26:52,560
libraries is you that python file is

4903
03:26:50,600 --> 03:26:55,560
using

4904
03:26:52,560 --> 03:26:57,160
um then we have just you can do this

4905
03:26:55,560 --> 03:26:58,520
some stuff over the command line like

4906
03:26:57,160 --> 03:27:01,520
this

4907
03:26:58,520 --> 03:27:04,760
so uh ncu kernel name you can you can do

4908
03:27:01,520 --> 03:27:06,319
stuff over the command line um but yeah

4909
03:27:04,760 --> 03:27:09,319
there's there's a bunch of useful tools

4910
03:27:06,319 --> 03:27:12,120
here uh so this will this might be

4911
03:27:09,319 --> 03:27:13,880
updated later on um it's not in like the

4912
03:27:12,120 --> 03:27:15,319
the best format yet so this might look a

4913
03:27:13,880 --> 03:27:17,439
bit different when you when you see it

4914
03:27:15,319 --> 03:27:19,880
but uh these are kind of the the main

4915
03:27:17,439 --> 03:27:24,239
ideas and then just to just to I guess

4916
03:27:19,880 --> 03:27:27,760
leave it off on an end note um cupti or

4917
03:27:24,239 --> 03:27:30,239
Cuda um Cuda profiling tools interface

4918
03:27:27,760 --> 03:27:34,080
the PTI at the end this is for like

4919
03:27:30,239 --> 03:27:35,760
creation creating your own uh the

4920
03:27:34,080 --> 03:27:38,840
creating your own custom profiling and

4921
03:27:35,760 --> 03:27:41,479
tracing tools that Target specific C

4922
03:27:38,840 --> 03:27:43,359
applications so you can you can like

4923
03:27:41,479 --> 03:27:45,160
design your own profiling tools with

4924
03:27:43,359 --> 03:27:46,359
this um if that's something that catches

4925
03:27:45,160 --> 03:27:49,160
your interest you might want to look

4926
03:27:46,359 --> 03:27:50,479
more into it so I'll leave this here uh

4927
03:27:49,160 --> 03:27:52,439
but that's that's how you profile cutic

4928
03:27:50,479 --> 03:27:54,880
kernels next up we have this thing

4929
03:27:52,439 --> 03:27:56,840
called an atomic operation and atomic

4930
03:27:54,880 --> 03:27:58,600
operations are used in very specific

4931
03:27:56,840 --> 03:28:02,040
cases so I'm going to try to cover these

4932
03:27:58,600 --> 03:28:04,160
as best I can by atomic we mean the

4933
03:28:02,040 --> 03:28:05,720
indivisibility concept in physics where

4934
03:28:04,160 --> 03:28:07,960
thing cannot be broken down further

4935
03:28:05,720 --> 03:28:09,279
right so you have an atom it's like oh I

4936
03:28:07,960 --> 03:28:10,439
mean technically there are quarks and

4937
03:28:09,279 --> 03:28:11,640
stuff but you don't worry about those

4938
03:28:10,439 --> 03:28:14,960
it's just like the indivis

4939
03:28:11,640 --> 03:28:16,520
indivisibility concept of this thing you

4940
03:28:14,960 --> 03:28:18,199
you cannot cut it in half right there

4941
03:28:16,520 --> 03:28:19,840
are Parts maybe inside of it that that

4942
03:28:18,199 --> 03:28:22,279
make it up but you cannot you cannot cut

4943
03:28:19,840 --> 03:28:24,680
it in half um and that's that's what

4944
03:28:22,279 --> 03:28:26,600
this Atomic operation is and it operates

4945
03:28:24,680 --> 03:28:29,279
as a software abstraction for us so the

4946
03:28:26,600 --> 03:28:33,520
hardware and the Cuda compiler take care

4947
03:28:29,279 --> 03:28:35,520
of all this for us um essentially an

4948
03:28:33,520 --> 03:28:37,040
atomic operation ensures that a

4949
03:28:35,520 --> 03:28:39,520
particular operation on a memory

4950
03:28:37,040 --> 03:28:41,199
location is completed entirely by one

4951
03:28:39,520 --> 03:28:43,399
thread before another thread can access

4952
03:28:41,199 --> 03:28:45,199
or modify the same memory location this

4953
03:28:43,399 --> 03:28:46,720
prevents raise conditions so remember

4954
03:28:45,199 --> 03:28:48,040
before when we were talking about how

4955
03:28:46,720 --> 03:28:49,680
there are like multiple threads that

4956
03:28:48,040 --> 03:28:52,160
like one might end up being faster and

4957
03:28:49,680 --> 03:28:54,080
hit the goal before this one and it's

4958
03:28:52,160 --> 03:28:56,040
they sort of need to like not modify

4959
03:28:54,080 --> 03:28:58,239
each other's uh they not they need to

4960
03:28:56,040 --> 03:29:00,000
like not mess with each other's things

4961
03:28:58,239 --> 03:29:03,279
so that that's what this idea is

4962
03:29:00,000 --> 03:29:06,359
referring to um

4963
03:29:03,279 --> 03:29:07,880
so cannot access or modify the same

4964
03:29:06,359 --> 03:29:10,640
memory location of another thread that's

4965
03:29:07,880 --> 03:29:12,439
very it's a very key point right um and

4966
03:29:10,640 --> 03:29:17,279
and we're going to see a very Crystal

4967
03:29:12,439 --> 03:29:19,840
Clear example of this in a second um

4968
03:29:17,279 --> 03:29:22,239
we might lose some speed

4969
03:29:19,840 --> 03:29:24,120
so if we limit the amount of work done

4970
03:29:22,239 --> 03:29:25,840
on a single piece of memory per unit

4971
03:29:24,120 --> 03:29:27,160
time through put an atomic operation

4972
03:29:25,840 --> 03:29:29,080
we're we're going to lose some speed

4973
03:29:27,160 --> 03:29:32,680
from that right if we're locking things

4974
03:29:29,080 --> 03:29:34,600
down and limiting how how fast the

4975
03:29:32,680 --> 03:29:36,359
program can finish by just having

4976
03:29:34,600 --> 03:29:38,960
everything like not wait for everything

4977
03:29:36,359 --> 03:29:42,239
else then it just finishes faster right

4978
03:29:38,960 --> 03:29:44,520
so uh when we use atomics things will

4979
03:29:42,239 --> 03:29:46,520
slow down but it is guaranteed to be

4980
03:29:44,520 --> 03:29:47,920
Memory safe um and that that's

4981
03:29:46,520 --> 03:29:49,560
ultimately what you might care about in

4982
03:29:47,920 --> 03:29:52,479
some cases it might be better to get the

4983
03:29:49,560 --> 03:29:55,680
memory safe aspect instead of instead of

4984
03:29:52,479 --> 03:29:57,000
the performance gain um so there's a

4985
03:29:55,680 --> 03:29:58,479
bunch of different Atomic operations

4986
03:29:57,000 --> 03:30:01,800
that we have I'm just going to make this

4987
03:29:58,479 --> 03:30:04,120
a bit easier to see um you have Atomic

4988
03:30:01,800 --> 03:30:07,479
ad so essentially what this is you have

4989
03:30:04,120 --> 03:30:09,239
a you have an in uh a pointer to an INT

4990
03:30:07,479 --> 03:30:13,279
some some memory address and you have a

4991
03:30:09,239 --> 03:30:15,479
value and all you do is adds value to

4992
03:30:13,279 --> 03:30:17,720
the value at address so when we when we

4993
03:30:15,479 --> 03:30:19,600
pass in like for example say the number

4994
03:30:17,720 --> 03:30:21,640
four and then we get the memory address

4995
03:30:19,600 --> 03:30:24,560
to that which is some hex code we put

4996
03:30:21,640 --> 03:30:27,040
that we put that hex code in here and

4997
03:30:24,560 --> 03:30:28,680
then we put a Val let's say like two and

4998
03:30:27,040 --> 03:30:31,680
so what that'll do is it'll say okay

4999
03:30:28,680 --> 03:30:33,359
well we have the memory address let's um

5000
03:30:31,680 --> 03:30:35,080
let's get the value for that memory

5001
03:30:33,359 --> 03:30:37,199
address which is four and then we're

5002
03:30:35,080 --> 03:30:38,720
going to add the value to that so it's

5003
03:30:37,199 --> 03:30:40,800
just like this memory address stays the

5004
03:30:38,720 --> 03:30:42,479
same there's nothing new being created

5005
03:30:40,800 --> 03:30:46,040
it's just you're taking this value and

5006
03:30:42,479 --> 03:30:48,040
you're adding it on top um and that's

5007
03:30:46,040 --> 03:30:50,239
that's kind of the whole philosophy of

5008
03:30:48,040 --> 03:30:54,120
everything in here so

5009
03:30:50,239 --> 03:30:56,760
substitution um exchange and and and the

5010
03:30:54,120 --> 03:30:59,840
return value will always be the old

5011
03:30:56,760 --> 03:31:02,120
value so when we do like Atomic add and

5012
03:30:59,840 --> 03:31:05,239
then we say like put int equals Atomic

5013
03:31:02,120 --> 03:31:07,040
ad or of of of whatever is in here it's

5014
03:31:05,239 --> 03:31:09,279
going to return the old value of

5015
03:31:07,040 --> 03:31:11,960
whatever this was so it's going to

5016
03:31:09,279 --> 03:31:14,239
essentially return um the value at

5017
03:31:11,960 --> 03:31:16,600
address right um so we can sort of

5018
03:31:14,239 --> 03:31:18,479
compare and contrast it's it lets us do

5019
03:31:16,600 --> 03:31:20,160
that um or you could just not like

5020
03:31:18,479 --> 03:31:22,279
return anything that's fine you could

5021
03:31:20,160 --> 03:31:24,520
just if you just want to add Val to that

5022
03:31:22,279 --> 03:31:28,080
to the value then to the address value

5023
03:31:24,520 --> 03:31:30,239
then you can just do that um but these

5024
03:31:28,080 --> 03:31:33,160
are all of the uh these are all the

5025
03:31:30,239 --> 03:31:36,239
operations that come with atomic uh

5026
03:31:33,160 --> 03:31:39,199
these are all the atomic operations

5027
03:31:36,239 --> 03:31:40,920
um there's also floating Point Atomic

5028
03:31:39,199 --> 03:31:44,439
operations

5029
03:31:40,920 --> 03:31:46,680
um so you you you can think of atomics

5030
03:31:44,439 --> 03:31:50,199
as like a very fast

5031
03:31:46,680 --> 03:31:52,399
uh Hardware Mutual exclusion operation

5032
03:31:50,199 --> 03:31:53,640
um and I'll I'll dig into mutexes in a

5033
03:31:52,399 --> 03:31:56,520
second here but essentially how this

5034
03:31:53,640 --> 03:31:59,319
goes is you lock down a memory location

5035
03:31:56,520 --> 03:32:02,160
um you set old value the The Returned

5036
03:31:59,319 --> 03:32:03,800
value equal to um like d referencing

5037
03:32:02,160 --> 03:32:05,319
that memory location so like the hex

5038
03:32:03,800 --> 03:32:07,359
code and then you get the value for that

5039
03:32:05,319 --> 03:32:09,000
you set the old you set this old value

5040
03:32:07,359 --> 03:32:13,720
that you're going to return equal to

5041
03:32:09,000 --> 03:32:17,040
that um that that D reference value

5042
03:32:13,720 --> 03:32:19,880
um and then we set

5043
03:32:17,040 --> 03:32:22,000
the we set the D referenced memory

5044
03:32:19,880 --> 03:32:25,560
location so that value that goes with

5045
03:32:22,000 --> 03:32:27,640
that hex code to the old Value Plus the

5046
03:32:25,560 --> 03:32:30,239
increment which is which is Val right

5047
03:32:27,640 --> 03:32:31,720
this is int Val um and then we unlock

5048
03:32:30,239 --> 03:32:34,160
the memory location we return it so it's

5049
03:32:31,720 --> 03:32:35,960
just like during this part where we're

5050
03:32:34,160 --> 03:32:37,760
incrementing and we're storing the old

5051
03:32:35,960 --> 03:32:39,199
value we're going to lock it down so

5052
03:32:37,760 --> 03:32:40,880
nothing else can interfere with that

5053
03:32:39,199 --> 03:32:43,399
it's just this has to complete this is

5054
03:32:40,880 --> 03:32:45,680
priority and that'll that priority will

5055
03:32:43,399 --> 03:32:47,439
exist through however many core threads

5056
03:32:45,680 --> 03:32:49,239
we we have right so that way they can't

5057
03:32:47,439 --> 03:32:53,160
interfere with each other so one has to

5058
03:32:49,239 --> 03:32:56,880
finish before another one accesses it

5059
03:32:53,160 --> 03:33:00,800
um and then we just return that right

5060
03:32:56,880 --> 03:33:02,399
so um in terms of mutual exclusion

5061
03:33:00,800 --> 03:33:06,560
there's a nice YouTube link on here that

5062
03:33:02,399 --> 03:33:08,560
I found which was very good um Mutual is

5063
03:33:06,560 --> 03:33:11,640
is like a like a shared relationship

5064
03:33:08,560 --> 03:33:14,319
between entities so all of us threads um

5065
03:33:11,640 --> 03:33:16,359
we're going to the act of keeping

5066
03:33:14,319 --> 03:33:19,359
something out or preventing ACD

5067
03:33:16,359 --> 03:33:21,800
so we're going to exclude uh everyone

5068
03:33:19,359 --> 03:33:23,319
else from accessing each other's thing

5069
03:33:21,800 --> 03:33:25,160
we're going to let each other finish

5070
03:33:23,319 --> 03:33:28,080
right that's that's that's Mutual

5071
03:33:25,160 --> 03:33:30,680
exclusion and this applies to atomics

5072
03:33:28,080 --> 03:33:32,920
right um so you don't have multiple

5073
03:33:30,680 --> 03:33:34,920
threads accessing the same thing at

5074
03:33:32,920 --> 03:33:37,160
once um and there's there's like an

5075
03:33:34,920 --> 03:33:39,600
intuitive example here of like what this

5076
03:33:37,160 --> 03:33:42,880
might look um at at a lower level what

5077
03:33:39,600 --> 03:33:47,720
this is actually doing so uh if we go

5078
03:33:42,880 --> 03:33:52,000
over to our Atomic ad over here

5079
03:33:47,720 --> 03:33:54,560
um if I if I nvcc compile this um we'll

5080
03:33:52,000 --> 03:33:56,239
see like first of all we import whatever

5081
03:33:54,560 --> 03:33:59,239
we need to the cudar

5082
03:33:56,239 --> 03:34:01,239
runtime. um we have a number of threads

5083
03:33:59,239 --> 03:34:03,560
so a th000 threads per block and then a

5084
03:34:01,239 --> 03:34:05,800
th000 blocks in the grid um these are

5085
03:34:03,560 --> 03:34:07,960
these are macros that we Define so if

5086
03:34:05,800 --> 03:34:09,359
there are a th000 blocks with each 1,000

5087
03:34:07,960 --> 03:34:11,080
threads inside of them then we're going

5088
03:34:09,359 --> 03:34:13,399
to have a total of a million threads

5089
03:34:11,080 --> 03:34:15,720
right um and then we have two kernels

5090
03:34:13,399 --> 03:34:17,880
here so one is going to increment count

5091
03:34:15,720 --> 03:34:21,000
counter non atomically so it's going to

5092
03:34:17,880 --> 03:34:23,199
take in a counter uh it's going to store

5093
03:34:21,000 --> 03:34:26,520
that old value as the D referenced

5094
03:34:23,199 --> 03:34:28,160
counter CU this is a pointer right um

5095
03:34:26,520 --> 03:34:29,720
we're going to set the new value to

5096
03:34:28,160 --> 03:34:31,840
whatever this is whatever that actual

5097
03:34:29,720 --> 03:34:34,080
integer value is plus one we're just

5098
03:34:31,840 --> 03:34:36,760
going to increment by one and then we're

5099
03:34:34,080 --> 03:34:40,760
going to um we're going to update

5100
03:34:36,760 --> 03:34:42,720
counter right um and then there's an

5101
03:34:40,760 --> 03:34:45,199
atomic version of this which does the

5102
03:34:42,720 --> 03:34:47,640
same thing except it locks instead so

5103
03:34:45,199 --> 03:34:51,840
this part here um this is actually like

5104
03:34:47,640 --> 03:34:51,840
you're adding you're essentially adding

5105
03:34:52,160 --> 03:34:55,880
uh not

5106
03:34:56,680 --> 03:35:00,399
locked and this is

5107
03:35:01,640 --> 03:35:05,840
not not

5108
03:35:04,000 --> 03:35:07,640
unlocked right so you're supposed to

5109
03:35:05,840 --> 03:35:11,120
lock here and then unlock there and

5110
03:35:07,640 --> 03:35:16,120
return whatever that is

5111
03:35:11,120 --> 03:35:18,880
so I'm if that's I think that's correct

5112
03:35:16,120 --> 03:35:21,720
yes

5113
03:35:18,880 --> 03:35:23,359
so we go down and everything here is is

5114
03:35:21,720 --> 03:35:25,239
fairly intuitive we have our numb blocks

5115
03:35:23,359 --> 03:35:26,399
and our num threads and it's the idea is

5116
03:35:25,239 --> 03:35:28,359
we're going to have a million threads

5117
03:35:26,399 --> 03:35:30,479
that are each trying to update this same

5118
03:35:28,359 --> 03:35:33,319
this same counter because we pass this

5119
03:35:30,479 --> 03:35:36,600
this counter this is a single variable

5120
03:35:33,319 --> 03:35:38,840
or a single pointer that we pass in um

5121
03:35:36,600 --> 03:35:42,600
and all of these threads have to have to

5122
03:35:38,840 --> 03:35:44,359
modify the same thing right so when we

5123
03:35:42,600 --> 03:35:46,760
actually run

5124
03:35:44,359 --> 03:35:49,720
this you're you're going to see

5125
03:35:46,760 --> 03:35:53,160
non-atomic counter value is 41 so this

5126
03:35:49,720 --> 03:35:55,359
means that all of these threads are

5127
03:35:53,160 --> 03:35:56,520
attacking the same the same memory

5128
03:35:55,359 --> 03:35:59,279
address and they're all performing

5129
03:35:56,520 --> 03:36:01,239
modifications on it at the same time but

5130
03:35:59,279 --> 03:36:02,600
Atomic it's going to take a little while

5131
03:36:01,239 --> 03:36:04,680
longer it might take a million

5132
03:36:02,600 --> 03:36:05,920
operations instead of 41 but it's going

5133
03:36:04,680 --> 03:36:08,800
to ensure that we get through this

5134
03:36:05,920 --> 03:36:10,239
properly so it's going to say okay well

5135
03:36:08,800 --> 03:36:12,439
this thread wants to access it so we

5136
03:36:10,239 --> 03:36:14,279
need to lock down uh only this thread

5137
03:36:12,439 --> 03:36:16,319
can access this value and then all the

5138
03:36:14,279 --> 03:36:17,760
other threads instead of racing to it

5139
03:36:16,319 --> 03:36:20,600
they're going to just wait because it's

5140
03:36:17,760 --> 03:36:22,279
an atomic operation right and so this

5141
03:36:20,600 --> 03:36:23,960
one gets to complete first and then

5142
03:36:22,279 --> 03:36:25,760
maybe this guy and then this guy and

5143
03:36:23,960 --> 03:36:28,000
then this guy and they and they all sort

5144
03:36:25,760 --> 03:36:30,120
of complete um and then you end up with

5145
03:36:28,000 --> 03:36:32,720
the actual true answer which is uh 1

5146
03:36:30,120 --> 03:36:35,239
million right because it increments uh

5147
03:36:32,720 --> 03:36:38,040
it increments from

5148
03:36:35,239 --> 03:36:41,279
uh increments from from

5149
03:36:38,040 --> 03:36:42,960
zero so that's that's pretty much

5150
03:36:41,279 --> 03:36:46,399
atomics

5151
03:36:42,960 --> 03:36:47,840
um they're pretty cool maybe you can

5152
03:36:46,399 --> 03:36:49,800
think of a way that you could use them

5153
03:36:47,840 --> 03:36:51,279
right now I don't know um but that's

5154
03:36:49,800 --> 03:36:53,439
that's just something that's super

5155
03:36:51,279 --> 03:36:55,399
important to cover because uh that's

5156
03:36:53,439 --> 03:36:56,920
that's one of the the risks with uh

5157
03:36:55,399 --> 03:36:58,720
kernels is that you have a bunch of

5158
03:36:56,920 --> 03:37:00,479
these a bunch of different threads

5159
03:36:58,720 --> 03:37:01,960
accessing the same the same thing and

5160
03:37:00,479 --> 03:37:04,520
making changes that maybe you don't want

5161
03:37:01,960 --> 03:37:06,080
to it um and not like getting any errors

5162
03:37:04,520 --> 03:37:08,399
or warnings about it right that's that's

5163
03:37:06,080 --> 03:37:10,800
a danger that you have so atomics helps

5164
03:37:08,399 --> 03:37:13,120
secure that and lock that down so now we

5165
03:37:10,800 --> 03:37:14,760
go into Cuda streams and Cuda streams

5166
03:37:13,120 --> 03:37:17,439
are one of the most useful things for

5167
03:37:14,760 --> 03:37:19,840
performance optimization uh in maybe

5168
03:37:17,439 --> 03:37:21,319
even large systems right so this

5169
03:37:19,840 --> 03:37:22,600
actually this especially Works in large

5170
03:37:21,319 --> 03:37:25,000
systems and you're going to see why in a

5171
03:37:22,600 --> 03:37:27,040
second here um so you can think of the

5172
03:37:25,000 --> 03:37:30,359
intuition here you can think of streams

5173
03:37:27,040 --> 03:37:32,239
as River streams where the uh direction

5174
03:37:30,359 --> 03:37:35,800
of operations flows only forward in time

5175
03:37:32,239 --> 03:37:39,199
so you have this this timeline and the

5176
03:37:35,800 --> 03:37:42,359
idea is normally you would copy some

5177
03:37:39,199 --> 03:37:44,000
data over from host to device and then

5178
03:37:42,359 --> 03:37:46,080
you would do something with that data

5179
03:37:44,000 --> 03:37:48,840
like a kernel launch and and then you

5180
03:37:46,080 --> 03:37:51,000
would copy that back from device to host

5181
03:37:48,840 --> 03:37:52,600
to do something useful with it um and

5182
03:37:51,000 --> 03:37:54,040
what you have here is you have these

5183
03:37:52,600 --> 03:37:56,840
little dependencies where it's like you

5184
03:37:54,040 --> 03:37:58,199
have to wait for the data to come in

5185
03:37:56,840 --> 03:38:00,640
before you actually start the kernel

5186
03:37:58,199 --> 03:38:02,080
launch wouldn't you always want to be

5187
03:38:00,640 --> 03:38:03,760
running kernels and wouldn't you always

5188
03:38:02,080 --> 03:38:05,239
want to be doing computation well

5189
03:38:03,760 --> 03:38:07,520
streams actually solves that issue for

5190
03:38:05,239 --> 03:38:09,640
us instead of just having one little

5191
03:38:07,520 --> 03:38:11,560
timeline you can have an extra layer

5192
03:38:09,640 --> 03:38:12,880
underneath it too and even even even

5193
03:38:11,560 --> 03:38:17,080
more you can have as many layers as you

5194
03:38:12,880 --> 03:38:19,399
want and the the whole idea is you can

5195
03:38:17,080 --> 03:38:21,720
copy some copy something over do a

5196
03:38:19,399 --> 03:38:23,800
kernel launch and then during that

5197
03:38:21,720 --> 03:38:26,120
kernel launch like when when that stuff

5198
03:38:23,800 --> 03:38:29,239
gets copied over you can start copying

5199
03:38:26,120 --> 03:38:31,840
the next stuff over in a separate stream

5200
03:38:29,239 --> 03:38:33,720
right so you can you'll be doing some

5201
03:38:31,840 --> 03:38:35,800
computation while you're copying stuff

5202
03:38:33,720 --> 03:38:39,040
over and then when this stuff is copied

5203
03:38:35,800 --> 03:38:40,439
over um you can do the next kernel so

5204
03:38:39,040 --> 03:38:43,560
it'll it'll look like sort of a

5205
03:38:40,439 --> 03:38:48,000
staircase and I have an example of this

5206
03:38:43,560 --> 03:38:51,040
in the uh Nvidia documentation here the

5207
03:38:48,000 --> 03:38:53,960
Nvidia streams and concurrency

5208
03:38:51,040 --> 03:38:57,800
slides um and essentially looks like

5209
03:38:53,960 --> 03:39:00,120
this so you have um this thing called

5210
03:38:57,800 --> 03:39:03,680
cuda mem copy async which is

5211
03:39:00,120 --> 03:39:05,160
asynchronous um and normally if you're

5212
03:39:03,680 --> 03:39:07,239
doing a Serial program which is what

5213
03:39:05,160 --> 03:39:08,720
we've worked with so far you'd be doing

5214
03:39:07,239 --> 03:39:10,760
you'd be going in this fashion where

5215
03:39:08,720 --> 03:39:13,800
you're you're not always doing you'd be

5216
03:39:10,760 --> 03:39:15,760
like C copy something over do something

5217
03:39:13,800 --> 03:39:17,960
with it copy it back copy something over

5218
03:39:15,760 --> 03:39:19,880
do something with back and then in this

5219
03:39:17,960 --> 03:39:24,479
example you like copy a bunch of stuff

5220
03:39:19,880 --> 03:39:26,000
over host to device and then you do um

5221
03:39:24,479 --> 03:39:28,800
maybe

5222
03:39:26,000 --> 03:39:32,840
uh maybe it's better Illustrated in this

5223
03:39:28,800 --> 03:39:35,399
example is like you you you copy some

5224
03:39:32,840 --> 03:39:38,600
stuff over you do you know you do like

5225
03:39:35,399 --> 03:39:40,560
say like three kernels in a row um and

5226
03:39:38,600 --> 03:39:42,399
then whilst a kernel is running you're

5227
03:39:40,560 --> 03:39:44,239
always copying new stuff over so you're

5228
03:39:42,399 --> 03:39:45,920
not like you're not you're always doing

5229
03:39:44,239 --> 03:39:48,640
work across all the Stream

5230
03:39:45,920 --> 03:39:51,920
right uh and this is super useful

5231
03:39:48,640 --> 03:39:55,680
especially when you have something like

5232
03:39:51,920 --> 03:39:57,520
um when you have things like training a

5233
03:39:55,680 --> 03:39:59,760
massive language model right when you're

5234
03:39:57,520 --> 03:40:01,720
trying to when you have this data loader

5235
03:39:59,760 --> 03:40:04,040
that is like constantly loading big

5236
03:40:01,720 --> 03:40:05,640
chunks of text in you don't want to be

5237
03:40:04,040 --> 03:40:07,760
waiting for that you don't want to just

5238
03:40:05,640 --> 03:40:09,560
do your your your training forward and

5239
03:40:07,760 --> 03:40:11,319
backward pass and then and then wait for

5240
03:40:09,560 --> 03:40:12,840
it again you want like you want it to be

5241
03:40:11,319 --> 03:40:14,120
loaded in while you're doing your

5242
03:40:12,840 --> 03:40:16,160
forward and backward pass like you want

5243
03:40:14,120 --> 03:40:17,800
it to be ready for you so that way you

5244
03:40:16,160 --> 03:40:20,600
can just start you can just do it again

5245
03:40:17,800 --> 03:40:22,359
so it's like non-stop forb back or right

5246
03:40:20,600 --> 03:40:24,199
just you never want to stop doing that

5247
03:40:22,359 --> 03:40:25,600
and that will that will greatly increase

5248
03:40:24,199 --> 03:40:29,520
performance and so this is where Cuda

5249
03:40:25,600 --> 03:40:31,160
streams come in right um so this this

5250
03:40:29,520 --> 03:40:33,439
whole idea that I'm talking about with

5251
03:40:31,160 --> 03:40:35,080
like fetching data um before before you

5252
03:40:33,439 --> 03:40:36,800
actually need it like it's literally

5253
03:40:35,080 --> 03:40:39,120
called prefetching software abstraction

5254
03:40:36,800 --> 03:40:41,160
called pre-etching um so you move data

5255
03:40:39,120 --> 03:40:43,160
around before it as needed and this

5256
03:40:41,160 --> 03:40:46,920
hides the latency of moving data around

5257
03:40:43,160 --> 03:40:49,600
like Cuda M Copy right um

5258
03:40:46,920 --> 03:40:52,720
so we have this we have this kernel

5259
03:40:49,600 --> 03:40:55,239
launch configuration seen this before we

5260
03:40:52,720 --> 03:40:56,960
have a grid size we have a block size

5261
03:40:55,239 --> 03:40:59,000
right and then there were two other

5262
03:40:56,960 --> 03:41:02,760
things that I talked about which are

5263
03:40:59,000 --> 03:41:04,840
here now this is the the number of bytes

5264
03:41:02,760 --> 03:41:06,600
in shared memory right so you're you're

5265
03:41:04,840 --> 03:41:08,080
doing stuff with shared memory which

5266
03:41:06,600 --> 03:41:10,160
don't even worry about that right now

5267
03:41:08,080 --> 03:41:13,279
and then there's this other one s which

5268
03:41:10,160 --> 03:41:15,800
is the associated stream right so you

5269
03:41:13,279 --> 03:41:20,120
can actually put a specific current on a

5270
03:41:15,800 --> 03:41:21,479
stream as we showed in here right um you

5271
03:41:20,120 --> 03:41:22,920
can have these are all the different

5272
03:41:21,479 --> 03:41:25,359
streams stream one stream two stream

5273
03:41:22,920 --> 03:41:27,439
three stream four so you have you you

5274
03:41:25,359 --> 03:41:28,760
launch this one on stream one and Etc

5275
03:41:27,439 --> 03:41:31,000
right so that's that's the whole idea

5276
03:41:28,760 --> 03:41:34,120
there

5277
03:41:31,000 --> 03:41:36,000
um and this this this is just a super

5278
03:41:34,120 --> 03:41:38,520
easy way to interface with the streams

5279
03:41:36,000 --> 03:41:39,600
when we do our Colonel launches so there

5280
03:41:38,520 --> 03:41:41,040
there's multiple things that come in

5281
03:41:39,600 --> 03:41:44,319
here when we're dealing with streams so

5282
03:41:41,040 --> 03:41:45,760
you get this this thing of uh priorities

5283
03:41:44,319 --> 03:41:48,690
so create streams with different

5284
03:41:45,760 --> 03:41:51,000
priorities and if we go into

5285
03:41:48,690 --> 03:41:53,520
[Music]

5286
03:41:51,000 --> 03:41:56,720
um I don't know if it's in this one

5287
03:41:53,520 --> 03:41:59,880
maybe this script we if we look at the

5288
03:41:56,720 --> 03:42:02,680
get priority range here we can see um

5289
03:41:59,880 --> 03:42:04,960
this takes in a pointer to a least

5290
03:42:02,680 --> 03:42:07,920
priority int just a variable and then a

5291
03:42:04,960 --> 03:42:09,279
greatest priority int and so it's like

5292
03:42:07,920 --> 03:42:11,880
this is the least priority we're going

5293
03:42:09,279 --> 03:42:13,800
to we're going to plug this in here uh

5294
03:42:11,880 --> 03:42:15,399
and then we have a greatest priority

5295
03:42:13,800 --> 03:42:18,159
which which we plug in here right and

5296
03:42:15,399 --> 03:42:22,239
that's our range and

5297
03:42:18,159 --> 03:42:24,600
so we can feed these in so that uh Cuda

5298
03:42:22,239 --> 03:42:27,120
will actually manage which ones get more

5299
03:42:24,600 --> 03:42:28,720
priority over others so if you want to

5300
03:42:27,120 --> 03:42:30,520
uh load in a bunch of data first and

5301
03:42:28,720 --> 03:42:32,479
that's your initial priority you want to

5302
03:42:30,520 --> 03:42:34,239
like get that part done as fast as

5303
03:42:32,479 --> 03:42:36,479
possible you can actually prioritize

5304
03:42:34,239 --> 03:42:39,040
that um with like least and greatest

5305
03:42:36,479 --> 03:42:42,840
priority right

5306
03:42:39,040 --> 03:42:46,399
um and then we go uh a little

5307
03:42:42,840 --> 03:42:49,080
bit a little bit further down

5308
03:42:46,399 --> 03:42:54,479
and we have some examples here so let me

5309
03:42:49,080 --> 03:42:55,960
just touch on the basics here so there's

5310
03:42:54,479 --> 03:42:58,080
some stuff that you may have not seen

5311
03:42:55,960 --> 03:43:00,720
before which I probably used earlier but

5312
03:42:58,080 --> 03:43:02,800
I did mention it these are macros uh

5313
03:43:00,720 --> 03:43:05,239
these are the error checking macros that

5314
03:43:02,800 --> 03:43:08,760
we have to essentially make sure that

5315
03:43:05,239 --> 03:43:11,359
operations go through successfully so

5316
03:43:08,760 --> 03:43:13,080
when when we do like a Cuda Malik we

5317
03:43:11,359 --> 03:43:15,279
want to make sure that that that that

5318
03:43:13,080 --> 03:43:17,080
went successfully right and that'll just

5319
03:43:15,279 --> 03:43:20,040
this will

5320
03:43:17,080 --> 03:43:22,520
um this will return a Cuda error type

5321
03:43:20,040 --> 03:43:24,439
meaning either success or error like

5322
03:43:22,520 --> 03:43:26,359
fail right just indicating whether that

5323
03:43:24,439 --> 03:43:29,399
went through or not um so that that's

5324
03:43:26,359 --> 03:43:31,880
what that is and

5325
03:43:29,399 --> 03:43:33,479
then if we scroll down a little bit more

5326
03:43:31,880 --> 03:43:36,760
and see where the actual streams are

5327
03:43:33,479 --> 03:43:40,479
happening um keep in

5328
03:43:36,760 --> 03:43:43,960
mind up here we we use this Cuda stream

5329
03:43:40,479 --> 03:43:45,720
type right so it's a Cuda stream type

5330
03:43:43,960 --> 03:43:47,359
and we Define two streams that stream 1

5331
03:43:45,720 --> 03:43:51,159
and

5332
03:43:47,359 --> 03:43:53,479
two we create the Stream So we actually

5333
03:43:51,159 --> 03:43:55,120
have to have custom uh handlers for this

5334
03:43:53,479 --> 03:43:56,600
to say like okay you made this you made

5335
03:43:55,120 --> 03:43:58,199
you defined it it's like now you have to

5336
03:43:56,600 --> 03:44:00,239
actually create the thing it's a it's a

5337
03:43:58,199 --> 03:44:02,800
weird context thing Nvidia has but it it

5338
03:44:00,239 --> 03:44:04,880
it ensures everything is safe um and

5339
03:44:02,800 --> 03:44:06,560
handled properly by the

5340
03:44:04,880 --> 03:44:09,439
compiler

5341
03:44:06,560 --> 03:44:11,120
so we get this other term kudam mam copy

5342
03:44:09,439 --> 03:44:13,680
async and this essentially just allows

5343
03:44:11,120 --> 03:44:15,359
us to um have like these asynchronous

5344
03:44:13,680 --> 03:44:18,239
copies I mean when you have these

5345
03:44:15,359 --> 03:44:21,040
ordered like if you go camm copy async

5346
03:44:18,239 --> 03:44:23,239
on stream one and then later on you have

5347
03:44:21,040 --> 03:44:25,399
like a like like a kernel launch like

5348
03:44:23,239 --> 03:44:28,040
right underneath it um it'll actually go

5349
03:44:25,399 --> 03:44:29,520
in that order so it won't just it won't

5350
03:44:28,040 --> 03:44:31,680
um it won't try to do like the kernel

5351
03:44:29,520 --> 03:44:33,720
launch before because it's asynchronous

5352
03:44:31,680 --> 03:44:35,720
um it'll just be asynchronous meaning in

5353
03:44:33,720 --> 03:44:39,239
the context of streams so you can have

5354
03:44:35,720 --> 03:44:41,279
things sort of um happening I guess

5355
03:44:39,239 --> 03:44:43,199
concurrently um but it'll still follow

5356
03:44:41,279 --> 03:44:46,080
that sequential order within that stream

5357
03:44:43,199 --> 03:44:48,319
as long as you assign them to the same

5358
03:44:46,080 --> 03:44:50,520
one um so then we have our thread for

5359
03:44:48,319 --> 03:44:53,080
block and blocks per grid configuration

5360
03:44:50,520 --> 03:44:56,680
we launch this on stream one and we have

5361
03:44:53,080 --> 03:44:58,159
this this B this uh this B uh array on

5362
03:44:56,680 --> 03:45:01,319
on stream

5363
03:44:58,159 --> 03:45:03,080
2 and we can do stuff with that as well

5364
03:45:01,319 --> 03:45:04,720
so and then notice how we have the

5365
03:45:03,080 --> 03:45:06,560
commment copy inputs the device

5366
03:45:04,720 --> 03:45:08,359
asynchronously so this is just a little

5367
03:45:06,560 --> 03:45:11,040
cheat where instead of instead of

5368
03:45:08,359 --> 03:45:13,680
copying copying uh a from hosted device

5369
03:45:11,040 --> 03:45:16,560
then copying B to from hosted device uh

5370
03:45:13,680 --> 03:45:18,319
sequentially you do at the same time so

5371
03:45:16,560 --> 03:45:19,720
a gets copied and B gets copied at the

5372
03:45:18,319 --> 03:45:21,600
same time and then you don't have this

5373
03:45:19,720 --> 03:45:23,080
extra barrier here where like nothing no

5374
03:45:21,600 --> 03:45:26,880
work is being done you can just get

5375
03:45:23,080 --> 03:45:28,520
right to uh kernel computation right um

5376
03:45:26,880 --> 03:45:30,800
so we we see that here we have a stream

5377
03:45:28,520 --> 03:45:32,720
one and stream two and this is all done

5378
03:45:30,800 --> 03:45:36,680
on stream one so all this all this

5379
03:45:32,720 --> 03:45:39,600
memory is going to be shared um

5380
03:45:36,680 --> 03:45:41,880
and uh we we go down further to this you

5381
03:45:39,600 --> 03:45:43,600
know we copy back we copy C back

5382
03:45:41,880 --> 03:45:44,720
asynchronously but there's only one so

5383
03:45:43,600 --> 03:45:47,080
it doesn't really matter which stream

5384
03:45:44,720 --> 03:45:50,520
that's on um we just do async because

5385
03:45:47,080 --> 03:45:51,960
why not and then the stream synchronize

5386
03:45:50,520 --> 03:45:54,040
so we're going to ensure that all of the

5387
03:45:51,960 --> 03:45:56,560
streams are then caught up and then we

5388
03:45:54,040 --> 03:45:58,040
can um and then we can do something with

5389
03:45:56,560 --> 03:46:02,080
that right so we're going to you know

5390
03:45:58,040 --> 03:46:04,319
free the device viice a BC and then

5391
03:46:02,080 --> 03:46:05,920
destroy both of the streams uh and then

5392
03:46:04,319 --> 03:46:09,199
it's and then it's done so if I go ahead

5393
03:46:05,920 --> 03:46:09,199
and actually compile and run

5394
03:46:13,239 --> 03:46:17,680
this test passed and we got everything

5395
03:46:15,920 --> 03:46:20,040
correctly and we did this vector

5396
03:46:17,680 --> 03:46:22,680
addition um so it's just kind of what

5397
03:46:20,040 --> 03:46:24,120
what we all we really did here was just

5398
03:46:22,680 --> 03:46:26,680
uh this is where the magic happens

5399
03:46:24,120 --> 03:46:29,600
instead of loading a and then B we load

5400
03:46:26,680 --> 03:46:32,359
and B at the same time um and then we go

5401
03:46:29,600 --> 03:46:34,199
into um what's it

5402
03:46:32,359 --> 03:46:37,239
called

5403
03:46:34,199 --> 03:46:39,840
Advanced so going to lower this a little

5404
03:46:37,239 --> 03:46:41,399
bit so you can see so when we go into

5405
03:46:39,840 --> 03:46:43,920
advanced streams things get a little

5406
03:46:41,399 --> 03:46:45,319
weirder uh but it's not too crazy so

5407
03:46:43,920 --> 03:46:48,399
there's a few things that I want to

5408
03:46:45,319 --> 03:46:52,520
introduce here we have pinned memory so

5409
03:46:48,399 --> 03:46:56,439
it's essentially saying on on the CPU uh

5410
03:46:52,520 --> 03:46:57,760
Global Dr it's going to reserve this

5411
03:46:56,439 --> 03:47:01,439
piece of memory we're going to we're

5412
03:46:57,760 --> 03:47:02,800
going to pin it using Cuda Malik host um

5413
03:47:01,439 --> 03:47:05,279
send it just saying you know this is a

5414
03:47:02,800 --> 03:47:07,159
part of Cuda it's reserved for uh the

5415
03:47:05,279 --> 03:47:08,880
GPU to use later so we're not going to

5416
03:47:07,159 --> 03:47:11,000
modify that we're not going to let the

5417
03:47:08,880 --> 03:47:12,640
operating system or anything play with

5418
03:47:11,000 --> 03:47:13,960
this we're just going to pin it nothing

5419
03:47:12,640 --> 03:47:15,680
can touch it and then we're going to

5420
03:47:13,960 --> 03:47:16,840
drag that over somewhere else for later

5421
03:47:15,680 --> 03:47:19,120
and we're just going to like reserve

5422
03:47:16,840 --> 03:47:20,080
that right um so we're going to use this

5423
03:47:19,120 --> 03:47:21,520
we're going to need this for later so

5424
03:47:20,080 --> 03:47:23,880
don't play with it is a good way to

5425
03:47:21,520 --> 03:47:27,600
think about this

5426
03:47:23,880 --> 03:47:31,399
um events are a critical part of using

5427
03:47:27,600 --> 03:47:34,520
streams so we can measure kernel

5428
03:47:31,399 --> 03:47:37,520
execution time given this uh given this

5429
03:47:34,520 --> 03:47:39,960
example here so we have an event type

5430
03:47:37,520 --> 03:47:42,720
start and stop so the these don't

5431
03:47:39,960 --> 03:47:46,399
actually time anything but they are um

5432
03:47:42,720 --> 03:47:49,080
they're part of uh the input to these uh

5433
03:47:46,399 --> 03:47:50,520
Event Event record functions so we go

5434
03:47:49,080 --> 03:47:52,880
and create these with the memory

5435
03:47:50,520 --> 03:47:54,520
addresses of start and stop and then we

5436
03:47:52,880 --> 03:47:57,199
can plug in whatever stream for example

5437
03:47:54,520 --> 03:47:59,560
just stream or stream one or stream two

5438
03:47:57,199 --> 03:48:03,080
we we do an event

5439
03:47:59,560 --> 03:48:05,680
record we do uh we launch our kernel on

5440
03:48:03,080 --> 03:48:08,239
this stream and then we do another event

5441
03:48:05,680 --> 03:48:11,760
record um in this

5442
03:48:08,239 --> 03:48:14,920
stream and we can take these values

5443
03:48:11,760 --> 03:48:16,960
start and stop and they might carry

5444
03:48:14,920 --> 03:48:20,279
metadata I don't know exactly how Cuda

5445
03:48:16,960 --> 03:48:23,040
handles this but start and stop we can

5446
03:48:20,279 --> 03:48:24,840
uh synchronize we can synchronize uh

5447
03:48:23,040 --> 03:48:28,239
everything and then we can plug it into

5448
03:48:24,840 --> 03:48:30,520
Cuda event lapse time which takes this

5449
03:48:28,239 --> 03:48:31,840
milliseconds float uh which is going to

5450
03:48:30,520 --> 03:48:33,439
be a milliseconds and then you have your

5451
03:48:31,840 --> 03:48:35,880
start and stop and that's going to tell

5452
03:48:33,439 --> 03:48:38,359
you how long your kernel took to run

5453
03:48:35,880 --> 03:48:41,080
right so instead of launching it in the

5454
03:48:38,359 --> 03:48:43,520
instead of going into the the ncu

5455
03:48:41,080 --> 03:48:45,840
profiler you can just do this instead if

5456
03:48:43,520 --> 03:48:47,920
you want to and this will this will not

5457
03:48:45,840 --> 03:48:49,720
have any computational overhead or it's

5458
03:48:47,920 --> 03:48:51,080
very minimal so this can be run in like

5459
03:48:49,720 --> 03:48:54,080
production environments it's not really

5460
03:48:51,080 --> 03:48:56,319
going to cost you anything um and then

5461
03:48:54,080 --> 03:49:01,600
you have the synchronization between

5462
03:48:56,319 --> 03:49:05,080
streams so um essentially these events

5463
03:49:01,600 --> 03:49:06,960
will synchronize they will be placed in

5464
03:49:05,080 --> 03:49:09,560
individual streams so instead of the

5465
03:49:06,960 --> 03:49:11,520
whole device or across all the streams

5466
03:49:09,560 --> 03:49:13,479
it's just one specific stream right that

5467
03:49:11,520 --> 03:49:16,159
that's the whole idea with these um and

5468
03:49:13,479 --> 03:49:18,800
then of course you can overlap uh

5469
03:49:16,159 --> 03:49:21,239
computation data transfer with uh the

5470
03:49:18,800 --> 03:49:23,040
prefetching idea which we talked about

5471
03:49:21,239 --> 03:49:27,880
uh before which I

5472
03:49:23,040 --> 03:49:29,720
believe pre fetching yes so events are

5473
03:49:27,880 --> 03:49:31,040
great for that um and then we have

5474
03:49:29,720 --> 03:49:32,960
callbacks which are used slightly

5475
03:49:31,040 --> 03:49:34,840
differently you you can essentially set

5476
03:49:32,960 --> 03:49:37,319
up a pipeline where the completion of

5477
03:49:34,840 --> 03:49:39,319
one operation on the GPU triggers the

5478
03:49:37,319 --> 03:49:42,159
start of another on the CPU so this is

5479
03:49:39,319 --> 03:49:44,279
going to have some more overhead but if

5480
03:49:42,159 --> 03:49:45,479
you want to log when something happens

5481
03:49:44,279 --> 03:49:48,120
when you want to log when something

5482
03:49:45,479 --> 03:49:50,680
happens on your GPU then you can use a

5483
03:49:48,120 --> 03:49:53,920
call back um so in in this context we

5484
03:49:50,680 --> 03:49:55,680
have a kernel uh and then you know in

5485
03:49:53,920 --> 03:49:58,399
like we have we have a like say say like

5486
03:49:55,680 --> 03:50:00,720
some stream like stream one and then we

5487
03:49:58,399 --> 03:50:03,120
place this call back right after the

5488
03:50:00,720 --> 03:50:05,399
kernel so in the timeline it's going to

5489
03:50:03,120 --> 03:50:06,880
show up as kernel and Cuda stream at

5490
03:50:05,399 --> 03:50:09,760
callback just as the way they are from

5491
03:50:06,880 --> 03:50:12,680
like top to bottom in the code and if

5492
03:50:09,760 --> 03:50:14,720
they're in the same stream um and we put

5493
03:50:12,680 --> 03:50:17,199
this this call back in and that's the

5494
03:50:14,720 --> 03:50:19,319
entally going to say when this finishes

5495
03:50:17,199 --> 03:50:22,199
when when this finishes we're going to

5496
03:50:19,319 --> 03:50:25,199
call this function

5497
03:50:22,199 --> 03:50:29,239
um and it's just going to print GPU

5498
03:50:25,199 --> 03:50:30,680
operation completed right so that's uh

5499
03:50:29,239 --> 03:50:31,880
that that's like one use case of

5500
03:50:30,680 --> 03:50:33,080
callbacks you might not use them all the

5501
03:50:31,880 --> 03:50:34,319
time you're probably going to use events

5502
03:50:33,080 --> 03:50:36,000
more if you're really trying to get

5503
03:50:34,319 --> 03:50:38,080
those optimizations out but let's go

5504
03:50:36,000 --> 03:50:41,199
ahead and look at the uh Advanced

5505
03:50:38,080 --> 03:50:44,840
section here so

5506
03:50:41,199 --> 03:50:46,840
um we have we have kernel one which is

5507
03:50:44,840 --> 03:50:48,399
going to multiply by two and we have

5508
03:50:46,840 --> 03:50:50,760
kernel 2 which is going to add one all

5509
03:50:48,399 --> 03:50:52,600
right very simple operations we have our

5510
03:50:50,760 --> 03:50:54,359
call back here stream call back

5511
03:50:52,600 --> 03:50:57,000
operation completed just print print out

5512
03:50:54,359 --> 03:50:58,720
when something happened right um and

5513
03:50:57,000 --> 03:51:01,359
then just sort of like flowing from top

5514
03:50:58,720 --> 03:51:03,840
to bottom here we do our our Cuda stream

5515
03:51:01,359 --> 03:51:05,800
type so we just declare some streams we

5516
03:51:03,840 --> 03:51:08,680
have our Cuda event type we're just

5517
03:51:05,800 --> 03:51:10,120
going to initialize an event um we can

5518
03:51:08,680 --> 03:51:11,640
print out whatever that event is I was

5519
03:51:10,120 --> 03:51:14,159
testing earlier so this that's why this

5520
03:51:11,640 --> 03:51:18,040
is still here um we do our Cuda malic

5521
03:51:14,159 --> 03:51:21,479
host for that pinned memory um we could

5522
03:51:18,040 --> 03:51:24,520
amalik our um our device

5523
03:51:21,479 --> 03:51:27,159
data we do our our greatest and least

5524
03:51:24,520 --> 03:51:30,239
priorities like I was talking about

5525
03:51:27,159 --> 03:51:32,680
before uh we create we actually create

5526
03:51:30,239 --> 03:51:35,560
the event itself using this previous

5527
03:51:32,680 --> 03:51:40,120
event type that we initialized here and

5528
03:51:35,560 --> 03:51:43,399
then we um we do our you know uh Cuda

5529
03:51:40,120 --> 03:51:44,880
Cuda M Copy async uh we we launch a

5530
03:51:43,399 --> 03:51:46,680
kernel and then this is where things

5531
03:51:44,880 --> 03:51:49,399
start to get a little tricky and I'll do

5532
03:51:46,680 --> 03:51:51,840
my best to explain so when we do Cuda

5533
03:51:49,399 --> 03:51:53,760
event record that's going to place a

5534
03:51:51,840 --> 03:51:55,560
little marker like a little tick right

5535
03:51:53,760 --> 03:51:57,720
in that stream so we have the stream one

5536
03:51:55,560 --> 03:51:59,120
here um you know this this is on stream

5537
03:51:57,720 --> 03:52:01,399
one this on stream one this is on stream

5538
03:51:59,120 --> 03:52:03,319
one so you're going to do your your copy

5539
03:52:01,399 --> 03:52:04,960
from hosted device and then you're going

5540
03:52:03,319 --> 03:52:07,479
to do the kernel and then you're going

5541
03:52:04,960 --> 03:52:11,279
to put a little tick mark right here um

5542
03:52:07,479 --> 03:52:14,960
and what that says is

5543
03:52:11,279 --> 03:52:16,760
um we we might want to do something when

5544
03:52:14,960 --> 03:52:18,680
when this gets reached so notice how

5545
03:52:16,760 --> 03:52:21,680
this is right below our kernel so when

5546
03:52:18,680 --> 03:52:23,920
the kernel is finished when it is done

5547
03:52:21,680 --> 03:52:26,040
uh this is going to this is this is

5548
03:52:23,920 --> 03:52:27,880
going to trigger right and then we have

5549
03:52:26,040 --> 03:52:29,800
this stream weight event as a little

5550
03:52:27,880 --> 03:52:32,840
dependency so it's essentially going to

5551
03:52:29,800 --> 03:52:34,720
wait for everything up to uh stream

5552
03:52:32,840 --> 03:52:37,439
stream one to finish and then it's going

5553
03:52:34,720 --> 03:52:39,880
to begin on on stream two so stream two

5554
03:52:37,439 --> 03:52:43,080
has to actually wait for this to happen

5555
03:52:39,880 --> 03:52:45,000
notice how we pass in uh a stream we

5556
03:52:43,080 --> 03:52:46,800
which is stream two so stream stream 2

5557
03:52:45,000 --> 03:52:49,520
will start doing its things like Colonel

5558
03:52:46,800 --> 03:52:51,840
execution and and the call back um we

5559
03:52:49,520 --> 03:52:55,399
have to wait

5560
03:52:51,840 --> 03:52:57,120
for where did it go we have to wait for

5561
03:52:55,399 --> 03:52:58,600
this event to trigger first and that's

5562
03:52:57,120 --> 03:53:01,000
essentially all this is is it just waits

5563
03:52:58,600 --> 03:53:03,120
for it and then it begins on stream to

5564
03:53:01,000 --> 03:53:04,640
when all of these previous ones are

5565
03:53:03,120 --> 03:53:08,159
completed

5566
03:53:04,640 --> 03:53:10,479
right or at least everything else in uh

5567
03:53:08,159 --> 03:53:14,120
everything else in stream stream one so

5568
03:53:10,479 --> 03:53:16,239
these two so then stream two comes along

5569
03:53:14,120 --> 03:53:19,680
after this is done after we've actually

5570
03:53:16,239 --> 03:53:22,680
done our kernel

5571
03:53:19,680 --> 03:53:24,800
execution and then stream two is going

5572
03:53:22,680 --> 03:53:26,319
to run this second kernel so it just

5573
03:53:24,800 --> 03:53:29,800
it's just kind of ordered in that way so

5574
03:53:26,319 --> 03:53:30,840
you have the async CM copy kernel one

5575
03:53:29,800 --> 03:53:33,000
and then it's going to wait for that to

5576
03:53:30,840 --> 03:53:35,199
finish and then drop down in stream two

5577
03:53:33,000 --> 03:53:37,439
it's going to start um it's going to

5578
03:53:35,199 --> 03:53:41,199
start the second kernel execution and

5579
03:53:37,439 --> 03:53:43,080
then when this is done so when when um

5580
03:53:41,199 --> 03:53:44,960
once we complete this point this is like

5581
03:53:43,080 --> 03:53:46,520
another marker in the timeline

5582
03:53:44,960 --> 03:53:49,040
it's going to wait for all that to

5583
03:53:46,520 --> 03:53:50,800
complete and then it's going to say okay

5584
03:53:49,040 --> 03:53:52,000
awesome we can now do a call back and

5585
03:53:50,800 --> 03:53:53,439
then it's going to go up to this

5586
03:53:52,000 --> 03:53:56,800
function here and it's going to and it's

5587
03:53:53,439 --> 03:53:59,479
going to run that um so that's just kind

5588
03:53:56,800 --> 03:54:02,319
of like stepping through uh one by one

5589
03:53:59,479 --> 03:54:05,479
what is happening there

5590
03:54:02,319 --> 03:54:08,399
um and then we'll just copy back to post

5591
03:54:05,479 --> 03:54:11,239
with Cuda Cuda mem copy async uh and

5592
03:54:08,399 --> 03:54:13,000
then to finalize we always want to uh

5593
03:54:11,239 --> 03:54:14,199
synchronize our streams so we have all

5594
03:54:13,000 --> 03:54:16,199
these streams that are happening we've

5595
03:54:14,199 --> 03:54:17,920
just added another layer of complexity

5596
03:54:16,199 --> 03:54:19,640
we need to synchronize those up too

5597
03:54:17,920 --> 03:54:20,680
right so there's the whole device

5598
03:54:19,640 --> 03:54:22,439
synchronized which is like you

5599
03:54:20,680 --> 03:54:24,159
synchronize all the threads in the

5600
03:54:22,439 --> 03:54:25,680
device and then there's this one which

5601
03:54:24,159 --> 03:54:27,479
is on the level of stream so you have

5602
03:54:25,680 --> 03:54:29,880
like maybe stream one 2 3 four and you

5603
03:54:27,479 --> 03:54:32,239
like wait for all of them to like finish

5604
03:54:29,880 --> 03:54:33,560
um before you before you continue right

5605
03:54:32,239 --> 03:54:37,159
you wait for all them to catch up by

5606
03:54:33,560 --> 03:54:38,239
adding a little barrier blocking um and

5607
03:54:37,159 --> 03:54:40,000
and that's what's happening here and

5608
03:54:38,239 --> 03:54:41,359
then we just destroy all of these we

5609
03:54:40,000 --> 03:54:43,359
just essentially remove all these

5610
03:54:41,359 --> 03:54:45,279
contexts and then we're good to go so

5611
03:54:43,359 --> 03:54:47,159
that's uh that's how stream work that's

5612
03:54:45,279 --> 03:54:51,600
how this Advanced thing works under the

5613
03:54:47,159 --> 03:54:51,600
hood um if I go ahead and run

5614
03:54:56,239 --> 03:55:01,720
this so we notice how when we when we

5615
03:54:58,880 --> 03:55:03,319
are printing out uh where did it go when

5616
03:55:01,720 --> 03:55:05,600
we're printing out the event the event

5617
03:55:03,319 --> 03:55:07,600
is just a pointer so it's like a it's

5618
03:55:05,600 --> 03:55:11,920
just like a memory address thing and

5619
03:55:07,600 --> 03:55:14,560
then we got our our operation completed

5620
03:55:11,920 --> 03:55:16,560
so that's when we do this with when we

5621
03:55:14,560 --> 03:55:19,640
do this call back and then we end up

5622
03:55:16,560 --> 03:55:20,960
with the test passed afterwards so uh

5623
03:55:19,640 --> 03:55:22,920
you know just referencing back to that

5624
03:55:20,960 --> 03:55:24,159
Nvidia diagram with all the all the

5625
03:55:22,920 --> 03:55:26,880
different streams like that that's

5626
03:55:24,159 --> 03:55:29,279
essentially what you care about

5627
03:55:26,880 --> 03:55:30,760
right so I hope the last part wasn't too

5628
03:55:29,279 --> 03:55:33,159
conceptually hard for you that that's

5629
03:55:30,760 --> 03:55:35,880
typically where uh people will sort of

5630
03:55:33,159 --> 03:55:38,560
break down and and question a lot of

5631
03:55:35,880 --> 03:55:40,000
things it it was probably hard but

5632
03:55:38,560 --> 03:55:41,960
anyways I'm glad you made it through

5633
03:55:40,000 --> 03:55:43,439
feel free to rewatch some parts that is

5634
03:55:41,960 --> 03:55:46,040
one of the most challenging parts of the

5635
03:55:43,439 --> 03:55:48,640
course there's lot to unpack it's very

5636
03:55:46,040 --> 03:55:50,279
spatially intuitive but this part is

5637
03:55:48,640 --> 03:55:53,080
supposed to not be very spatially

5638
03:55:50,279 --> 03:55:55,199
intuitive it is supposed to be just like

5639
03:55:53,080 --> 03:55:56,880
textbook examples uh this is how you

5640
03:55:55,199 --> 03:55:59,720
navigate things it's it's not supposed

5641
03:55:56,880 --> 03:56:03,520
to be very hard mathematically spatially

5642
03:55:59,720 --> 03:56:07,840
anything so this is this chapter is on

5643
03:56:03,520 --> 03:56:10,040
the Cuda API so this is chapter six Cuda

5644
03:56:07,840 --> 03:56:13,560
apis um we have a few to go through

5645
03:56:10,040 --> 03:56:16,279
kublos CNN Etc but I want you to

5646
03:56:13,560 --> 03:56:20,960
navigate over to docs. nvidia.com

5647
03:56:16,279 --> 03:56:23,040
Cuda so here we have a lot of resources

5648
03:56:20,960 --> 03:56:25,479
we have a lot of cool things to look at

5649
03:56:23,040 --> 03:56:28,520
um and I just kind of want to point this

5650
03:56:25,479 --> 03:56:30,520
out not that it's specific to uh the cud

5651
03:56:28,520 --> 03:56:32,279
API section but because there's a lot of

5652
03:56:30,520 --> 03:56:34,399
there's a lot of useful things here so

5653
03:56:32,279 --> 03:56:36,239
you have your installation guides for

5654
03:56:34,399 --> 03:56:38,040
like Windows and Linux right like this

5655
03:56:36,239 --> 03:56:40,359
is just like everything you need to get

5656
03:56:38,040 --> 03:56:42,199
started um programming guide best

5657
03:56:40,359 --> 03:56:44,960
practices all the different you know

5658
03:56:42,199 --> 03:56:47,680
Maxwell Pascal VTA Turing m here Hopper

5659
03:56:44,960 --> 03:56:50,520
Ada Maxwell um all these different

5660
03:56:47,680 --> 03:56:52,600
compatibility guides and tuning guides

5661
03:56:50,520 --> 03:56:54,680
uh for different architectures and then

5662
03:56:52,600 --> 03:56:56,760
you have like your PTX which is the

5663
03:56:54,680 --> 03:56:59,120
assembly instructions for Cuda that's

5664
03:56:56,760 --> 03:57:01,399
what it compiles down to uh and then

5665
03:56:59,120 --> 03:57:05,479
just like API references miscellaneous

5666
03:57:01,399 --> 03:57:07,680
stuff um and tools like nbcc uh GDB for

5667
03:57:05,479 --> 03:57:09,960
Cuda so when we covered GDB earlier in

5668
03:57:07,680 --> 03:57:11,239
the C+ plus review section this is the

5669
03:57:09,960 --> 03:57:13,479
equivalent for Cuda so when you're

5670
03:57:11,239 --> 03:57:15,359
debugging Cuda programs you'd use that

5671
03:57:13,479 --> 03:57:18,520
uh and then there's like Insite compute

5672
03:57:15,359 --> 03:57:21,080
which we used earlier um and that's yeah

5673
03:57:18,520 --> 03:57:23,760
just a lot of very informative tools

5674
03:57:21,080 --> 03:57:26,560
here what we mostly care about is the C

5675
03:57:23,760 --> 03:57:29,000
API references so in here we have

5676
03:57:26,560 --> 03:57:30,920
runtime API driver API math API you can

5677
03:57:29,000 --> 03:57:33,760
go through those if you want but mainly

5678
03:57:30,920 --> 03:57:36,399
what I'm going to cover is kublos and

5679
03:57:33,760 --> 03:57:39,760
cdnn which is over here if you go to

5680
03:57:36,399 --> 03:57:42,720
docs. video.com deeplearning CNN you can

5681
03:57:39,760 --> 03:57:45,840
find this and these are the main ones

5682
03:57:42,720 --> 03:57:49,680
which I expect to cover in this section

5683
03:57:45,840 --> 03:57:52,239
so you can think of these uh like kublos

5684
03:57:49,680 --> 03:57:54,399
and CNN as they're not you're not

5685
03:57:52,239 --> 03:57:55,800
actually writing you're not writing

5686
03:57:54,399 --> 03:57:58,080
things out

5687
03:57:55,800 --> 03:58:01,439
manually you're not writing out your own

5688
03:57:58,080 --> 03:58:04,159
kernels it's the the whole idea here is

5689
03:58:01,439 --> 03:58:06,080
you have like this this black box

5690
03:58:04,159 --> 03:58:08,520
function that you call or like a it's

5691
03:58:06,080 --> 03:58:12,479
binded to a shared object file like an

5692
03:58:08,520 --> 03:58:14,520
so and it's opaque so they use these

5693
03:58:12,479 --> 03:58:16,399
these this word called an opaque struct

5694
03:58:14,520 --> 03:58:18,600
type and what that is is is you're just

5695
03:58:16,399 --> 03:58:21,000
calling something that is compiled down

5696
03:58:18,600 --> 03:58:23,600
to to run on the hardware you do not get

5697
03:58:21,000 --> 03:58:25,720
to see it because it's you know in

5698
03:58:23,600 --> 03:58:28,600
encoded in some binary format that you

5699
03:58:25,720 --> 03:58:30,479
can't really read as a human and so you

5700
03:58:28,600 --> 03:58:32,720
have to refer to these opaque struct

5701
03:58:30,479 --> 03:58:34,479
types to be able to call those these are

5702
03:58:32,720 --> 03:58:37,520
highly optimized so like the

5703
03:58:34,479 --> 03:58:39,000
state-of-the-art algorithms in the world

5704
03:58:37,520 --> 03:58:40,920
uh for running you know deep learning

5705
03:58:39,000 --> 03:58:43,279
algorithms that these are the fastest

5706
03:58:40,920 --> 03:58:44,800
ones um sometimes you might get

5707
03:58:43,279 --> 03:58:46,680
something faster depending on the use

5708
03:58:44,800 --> 03:58:48,399
case but we'll generally assume that

5709
03:58:46,680 --> 03:58:52,960
these that the Cuda API provides the

5710
03:58:48,399 --> 03:58:55,560
fastest functions uh generally speaking

5711
03:58:52,960 --> 03:58:57,960
so when you're trying to figure out how

5712
03:58:55,560 --> 03:58:59,920
to get the fastest possible inference to

5713
03:58:57,960 --> 03:59:05,199
work on your GPU

5714
03:58:59,920 --> 03:59:08,080
cluster uh you might want to uh you know

5715
03:59:05,199 --> 03:59:11,560
use something like Cuda API um and then

5716
03:59:08,080 --> 03:59:13,080
going through uh you know just going

5717
03:59:11,560 --> 03:59:14,880
through and researching and figuring out

5718
03:59:13,080 --> 03:59:18,239
how to get it done by going to like

5719
03:59:14,880 --> 03:59:20,040
Google search perplexity Chad gbt um you

5720
03:59:18,239 --> 03:59:22,279
know maybe anthropic

5721
03:59:20,040 --> 03:59:25,359
models and then keyword searching in the

5722
03:59:22,279 --> 03:59:28,040
Nvidia docs like just a crlf like that

5723
03:59:25,359 --> 03:59:31,040
um but the the C API is going to give

5724
03:59:28,040 --> 03:59:32,640
you the fastest stuff right um You may

5725
03:59:31,040 --> 03:59:35,720
have seen this before how we did this

5726
03:59:32,640 --> 03:59:37,720
these like error checks um and these

5727
03:59:35,720 --> 03:59:41,600
essentially just just say like when you

5728
03:59:37,720 --> 03:59:43,680
call a function and say like kublos um

5729
03:59:41,600 --> 03:59:45,279
you're you're going to check if if that

5730
03:59:43,680 --> 03:59:46,880
return an error or not and if it does

5731
03:59:45,279 --> 03:59:48,520
you're going to print the error and the

5732
03:59:46,880 --> 03:59:50,479
line it was at right so these are just

5733
03:59:48,520 --> 03:59:51,760
custom ways of of printing out errors

5734
03:59:50,479 --> 03:59:54,840
and when things don't go according to

5735
03:59:51,760 --> 03:59:56,840
plan so I have these both for um kblast

5736
03:59:54,840 --> 03:59:59,840
and qnn so it just checks the function

5737
03:59:56,840 --> 04:00:02,159
make sure it went through properly

5738
03:59:59,840 --> 04:00:07,199
um

5739
04:00:02,159 --> 04:00:10,680
now kuas is short for Cuda so the CU is

5740
04:00:07,199 --> 04:00:12,720
for Cuda basic linear algebra sub

5741
04:00:10,680 --> 04:00:15,359
routines or subsystems I can't remember

5742
04:00:12,720 --> 04:00:16,880
which one but it's it's for it's for

5743
04:00:15,359 --> 04:00:19,960
linear algebra stuff like matrix

5744
04:00:16,880 --> 04:00:22,000
multiplication right and sgem which is a

5745
04:00:19,960 --> 04:00:23,080
short for single Precision General

5746
04:00:22,000 --> 04:00:25,120
matrix

5747
04:00:23,080 --> 04:00:27,680
multiplication

5748
04:00:25,120 --> 04:00:29,319
um and that's that that's like pretty

5749
04:00:27,680 --> 04:00:30,800
much what this whole like GRE me file is

5750
04:00:29,319 --> 04:00:33,080
about I'm kind of like reciting it as we

5751
04:00:30,800 --> 04:00:34,640
go down but um you know there there are

5752
04:00:33,080 --> 04:00:37,720
resources on this like proper error

5753
04:00:34,640 --> 04:00:39,920
checking Library samples um if we were

5754
04:00:37,720 --> 04:00:41,359
to go to this there's like a library

5755
04:00:39,920 --> 04:00:44,359
samples where you can test out each of

5756
04:00:41,359 --> 04:00:46,640
these um

5757
04:00:44,359 --> 04:00:48,120
but the the whole idea with kublos and

5758
04:00:46,640 --> 04:00:51,239
and how it's important to deep learning

5759
04:00:48,120 --> 04:00:53,800
is in in something like the transformer

5760
04:00:51,239 --> 04:00:54,880
or an MLP in the Transformer itself

5761
04:00:53,800 --> 04:00:56,640
you're going to use this algorithm

5762
04:00:54,880 --> 04:00:58,640
called matrix multiplication and when

5763
04:00:56,640 --> 04:00:59,880
you want the MLP to run really fast or

5764
04:00:58,640 --> 04:01:01,880
you want this language model to have

5765
04:00:59,880 --> 04:01:04,760
really really fast inference time you

5766
04:01:01,880 --> 04:01:06,159
want the algorithms to not really have

5767
04:01:04,760 --> 04:01:08,680
bottlenecks right you want them to run

5768
04:01:06,159 --> 04:01:11,760
as fast as possible on the hardware and

5769
04:01:08,680 --> 04:01:14,080
so using the sub routines in kuas you

5770
04:01:11,760 --> 04:01:15,720
can actually get that um there are other

5771
04:01:14,080 --> 04:01:17,560
ways where you can like combine and mix

5772
04:01:15,720 --> 04:01:20,120
things together but that's more advanced

5773
04:01:17,560 --> 04:01:22,560
for now we're just assume that the

5774
04:01:20,120 --> 04:01:27,080
fastest algorithms exist in kublos and

5775
04:01:22,560 --> 04:01:28,399
CNN for deep learning purposes um so

5776
04:01:27,080 --> 04:01:30,080
we're going to go ahead and start here

5777
04:01:28,399 --> 04:01:33,080
with uh

5778
04:01:30,080 --> 04:01:33,080
kublos

5779
04:01:34,120 --> 04:01:42,680
now basic linear algebra sub programs

5780
04:01:39,000 --> 04:01:46,000
um for accelerating AI high performance

5781
04:01:42,680 --> 04:01:49,720
applications like I said before

5782
04:01:46,000 --> 04:01:52,040
um industry standard blast apis and Gem

5783
04:01:49,720 --> 04:01:53,479
API so General matrix multiplication

5784
04:01:52,040 --> 04:01:55,520
with support for fusions highly

5785
04:01:53,479 --> 04:01:56,800
optimized for NVIDIA gpus I'll dig into

5786
04:01:55,520 --> 04:01:59,760
fusions in a second here don't worry

5787
04:01:56,800 --> 04:02:02,000
about that um but what what I've

5788
04:01:59,760 --> 04:02:04,399
essentially done with each of these is

5789
04:02:02,000 --> 04:02:06,560
I've laid them out into testing so

5790
04:02:04,399 --> 04:02:08,120
before I go into actually like uh

5791
04:02:06,560 --> 04:02:10,920
printing out what the results are and

5792
04:02:08,120 --> 04:02:12,800
and how well these work um and what the

5793
04:02:10,920 --> 04:02:14,880
differences are it's important to cover

5794
04:02:12,800 --> 04:02:18,720
what they actually do what is the

5795
04:02:14,880 --> 04:02:23,000
difference between these so kubalas

5796
04:02:18,720 --> 04:02:25,199
itself is just the super high it's it's

5797
04:02:23,000 --> 04:02:27,000
it's essentially the easiest one to to

5798
04:02:25,199 --> 04:02:28,479
use and get working it's just like the

5799
04:02:27,000 --> 04:02:31,359
the standard that you typically start

5800
04:02:28,479 --> 04:02:34,399
with um and it's going to support uh

5801
04:02:31,359 --> 04:02:35,560
your basic you know uh single Precision

5802
04:02:34,399 --> 04:02:40,080
so

5803
04:02:35,560 --> 04:02:44,560
fp32 um and uh fp16 matrix

5804
04:02:40,080 --> 04:02:47,080
multiplication right um kuas LT is a

5805
04:02:44,560 --> 04:02:50,120
lightweight extension of kuas that

5806
04:02:47,080 --> 04:02:51,800
provides a more flexible API primarily

5807
04:02:50,120 --> 04:02:54,680
aimed at improving performance for

5808
04:02:51,800 --> 04:02:58,239
specific workloads

5809
04:02:54,680 --> 04:03:02,159
um except this is more oriented around

5810
04:02:58,239 --> 04:03:03,800
larger matrices so kuas LT is optimized

5811
04:03:02,159 --> 04:03:06,960
a little differently and it can be

5812
04:03:03,800 --> 04:03:10,359
faster than uh just regular kuas in

5813
04:03:06,960 --> 04:03:12,199
cases so when you have when you have

5814
04:03:10,359 --> 04:03:14,080
something that's a lightweight ideally

5815
04:03:12,199 --> 04:03:15,319
it's going to be lower precision right

5816
04:03:14,080 --> 04:03:17,040
you can think of the L as like

5817
04:03:15,319 --> 04:03:19,159
lightweight or lower Precision whatever

5818
04:03:17,040 --> 04:03:21,159
you want and and essentially what this

5819
04:03:19,159 --> 04:03:23,399
means is is it's the same as kublos

5820
04:03:21,159 --> 04:03:27,040
except uh when you use lower Precision

5821
04:03:23,399 --> 04:03:28,880
like fp16 fp8 and 8 um they're they're

5822
04:03:27,040 --> 04:03:32,319
going to run way way faster and that's

5823
04:03:28,880 --> 04:03:35,439
what LT has designed for so same idea

5824
04:03:32,319 --> 04:03:37,080
just lower Precision bigger matrices um

5825
04:03:35,439 --> 04:03:40,279
different kind of

5826
04:03:37,080 --> 04:03:42,239
workloads and then you have XT which is

5827
04:03:40,279 --> 04:03:44,279
I don't actually recommend this because

5828
04:03:42,239 --> 04:03:47,399
it's ridiculously slow

5829
04:03:44,279 --> 04:03:50,040
um but you can you can interconnect

5830
04:03:47,399 --> 04:03:52,120
multiple gpus and CPUs together to solve

5831
04:03:50,040 --> 04:03:54,720
a problem so if you have a massive

5832
04:03:52,120 --> 04:03:56,880
Matrix uh you have a giant matrix

5833
04:03:54,720 --> 04:03:59,680
multiplication to do you can actually

5834
04:03:56,880 --> 04:04:01,680
split this across the CPU and GPU and

5835
04:03:59,680 --> 04:04:05,159
they will talk to each other and get

5836
04:04:01,680 --> 04:04:09,000
things done um however the the memory

5837
04:04:05,159 --> 04:04:10,040
bandwidth bottlenecks um really limit

5838
04:04:09,000 --> 04:04:12,640
the

5839
04:04:10,040 --> 04:04:15,000
compute because you don't just have this

5840
04:04:12,640 --> 04:04:16,479
super fast like uh this High memory

5841
04:04:15,000 --> 04:04:18,920
bandwidth on the GPU that you can just

5842
04:04:16,479 --> 04:04:20,960
go back and forth like it the memory

5843
04:04:18,920 --> 04:04:23,000
bandwidth between the CPU and GPU is

5844
04:04:20,960 --> 04:04:25,159
really low that's why it takes so long

5845
04:04:23,000 --> 04:04:27,520
to copy things over so you have to worry

5846
04:04:25,159 --> 04:04:30,159
about that uh your your solving speed

5847
04:04:27,520 --> 04:04:32,920
actually gets slowed down a lot um and

5848
04:04:30,159 --> 04:04:35,800
this is one of the the holdbacks with XT

5849
04:04:32,920 --> 04:04:37,920
um but you can run multiple gpus it's

5850
04:04:35,800 --> 04:04:40,080
designed to be thread safe ideal for

5851
04:04:37,920 --> 04:04:43,920
large scale computations from you know

5852
04:04:40,080 --> 04:04:46,319
in distributed workloads um large scale

5853
04:04:43,920 --> 04:04:48,800
algebra that exceeds GPU memory so if

5854
04:04:46,319 --> 04:04:50,800
you have like uh if you have giant

5855
04:04:48,800 --> 04:04:54,120
matrices that it's like

5856
04:04:50,800 --> 04:04:56,840
16,384 by 16,384 and you multiply that

5857
04:04:54,120 --> 04:05:00,080
by itself uh that might not all fit on

5858
04:04:56,840 --> 04:05:06,080
like an 8 GB card you know if I do I go

5859
04:05:00,080 --> 04:05:08,760
Python and I do 16 384 squared that's uh

5860
04:05:06,080 --> 04:05:12,080
what 8 m that's like

5861
04:05:08,760 --> 04:05:14,000
268 that's like 268 Million numbers

5862
04:05:12,080 --> 04:05:15,800
that's a lot of numbers um and if you

5863
04:05:14,000 --> 04:05:19,760
have three of these if you have an a b

5864
04:05:15,800 --> 04:05:23,000
and a c all allocated then that's like

5865
04:05:19,760 --> 04:05:25,159
700 that's like 800 Million numbers and

5866
04:05:23,000 --> 04:05:28,600
then if you you know kick this up to

5867
04:05:25,159 --> 04:05:32,359
like uh fp32 you multiply by the number

5868
04:05:28,600 --> 04:05:34,840
of um you multiply by four that's the

5869
04:05:32,359 --> 04:05:37,920
that's the size of the that's the size

5870
04:05:34,840 --> 04:05:40,960
of a float so four four bytes and you

5871
04:05:37,920 --> 04:05:43,439
get ridiculous numbers like 3.2 GB of

5872
04:05:40,960 --> 04:05:45,199
space so if you have a 2 GB card or it's

5873
04:05:43,439 --> 04:05:47,800
like an embedded system it's not going

5874
04:05:45,199 --> 04:05:50,720
to fit all that um or if you were to

5875
04:05:47,800 --> 04:05:52,640
bump this up to like say

5876
04:05:50,720 --> 04:05:54,239
100,000 that that's not going to fit

5877
04:05:52,640 --> 04:05:56,120
right like these type of numbers are so

5878
04:05:54,239 --> 04:05:58,479
massive that you just need to use

5879
04:05:56,120 --> 04:06:00,680
external CPU dram in order to actually

5880
04:05:58,479 --> 04:06:02,319
store them right um so you can there's

5881
04:06:00,680 --> 04:06:07,159
obviously other ways of optimizing that

5882
04:06:02,319 --> 04:06:12,560
but XT allows you to do this um so I did

5883
04:06:07,159 --> 04:06:15,279
a run of these um where the size was 16

5884
04:06:12,560 --> 04:06:17,319
384 and this all actually fit on my card

5885
04:06:15,279 --> 04:06:23,279
because um you

5886
04:06:17,319 --> 04:06:26,920
know I have uh 8 GB of gpv RAM which is

5887
04:06:23,279 --> 04:06:30,439
low but high in some cases

5888
04:06:26,920 --> 04:06:33,479
um kublos versus kublos XT so I did five

5889
04:06:30,439 --> 04:06:35,800
runs each it's about 0.59 seconds on

5890
04:06:33,479 --> 04:06:39,000
average and then this took about 3.5

5891
04:06:35,800 --> 04:06:41,279
seconds on average so the results ended

5892
04:06:39,000 --> 04:06:44,239
up matching so it was they were like

5893
04:06:41,279 --> 04:06:46,199
pretty much identical uh and then yeah

5894
04:06:44,239 --> 04:06:48,159
it's it's it's just like insane how much

5895
04:06:46,199 --> 04:06:52,359
of a speed up that gets when you just

5896
04:06:48,159 --> 04:06:53,760
stick with using the GPU right and so

5897
04:06:52,359 --> 04:06:55,080
that's like one of the examples why you

5898
04:06:53,760 --> 04:06:59,920
want to be careful when you use things

5899
04:06:55,080 --> 04:07:00,880
like kuas XT um kuas DX is we're not

5900
04:06:59,920 --> 04:07:02,720
going to be doing that we're not going

5901
04:07:00,880 --> 04:07:05,159
to be using that in this course um but

5902
04:07:02,720 --> 04:07:08,000
you can look more into it here with this

5903
04:07:05,159 --> 04:07:09,600
documentation um now cutless is

5904
04:07:08,000 --> 04:07:11,080
something I'll you know I'll cover in

5905
04:07:09,600 --> 04:07:14,399
the in the extra section A little bit

5906
04:07:11,080 --> 04:07:16,840
more but um

5907
04:07:14,399 --> 04:07:20,800
kublos and its variance run on the

5908
04:07:16,840 --> 04:07:24,920
host whatever comes with kublos DX isn't

5909
04:07:20,800 --> 04:07:27,399
well super documented or optimized um

5910
04:07:24,920 --> 04:07:29,960
and when we're trying to do things like

5911
04:07:27,399 --> 04:07:31,800
matrix multiplication in a Transformer

5912
04:07:29,960 --> 04:07:33,359
we may not want to rely on something

5913
04:07:31,800 --> 04:07:35,159
where the where the operations are

5914
04:07:33,359 --> 04:07:37,479
scheduled from the host when it when we

5915
04:07:35,159 --> 04:07:39,760
want to call like a Koss operation the

5916
04:07:37,479 --> 04:07:41,319
CPU tells it to do that right whereas on

5917
04:07:39,760 --> 04:07:43,159
GPU it's just like a kernel that's

5918
04:07:41,319 --> 04:07:44,600
launched and all the operations are are

5919
04:07:43,159 --> 04:07:46,720
done so they're a little bit different

5920
04:07:44,600 --> 04:07:50,080
in that regard

5921
04:07:46,720 --> 04:07:52,640
um when we do like matrix multiplication

5922
04:07:50,080 --> 04:07:54,239
along with like a r activation and then

5923
04:07:52,640 --> 04:07:55,439
another matrix multiplication and then

5924
04:07:54,239 --> 04:07:58,479
another value and then like a

5925
04:07:55,439 --> 04:08:00,439
convolutional layer it's like you you

5926
04:07:58,479 --> 04:08:02,960
typically don't have that in a single

5927
04:08:00,439 --> 04:08:04,600
operation you fuse those together and

5928
04:08:02,960 --> 04:08:05,720
that's this idea of fusion which we

5929
04:08:04,600 --> 04:08:09,960
talked about

5930
04:08:05,720 --> 04:08:11,800
before um and when when you when you

5931
04:08:09,960 --> 04:08:13,720
have something like cutless which is a

5932
04:08:11,800 --> 04:08:17,720
template library that is able to use

5933
04:08:13,720 --> 04:08:22,239
things together um you know you can uh

5934
04:08:17,720 --> 04:08:25,920
you can get a lot higher performance

5935
04:08:22,239 --> 04:08:27,720
so for example um like flash attention

5936
04:08:25,920 --> 04:08:30,399
it doesn't actually use cutless but it's

5937
04:08:27,720 --> 04:08:33,119
an example of what fused Cuda kernels

5938
04:08:30,399 --> 04:08:35,000
look like so flash attention was a paper

5939
04:08:33,119 --> 04:08:37,040
that came out you know I think two three

5940
04:08:35,000 --> 04:08:41,560
years ago something if we just pull this

5941
04:08:37,040 --> 04:08:43,439
up it's uh you know it's essentially for

5942
04:08:41,560 --> 04:08:46,399
the attention mechanism in Transformers

5943
04:08:43,439 --> 04:08:48,279
and it has a whole thing and the idea is

5944
04:08:46,399 --> 04:08:50,720
um you have this this this this

5945
04:08:48,279 --> 04:08:53,359
attention layer in the GPT it's like

5946
04:08:50,720 --> 04:08:55,600
matal drop out soft Max mask and another

5947
04:08:53,359 --> 04:08:57,439
mmal and if you feed these together with

5948
04:08:55,600 --> 04:08:59,159
custom handwritten highly optimized

5949
04:08:57,439 --> 04:09:01,560
kernels you can make this really really

5950
04:08:59,159 --> 04:09:04,439
fast and you can speed this up by like 5

5951
04:09:01,560 --> 04:09:07,000
to 10x on certain hardware and that's

5952
04:09:04,439 --> 04:09:08,960
the idea of fusion so cutless will allow

5953
04:09:07,000 --> 04:09:12,920
you to develop you know faster matrix

5954
04:09:08,960 --> 04:09:14,600
multiplication algorithms um and then

5955
04:09:12,920 --> 04:09:16,159
you would take something like fusion and

5956
04:09:14,600 --> 04:09:17,600
you would combine what you've maybe

5957
04:09:16,159 --> 04:09:19,239
written in cutless and you would you

5958
04:09:17,600 --> 04:09:20,960
would just combine everything so that

5959
04:09:19,239 --> 04:09:22,560
you don't have to rely on whatever

5960
04:09:20,960 --> 04:09:24,279
Nvidia provides and you can just do your

5961
04:09:22,560 --> 04:09:26,439
own thing right that's that's kind of

5962
04:09:24,279 --> 04:09:29,600
the reason why we do things like fusion

5963
04:09:26,439 --> 04:09:32,439
and uh and and we use cutless so

5964
04:09:29,600 --> 04:09:35,520
template uh template linear algebra sub

5965
04:09:32,439 --> 04:09:36,920
routines um but don't worry too much

5966
04:09:35,520 --> 04:09:38,800
about cutless we're not going to go over

5967
04:09:36,920 --> 04:09:41,760
cut list in this course uh there's a lot

5968
04:09:38,800 --> 04:09:43,920
to cover there uh but anyways that's

5969
04:09:41,760 --> 04:09:44,800
like the whole idea of that's the whole

5970
04:09:43,920 --> 04:09:48,960
idea of

5971
04:09:44,800 --> 04:09:50,600
kuas now we dig into these I'm going to

5972
04:09:48,960 --> 04:09:53,399
sort of go through this as best I can to

5973
04:09:50,600 --> 04:09:57,520
illustrate what's happening so we need

5974
04:09:53,399 --> 04:09:58,960
to import this uh kublos V2 right so

5975
04:09:57,520 --> 04:10:01,199
this is this is a new thing we're going

5976
04:09:58,960 --> 04:10:04,279
to add we're going to add F

5977
04:10:01,199 --> 04:10:06,800
fp16 uh inclusions and then just some

5978
04:10:04,279 --> 04:10:09,119
Matrix sizes right as macros then we're

5979
04:10:06,800 --> 04:10:11,319
going to include these these macers that

5980
04:10:09,119 --> 04:10:13,600
check for errors so we wrap those around

5981
04:10:11,319 --> 04:10:15,720
like for example here when we when we do

5982
04:10:13,600 --> 04:10:17,239
a Cuda Malik it's it's running on Cuda

5983
04:10:15,720 --> 04:10:19,319
so we have to make sure that doesn't

5984
04:10:17,239 --> 04:10:20,840
return an error uh we just do this

5985
04:10:19,319 --> 04:10:23,359
consistently for everything to make sure

5986
04:10:20,840 --> 04:10:26,920
that nothing uh just breaks everything

5987
04:10:23,359 --> 04:10:29,520
goes according to plan right um and so

5988
04:10:26,920 --> 04:10:31,720
in this example

5989
04:10:29,520 --> 04:10:33,159
um I'm actually going to go back to a

5990
04:10:31,720 --> 04:10:36,040
section there was a there was a point I

5991
04:10:33,159 --> 04:10:36,040
missed

5992
04:10:37,239 --> 04:10:43,359
um as we saw in the script I make these

5993
04:10:41,760 --> 04:10:45,040
arrays

5994
04:10:43,359 --> 04:10:46,560
and notice how these are small right

5995
04:10:45,040 --> 04:10:48,439
there's a few th there there's a few

5996
04:10:46,560 --> 04:10:50,279
other things like this that you want to

5997
04:10:48,439 --> 04:10:52,720
watch out for when you're comparing

5998
04:10:50,279 --> 04:10:55,439
things because what we're doing here is

5999
04:10:52,720 --> 04:10:58,399
we're essentially comparing the speeds

6000
04:10:55,439 --> 04:10:59,880
and uh how similar things are together

6001
04:10:58,399 --> 04:11:02,840
we want to we want to compare like the

6002
04:10:59,880 --> 04:11:05,640
fp32 with the fp16 see how they perform

6003
04:11:02,840 --> 04:11:06,600
make sure they match up in the end um

6004
04:11:05,640 --> 04:11:10,040
and so there's a lot of things you have

6005
04:11:06,600 --> 04:11:13,359
to watch out for so doing warm-up runs

6006
04:11:10,040 --> 04:11:14,880
is good because uh Cuda might encour

6007
04:11:13,359 --> 04:11:17,000
some additional overhead when you do

6008
04:11:14,880 --> 04:11:21,279
like the first few runs so you want to

6009
04:11:17,000 --> 04:11:22,760
get those done with um and then continue

6010
04:11:21,279 --> 04:11:24,560
with the Benchmark runs so make sure

6011
04:11:22,760 --> 04:11:26,319
that the overhead like just gets like

6012
04:11:24,560 --> 04:11:28,640
removed you want it to like just sort

6013
04:11:26,319 --> 04:11:30,119
that out on its own for like a few runs

6014
04:11:28,640 --> 04:11:31,920
and then you do like a 100 Benchmark

6015
04:11:30,119 --> 04:11:33,760
runs and you take the average time of

6016
04:11:31,920 --> 04:11:35,520
those and that's way you get an accurate

6017
04:11:33,760 --> 04:11:38,600
measurement of how long it takes to

6018
04:11:35,520 --> 04:11:40,399
execute a function or a kernel right um

6019
04:11:38,600 --> 04:11:42,600
without doing any marup runs you might

6020
04:11:40,399 --> 04:11:44,000
see like the first thing take like 50

6021
04:11:42,600 --> 04:11:46,239
milliseconds and then the next one take

6022
04:11:44,000 --> 04:11:47,279
2 milliseconds and it's like whoa what

6023
04:11:46,239 --> 04:11:49,720
happened there well that was the

6024
04:11:47,279 --> 04:11:51,720
overhead that you needed that that kuo

6025
04:11:49,720 --> 04:11:53,439
actually took over and required so

6026
04:11:51,720 --> 04:11:58,000
that's why you do the the warm-up

6027
04:11:53,439 --> 04:12:00,319
runs um so warm-up and Benchmark runs

6028
04:11:58,000 --> 04:12:06,399
you want to verify all of your results

6029
04:12:00,319 --> 04:12:06,399
so uh in in here where was

6030
04:12:07,600 --> 04:12:11,359
it maybe we didn't maybe we didn't

6031
04:12:09,800 --> 04:12:14,040
compare results in here but we do

6032
04:12:11,359 --> 04:12:15,479
somewhere else um

6033
04:12:14,040 --> 04:12:18,399
you want to verify everything so that it

6034
04:12:15,479 --> 04:12:20,760
all matches up and then the last one is

6035
04:12:18,399 --> 04:12:22,920
when you're testing things from scratch

6036
04:12:20,760 --> 04:12:25,080
instead of like randomly initializing

6037
04:12:22,920 --> 04:12:27,119
massive matrices instead of having like

6038
04:12:25,080 --> 04:12:29,520
thousand by thousand and just like

6039
04:12:27,119 --> 04:12:31,359
random distribution you want to populate

6040
04:12:29,520 --> 04:12:32,800
it with values that you can actually

6041
04:12:31,359 --> 04:12:34,760
calculate on your own like something

6042
04:12:32,800 --> 04:12:36,920
that you could take to your whiteboard

6043
04:12:34,760 --> 04:12:38,359
or do it in your head right uh so when

6044
04:12:36,920 --> 04:12:40,000
you have something that's laid out like

6045
04:12:38,359 --> 04:12:42,199
this you can go ahead and write out okay

6046
04:12:40,000 --> 04:12:44,119
well why did it not work right then it's

6047
04:12:42,199 --> 04:12:46,040
more more easy to break down the problem

6048
04:12:44,119 --> 04:12:48,399
and understand what went wrong there as

6049
04:12:46,040 --> 04:12:51,319
opposed to taking apart a

6050
04:12:48,399 --> 04:12:54,040
non-reproducible 1,00 by 1000 Matrix you

6051
04:12:51,319 --> 04:12:56,880
just can't do that um but anyways going

6052
04:12:54,040 --> 04:13:02,040
back to the point here did we initialize

6053
04:12:56,880 --> 04:13:03,479
some matrices so a 3x4 and then a 4X two

6054
04:13:02,040 --> 04:13:07,159
so they should have 12

6055
04:13:03,479 --> 04:13:09,800
elements right we scroll back up this

6056
04:13:07,159 --> 04:13:13,720
goes up to 12 and then this goes up to

6057
04:13:09,800 --> 04:13:13,720
eight that's a 2x4

6058
04:13:13,960 --> 04:13:21,199
we initialize uh these on the CPU so C

6059
04:13:18,119 --> 04:13:22,680
on the CPU the single Precision CU loss

6060
04:13:21,199 --> 04:13:28,920
output on the

6061
04:13:22,680 --> 04:13:31,399
CPU and then uh the the CQ loss output

6062
04:13:28,920 --> 04:13:34,720
uh for the for the half

6063
04:13:31,399 --> 04:13:36,840
Precision uh and then we do a CPU mapal

6064
04:13:34,720 --> 04:13:39,359
just to just to test the results just to

6065
04:13:36,840 --> 04:13:41,000
populate um because again the CPU might

6066
04:13:39,359 --> 04:13:42,720
be the easiest one to actually write out

6067
04:13:41,000 --> 04:13:44,680
for us if we're transferring this from

6068
04:13:42,720 --> 04:13:47,439
python our numpy we just write out the

6069
04:13:44,680 --> 04:13:50,399
CPU everything will be super easy to uh

6070
04:13:47,439 --> 04:13:53,920
compare back to it um so we just we just

6071
04:13:50,399 --> 04:13:56,080
do that first we have this kublos handle

6072
04:13:53,920 --> 04:13:58,359
thing which I'm going to go in into in a

6073
04:13:56,080 --> 04:14:00,479
second here um how like all of these are

6074
04:13:58,359 --> 04:14:03,239
how these how all of these work

6075
04:14:00,479 --> 04:14:05,080
together but you have this kublos handle

6076
04:14:03,239 --> 04:14:06,560
um which just gives like a kublos

6077
04:14:05,080 --> 04:14:08,600
context on like what you're doing you

6078
04:14:06,560 --> 04:14:11,640
just have to initialize this for safety

6079
04:14:08,600 --> 04:14:15,159
we create that with a separate function

6080
04:14:11,640 --> 04:14:16,600
um we do all of our our malx and then we

6081
04:14:15,159 --> 04:14:18,359
have this this new function here which

6082
04:14:16,600 --> 04:14:22,560
you haven't seen before this is called

6083
04:14:18,359 --> 04:14:25,880
This is called sjem so single Precision

6084
04:14:22,560 --> 04:14:30,199
General General with the GE matrix

6085
04:14:25,880 --> 04:14:33,159
multiplication and inside of here if I

6086
04:14:30,199 --> 04:14:34,680
control click on this in vs code we can

6087
04:14:33,159 --> 04:14:36,040
see where this come from we can see

6088
04:14:34,680 --> 04:14:38,560
where this comes from and I can right

6089
04:14:36,040 --> 04:14:40,080
click on this and see where the root of

6090
04:14:38,560 --> 04:14:42,680
this actually is

6091
04:14:40,080 --> 04:14:47,040
right so we look at in order what all

6092
04:14:42,680 --> 04:14:49,359
these are so we have the the handle the

6093
04:14:47,040 --> 04:14:50,520
operation these operations I'm actually

6094
04:14:49,359 --> 04:14:52,680
going to pull these up in a second so

6095
04:14:50,520 --> 04:14:55,640
this makes more sense we have the shape

6096
04:14:52,680 --> 04:14:58,479
so M and K the alpha term which will

6097
04:14:55,640 --> 04:15:00,600
make sense in a second we have a the

6098
04:14:58,479 --> 04:15:03,520
leading dimension of a so in this case

6099
04:15:00,600 --> 04:15:06,560
it'll it it would be M

6100
04:15:03,520 --> 04:15:09,359
um B the leading dimension of B which in

6101
04:15:06,560 --> 04:15:13,159
this case would be n because it's an MN

6102
04:15:09,359 --> 04:15:14,840
time n by K and so the first one going

6103
04:15:13,159 --> 04:15:17,080
to have leading M and then the second

6104
04:15:14,840 --> 04:15:20,840
one's going to have leading uh or sorry

6105
04:15:17,080 --> 04:15:22,560
leading K I mean leading k um B is going

6106
04:15:20,840 --> 04:15:24,560
to have a leading K the leading

6107
04:15:22,560 --> 04:15:27,239
Dimension is going to be a k and then we

6108
04:15:24,560 --> 04:15:29,199
have this beta and a c which we uh

6109
04:15:27,239 --> 04:15:30,640
element wise multiply by and then

6110
04:15:29,199 --> 04:15:32,560
leading dimension of C so there's a lot

6111
04:15:30,640 --> 04:15:35,920
of things you have to add to this and it

6112
04:15:32,560 --> 04:15:38,119
gets it gets quite bloated fast um so I

6113
04:15:35,920 --> 04:15:39,520
guess to just sort of illustrate what

6114
04:15:38,119 --> 04:15:43,960
all of those mean there's a lot to

6115
04:15:39,520 --> 04:15:46,920
unpack here if we scroll down to

6116
04:15:43,960 --> 04:15:49,920
um

6117
04:15:46,920 --> 04:15:49,920
sjem

6118
04:15:54,640 --> 04:16:03,119
sjem kuas sjem awesome so this is a

6119
04:15:59,439 --> 04:16:04,159
super important part um this is exactly

6120
04:16:03,119 --> 04:16:07,439
what we just

6121
04:16:04,159 --> 04:16:09,439
saw and look at this so this is this is

6122
04:16:07,439 --> 04:16:12,520
what the Matrix Matrix multiplication

6123
04:16:09,439 --> 04:16:15,520
looks like so we have a c and we do

6124
04:16:12,520 --> 04:16:16,760
alpha times an OP whatever operation

6125
04:16:15,520 --> 04:16:20,119
which is going to be like maybe a

6126
04:16:16,760 --> 04:16:22,560
transpose on a um Matrix multiply that

6127
04:16:20,119 --> 04:16:25,880
with some operation on B which might be

6128
04:16:22,560 --> 04:16:31,479
a transpose and then plus uh the beta

6129
04:16:25,880 --> 04:16:31,479
term which beta is a

6130
04:16:33,880 --> 04:16:39,359
um beta is just going to be a constant

6131
04:16:36,880 --> 04:16:43,119
float number so like in this case you

6132
04:16:39,359 --> 04:16:46,880
might want to have a as 1.0 and B as 0.0

6133
04:16:43,119 --> 04:16:49,920
just so that you're only doing um a * b

6134
04:16:46,880 --> 04:16:52,040
equals c right um but it does gener it

6135
04:16:49,920 --> 04:16:53,920
does provide this this abstraction for

6136
04:16:52,040 --> 04:16:55,800
you so that you can do more with it if

6137
04:16:53,920 --> 04:16:59,359
you want to like add something on later

6138
04:16:55,800 --> 04:17:01,600
like maybe a like maybe a bias right

6139
04:16:59,359 --> 04:17:04,960
um and then we have all these other

6140
04:17:01,600 --> 04:17:08,640
operations so kublos op n kublos op T

6141
04:17:04,960 --> 04:17:11,040
and kublos op C so

6142
04:17:08,640 --> 04:17:13,680
um we want to we want to worry about the

6143
04:17:11,040 --> 04:17:15,840
n and the T here so the n is

6144
04:17:13,680 --> 04:17:18,399
like no

6145
04:17:15,840 --> 04:17:21,399
operations the t is a

6146
04:17:18,399 --> 04:17:21,399
transpose

6147
04:17:23,720 --> 04:17:30,279
um and then you have this column major

6148
04:17:26,520 --> 04:17:31,880
format so column major throws a lot of

6149
04:17:30,279 --> 04:17:34,680
this off so to illustrate this

6150
04:17:31,880 --> 04:17:38,159
difference of column versus row major as

6151
04:17:34,680 --> 04:17:40,800
we saw in uh as we saw here so matrices

6152
04:17:38,159 --> 04:17:44,920
are stored in column major format with

6153
04:17:40,800 --> 04:17:48,479
these um with these Dimensions m m by K

6154
04:17:44,920 --> 04:17:50,520
and then K byn and c m byn um column

6155
04:17:48,479 --> 04:17:53,040
Major versus row major is very important

6156
04:17:50,520 --> 04:17:55,560
so it's actually a little harder because

6157
04:17:53,040 --> 04:18:00,080
it's column major but we'll make do

6158
04:17:55,560 --> 04:18:00,080
anyways column major

6159
04:18:01,080 --> 04:18:07,760
Matrix we'll just have

6160
04:18:05,199 --> 04:18:09,520
a well actually we make it row major

6161
04:18:07,760 --> 04:18:14,760
first that's a good

6162
04:18:09,520 --> 04:18:14,760
idea R major a equals

6163
04:18:22,040 --> 04:18:27,479
uh and then call

6164
04:18:24,479 --> 04:18:27,479
major

6165
04:18:33,279 --> 04:18:38,199
a and then the memory layout

6166
04:18:45,680 --> 04:18:50,560
so notice the difference here so we have

6167
04:18:48,479 --> 04:18:54,760
a we have a

6168
04:18:50,560 --> 04:18:58,399
uh a 2x4 Matrix right it goes 1 2 3 4 5

6169
04:18:54,760 --> 04:18:59,880
6 7 8 then here we have 1 5 2 6 3 7 4 8

6170
04:18:58,399 --> 04:19:03,800
right so the whole idea here is we

6171
04:18:59,880 --> 04:19:03,800
essentially transposed it

6172
04:19:04,800 --> 04:19:09,760
we the way this row goes from left to

6173
04:19:07,439 --> 04:19:13,080
right it is now going from top to

6174
04:19:09,760 --> 04:19:14,640
bottom and now we have to deal with this

6175
04:19:13,080 --> 04:19:16,239
so if we have something that's like I

6176
04:19:14,640 --> 04:19:19,319
mean typically when you're going to feed

6177
04:19:16,239 --> 04:19:22,479
a matrix into a matrix multiplication

6178
04:19:19,319 --> 04:19:25,600
you expect it to be in row major and

6179
04:19:22,479 --> 04:19:28,520
kublos kublos sjem expects it to be in

6180
04:19:25,600 --> 04:19:30,760
column major so there's a way to get

6181
04:19:28,520 --> 04:19:31,960
around this and notice how this not by

6182
04:19:30,760 --> 04:19:33,720
the way notice how this is laid out in

6183
04:19:31,960 --> 04:19:36,279
memory this is like a very simple way of

6184
04:19:33,720 --> 04:19:37,560
looking at it and then this is like okay

6185
04:19:36,279 --> 04:19:39,119
that's I guess so but a little

6186
04:19:37,560 --> 04:19:45,119
interesting because of you know it's

6187
04:19:39,119 --> 04:19:47,319
just like 1526 1526 and that order but

6188
04:19:45,119 --> 04:19:50,880
uh there's an interesting article I

6189
04:19:47,319 --> 04:19:52,319
found on how to uh how to deal with this

6190
04:19:50,880 --> 04:19:56,880
so this is why you see our dimensions

6191
04:19:52,319 --> 04:19:58,960
are a little messed up um but on this

6192
04:19:56,880 --> 04:20:01,119
pay attention to your shaping from stack

6193
04:19:58,960 --> 04:20:03,399
Overflow I found an answer to this and

6194
04:20:01,119 --> 04:20:06,399
how to make it actually work to your

6195
04:20:03,399 --> 04:20:06,399
favor

6196
04:20:07,800 --> 04:20:14,279
so where is

6197
04:20:10,319 --> 04:20:17,000
it this guy pretty much said

6198
04:20:14,279 --> 04:20:19,680
um kuas interprets matrices as column

6199
04:20:17,000 --> 04:20:21,840
ordered so when you execute this you are

6200
04:20:19,680 --> 04:20:24,080
correctly transposing uh cuz you're

6201
04:20:21,840 --> 04:20:26,520
doing an opt which is going to transpose

6202
04:20:24,080 --> 04:20:29,479
a and then an opt is going to transpose

6203
04:20:26,520 --> 04:20:32,000
uh B you're correctly transposing each

6204
04:20:29,479 --> 04:20:33,680
input um but KU stumps will result in

6205
04:20:32,000 --> 04:20:35,520
column major order so you want it to

6206
04:20:33,680 --> 04:20:37,560
come back in row major order so that you

6207
04:20:35,520 --> 04:20:39,279
can use it for something else so what

6208
04:20:37,560 --> 04:20:43,279
you end up having to do to avoid this

6209
04:20:39,279 --> 04:20:46,720
whole column major mess is Trick KU into

6210
04:20:43,279 --> 04:20:48,680
Computing differently um and so the way

6211
04:20:46,720 --> 04:20:52,680
that you the way that you call this is

6212
04:20:48,680 --> 04:20:54,279
you say handle op n OPN and you go n MK

6213
04:20:52,680 --> 04:20:59,040
instead of

6214
04:20:54,279 --> 04:21:02,640
mkn uh and then you go um Alpha and then

6215
04:20:59,040 --> 04:21:04,159
your your B Matrix instead of a um

6216
04:21:02,640 --> 04:21:06,199
because normally you would do a here but

6217
04:21:04,159 --> 04:21:10,239
instead you do

6218
04:21:06,199 --> 04:21:12,640
B um and then you do you're leading a

6219
04:21:10,239 --> 04:21:15,840
dimension so remember n n is the leading

6220
04:21:12,640 --> 04:21:17,720
a dimension now and then you have um

6221
04:21:15,840 --> 04:21:20,239
then you have your a and the leading

6222
04:21:17,720 --> 04:21:22,840
dimension for that is going to be um

6223
04:21:20,239 --> 04:21:24,479
it's going to be K I know it's confusing

6224
04:21:22,840 --> 04:21:27,239
but just bear with me and then you have

6225
04:21:24,479 --> 04:21:30,319
the beta which is just like that that

6226
04:21:27,239 --> 04:21:31,960
number you're going to multiply C by and

6227
04:21:30,319 --> 04:21:34,439
then the leading dimension for C is

6228
04:21:31,960 --> 04:21:35,560
going to be n so there's a lot there's a

6229
04:21:34,439 --> 04:21:38,680
lot of different there's a lot of weird

6230
04:21:35,560 --> 04:21:39,920
changes there but essentially that's

6231
04:21:38,680 --> 04:21:42,880
going to that's going to avoid the

6232
04:21:39,920 --> 04:21:47,920
column major issue so when we do this in

6233
04:21:42,880 --> 04:21:51,479
DS code you can see that I call uh Kos

6234
04:21:47,920 --> 04:21:55,000
sjem with that same idea so n n and then

6235
04:21:51,479 --> 04:21:59,319
go n MK When we put our when we put

6236
04:21:55,000 --> 04:22:02,479
device B Matrix we put n device a matrix

6237
04:21:59,319 --> 04:22:04,159
K beta device C Matrix then n again so

6238
04:22:02,479 --> 04:22:06,239
just exactly like copy and paste from

6239
04:22:04,159 --> 04:22:09,239
that um and this

6240
04:22:06,239 --> 04:22:09,239
works

6241
04:22:10,520 --> 04:22:17,760
so this this this concept applies to H

6242
04:22:14,800 --> 04:22:20,199
gem as well which is what we have below

6243
04:22:17,760 --> 04:22:22,239
so I've done a single precision and a

6244
04:22:20,199 --> 04:22:24,040
half precision as well um and there's

6245
04:22:22,239 --> 04:22:26,119
like a little casting operation that you

6246
04:22:24,040 --> 04:22:30,000
do here it's like a flow to half

6247
04:22:26,119 --> 04:22:32,040
function which um we we initialize these

6248
04:22:30,000 --> 04:22:34,000
as as half Precision matrices and then

6249
04:22:32,040 --> 04:22:36,000
we we do a float to have conversion and

6250
04:22:34,000 --> 04:22:38,159
store the result in those based on our

6251
04:22:36,000 --> 04:22:40,600
index

6252
04:22:38,159 --> 04:22:43,119
um camm

6253
04:22:40,600 --> 04:22:46,640
copy High Precision data to the device

6254
04:22:43,119 --> 04:22:49,319
so the host right this is a host sorry a

6255
04:22:46,640 --> 04:22:53,720
half

6256
04:22:49,319 --> 04:22:56,479
um and then we just copy back we we do

6257
04:22:53,720 --> 04:23:00,880
the hgem which is the half Precision uh

6258
04:22:56,479 --> 04:23:03,399
version of this and then we copy back

6259
04:23:00,880 --> 04:23:04,960
um and then we we can just print out the

6260
04:23:03,399 --> 04:23:08,439
results to make sure everything matches

6261
04:23:04,960 --> 04:23:11,359
up so when I go

6262
04:23:08,439 --> 04:23:13,239
nbcc and we have to add this little L

6263
04:23:11,359 --> 04:23:14,920
like link this is for link and then we

6264
04:23:13,239 --> 04:23:17,119
put kublos at the end of it that's what

6265
04:23:14,920 --> 04:23:18,319
that means cuz cuos doesn't come just

6266
04:23:17,119 --> 04:23:20,319
right out of the B you have to actually

6267
04:23:18,319 --> 04:23:22,640
manually link it because it has to has

6268
04:23:20,319 --> 04:23:25,560
to do that part separately we just go

6269
04:23:22,640 --> 04:23:29,399
ahead and run this we can see our

6270
04:23:25,560 --> 04:23:33,640
matrices so when we go up this is uh row

6271
04:23:29,399 --> 04:23:36,319
major right 1 2 3 4 1 2 3 4 5 6 7 8 9 10

6272
04:23:33,640 --> 04:23:39,800
11 12 and same one same idea with this

6273
04:23:36,319 --> 04:23:42,119
one it's you know it's a 4X two so it's

6274
04:23:39,800 --> 04:23:44,439
going to be four high and then two two

6275
04:23:42,119 --> 04:23:44,439
wide

6276
04:23:46,080 --> 04:23:54,359
um so the two the two uh width is one

6277
04:23:50,960 --> 04:23:56,880
two and then 1 2 3 4 5 6 7 8 right

6278
04:23:54,359 --> 04:23:59,920
everything is lined up as we want it to

6279
04:23:56,880 --> 04:24:02,279
um and when we do a a row major matrix

6280
04:23:59,920 --> 04:24:03,560
multiplication on the CPU we end up with

6281
04:24:02,279 --> 04:24:05,520
what we're supposed to get so these

6282
04:24:03,560 --> 04:24:07,760
inner Dimensions cancel out and we end

6283
04:24:05,520 --> 04:24:09,479
up with the three and two like we were

6284
04:24:07,760 --> 04:24:11,279
we were practicing before right and we

6285
04:24:09,479 --> 04:24:12,760
end up with the three and two here these

6286
04:24:11,279 --> 04:24:16,800
results these are these are our

6287
04:24:12,760 --> 04:24:19,319
verification results 50 60 114 140 Etc

6288
04:24:16,800 --> 04:24:23,760
we do the kuas sjem result we get the

6289
04:24:19,319 --> 04:24:26,479
same shape um same

6290
04:24:23,760 --> 04:24:28,199
numbers and then the HD result after we

6291
04:24:26,479 --> 04:24:31,880
cast back to single

6292
04:24:28,199 --> 04:24:36,119
Precision 3x two boom boom boom boom

6293
04:24:31,880 --> 04:24:38,600
boom boom right so that is just a very

6294
04:24:36,119 --> 04:24:40,560
uh clear concise example on how on on

6295
04:24:38,600 --> 04:24:42,520
like the differences between kublos sjem

6296
04:24:40,560 --> 04:24:45,439
and H gem there all all the differences

6297
04:24:42,520 --> 04:24:49,880
just half single versus half precision

6298
04:24:45,439 --> 04:24:51,840
and the key point of uh changing how you

6299
04:24:49,880 --> 04:24:53,920
uh set this up this is the most

6300
04:24:51,840 --> 04:24:56,479
important part of this entire script is

6301
04:24:53,920 --> 04:24:59,119
just what it what it is what it does and

6302
04:24:56,479 --> 04:25:01,080
then everything in between uh that we

6303
04:24:59,119 --> 04:25:03,080
need to pay attention to in order to

6304
04:25:01,080 --> 04:25:07,640
keep everything in row

6305
04:25:03,080 --> 04:25:11,479
majure so now we move on to kuas LT

6306
04:25:07,640 --> 04:25:13,640
so if we just pop over to um right here

6307
04:25:11,479 --> 04:25:16,439
Kos LT

6308
04:25:13,640 --> 04:25:18,520
so kuas LT is the lightweight version

6309
04:25:16,439 --> 04:25:21,159
and it's designed to hold much bigger

6310
04:25:18,520 --> 04:25:22,399
matrices and so I figured little

6311
04:25:21,159 --> 04:25:25,800
something to do with that out the hard

6312
04:25:22,399 --> 04:25:29,319
way and I was playing around um you

6313
04:25:25,800 --> 04:25:32,720
cannot have matricies without multiples

6314
04:25:29,319 --> 04:25:34,520
of four so if you have like a 3x4 um

6315
04:25:32,720 --> 04:25:36,600
like three is not a multiple of four or

6316
04:25:34,520 --> 04:25:38,920
a 2x4 both of them aren't multiples of

6317
04:25:36,600 --> 04:25:40,119
four uh those that does not work that

6318
04:25:38,920 --> 04:25:41,600
will not that will not return

6319
04:25:40,119 --> 04:25:45,560
successfully but if you do have like a

6320
04:25:41,600 --> 04:25:48,760
4X 4 or 4X 8 or like a 12 by 16 or

6321
04:25:45,560 --> 04:25:51,840
something like that'll work fine so if

6322
04:25:48,760 --> 04:25:54,040
we like right click on this just like I

6323
04:25:51,840 --> 04:25:55,560
found this uh in the kublos

6324
04:25:54,040 --> 04:25:58,920
documentation

6325
04:25:55,560 --> 04:26:03,000
um where did where did it

6326
04:25:58,920 --> 04:26:03,000
go let's search this

6327
04:26:04,000 --> 04:26:10,960
up oh maybe I should copy this part um

6328
04:26:08,960 --> 04:26:14,680
but yeah so if you plug if you plug that

6329
04:26:10,960 --> 04:26:18,920
string into here you'll see um in the

6330
04:26:14,680 --> 04:26:21,960
kuas LT matmo um it's going to be scroll

6331
04:26:18,920 --> 04:26:23,960
down data ordering so four by a line

6332
04:26:21,960 --> 04:26:26,520
leading dimens leading Dimensions must

6333
04:26:23,960 --> 04:26:28,560
be multiples of four um Dimensions M

6334
04:26:26,520 --> 04:26:30,600
andk must be multiples of four right so

6335
04:26:28,560 --> 04:26:32,319
it's just universally a good idea when

6336
04:26:30,600 --> 04:26:36,840
you're working with big majores don't do

6337
04:26:32,319 --> 04:26:40,479
like 4,091 do 496 it's just kind of like

6338
04:26:36,840 --> 04:26:42,600
makes logical sense to do that so uh

6339
04:26:40,479 --> 04:26:45,159
that's that's all I kind of I wanted to

6340
04:26:42,600 --> 04:26:46,600
say for that is to watch out there um

6341
04:26:45,159 --> 04:26:49,880
but let's go ahead and just dive into

6342
04:26:46,600 --> 04:26:54,479
like what the actual LT MCO is doing so

6343
04:26:49,880 --> 04:26:56,239
we have this new include header or or um

6344
04:26:54,479 --> 04:26:59,040
this new include that we have to do so

6345
04:26:56,239 --> 04:27:01,080
Koss lt. and then we have the regular

6346
04:26:59,040 --> 04:27:04,040
macros for checking Cuda and checking uh

6347
04:27:01,080 --> 04:27:07,000
kuas for errors we have the CPU imple

6348
04:27:04,040 --> 04:27:08,399
implementation to compare against um we

6349
04:27:07,000 --> 04:27:09,760
have the print Matrix so that's just

6350
04:27:08,399 --> 04:27:11,159
going to like conveniently print things

6351
04:27:09,760 --> 04:27:13,040
for us that we can look at them and make

6352
04:27:11,159 --> 04:27:15,880
sure everything lines up

6353
04:27:13,040 --> 04:27:18,600
um and then notice how I make the sizes

6354
04:27:15,880 --> 04:27:20,640
four four and four right so nothing less

6355
04:27:18,600 --> 04:27:23,239
than that just Square Matrix Square

6356
04:27:20,640 --> 04:27:25,840
matrices um I do also make sure to make

6357
04:27:23,239 --> 04:27:28,159
these uh these like different elements

6358
04:27:25,840 --> 04:27:31,520
so if these were a clone this was like

6359
04:27:28,159 --> 04:27:32,880
one 1 to 16 and then 1 to 16 like you

6360
04:27:31,520 --> 04:27:34,000
might not get the results you want so

6361
04:27:32,880 --> 04:27:36,239
you want to make them a little bit

6362
04:27:34,000 --> 04:27:38,439
unique so that you don't run into like

6363
04:27:36,239 --> 04:27:41,119
any weird cases where you think you have

6364
04:27:38,439 --> 04:27:43,800
the right answer but you don't so I

6365
04:27:41,119 --> 04:27:45,560
changed this three right here to a four

6366
04:27:43,800 --> 04:27:49,359
so it's four and then four and then

6367
04:27:45,560 --> 04:27:50,640
instead of 13 to 16 I did 17 to 20 um

6368
04:27:49,359 --> 04:27:54,960
just to mix it up a

6369
04:27:50,640 --> 04:27:57,199
bit now the actual magic I mean we have

6370
04:27:54,960 --> 04:27:59,319
a lot of stuff happening here where we

6371
04:27:57,199 --> 04:28:02,880
um where we essentially where we

6372
04:27:59,319 --> 04:28:05,520
essentially uh make like a like a fp32

6373
04:28:02,880 --> 04:28:07,520
matrix we cam malic and we we populate

6374
04:28:05,520 --> 04:28:11,199
those and we also have a half so this is

6375
04:28:07,520 --> 04:28:14,000
just the half of a float so it's fp16 is

6376
04:28:11,199 --> 04:28:18,000
what this is um and then we just

6377
04:28:14,000 --> 04:28:18,000
populate those with

6378
04:28:18,720 --> 04:28:26,880
uh we we we we populate these where are

6379
04:28:22,800 --> 04:28:29,000
we populating it I think

6380
04:28:26,880 --> 04:28:30,600
that's oh no we already populated them

6381
04:28:29,000 --> 04:28:33,880
up here I'm being

6382
04:28:30,600 --> 04:28:35,880
silly but we go down after we've like

6383
04:28:33,880 --> 04:28:39,840
CMM copied everything this is all on the

6384
04:28:35,880 --> 04:28:41,600
device now um we have this kublos LT

6385
04:28:39,840 --> 04:28:43,399
handle so this is this just the handle

6386
04:28:41,600 --> 04:28:46,040
that we need to create the context we go

6387
04:28:43,399 --> 04:28:48,800
and create that um and then we have this

6388
04:28:46,040 --> 04:28:51,080
new term called a kuas LT Matrix layout

6389
04:28:48,800 --> 04:28:52,640
type um so this is just there's a few

6390
04:28:51,080 --> 04:28:54,439
types we need to ensure that this goes

6391
04:28:52,640 --> 04:28:57,159
properly and this is one of them so

6392
04:28:54,439 --> 04:28:58,479
essentially just the shapes um and the

6393
04:28:57,159 --> 04:29:00,399
data type that we're going to use so

6394
04:28:58,479 --> 04:29:03,359
Cuda data type

6395
04:29:00,399 --> 04:29:06,439
uh and then this this kind of follows

6396
04:29:03,359 --> 04:29:10,560
the same idea as the whole column major

6397
04:29:06,439 --> 04:29:13,680
thing so if we I don't know where

6398
04:29:10,560 --> 04:29:16,600
exactly this was in the box but if we go

6399
04:29:13,680 --> 04:29:20,080
to uh

6400
04:29:16,600 --> 04:29:23,720
kuas what was it it was The Matrix kuas

6401
04:29:20,080 --> 04:29:24,960
LT Matrix layout create if we just enter

6402
04:29:23,720 --> 04:29:28,920
this

6403
04:29:24,960 --> 04:29:31,960
in we see it's down here and if we take

6404
04:29:28,920 --> 04:29:33,479
a look at the rows and columns number

6405
04:29:31,960 --> 04:29:35,319
and rows and Columns of the Matrix and

6406
04:29:33,479 --> 04:29:37,680
then leading Dimension leading dimension

6407
04:29:35,319 --> 04:29:39,560
of the Matrix in column major layout so

6408
04:29:37,680 --> 04:29:41,119
this is the same idea um and I'm going

6409
04:29:39,560 --> 04:29:42,359
to show you how to like get through that

6410
04:29:41,119 --> 04:29:44,680
it's it's a little bit EAS easier

6411
04:29:42,359 --> 04:29:47,159
intuitively to sort of see how that how

6412
04:29:44,680 --> 04:29:50,920
that pans out but yes uh we do need to

6413
04:29:47,159 --> 04:29:54,159
abide by that column major rule there so

6414
04:29:50,920 --> 04:29:57,399
popping back here um for for fp32 we

6415
04:29:54,159 --> 04:29:59,960
just use real um and then 32f so if we

6416
04:29:57,399 --> 04:30:01,399
actually look at this specific type like

6417
04:29:59,960 --> 04:30:02,880
where does this come from the Cuda data

6418
04:30:01,399 --> 04:30:05,920
type you can actually see a list of

6419
04:30:02,880 --> 04:30:09,239
these so we have anywhere from like um

6420
04:30:05,920 --> 04:30:11,560
like real num so R is real C is complex

6421
04:30:09,239 --> 04:30:14,000
and then this number is the Precision so

6422
04:30:11,560 --> 04:30:16,199
16 is half and then 32 is single and

6423
04:30:14,000 --> 04:30:18,159
then 64 is full and then you have like

6424
04:30:16,199 --> 04:30:21,040
the other you know smaller types that

6425
04:30:18,159 --> 04:30:23,840
you can use uh and then like just normal

6426
04:30:21,040 --> 04:30:27,040
f is uh essentially what what that

6427
04:30:23,840 --> 04:30:29,279
difference is is like normal fp16 will

6428
04:30:27,040 --> 04:30:31,040
have a like it'll have a sign bit that's

6429
04:30:29,279 --> 04:30:32,720
either positive or negative and then

6430
04:30:31,040 --> 04:30:35,800
it'll have a certain number of exponent

6431
04:30:32,720 --> 04:30:37,600
bits so how how big is like the integer

6432
04:30:35,800 --> 04:30:40,680
half like before before the decimal

6433
04:30:37,600 --> 04:30:42,640
place and then the mantisa bits which is

6434
04:30:40,680 --> 04:30:46,199
uh how precise can it be in the decimal

6435
04:30:42,640 --> 04:30:48,720
places right so fp16 is going to be more

6436
04:30:46,199 --> 04:30:50,920
precise in decimals and then bf16 or

6437
04:30:48,720 --> 04:30:52,239
brain float 16 the reason it's called

6438
04:30:50,920 --> 04:30:55,199
brain flow is because it came from

6439
04:30:52,239 --> 04:30:57,920
Google brain um that that is going to

6440
04:30:55,199 --> 04:31:00,920
have less mantisa and more

6441
04:30:57,920 --> 04:31:03,239
exponent exponent bits so that's how

6442
04:31:00,920 --> 04:31:04,720
that kind of naming scheme goes uh but

6443
04:31:03,239 --> 04:31:08,960
all we need to worry about in this case

6444
04:31:04,720 --> 04:31:11,720
is the um the real 32bit floating point

6445
04:31:08,960 --> 04:31:13,800
and then the uh real 16bit floating

6446
04:31:11,720 --> 04:31:16,760
point Point

6447
04:31:13,800 --> 04:31:19,239
um so we can see that we use those here

6448
04:31:16,760 --> 04:31:21,720
uh just for the 32-bit we use that and

6449
04:31:19,239 --> 04:31:24,479
then when we're ordering this we want to

6450
04:31:21,720 --> 04:31:28,279
do um so the first Matrix a is going to

6451
04:31:24,479 --> 04:31:30,840
be of shape n m m by K so we want to

6452
04:31:28,279 --> 04:31:32,920
flip that so it's going to be K by m and

6453
04:31:30,840 --> 04:31:34,800
then because we've flipped it uh we need

6454
04:31:32,920 --> 04:31:39,439
to put the leading Dimension here right

6455
04:31:34,800 --> 04:31:42,399
so uh leading Dimension is at the end um

6456
04:31:39,439 --> 04:31:44,920
and that's that's that so

6457
04:31:42,399 --> 04:31:47,520
Matrix B same idea we have it's normally

6458
04:31:44,920 --> 04:31:50,199
K byn so we flip that and then put the

6459
04:31:47,520 --> 04:31:53,159
put the new n as the leading Dimension

6460
04:31:50,199 --> 04:31:56,080
and then same idea here as well um then

6461
04:31:53,159 --> 04:31:59,199
we go to the fp6 which literally just

6462
04:31:56,080 --> 04:32:00,640
uses real but we replace the 32 with 16

6463
04:31:59,199 --> 04:32:03,800
and we do the same exact thing with

6464
04:32:00,640 --> 04:32:06,119
shapes um and then we go down to the uh

6465
04:32:03,800 --> 04:32:08,040
the map description type which we just

6466
04:32:06,119 --> 04:32:11,399
have to create we just have to create

6467
04:32:08,040 --> 04:32:13,800
that and essentially we we pass in this

6468
04:32:11,399 --> 04:32:16,319
this typ type that we Define the memory

6469
04:32:13,800 --> 04:32:19,159
address to that the compute type that

6470
04:32:16,319 --> 04:32:21,199
we're doing and then the data type so we

6471
04:32:19,159 --> 04:32:23,720
look at this it's going to be the map M

6472
04:32:21,199 --> 04:32:26,319
description with the kublos LT map M

6473
04:32:23,720 --> 04:32:28,680
description type like we had here and

6474
04:32:26,319 --> 04:32:30,199
then the kublos compute type which I'll

6475
04:32:28,680 --> 04:32:31,600
show you in a second here and then the

6476
04:32:30,199 --> 04:32:35,159
data type which we were doing before

6477
04:32:31,600 --> 04:32:36,680
which is just going to be um fp32 so we

6478
04:32:35,159 --> 04:32:38,960
go to this compute

6479
04:32:36,680 --> 04:32:40,840
type we actually see there's a few of

6480
04:32:38,960 --> 04:32:43,319
these uh you just do like control click

6481
04:32:40,840 --> 04:32:46,239
to look at those um but there's a few

6482
04:32:43,319 --> 04:32:48,000
here so we have like kuas compute um 16

6483
04:32:46,239 --> 04:32:49,520
F which is which is what we're going to

6484
04:32:48,000 --> 04:32:52,960
want for the the next one but we're

6485
04:32:49,520 --> 04:32:56,000
using this one right now um you can do

6486
04:32:52,960 --> 04:32:57,720
like fast you can do like fast so it'll

6487
04:32:56,000 --> 04:33:00,319
it'll like change I can't remember

6488
04:32:57,720 --> 04:33:02,119
exactly how it changes those inside but

6489
04:33:00,319 --> 04:33:06,119
it's something in the realm of like

6490
04:33:02,119 --> 04:33:08,920
accumulating and this and that um but

6491
04:33:06,119 --> 04:33:10,439
yeah so these can be uh you it kind of

6492
04:33:08,920 --> 04:33:12,199
just depends on like what you're doing

6493
04:33:10,439 --> 04:33:13,520
if you're sticking with brain float then

6494
04:33:12,199 --> 04:33:17,920
you can you could pick that type you

6495
04:33:13,520 --> 04:33:22,080
could do like Fast um up to you but um

6496
04:33:17,920 --> 04:33:22,080
yeah that's that's the whole idea there

6497
04:33:22,520 --> 04:33:28,039
so if we pop back to this one you notice

6498
04:33:25,920 --> 04:33:30,840
we're just using the compute 16 float

6499
04:33:28,039 --> 04:33:34,561
and then the the data type of 16 uh

6500
04:33:30,840 --> 04:33:37,119
16bit float as well um and then we're

6501
04:33:34,561 --> 04:33:39,039
going to we're going to set an attribute

6502
04:33:37,119 --> 04:33:40,760
and the parts of this attribute that

6503
04:33:39,039 --> 04:33:43,160
we're going to need are the M

6504
04:33:40,760 --> 04:33:46,279
description type which we finded earlier

6505
04:33:43,160 --> 04:33:50,600
um we're going to need a uh description

6506
04:33:46,279 --> 04:33:54,680
attribute which we uh which is if you

6507
04:33:50,600 --> 04:33:58,039
can literally go to these um and these

6508
04:33:54,680 --> 04:33:59,439
are these are essentially just the um I

6509
04:33:58,039 --> 04:34:01,160
can't remember exactly what this is but

6510
04:33:59,439 --> 04:34:03,879
it's probably like some transpose like

6511
04:34:01,160 --> 04:34:05,799
transpose a and then transpose b or

6512
04:34:03,879 --> 04:34:07,279
whatever that is um that that's that's

6513
04:34:05,799 --> 04:34:10,920
what I'd assume this is I haven't looked

6514
04:34:07,279 --> 04:34:13,000
into this in depth but um and then this

6515
04:34:10,920 --> 04:34:15,160
one is just going to be the kuas

6516
04:34:13,000 --> 04:34:18,039
operation so yeah so essentially just

6517
04:34:15,160 --> 04:34:21,879
like transposing or not and then the the

6518
04:34:18,039 --> 04:34:24,561
size of uh this CU loss operation type

6519
04:34:21,879 --> 04:34:26,799
so uh that's just like this essentially

6520
04:34:24,561 --> 04:34:28,799
so we're taking this this transpose

6521
04:34:26,799 --> 04:34:30,799
operations um and that we're just

6522
04:34:28,799 --> 04:34:33,799
setting what that that op is so when it

6523
04:34:30,799 --> 04:34:35,840
does like op and then in Brackets a like

6524
04:34:33,799 --> 04:34:38,719
that's what this is

6525
04:34:35,840 --> 04:34:42,799
um and then we go ahead and do the kuas

6526
04:34:38,719 --> 04:34:46,920
LTM which uh itself you know takes in a

6527
04:34:42,799 --> 04:34:52,760
handle um the map uh M description

6528
04:34:46,920 --> 04:34:57,000
type um an alpha a matrix layout B

6529
04:34:52,760 --> 04:35:02,080
Matrix layout for B beta C uh C Matrix

6530
04:34:57,000 --> 04:35:02,080
layout uh D and and Etc right

6531
04:35:02,199 --> 04:35:06,160
um You can pretty much just directly

6532
04:35:04,840 --> 04:35:07,359
paste these like there's a lot of these

6533
04:35:06,160 --> 04:35:08,959
that we don't actually need that we can

6534
04:35:07,359 --> 04:35:10,799
just set to null and aren't really

6535
04:35:08,959 --> 04:35:12,160
required so don't worry too much about

6536
04:35:10,799 --> 04:35:14,799
that you just kind of want things to be

6537
04:35:12,160 --> 04:35:16,920
in the right order and so similar to how

6538
04:35:14,799 --> 04:35:21,639
we did for regular kuas you're going to

6539
04:35:16,920 --> 04:35:22,959
do the B Matrix first and then uh and

6540
04:35:21,639 --> 04:35:24,400
then you're going to do the a matrix

6541
04:35:22,959 --> 04:35:26,680
after

6542
04:35:24,400 --> 04:35:28,400
right so that's that's kind of how that

6543
04:35:26,680 --> 04:35:30,719
goes then we just do the check KU blast

6544
04:35:28,400 --> 04:35:32,160
to make sure everything goes properly um

6545
04:35:30,719 --> 04:35:33,400
and then and then there's a there's an

6546
04:35:32,160 --> 04:35:35,400
important part that I kind of like

6547
04:35:33,400 --> 04:35:39,080
messed up here and it was a little silly

6548
04:35:35,400 --> 04:35:43,240
but I was trying to do this uh this uh

6549
04:35:39,080 --> 04:35:47,561
16 bit floating Point kuas LT ml with uh

6550
04:35:43,240 --> 04:35:49,279
with the regular Alpha types so um you

6551
04:35:47,561 --> 04:35:51,080
know notice how this is like this that

6552
04:35:49,279 --> 04:35:53,760
this is void it doesn't specifically say

6553
04:35:51,080 --> 04:35:56,039
float so I looked I looked at that and I

6554
04:35:53,760 --> 04:35:58,080
was like hm maybe maybe like we

6555
04:35:56,039 --> 04:36:00,480
shouldn't use a 32-bit number and

6556
04:35:58,080 --> 04:36:02,719
multiply that by a 16bit floating Point

6557
04:36:00,480 --> 04:36:06,160
number maybe that that's not how it goes

6558
04:36:02,719 --> 04:36:07,760
so I was like okay um I and at the time

6559
04:36:06,160 --> 04:36:09,320
I was getting a bunch of zero output so

6560
04:36:07,760 --> 04:36:11,719
I figured this might be a good fix and

6561
04:36:09,320 --> 04:36:14,240
it ended up working so uh you pretty

6562
04:36:11,719 --> 04:36:16,799
much just have to uh typ cast this so

6563
04:36:14,240 --> 04:36:18,160
float to half um and then you just set

6564
04:36:16,799 --> 04:36:19,320
you know your Alpha is going to be one

6565
04:36:18,160 --> 04:36:20,680
your beta is going to be zero you just

6566
04:36:19,320 --> 04:36:22,160
want to do a map that that's all you

6567
04:36:20,680 --> 04:36:24,039
care about so that's how you're going to

6568
04:36:22,160 --> 04:36:25,959
set them and then just have Alpha half

6569
04:36:24,039 --> 04:36:28,279
and beta half and just set these

6570
04:36:25,959 --> 04:36:30,400
accordingly um and everything will work

6571
04:36:28,279 --> 04:36:34,359
according to plan

6572
04:36:30,400 --> 04:36:38,520
so that's how it goes um once we're once

6573
04:36:34,359 --> 04:36:41,000
we're done those we copy the uh results

6574
04:36:38,520 --> 04:36:42,480
back to host uh we're not doing any any

6575
04:36:41,000 --> 04:36:43,600
like benchmark here we don't worry about

6576
04:36:42,480 --> 04:36:47,840
that we just want to make sure we have

6577
04:36:43,600 --> 04:36:52,000
the correct results um for both the kuas

6578
04:36:47,840 --> 04:36:53,799
LT uh single and the half Precision um

6579
04:36:52,000 --> 04:36:57,439
and so we just we essentially copy these

6580
04:36:53,799 --> 04:37:00,080
back we do a CPU ml to uh you know

6581
04:36:57,439 --> 04:37:03,639
essentially get a ver a verification uh

6582
04:37:00,080 --> 04:37:06,000
output so that we can compare it to

6583
04:37:03,639 --> 04:37:07,480
um and then we we do the actual

6584
04:37:06,000 --> 04:37:10,199
comparison itself so we use a standard

6585
04:37:07,480 --> 04:37:11,879
Library absolute value we go this minus

6586
04:37:10,199 --> 04:37:14,439
that it's going to get give us some

6587
04:37:11,879 --> 04:37:16,719
value that's like um you know hopefully

6588
04:37:14,439 --> 04:37:20,760
less than 1 * 10

6589
04:37:16,719 --> 04:37:23,080
^5 um and if that's false or or sorry if

6590
04:37:20,760 --> 04:37:25,520
if this is bigger than that number if if

6591
04:37:23,080 --> 04:37:28,080
it's bigger than what it tolers

6592
04:37:25,520 --> 04:37:29,760
tolerates then um they it says they

6593
04:37:28,080 --> 04:37:32,039
don't match and we end up returning uh

6594
04:37:29,760 --> 04:37:34,359
they they don't match um so if I go

6595
04:37:32,039 --> 04:37:37,320
ahead and

6596
04:37:34,359 --> 04:37:40,799
actually

6597
04:37:37,320 --> 04:37:44,039
uh run this we have to do link Coss and

6598
04:37:40,799 --> 04:37:46,879
Link C LT cuz remember at the top here

6599
04:37:44,039 --> 04:37:50,760
we did kuas lt. so we have to include

6600
04:37:46,879 --> 04:37:54,119
that and then if we just uh run this we

6601
04:37:50,760 --> 04:37:58,039
can see that uh we get a matrix a so row

6602
04:37:54,119 --> 04:38:00,279
major Matrix B um four four and then 17

6603
04:37:58,039 --> 04:38:05,959
through 20 as we as we wanted and then

6604
04:38:00,279 --> 04:38:07,719
we get a c output so 106 664 106 664 106

6605
04:38:05,959 --> 04:38:09,439
664 for all of these different

6606
04:38:07,719 --> 04:38:12,480
precisions and we can see that these

6607
04:38:09,439 --> 04:38:14,039
match with intolerance awesome so that's

6608
04:38:12,480 --> 04:38:15,439
just like comparing them and making sure

6609
04:38:14,039 --> 04:38:17,240
that they work the way we want them to

6610
04:38:15,439 --> 04:38:19,400
so we can like carry that and Port it to

6611
04:38:17,240 --> 04:38:22,199
something else

6612
04:38:19,400 --> 04:38:23,561
but then for actually comparing it for

6613
04:38:22,199 --> 04:38:26,439
measuring performance we have a

6614
04:38:23,561 --> 04:38:29,160
different script here so I essentially

6615
04:38:26,439 --> 04:38:30,359
did the same thing um don't like worry

6616
04:38:29,160 --> 04:38:33,279
about a bunch of what's happening in

6617
04:38:30,359 --> 04:38:36,520
here this is this is just a uh this is

6618
04:38:33,279 --> 04:38:40,279
just a uh a benchmarking script that I

6619
04:38:36,520 --> 04:38:44,561
wrote for for comparing very large

6620
04:38:40,279 --> 04:38:47,561
matrices so we do a 496 by 1024 Times

6621
04:38:44,561 --> 04:38:49,840
a24 by 496 so the inner Dimensions

6622
04:38:47,561 --> 04:38:54,279
cancel out the the K is cancel out and

6623
04:38:49,840 --> 04:38:58,119
then we end up with a 496 by 496 Matrix

6624
04:38:54,279 --> 04:38:59,920
um and so I pretty much do a naive

6625
04:38:58,119 --> 04:39:02,039
Matrix multiply because these are very

6626
04:38:59,920 --> 04:39:04,320
big the CPU will take forever to do

6627
04:39:02,039 --> 04:39:06,119
these so I I wrote a naive Matrix

6628
04:39:04,320 --> 04:39:08,719
multiply that still gives you know a

6629
04:39:06,119 --> 04:39:12,520
verifiably true answer in row major

6630
04:39:08,719 --> 04:39:15,240
order and then you know we have our our

6631
04:39:12,520 --> 04:39:18,439
our normal distribution uh random number

6632
04:39:15,240 --> 04:39:22,680
generator here that you know ensures

6633
04:39:18,439 --> 04:39:24,840
everything is is kind of just goes as we

6634
04:39:22,680 --> 04:39:28,080
want some essentially like when you do

6635
04:39:24,840 --> 04:39:30,279
torch. randn it's going to just do that

6636
04:39:28,080 --> 04:39:33,359
um it's going to make it nor like Rand n

6637
04:39:30,279 --> 04:39:35,600
is for normally uh randomly normally

6638
04:39:33,359 --> 04:39:38,000
distributed um that's that's what this

6639
04:39:35,600 --> 04:39:39,798
is doing and then verify results same

6640
04:39:38,000 --> 04:39:41,760
idea does it does the relative error

6641
04:39:39,798 --> 04:39:45,480
match with intolerance

6642
04:39:41,760 --> 04:39:47,840
um we do a timing so with it with our uh

6643
04:39:45,480 --> 04:39:49,718
with our our previous streams so we

6644
04:39:47,840 --> 04:39:52,080
don't actually like put the stream in

6645
04:39:49,718 --> 04:39:54,520
but we just want to uh record the time

6646
04:39:52,080 --> 04:39:57,160
and do this elapse time thing uh and

6647
04:39:54,520 --> 04:39:59,958
then just return that to measure it um

6648
04:39:57,160 --> 04:40:02,520
on the actual device itself uh and then

6649
04:39:59,958 --> 04:40:04,000
we Benchmark and we just have a bunch of

6650
04:40:02,520 --> 04:40:05,520
other stuff filled in in between here

6651
04:40:04,000 --> 04:40:08,920
the same as what you saw in the in the

6652
04:40:05,520 --> 04:40:10,680
previous script um and then we we just

6653
04:40:08,920 --> 04:40:12,320
end up printing out the average time

6654
04:40:10,680 --> 04:40:15,000
afterwards

6655
04:40:12,320 --> 04:40:16,718
so um it'll also return the max error we

6656
04:40:15,000 --> 04:40:19,240
get as well consider there is some error

6657
04:40:16,718 --> 04:40:23,200
with fp16 so we should know about that

6658
04:40:19,240 --> 04:40:26,680
too um but if I I just print this out

6659
04:40:23,200 --> 04:40:26,680
here two

6660
04:40:29,280 --> 04:40:36,718
compare we print this out notice

6661
04:40:33,360 --> 04:40:39,240
so okay so KU loss results match the

6662
04:40:36,718 --> 04:40:41,360
naive kernel with intolerance results

6663
04:40:39,240 --> 04:40:43,480
match results match and results match

6664
04:40:41,360 --> 04:40:44,798
awesome so everything is lined up um I

6665
04:40:43,480 --> 04:40:46,400
did give it some additional tolerance

6666
04:40:44,798 --> 04:40:48,480
here just because of some of the error

6667
04:40:46,400 --> 04:40:51,560
but here uh we notice that this error is

6668
04:40:48,480 --> 04:40:53,638
not super significant um it it's not

6669
04:40:51,560 --> 04:40:55,320
this is the maximum error so most of it

6670
04:40:53,638 --> 04:40:57,040
is going to be very insignificant

6671
04:40:55,320 --> 04:40:59,160
compared to this this is just like a

6672
04:40:57,040 --> 04:41:03,760
side edge case which might pop up a few

6673
04:40:59,160 --> 04:41:05,480
times uh very like not often at all so

6674
04:41:03,760 --> 04:41:08,040
when we actually look at the times we

6675
04:41:05,480 --> 04:41:11,878
can see normal CU loss gives us fp32

6676
04:41:08,040 --> 04:41:14,440
average time of 2.5 milliseconds uh kuas

6677
04:41:11,878 --> 04:41:16,760
LT gives us an an average of 2 point

6678
04:41:14,440 --> 04:41:21,200
about 2.8 milliseconds which isn't

6679
04:41:16,760 --> 04:41:25,958
amazing uh and then kuas LT FP 16 gives

6680
04:41:21,200 --> 04:41:27,680
us 63 milliseconds and then LT gives us

6681
04:41:25,958 --> 04:41:32,280
um about four

6682
04:41:27,680 --> 04:41:33,760
uh 46 milliseconds which is really fast

6683
04:41:32,280 --> 04:41:35,718
compared to this naive kernel that we've

6684
04:41:33,760 --> 04:41:37,520
written before so this took 28

6685
04:41:35,718 --> 04:41:39,878
milliseconds to right this is about the

6686
04:41:37,520 --> 04:41:42,920
time that it takes me to Ping the Google

6687
04:41:39,878 --> 04:41:46,320
servers about 28 Mill seconds for it to

6688
04:41:42,920 --> 04:41:48,878
run the complete naive kernel and the LT

6689
04:41:46,320 --> 04:41:52,400
took Point about5 milliseconds that is

6690
04:41:48,878 --> 04:41:54,680
insanely fast um like look at the if you

6691
04:41:52,400 --> 04:41:57,120
just look back at at at how we were we

6692
04:41:54,680 --> 04:42:00,840
were doing a matrix multiplication of

6693
04:41:57,120 --> 04:42:04,200
like taking the column and do producting

6694
04:42:00,840 --> 04:42:06,320
it with a row like you have a it's like

6695
04:42:04,200 --> 04:42:09,520
size uh

6696
04:42:06,320 --> 04:42:11,760
1,24 and it does that and it it does for

6697
04:42:09,520 --> 04:42:15,400
every single combination it's it and it

6698
04:42:11,760 --> 04:42:18,200
does all of that in uh half a

6699
04:42:15,400 --> 04:42:21,680
millisecond so that's that's about 5

6700
04:42:18,200 --> 04:42:25,798
10,000 of a second uh but yeah anyways

6701
04:42:21,680 --> 04:42:27,480
that's that is a kublos LT at this point

6702
04:42:25,798 --> 04:42:30,160
you might be a little upset or

6703
04:42:27,480 --> 04:42:32,760
frustrated about why I'm just uh showing

6704
04:42:30,160 --> 04:42:34,160
you code and then reviewing it and not

6705
04:42:32,760 --> 04:42:35,718
like writing it from scratch like

6706
04:42:34,160 --> 04:42:37,040
starting from the very top like let's

6707
04:42:35,718 --> 04:42:39,600
define these and then write this and

6708
04:42:37,040 --> 04:42:41,958
then write this the point is like all of

6709
04:42:39,600 --> 04:42:43,878
this it is a lot there's a lot of lines

6710
04:42:41,958 --> 04:42:46,320
I'm not going to type this all manually

6711
04:42:43,878 --> 04:42:48,400
the course is uh long enough already as

6712
04:42:46,320 --> 04:42:51,520
it is I'm not going to make it 10 times

6713
04:42:48,400 --> 04:42:54,440
longer by writing everything uh by hand

6714
04:42:51,520 --> 04:42:58,400
um but yeah you you you kind of get the

6715
04:42:54,440 --> 04:43:00,080
point you I identify the main the main

6716
04:42:58,400 --> 04:43:02,320
uh material that you need to learn the

6717
04:43:00,080 --> 04:43:03,958
most important stuff and I highlight it

6718
04:43:02,320 --> 04:43:05,760
um but I don't need to highlight

6719
04:43:03,958 --> 04:43:08,480
everything like writing check KU loss is

6720
04:43:05,760 --> 04:43:11,160
just redundant writing

6721
04:43:08,480 --> 04:43:12,520
um I don't know like writing the writing

6722
04:43:11,160 --> 04:43:15,320
this stuff it's just redundant like you

6723
04:43:12,520 --> 04:43:17,080
already know what that is um so that's

6724
04:43:15,320 --> 04:43:18,320
that's kind of why I'm sticking away

6725
04:43:17,080 --> 04:43:20,718
from that side and just trying to kind

6726
04:43:18,320 --> 04:43:22,638
of trying to make like Fast progress

6727
04:43:20,718 --> 04:43:24,920
here um we might write out some things

6728
04:43:22,638 --> 04:43:25,840
later on just just to kind of help you

6729
04:43:24,920 --> 04:43:27,240
understand it when it gets more

6730
04:43:25,840 --> 04:43:29,638
intuitive and and you're actually like

6731
04:43:27,240 --> 04:43:32,440
building stuff but right now uh we don't

6732
04:43:29,638 --> 04:43:34,040
this is not very conceptually hard so

6733
04:43:32,440 --> 04:43:37,718
we're just kind of flying through it but

6734
04:43:34,040 --> 04:43:39,000
this next part is on kublos XT so it's

6735
04:43:37,718 --> 04:43:41,320
essentially the same thing as what we've

6736
04:43:39,000 --> 04:43:43,680
just done except it's a bit different we

6737
04:43:41,320 --> 04:43:46,000
still do the handle um we still create

6738
04:43:43,680 --> 04:43:47,600
it we do this new thing called device

6739
04:43:46,000 --> 04:43:49,920
select which remember when I talked

6740
04:43:47,600 --> 04:43:51,798
about um how you can have multiple gpus

6741
04:43:49,920 --> 04:43:55,120
and and do a computation across multiple

6742
04:43:51,798 --> 04:43:58,040
gpus in the host that's what this is so

6743
04:43:55,120 --> 04:44:01,080
we we essentially just do this little

6744
04:43:58,040 --> 04:44:03,718
hack um and we just device select

6745
04:44:01,080 --> 04:44:05,480
whatever the main GPU device is um and

6746
04:44:03,718 --> 04:44:06,840
that's that's how we do things across so

6747
04:44:05,480 --> 04:44:09,360
this is this is just a little hack when

6748
04:44:06,840 --> 04:44:12,878
you have one one GPU so that you do

6749
04:44:09,360 --> 04:44:14,280
this um and then we do this XTS gem

6750
04:44:12,878 --> 04:44:16,360
which is the same exact thing as we've

6751
04:44:14,280 --> 04:44:17,878
been doing before except you have these

6752
04:44:16,360 --> 04:44:21,080
things that are on the host and then

6753
04:44:17,878 --> 04:44:22,480
they're managed by um this this sjem

6754
04:44:21,080 --> 04:44:24,440
function so you don't have to actually

6755
04:44:22,480 --> 04:44:26,680
move anything to device you just you

6756
04:44:24,440 --> 04:44:28,520
don't even I I don't have a single camm

6757
04:44:26,680 --> 04:44:31,520
copy in here

6758
04:44:28,520 --> 04:44:33,680
um you just pass in your matrices on the

6759
04:44:31,520 --> 04:44:35,760
host and it'll manage all of that memory

6760
04:44:33,680 --> 04:44:37,360
back and forth but at some performance

6761
04:44:35,760 --> 04:44:41,798
cost so you'll see that in a second when

6762
04:44:37,360 --> 04:44:41,798
I compile this um

6763
04:44:44,080 --> 04:44:48,000
we don't actually need to link LT but

6764
04:44:45,760 --> 04:44:48,000
that's

6765
04:44:48,280 --> 04:44:52,760
fine um and so you can see maximum

6766
04:44:50,600 --> 04:44:57,280
difference between CPU and GPU results

6767
04:44:52,760 --> 04:44:57,280
right um and then if we go to

6768
04:44:58,480 --> 04:45:03,040
say and then we just link KU

6769
04:45:06,440 --> 04:45:10,958
Bloss we'll give that a second here I

6770
04:45:08,920 --> 04:45:14,080
did I did the exact same thing as as our

6771
04:45:10,958 --> 04:45:17,718
Koss LT but for XT instead and made very

6772
04:45:14,080 --> 04:45:21,760
big matrices so 16 384 you notice the

6773
04:45:17,718 --> 04:45:23,840
kuo Run does uh but you know like I like

6774
04:45:21,760 --> 04:45:26,680
I think I highlighted this before but 0.

6775
04:45:23,840 --> 04:45:28,638
59 seconds or 6 seconds on average and

6776
04:45:26,680 --> 04:45:32,520
then the kuas LT is going to be way

6777
04:45:28,638 --> 04:45:35,958
longer than that so uh we'll we'll just

6778
04:45:32,520 --> 04:45:37,920
give this a second here to finish um but

6779
04:45:35,958 --> 04:45:40,280
yeah you you you get my point it takes a

6780
04:45:37,920 --> 04:45:43,040
while you don't want to run this maybe

6781
04:45:40,280 --> 04:45:45,760
you don't want to run this in

6782
04:45:43,040 --> 04:45:47,320
production um so we get the average time

6783
04:45:45,760 --> 04:45:49,560
everything matches with in toolerance

6784
04:45:47,320 --> 04:45:54,718
and we're good um but yeah that's that's

6785
04:45:49,560 --> 04:45:54,718
kublos XT for you um go and delete

6786
04:45:55,080 --> 04:46:01,160
these I hope you enjoy that section on

6787
04:45:57,718 --> 04:46:03,160
kuas or Cuda basic linear algebra

6788
04:46:01,160 --> 04:46:07,160
subprograms uh that was that was quite a

6789
04:46:03,160 --> 04:46:09,760
bit hey uh we got a second part on CNN

6790
04:46:07,160 --> 04:46:13,920
so when you do when you do um for

6791
04:46:09,760 --> 04:46:17,360
example like P install torch like

6792
04:46:13,920 --> 04:46:20,200
this uh and you see like uh where is it

6793
04:46:17,360 --> 04:46:21,520
kublos qnn right that's this is where

6794
04:46:20,200 --> 04:46:22,920
this is where it's kind of coming from

6795
04:46:21,520 --> 04:46:24,760
right and you have all the other things

6796
04:46:22,920 --> 04:46:26,718
like q fft and C random number

6797
04:46:24,760 --> 04:46:29,000
generators and CP solver and CP sparse

6798
04:46:26,718 --> 04:46:31,760
and Collective Communications across

6799
04:46:29,000 --> 04:46:33,718
multiple nodes profilers Triton which

6800
04:46:31,760 --> 04:46:35,878
we'll go into later right I mean this is

6801
04:46:33,718 --> 04:46:38,760
this is why I'm covering this stuff so

6802
04:46:35,878 --> 04:46:40,360
that you can uh so you can kind of work

6803
04:46:38,760 --> 04:46:42,040
with it and you understand how Pi torch

6804
04:46:40,360 --> 04:46:45,240
Works under the Hood right that's one of

6805
04:46:42,040 --> 04:46:48,520
the main reasons I'm putting this out so

6806
04:46:45,240 --> 04:46:50,320
when you go into CNN um there's there's

6807
04:46:48,520 --> 04:46:51,600
actually a lot more to unpack here as

6808
04:46:50,320 --> 04:46:54,000
compared to kuas but it's not

6809
04:46:51,600 --> 04:46:56,920
conceptually hard um some of it is a

6810
04:46:54,000 --> 04:47:01,600
little bit intuitive but not really um

6811
04:46:56,920 --> 04:47:03,520
so CNN is not entirely for matrix

6812
04:47:01,600 --> 04:47:05,240
multiplication it does matrix

6813
04:47:03,520 --> 04:47:07,240
multiplication in some operations to

6814
04:47:05,240 --> 04:47:08,520
really speed things up uh but it's not a

6815
04:47:07,240 --> 04:47:10,120
performance bottleneck you don't

6816
04:47:08,520 --> 04:47:12,600
actually explicitly do matrix

6817
04:47:10,120 --> 04:47:14,920
multiplication in CNN that's not a thing

6818
04:47:12,600 --> 04:47:17,120
that's what kblast handles right um so

6819
04:47:14,920 --> 04:47:18,000
in CNN you're going to deal with things

6820
04:47:17,120 --> 04:47:22,520
like

6821
04:47:18,000 --> 04:47:25,200
convolutions um pooling layers soft Max

6822
04:47:22,520 --> 04:47:27,718
um Dropout right batch

6823
04:47:25,200 --> 04:47:30,558
normalization uh tensor Transformations

6824
04:47:27,718 --> 04:47:33,280
like reshaping and concatenation layer

6825
04:47:30,558 --> 04:47:35,080
Norm all this right so all these other

6826
04:47:33,280 --> 04:47:37,360
deep learning operations other than

6827
04:47:35,080 --> 04:47:39,680
matrix multiplication is what qnn is

6828
04:47:37,360 --> 04:47:41,440
going to cover a lot of um a lot of the

6829
04:47:39,680 --> 04:47:44,280
a lot of the common most Comm L used

6830
04:47:41,440 --> 04:47:45,798
ones um so this is this is kind of why

6831
04:47:44,280 --> 04:47:47,958
I'm bringing you to the docs here is

6832
04:47:45,798 --> 04:47:50,760
because this there's this is a super

6833
04:47:47,958 --> 04:47:52,558
direct interface with everything um so

6834
04:47:50,760 --> 04:47:55,480
they actually have their own thing Doc

6835
04:47:52,558 --> 04:47:57,480
Nvidia deeplearning CNN and you go there

6836
04:47:55,480 --> 04:47:59,200
and it brings you to this page so

6837
04:47:57,480 --> 04:48:00,680
there's multiple things here we have

6838
04:47:59,200 --> 04:48:02,600
like a getting started or installation

6839
04:48:00,680 --> 04:48:04,280
guide which we don't care about and then

6840
04:48:02,600 --> 04:48:06,958
the other ones the important ones which

6841
04:48:04,280 --> 04:48:08,400
is backend API and developer guide so

6842
04:48:06,958 --> 04:48:11,240
we're going to be looking at these two

6843
04:48:08,400 --> 04:48:14,638
today and doing some examples of C DNN

6844
04:48:11,240 --> 04:48:18,200
operations and comparing them so if we

6845
04:48:14,638 --> 04:48:20,320
go to like backend API overview um we

6846
04:48:18,200 --> 04:48:23,240
can see it's like

6847
04:48:20,320 --> 04:48:26,080
um Cuda so it supports the Cuda streams

6848
04:48:23,240 --> 04:48:27,760
that we were talking about before um it

6849
04:48:26,080 --> 04:48:30,320
has multiple things that it's kind of

6850
04:48:27,760 --> 04:48:31,558
like linked to so there's like I I don't

6851
04:48:30,320 --> 04:48:33,958
I wouldn't pay attention to that

6852
04:48:31,558 --> 04:48:35,600
entirely but um you have these you have

6853
04:48:33,958 --> 04:48:40,400
essentially have these three parts so

6854
04:48:35,600 --> 04:48:46,000
you have qnn graph qnn Ops qnn CNN and

6855
04:48:40,400 --> 04:48:49,160
adver serial so graph is going to it

6856
04:48:46,000 --> 04:48:51,000
it's not uh it's not it doesn't support

6857
04:48:49,160 --> 04:48:53,600
like graph operations where you're like

6858
04:48:51,000 --> 04:48:55,360
dealing with graphs it's more so how do

6859
04:48:53,600 --> 04:48:57,280
you combine operations together in the

6860
04:48:55,360 --> 04:48:59,480
form of a graph so when you're doing

6861
04:48:57,280 --> 04:49:00,798
like a like a convolution layer and then

6862
04:48:59,480 --> 04:49:03,558
you're adding a bias and then you're

6863
04:49:00,798 --> 04:49:04,760
doing a Max pool after it like that

6864
04:49:03,558 --> 04:49:06,360
that's going to look like a graph right

6865
04:49:04,760 --> 04:49:09,200
you're going to have these nodes

6866
04:49:06,360 --> 04:49:11,480
essentially where it's like a node is an

6867
04:49:09,200 --> 04:49:14,520
operation and an edge is a a tensor

6868
04:49:11,480 --> 04:49:17,280
right or a matrix and so it' be like

6869
04:49:14,520 --> 04:49:19,240
convolution 2D and then there's going to

6870
04:49:17,280 --> 04:49:21,160
be an edge which is the which is the

6871
04:49:19,240 --> 04:49:23,480
data flowing from from the output of

6872
04:49:21,160 --> 04:49:25,400
here to the input of the next node and

6873
04:49:23,480 --> 04:49:27,718
that next node might be like the bias

6874
04:49:25,400 --> 04:49:30,638
that it adds right uh and then and then

6875
04:49:27,718 --> 04:49:33,480
it's going to flow out of the bias into

6876
04:49:30,638 --> 04:49:35,360
uh like a Max pool layer or average pool

6877
04:49:33,480 --> 04:49:36,958
layer and then it's going to go from

6878
04:49:35,360 --> 04:49:38,240
there and so instead of doing these

6879
04:49:36,958 --> 04:49:40,920
separately where you do a separate

6880
04:49:38,240 --> 04:49:42,480
function call for uh convolution bias so

6881
04:49:40,920 --> 04:49:44,840
you do like a manual bias kernel and

6882
04:49:42,480 --> 04:49:47,000
then a manual Max pool kernel you just

6883
04:49:44,840 --> 04:49:49,558
fuse these all into one and you keep

6884
04:49:47,000 --> 04:49:52,760
track of all of your all of your uh data

6885
04:49:49,558 --> 04:49:54,600
in between and it does that for you so q

6886
04:49:52,760 --> 04:49:56,440
that that's what the whole qnn graph

6887
04:49:54,600 --> 04:49:58,240
thing is um it supports you know both

6888
04:49:56,440 --> 04:50:00,840
forward and backward path so when you're

6889
04:49:58,240 --> 04:50:03,600
going through calculating the prediction

6890
04:50:00,840 --> 04:50:05,638
predictions uh and when you're uh

6891
04:50:03,600 --> 04:50:07,600
modifying all the gradients and and back

6892
04:50:05,638 --> 04:50:09,840
propagating through it supports both of

6893
04:50:07,600 --> 04:50:11,280
those right so it's designed for it's

6894
04:50:09,840 --> 04:50:12,840
designed for kind of

6895
04:50:11,280 --> 04:50:14,160
uh just putting something in place

6896
04:50:12,840 --> 04:50:16,040
instead of having to write like all the

6897
04:50:14,160 --> 04:50:20,798
kernels from scratch is just kind of

6898
04:50:16,040 --> 04:50:23,520
makes your life easier that way um so

6899
04:50:20,798 --> 04:50:26,958
there are multiple things within a CNN

6900
04:50:23,520 --> 04:50:30,638
graph so we have these pre-compiled

6901
04:50:26,958 --> 04:50:32,480
engines runtime compiled engines um I

6902
04:50:30,638 --> 04:50:36,920
have this pulled up on my second monitor

6903
04:50:32,480 --> 04:50:36,920
here which I'll probably just bring over

6904
04:50:39,638 --> 04:50:43,120
um where did it

6905
04:50:44,080 --> 04:50:48,600
go yes so the pre-compiled single

6906
04:50:47,080 --> 04:50:50,958
operation engines I'm going to make this

6907
04:50:48,600 --> 04:50:52,840
more readable the pre-compiled single

6908
04:50:50,958 --> 04:50:54,440
operation engines pre-compiled and

6909
04:50:52,840 --> 04:50:56,680
optimized for a single specific

6910
04:50:54,440 --> 04:50:58,120
operation like a convolution because

6911
04:50:56,680 --> 04:51:00,360
they're pre-compiled they offer very

6912
04:50:58,120 --> 04:51:02,558
efficient execution and are inflexible

6913
04:51:00,360 --> 04:51:04,240
in terms of operations they can perform

6914
04:51:02,558 --> 04:51:06,558
right they're comp compiled down to

6915
04:51:04,240 --> 04:51:08,200
machine code they only do one specific

6916
04:51:06,558 --> 04:51:10,240
function on something but it goes very

6917
04:51:08,200 --> 04:51:13,440
fast because of all the optimizations in

6918
04:51:10,240 --> 04:51:15,480
b area that it has right um so like for

6919
04:51:13,440 --> 04:51:16,958
example a major multiplication engine

6920
04:51:15,480 --> 04:51:18,400
that is pre-compiled and optimized

6921
04:51:16,958 --> 04:51:20,760
specifically for that operation right

6922
04:51:18,400 --> 04:51:23,360
like similar to convolutions um and then

6923
04:51:20,760 --> 04:51:26,600
there's generic runtime Fusion

6924
04:51:23,360 --> 04:51:29,520
engines designed to dynamically fuse

6925
04:51:26,600 --> 04:51:31,040
mult multiple operations at runtime so

6926
04:51:29,520 --> 04:51:32,600
offer more flexibility compared to

6927
04:51:31,040 --> 04:51:34,878
pre-compiled because they're generic and

6928
04:51:32,600 --> 04:51:36,840
they can adapt right uh these are things

6929
04:51:34,878 --> 04:51:40,600
that would that would happen during the

6930
04:51:36,840 --> 04:51:42,958
compilation um not might not be as as

6931
04:51:40,600 --> 04:51:44,760
high performance optimized but they're

6932
04:51:42,958 --> 04:51:46,958
they are generic and they can and they

6933
04:51:44,760 --> 04:51:48,760
can they act as like a generic fuser of

6934
04:51:46,958 --> 04:51:50,280
operations together so you will get

6935
04:51:48,760 --> 04:51:52,120
those performance benefits but they're

6936
04:51:50,280 --> 04:51:54,440
not going to be as high so you still get

6937
04:51:52,120 --> 04:51:55,520
them but they're not going to be um

6938
04:51:54,440 --> 04:51:57,280
they're not going to be as high as

6939
04:51:55,520 --> 04:51:59,600
something customly written for that

6940
04:51:57,280 --> 04:52:02,240
algorithm then you have a specialized

6941
04:51:59,600 --> 04:52:04,320
runtime Fusion engine similar to generic

6942
04:52:02,240 --> 04:52:06,320
runtime Fusion engines uh but they're

6943
04:52:04,320 --> 04:52:08,200
typically uh specifically optimized for

6944
04:52:06,320 --> 04:52:12,000
certain patterns or combinations of

6945
04:52:08,200 --> 04:52:14,680
operations right so offering runtime

6946
04:52:12,000 --> 04:52:18,638
flexibility and leveraging optimizations

6947
04:52:14,680 --> 04:52:20,798
for particular use cases or operation

6948
04:52:18,638 --> 04:52:23,200
sequences and then for example an engine

6949
04:52:20,798 --> 04:52:24,480
optimized for fusing convolution layers

6950
04:52:23,200 --> 04:52:26,558
followed by activation functions in

6951
04:52:24,480 --> 04:52:28,360
neural networks like similar how similar

6952
04:52:26,558 --> 04:52:29,920
to how I was talking about before you

6953
04:52:28,360 --> 04:52:32,320
have like convolution and then a bias

6954
04:52:29,920 --> 04:52:34,878
and then uh convolution bias and then a

6955
04:52:32,320 --> 04:52:37,958
Max pull layer uh similar similar to

6956
04:52:34,878 --> 04:52:39,958
that right um It'll recognize your code

6957
04:52:37,958 --> 04:52:42,040
architecture uh and it'll find the fuse

6958
04:52:39,958 --> 04:52:44,920
patterns where you would get a speed up

6959
04:52:42,040 --> 04:52:47,760
so um it's it's going to just it's going

6960
04:52:44,920 --> 04:52:49,320
to be um it's going to be smart right

6961
04:52:47,760 --> 04:52:50,718
it's going to try to be smart when it's

6962
04:52:49,320 --> 04:52:52,040
when it's compil this down and seeing

6963
04:52:50,718 --> 04:52:54,400
where you could actually get a speed up

6964
04:52:52,040 --> 04:52:57,040
from

6965
04:52:54,400 --> 04:52:59,760
um and then you have the specialized

6966
04:52:57,040 --> 04:53:01,160
pre-compiled so pre-compiled for

6967
04:52:59,760 --> 04:53:03,958
specific

6968
04:53:01,160 --> 04:53:05,718
sequences they offer the same high

6969
04:53:03,958 --> 04:53:07,400
performance as pre-compiled single

6970
04:53:05,718 --> 04:53:09,638
operations so these ones that were

6971
04:53:07,400 --> 04:53:11,400
really fast uh but can handle sequences

6972
04:53:09,638 --> 04:53:13,480
of operations rather than just single

6973
04:53:11,400 --> 04:53:16,320
ones so these are

6974
04:53:13,480 --> 04:53:18,280
actually these are actually amazing if

6975
04:53:16,320 --> 04:53:19,558
you are trying to do a lot of layers

6976
04:53:18,280 --> 04:53:22,080
like if you have a whole like say a

6977
04:53:19,558 --> 04:53:23,718
Transformer Block in a neural network uh

6978
04:53:22,080 --> 04:53:25,600
and you want to do that entire attention

6979
04:53:23,718 --> 04:53:27,440
block this is an example of what that

6980
04:53:25,600 --> 04:53:29,000
would be so you have a lot of different

6981
04:53:27,440 --> 04:53:31,080
operations that you're doing in there

6982
04:53:29,000 --> 04:53:35,000
but if you just have this wrapper that

6983
04:53:31,080 --> 04:53:36,558
says multi-ad attention you call that uh

6984
04:53:35,000 --> 04:53:38,000
you get everything you put everything in

6985
04:53:36,558 --> 04:53:40,080
that you need and you get everything out

6986
04:53:38,000 --> 04:53:42,400
that's useful and then you can continue

6987
04:53:40,080 --> 04:53:44,558
on and it's going to be highly optimized

6988
04:53:42,400 --> 04:53:46,400
pre-compiled into binary specifically

6989
04:53:44,558 --> 04:53:48,638
for that right so this is kind of how

6990
04:53:46,400 --> 04:53:49,600
qnn is structured and this is these are

6991
04:53:48,638 --> 04:53:50,798
the things you're going to you know want

6992
04:53:49,600 --> 04:53:52,558
to pay attention to when you're trying

6993
04:53:50,798 --> 04:53:54,440
to optimize when you're looking out for

6994
04:53:52,558 --> 04:53:57,718
how you can take advantage of underlying

6995
04:53:54,440 --> 04:54:01,000
qnn features

6996
04:53:57,718 --> 04:54:04,240
um so uh you know there's an example of

6997
04:54:01,000 --> 04:54:07,240
like runtime Fusion right here um and

6998
04:54:04,240 --> 04:54:09,160
then if we go back to the graph API if

6999
04:54:07,240 --> 04:54:11,638
we go back I know it's bright you'll be

7000
04:54:09,160 --> 04:54:14,360
fine um

7001
04:54:11,638 --> 04:54:16,558
we pop over to this this graph API I

7002
04:54:14,360 --> 04:54:21,160
think that's where it was yeah graph API

7003
04:54:16,558 --> 04:54:23,480
with operation Fusion so

7004
04:54:21,160 --> 04:54:27,280
um convolution

7005
04:54:23,480 --> 04:54:30,638
forward pointwise bias pointwise value

7006
04:54:27,280 --> 04:54:33,240
right um that's that's kind of the whole

7007
04:54:30,638 --> 04:54:35,320
idea and if you wanted to fuse these

7008
04:54:33,240 --> 04:54:37,480
together you would do like uh you'd have

7009
04:54:35,320 --> 04:54:40,440
like essentially some organization you

7010
04:54:37,480 --> 04:54:41,840
have three tensors that you input um and

7011
04:54:40,440 --> 04:54:44,240
and that would be like their variable

7012
04:54:41,840 --> 04:54:46,920
names uh and then like these two would

7013
04:54:44,240 --> 04:54:49,240
go through here like your um like your X

7014
04:54:46,920 --> 04:54:52,080
and then your your W your weight kernel

7015
04:54:49,240 --> 04:54:53,520
the convolution filter itself is your W

7016
04:54:52,080 --> 04:54:56,600
and that would output something with

7017
04:54:53,520 --> 04:54:58,400
this arrow and then this one this bias

7018
04:54:56,600 --> 04:54:59,878
would come in um and it would

7019
04:54:58,400 --> 04:55:01,600
essentially add to the output of that

7020
04:54:59,878 --> 04:55:03,958
convolution and then you would do a

7021
04:55:01,600 --> 04:55:06,798
pointwise rue which is it's just

7022
04:55:03,958 --> 04:55:10,718
a it literally just goes one by one

7023
04:55:06,798 --> 04:55:12,360
through through each element um

7024
04:55:10,718 --> 04:55:14,080
and then you get your output so that

7025
04:55:12,360 --> 04:55:16,040
that's the whole idea of a graph is like

7026
04:55:14,080 --> 04:55:18,040
you have these which is your data flow

7027
04:55:16,040 --> 04:55:20,080
the edges is your is your actual data

7028
04:55:18,040 --> 04:55:21,638
and where it's flowing to and then the

7029
04:55:20,080 --> 04:55:24,798
points the nodes themselves are the

7030
04:55:21,638 --> 04:55:27,160
operations so um continuing to go

7031
04:55:24,798 --> 04:55:31,520
through this um you know inputs

7032
04:55:27,160 --> 04:55:34,400
convolution backward so Alpha Beta Dy uh

7033
04:55:31,520 --> 04:55:35,920
W and DX um and then you would end up

7034
04:55:34,400 --> 04:55:38,798
getting

7035
04:55:35,920 --> 04:55:40,760
um yeah you you you kind of get the

7036
04:55:38,798 --> 04:55:42,718
point you put whatever in is required

7037
04:55:40,760 --> 04:55:45,680
for an operation and you get whatever

7038
04:55:42,718 --> 04:55:46,878
out is is useful right um especially

7039
04:55:45,680 --> 04:55:48,440
important you pay attention to that in

7040
04:55:46,878 --> 04:55:49,558
the backward pass of things because

7041
04:55:48,440 --> 04:55:52,280
there's going to be more data you'll

7042
04:55:49,558 --> 04:55:54,760
have to take care of

7043
04:55:52,280 --> 04:55:57,600
um but yeah these these kind of work the

7044
04:55:54,760 --> 04:56:00,440
same way all around um normalization you

7045
04:55:57,600 --> 04:56:01,638
have like your mean Epsilon and variance

7046
04:56:00,440 --> 04:56:05,718
um then your

7047
04:56:01,638 --> 04:56:08,520
scale continuing to go forward um same

7048
04:56:05,718 --> 04:56:09,958
ideas generic runtime Fusion engines you

7049
04:56:08,520 --> 04:56:12,400
can kind of just scroll through this and

7050
04:56:09,958 --> 04:56:15,000
get the idea about how everything is

7051
04:56:12,400 --> 04:56:17,400
architected

7052
04:56:15,000 --> 04:56:19,000
um there's there's quite a bit here I

7053
04:56:17,400 --> 04:56:21,798
don't expect that you'll read all of

7054
04:56:19,000 --> 04:56:24,480
this um

7055
04:56:21,798 --> 04:56:27,798
but yeah that's that's pretty much how

7056
04:56:24,480 --> 04:56:31,240
the whole uh Fusion engine thing works I

7057
04:56:27,798 --> 04:56:33,280
know it's a kind of a a silly term but

7058
04:56:31,240 --> 04:56:35,320
it it is you're effectively fusing

7059
04:56:33,280 --> 04:56:38,000
operations together and you could say

7060
04:56:35,320 --> 04:56:39,878
that acts as like an engine right so if

7061
04:56:38,000 --> 04:56:41,558
we go back to this VSS code here that

7062
04:56:39,878 --> 04:56:44,120
I've opened the read me file you can

7063
04:56:41,558 --> 04:56:46,320
actually find these in here so just you

7064
04:56:44,120 --> 04:56:49,480
know a bunch of um bunch of stuff but

7065
04:56:46,320 --> 04:56:53,480
like the graph API very important um

7066
04:56:49,480 --> 04:56:55,638
Matt Mo um convolution forward backward

7067
04:56:53,480 --> 04:56:57,440
backward data

7068
04:56:55,638 --> 04:56:59,920
pointwise

7069
04:56:57,440 --> 04:57:02,200
um yeah just like pretty much some of

7070
04:56:59,920 --> 04:57:04,440
the images copy and pasted and then this

7071
04:57:02,200 --> 04:57:07,798
one was this one was actually

7072
04:57:04,440 --> 04:57:09,480
support uh that this one is support for

7073
04:57:07,798 --> 04:57:12,320
the different Compu capabilities right

7074
04:57:09,480 --> 04:57:15,040
so if you have like for example if if I

7075
04:57:12,320 --> 04:57:16,840
did um can't remember what it was it was

7076
04:57:15,040 --> 04:57:18,360
like device query remember that when we

7077
04:57:16,840 --> 04:57:19,958
printed out the computer capability and

7078
04:57:18,360 --> 04:57:25,360
mine was 8 uh

7079
04:57:19,958 --> 04:57:28,120
8.6 so um I would actually not get the

7080
04:57:25,360 --> 04:57:29,680
uh convolution backward filter fusions I

7081
04:57:28,120 --> 04:57:32,920
would not get this because it's only

7082
04:57:29,680 --> 04:57:33,958
supported on 9.0 and up right so pay

7083
04:57:32,920 --> 04:57:35,120
attention to things like that when

7084
04:57:33,958 --> 04:57:37,360
you're trying to fuse things together

7085
04:57:35,120 --> 04:57:39,240
for like research production purposes

7086
04:57:37,360 --> 04:57:41,200
you want to pay attention to what is

7087
04:57:39,240 --> 04:57:43,878
supported on your Ware or whatever

7088
04:57:41,200 --> 04:57:45,320
Hardware you're working on um so that

7089
04:57:43,878 --> 04:57:47,600
you don't like try to do something and

7090
04:57:45,320 --> 04:57:49,040
have it not work and waste time so it's

7091
04:57:47,600 --> 04:57:51,440
good to just like double check with this

7092
04:57:49,040 --> 04:57:53,320
stuff and see this is all in the CNN

7093
04:57:51,440 --> 04:57:56,080
docs

7094
04:57:53,320 --> 04:57:57,440
but there's still a few more sections we

7095
04:57:56,080 --> 04:58:00,760
have to cover so I'm going to dig into

7096
04:57:57,440 --> 04:58:03,600
those um we have the Ops API which I'll

7097
04:58:00,760 --> 04:58:04,440
dig into next is is very simple um if we

7098
04:58:03,600 --> 04:58:09,480
just go

7099
04:58:04,440 --> 04:58:11,958
to here go to Ops um essentially you

7100
04:58:09,480 --> 04:58:14,200
have these these same opaque stru types

7101
04:58:11,958 --> 04:58:16,920
as you did with kuas um except they do

7102
04:58:14,200 --> 04:58:19,200
different operations so you can do like

7103
04:58:16,920 --> 04:58:22,440
um like you can like create tensors

7104
04:58:19,200 --> 04:58:24,638
pooling um filter Dropout loss

7105
04:58:22,440 --> 04:58:26,400
activation all of this so the actual

7106
04:58:24,638 --> 04:58:28,760
functions here might be hard to read

7107
04:58:26,400 --> 04:58:32,080
it's very bright

7108
04:58:28,760 --> 04:58:33,600
um but activation backward activation

7109
04:58:32,080 --> 04:58:34,798
forward you just have like a massive

7110
04:58:33,600 --> 04:58:37,200
list of stuff I'm not going to go

7111
04:58:34,798 --> 04:58:38,160
through these one by one um but but you

7112
04:58:37,200 --> 04:58:40,080
get the point these are all the

7113
04:58:38,160 --> 04:58:42,558
operations that are supported with kunan

7114
04:58:40,080 --> 04:58:44,080
to CNN and we're going to test we're

7115
04:58:42,558 --> 04:58:47,718
going to test some of them

7116
04:58:44,080 --> 04:58:49,280
out so you know you get a bunch of like

7117
04:58:47,718 --> 04:58:51,040
uh descriptions about each what if each

7118
04:58:49,280 --> 04:58:52,200
of these do so it you know in case

7119
04:58:51,040 --> 04:58:53,638
you're wondering about something or

7120
04:58:52,200 --> 04:58:55,840
you're not getting an output as you'd

7121
04:58:53,638 --> 04:58:57,798
expect um you would generally refer to

7122
04:58:55,840 --> 04:58:59,680
these doc so you could whatever type

7123
04:58:57,798 --> 04:59:00,920
you're working with or whatever function

7124
04:58:59,680 --> 04:59:03,920
you're trying to call like let's say

7125
04:59:00,920 --> 04:59:06,200
you're you know working with this or

7126
04:59:03,920 --> 04:59:07,958
maybe you're doing let's see maybe

7127
04:59:06,200 --> 04:59:12,520
something simple like

7128
04:59:07,958 --> 04:59:15,760
uh like U activate backward right so you

7129
04:59:12,520 --> 04:59:17,400
have this you copy it f and you you can

7130
04:59:15,760 --> 04:59:19,680
find these all across so you have the

7131
04:59:17,400 --> 04:59:20,920
qnn activation backward and it has all

7132
04:59:19,680 --> 04:59:24,000
the different types in here that you

7133
04:59:20,920 --> 04:59:25,280
would use um and then you have the this

7134
04:59:24,000 --> 04:59:26,480
one this original one that we

7135
04:59:25,280 --> 04:59:27,760
highlighted before so that's how you

7136
04:59:26,480 --> 04:59:29,638
navigate these you just search for

7137
04:59:27,760 --> 04:59:32,120
whatever is wrong and then kind of just

7138
04:59:29,638 --> 04:59:33,798
like look at that and see any of the any

7139
04:59:32,120 --> 04:59:36,520
of the notes on it and see if you miss

7140
04:59:33,798 --> 04:59:40,440
something um that's that's kind of how

7141
04:59:36,520 --> 04:59:43,360
uh you're supposed to approach these um

7142
04:59:40,440 --> 04:59:45,480
and then going into uh CNN API so this

7143
04:59:43,360 --> 04:59:48,000
is where stuff might get a little bit

7144
04:59:45,480 --> 04:59:49,520
interesting um it's not like any graph

7145
04:59:48,000 --> 04:59:54,160
Fusion stuff you're doing it's just like

7146
04:59:49,520 --> 04:59:56,440
raw algorithms um so you'll have um you

7147
04:59:54,160 --> 04:59:58,558
know convolution backward bias like all

7148
04:59:56,440 --> 05:00:01,280
just all the different convolution stuff

7149
04:59:58,558 --> 05:00:04,080
um there is I mean there there is fused

7150
05:00:01,280 --> 05:00:08,558
Ops um so that's where like some of this

7151
05:00:04,080 --> 05:00:09,718
would come in but um yeah this is this

7152
05:00:08,558 --> 05:00:11,718
is where all the convolution stuff is

7153
05:00:09,718 --> 05:00:14,200
going to be for like image processing

7154
05:00:11,718 --> 05:00:15,718
and you name it right so we're going to

7155
05:00:14,200 --> 05:00:17,040
actually use convolutions in a second

7156
05:00:15,718 --> 05:00:18,798
here so I'm going to I'm going to save

7157
05:00:17,040 --> 05:00:21,400
this but it's you you approach it the

7158
05:00:18,798 --> 05:00:23,798
similar way as you would with with Ops

7159
05:00:21,400 --> 05:00:25,400
operations and then you have the uh

7160
05:00:23,798 --> 05:00:30,320
adversarial

7161
05:00:25,400 --> 05:00:33,680
API which is um same idea but you know

7162
05:00:30,320 --> 05:00:37,040
other functions so like you know rnn's

7163
05:00:33,680 --> 05:00:39,120
um C uh CTC loss multi-head

7164
05:00:37,040 --> 05:00:42,320
attention uh we'll do yeah see

7165
05:00:39,120 --> 05:00:46,718
multi-head attention weight so if I go

7166
05:00:42,320 --> 05:00:49,680
um multi head attention multi-ad

7167
05:00:46,718 --> 05:00:55,160
attention forward awesome how do we use

7168
05:00:49,680 --> 05:00:55,160
this right there's there's multi head

7169
05:00:57,160 --> 05:01:03,638
attention forward there's 22 of these in

7170
05:01:01,520 --> 05:01:07,160
here 21

7171
05:01:03,638 --> 05:01:08,920
now so you kind of get the point we can

7172
05:01:07,160 --> 05:01:12,958
scroll through

7173
05:01:08,920 --> 05:01:12,958
these there's a lot

7174
05:01:13,440 --> 05:01:19,080
jeez um so this is how you would do a a

7175
05:01:17,120 --> 05:01:21,280
multi-head attention block forward and

7176
05:01:19,080 --> 05:01:23,320
the forward pass there's a lot of stuff

7177
05:01:21,280 --> 05:01:25,520
in here but that's that that that that's

7178
05:01:23,320 --> 05:01:27,920
kind of how this goes right um just the

7179
05:01:25,520 --> 05:01:29,360
adversarial like extra the other section

7180
05:01:27,920 --> 05:01:33,520
miscellaneous whatever you want to call

7181
05:01:29,360 --> 05:01:36,080
it um but now we can actually go into

7182
05:01:33,520 --> 05:01:39,558
some of the uh

7183
05:01:36,080 --> 05:01:42,080
comparisons to understand uh how to

7184
05:01:39,558 --> 05:01:43,480
actually use CNN in a Cuda script now we

7185
05:01:42,080 --> 05:01:46,360
can actually go into some of the code

7186
05:01:43,480 --> 05:01:48,360
and examples behind CNN and how it works

7187
05:01:46,360 --> 05:01:50,520
under the hood well not not how it works

7188
05:01:48,360 --> 05:01:52,320
under the hood but how we can use things

7189
05:01:50,520 --> 05:01:55,480
like P torch under the hood to make

7190
05:01:52,320 --> 05:01:58,680
operations really fast so in this

7191
05:01:55,480 --> 05:02:00,798
example um you know we we do the Cuda

7192
05:01:58,680 --> 05:02:03,320
runtime and the cnn. H I'm just going to

7193
05:02:00,798 --> 05:02:04,840
do the 10 function for example um and in

7194
05:02:03,320 --> 05:02:08,680
case you haven't seen the 10 function

7195
05:02:04,840 --> 05:02:08,680
yet um

7196
05:02:10,080 --> 05:02:14,040
we go to Google

7197
05:02:11,440 --> 05:02:16,600
Images uh it literally just looks like

7198
05:02:14,040 --> 05:02:19,160
like this 10 H

7199
05:02:16,600 --> 05:02:22,520
um or like this maybe this is a better

7200
05:02:19,160 --> 05:02:24,120
one it's like between -1 and one it's

7201
05:02:22,520 --> 05:02:26,320
just a little activation function that

7202
05:02:24,120 --> 05:02:30,320
you do it's like a nice smooth S curve

7203
05:02:26,320 --> 05:02:31,558
and uh yeah so that's that's all we're

7204
05:02:30,320 --> 05:02:33,600
really doing here we don't actually need

7205
05:02:31,558 --> 05:02:35,200
to like do the type in the formula it's

7206
05:02:33,600 --> 05:02:37,240
already done for us um I've actually

7207
05:02:35,200 --> 05:02:40,280
written out the 10h kernel here and

7208
05:02:37,240 --> 05:02:46,680
operation is literally just 10 HF for P

7209
05:02:40,280 --> 05:02:49,840
function on device um so very simple um

7210
05:02:46,680 --> 05:02:56,440
we we want to have a tensor with the

7211
05:02:49,840 --> 05:02:58,760
shape um what's it called n by n by C by

7212
05:02:56,440 --> 05:03:01,440
height by width that's the format we

7213
05:02:58,760 --> 05:03:04,718
want to use so it's going to be like n

7214
05:03:01,440 --> 05:03:07,360
is batch size channels is uh Channel C

7215
05:03:04,718 --> 05:03:09,320
is channels height and then width right

7216
05:03:07,360 --> 05:03:11,160
so the whole idea of like channels is if

7217
05:03:09,320 --> 05:03:12,600
you have uh like like an image for

7218
05:03:11,160 --> 05:03:14,840
example like the image you're video

7219
05:03:12,600 --> 05:03:16,120
you're watching me on right now this is

7220
05:03:14,840 --> 05:03:19,360
going to have three channels it's going

7221
05:03:16,120 --> 05:03:21,400
to have RGB right so this is if uh say

7222
05:03:19,360 --> 05:03:23,440
you've you've done some convolutions and

7223
05:03:21,400 --> 05:03:26,280
now your now your channel Dimension is

7224
05:03:23,440 --> 05:03:29,120
very big so instead of three you've got

7225
05:03:26,280 --> 05:03:30,920
like 32 elements per pixel that you have

7226
05:03:29,120 --> 05:03:32,718
to keep track of and and do operations

7227
05:03:30,920 --> 05:03:35,440
on right so we're making a big tensor

7228
05:03:32,718 --> 05:03:38,958
here big tensor um if I actually go into

7229
05:03:35,440 --> 05:03:41,160
python new 256 we going to go well four

7230
05:03:38,958 --> 05:03:47,958
times cuz four is the number of of bytes

7231
05:03:41,160 --> 05:03:51,558
it's going to occupy 4 * 256 * 32 * 2 *

7232
05:03:47,958 --> 05:03:54,760
224 uh squared we end up getting this

7233
05:03:51,558 --> 05:03:57,718
number and if we divide this number by

7234
05:03:54,760 --> 05:04:01,718
uh 1 million it's

7235
05:03:57,718 --> 05:04:02,958
about 1.6 gabt that's how big this

7236
05:04:01,718 --> 05:04:04,280
tensor is and then we're going to do a

7237
05:04:02,958 --> 05:04:06,958
10h on

7238
05:04:04,280 --> 05:04:08,040
that so Cuda DN is going to handle this

7239
05:04:06,958 --> 05:04:10,840
and we're going to compare that to the

7240
05:04:08,040 --> 05:04:14,480
naive kernel um

7241
05:04:10,840 --> 05:04:17,360
and yeah so just stepping through this

7242
05:04:14,480 --> 05:04:19,000
same Cuda Malik initialized data Cuda M

7243
05:04:17,360 --> 05:04:21,240
Copy we're going to create some events

7244
05:04:19,000 --> 05:04:23,000
that we can Time stuff on the GPU we're

7245
05:04:21,240 --> 05:04:26,200
going to do our our warmup and and

7246
05:04:23,000 --> 05:04:29,718
Benchmark runs um we're going to do some

7247
05:04:26,200 --> 05:04:33,040
warmup runs for the knif kernel

7248
05:04:29,718 --> 05:04:34,718
um we're going to do benchmarks for it

7249
05:04:33,040 --> 05:04:35,638
we're going to set up qnn and this is

7250
05:04:34,718 --> 05:04:37,480
where we're actually going to learn a

7251
05:04:35,638 --> 05:04:39,878
little bit um and then like benchmarks

7252
05:04:37,480 --> 05:04:42,878
for CNN of course this is where it m

7253
05:04:39,878 --> 05:04:45,000
takes place so we have this qnn handle

7254
05:04:42,878 --> 05:04:47,760
type we create

7255
05:04:45,000 --> 05:04:50,480
that we have this tensor descriptor type

7256
05:04:47,760 --> 05:04:52,638
so you actually need to um it's like the

7257
05:04:50,480 --> 05:04:54,520
when you did like a matrix a descriptor

7258
05:04:52,638 --> 05:04:56,760
or Matrix B descriptor it's the same

7259
05:04:54,520 --> 05:04:58,200
idea but we do that for tensors because

7260
05:04:56,760 --> 05:04:59,480
it's you know it's more in the Deep

7261
05:04:58,200 --> 05:05:01,798
learning context there's more deep

7262
05:04:59,480 --> 05:05:04,240
learning operations in Cuda CP DNN since

7263
05:05:01,798 --> 05:05:08,878
it's deep neural network C deep neural

7264
05:05:04,240 --> 05:05:10,120
net right um so we have create Tor

7265
05:05:08,878 --> 05:05:11,320
descriptor which is is going to just

7266
05:05:10,120 --> 05:05:12,760
take the memory address of this and

7267
05:05:11,320 --> 05:05:15,638
actually create the tensor descriptor

7268
05:05:12,760 --> 05:05:17,320
based on the type um we're going to set

7269
05:05:15,638 --> 05:05:20,680
the tensor descriptor so it's going to

7270
05:05:17,320 --> 05:05:23,958
be um this type which we which we did

7271
05:05:20,680 --> 05:05:28,680
already the tensor format the data type

7272
05:05:23,958 --> 05:05:30,600
and then each of those uh NCH HW format

7273
05:05:28,680 --> 05:05:32,920
right so we do this we do this format

7274
05:05:30,600 --> 05:05:37,080
and you can you can look this up and

7275
05:05:32,920 --> 05:05:41,840
there's um it's right here tensor format

7276
05:05:37,080 --> 05:05:46,400
so we can do NCH HW we can do NW C or n

7277
05:05:41,840 --> 05:05:49,160
nhwc um and Etc right so that's we just

7278
05:05:46,400 --> 05:05:52,240
we just pick this one and then this data

7279
05:05:49,160 --> 05:05:53,360
float we look at this um we can have

7280
05:05:52,240 --> 05:05:57,440
like a bunch of different ones fast

7281
05:05:53,360 --> 05:05:59,958
float for fp8 um fp8 so we'd have one

7282
05:05:57,440 --> 05:06:02,120
sign bit and then five exponent bits and

7283
05:05:59,958 --> 05:06:05,000
then two mantisa bits right so it's

7284
05:06:02,120 --> 05:06:07,200
eight total right um and then fp8 you

7285
05:06:05,000 --> 05:06:08,958
know more more mantisa bits and then

7286
05:06:07,200 --> 05:06:10,558
Boolean we have all these different

7287
05:06:08,958 --> 05:06:13,200
types but we want the float there's no

7288
05:06:10,558 --> 05:06:17,798
like float 32 here we just want a normal

7289
05:06:13,200 --> 05:06:20,240
float no no half no bf16 none of that um

7290
05:06:17,798 --> 05:06:23,040
just something basic to to to

7291
05:06:20,240 --> 05:06:24,240
use and then we create not only the

7292
05:06:23,040 --> 05:06:26,558
tensor descriptor because we have the

7293
05:06:24,240 --> 05:06:28,280
tensor itself and then we have the

7294
05:06:26,558 --> 05:06:30,080
activation descriptor which is about

7295
05:06:28,280 --> 05:06:32,558
what the activation is going to do and

7296
05:06:30,080 --> 05:06:34,320
we have a custom type for that um and

7297
05:06:32,558 --> 05:06:37,160
you can go here and there's there's a

7298
05:06:34,320 --> 05:06:39,878
bunch of different um I think

7299
05:06:37,160 --> 05:06:41,440
it's I don't know where exactly this is

7300
05:06:39,878 --> 05:06:42,958
but I just like to right click on things

7301
05:06:41,440 --> 05:06:46,080
to learn

7302
05:06:42,958 --> 05:06:49,240
yeah good

7303
05:06:46,080 --> 05:06:51,760
luck uh we create the ACT activation

7304
05:06:49,240 --> 05:06:53,480
descriptor with its memory address and

7305
05:06:51,760 --> 05:06:56,958
here we have the activation descriptor

7306
05:06:53,480 --> 05:06:59,520
type as before we have the CNN or uh we

7307
05:06:56,958 --> 05:07:02,000
have the activation type uh mode type so

7308
05:06:59,520 --> 05:07:05,480
the activation mode uh is going to be

7309
05:07:02,000 --> 05:07:08,920
10h and then the uh propagation type is

7310
05:07:05,480 --> 05:07:10,480
just Nan um we're we're not we're not

7311
05:07:08,920 --> 05:07:13,840
doing that

7312
05:07:10,480 --> 05:07:15,920
um and then the coefficient is going to

7313
05:07:13,840 --> 05:07:17,638
be uh zero so there's not going to I

7314
05:07:15,920 --> 05:07:20,200
don't know what what is KF I don't know

7315
05:07:17,638 --> 05:07:22,120
what that is um but anyways this is just

7316
05:07:20,200 --> 05:07:23,798
like the template layout we're having

7317
05:07:22,120 --> 05:07:25,360
and I don't expect you to like go

7318
05:07:23,798 --> 05:07:27,040
through this and understand every single

7319
05:07:25,360 --> 05:07:28,920
character that's happening here this is

7320
05:07:27,040 --> 05:07:30,400
just kind of a more template example to

7321
05:07:28,920 --> 05:07:32,520
show you like how we're comparing these

7322
05:07:30,400 --> 05:07:34,000
how we're running and testing and you

7323
05:07:32,520 --> 05:07:35,520
can like take these out and put them

7324
05:07:34,000 --> 05:07:36,958
into other pieces of code so don't like

7325
05:07:35,520 --> 05:07:39,840
feel bad at all if you don't understand

7326
05:07:36,958 --> 05:07:41,440
this it is there there is a lot um but

7327
05:07:39,840 --> 05:07:45,240
if we go to like this for example we

7328
05:07:41,440 --> 05:07:48,000
have like sigmoid R 10 clip R um swish

7329
05:07:45,240 --> 05:07:51,080
all this right so we just want the tan H

7330
05:07:48,000 --> 05:07:54,440
function um

7331
05:07:51,080 --> 05:07:57,360
now we go to activation forward as we

7332
05:07:54,440 --> 05:07:59,120
saw just like a a few minutes ago um we

7333
05:07:57,360 --> 05:08:01,160
have the scann handle which we defined

7334
05:07:59,120 --> 05:08:02,958
before activation descriptor which we

7335
05:08:01,160 --> 05:08:05,600
which we just covered the alpha

7336
05:08:02,958 --> 05:08:10,480
parameter the tensor descriptor which we

7337
05:08:05,600 --> 05:08:12,680
covered uh up here we have this um

7338
05:08:10,480 --> 05:08:15,080
we have this void X so that's just

7339
05:08:12,680 --> 05:08:18,638
that's just the input a beta term which

7340
05:08:15,080 --> 05:08:21,680
we don't need um and then tensor

7341
05:08:18,638 --> 05:08:24,798
descriptor type for y so and the output

7342
05:08:21,680 --> 05:08:26,480
itself is just a uh it's it's nothing

7343
05:08:24,798 --> 05:08:28,480
it's nothing particularly special it's

7344
05:08:26,480 --> 05:08:29,958
just a void so we don't need any like

7345
05:08:28,480 --> 05:08:32,520
special types for it it's just like a

7346
05:08:29,958 --> 05:08:34,798
raw data like array output essentially

7347
05:08:32,520 --> 05:08:36,680
um going to be a bunch of floats right

7348
05:08:34,798 --> 05:08:38,558
um so that's that's that and it's going

7349
05:08:36,680 --> 05:08:40,120
to be on device that's why we have the D

7350
05:08:38,558 --> 05:08:42,240
there um

7351
05:08:40,120 --> 05:08:43,558
um we're going to synchronize everything

7352
05:08:42,240 --> 05:08:44,798
and then do our benchmarks run

7353
05:08:43,558 --> 05:08:46,600
benchmarks runs we're going to find the

7354
05:08:44,798 --> 05:08:49,000
average time we're going to verify

7355
05:08:46,600 --> 05:08:49,000
against the

7356
05:08:49,480 --> 05:08:53,840
CPU like a CPU tan for example it's not

7357
05:08:52,280 --> 05:08:55,520
not going to take long since it's like a

7358
05:08:53,840 --> 05:08:57,360
like a pointwise operation it's just

7359
05:08:55,520 --> 05:09:00,958
going to go one by one through it it's

7360
05:08:57,360 --> 05:09:05,000
very it's like linear time um so when we

7361
05:09:00,958 --> 05:09:10,920
actually uh when we actually compile

7362
05:09:05,000 --> 05:09:12,760
this 10 H to DNN see it knows

7363
05:09:10,920 --> 05:09:15,558
we run this you're actually going to be

7364
05:09:12,760 --> 05:09:17,400
surprised by something so give this a

7365
05:09:15,558 --> 05:09:20,080
second to

7366
05:09:17,400 --> 05:09:22,400
run my machine might be lagging a little

7367
05:09:20,080 --> 05:09:24,798
bit I'm not sure

7368
05:09:22,400 --> 05:09:27,320
but we're going to let that run for a

7369
05:09:24,798 --> 05:09:30,878
second here our Matrix sizes are are

7370
05:09:27,320 --> 05:09:34,600
quite big remember we have these

7371
05:09:30,878 --> 05:09:37,680
so um you know tensor size as we wanted

7372
05:09:34,600 --> 05:09:41,280
um the naive crud kernel time notice how

7373
05:09:37,680 --> 05:09:45,680
this is actually faster than the CNN

7374
05:09:41,280 --> 05:09:49,080
activation time um they're both correct

7375
05:09:45,680 --> 05:09:53,760
we just compare the results um you know

7376
05:09:49,080 --> 05:09:55,600
pointwise and this is faster why that

7377
05:09:53,760 --> 05:09:57,040
really made me angry when I when I saw

7378
05:09:55,600 --> 05:09:59,920
this I was like what is the point of

7379
05:09:57,040 --> 05:10:01,718
having a having a CNN activation like

7380
05:09:59,920 --> 05:10:04,680
what is the point of this um and the

7381
05:10:01,718 --> 05:10:06,440
point of that is just to give give

7382
05:10:04,680 --> 05:10:07,958
yourself things like Alpha and beta

7383
05:10:06,440 --> 05:10:09,320
right so when you have Alpha and beta

7384
05:10:07,958 --> 05:10:11,240
these are extra numbers that you have

7385
05:10:09,320 --> 05:10:13,280
have to consider in the operation and

7386
05:10:11,240 --> 05:10:16,558
when it's so simple when it's as simple

7387
05:10:13,280 --> 05:10:18,760
as just a like exponentiation or like a

7388
05:10:16,558 --> 05:10:20,718
multiplication um and it's it's like a

7389
05:10:18,760 --> 05:10:22,958
very simple like for example Ru takes

7390
05:10:20,718 --> 05:10:24,718
almost no time to complete 10h is like a

7391
05:10:22,958 --> 05:10:26,920
very simple operation just on a single

7392
05:10:24,718 --> 05:10:28,958
number um you just output that to the

7393
05:10:26,920 --> 05:10:31,280
same index it's very computationally

7394
05:10:28,958 --> 05:10:34,080
simple but when you add in little things

7395
05:10:31,280 --> 05:10:35,798
like Alpha and beta I suspect these are

7396
05:10:34,080 --> 05:10:38,160
what actually cause the performance

7397
05:10:35,798 --> 05:10:40,520
difference it might be mostly these or

7398
05:10:38,160 --> 05:10:42,680
some cdnn over head I mean again we

7399
05:10:40,520 --> 05:10:44,480
don't even know what this what this is

7400
05:10:42,680 --> 05:10:46,480
doing under the hood it's a complete

7401
05:10:44,480 --> 05:10:48,638
Black Box opaque struct we don't know

7402
05:10:46,480 --> 05:10:50,878
what's happening so it's hard to

7403
05:10:48,638 --> 05:10:53,920
actually know why this happens um

7404
05:10:50,878 --> 05:10:55,520
there's not very many resources on why

7405
05:10:53,920 --> 05:10:57,958
custom kernels might be faster than C

7406
05:10:55,520 --> 05:10:59,400
andn I haven't really found any of that

7407
05:10:57,958 --> 05:11:01,718
so we're just going to hold the

7408
05:10:59,400 --> 05:11:03,440
assumption that uh there's this big

7409
05:11:01,718 --> 05:11:05,520
opaque struct Black Box thing that we

7410
05:11:03,440 --> 05:11:07,440
don't know um and then you also have

7411
05:11:05,520 --> 05:11:09,840
just the alpha and beta as well that

7412
05:11:07,440 --> 05:11:11,638
you're you're um timesing

7413
05:11:09,840 --> 05:11:13,600
and adding things by right so that's

7414
05:11:11,638 --> 05:11:15,520
going to add some extra compute compute

7415
05:11:13,600 --> 05:11:17,240
overhead but these are not very big

7416
05:11:15,520 --> 05:11:20,798
differences at all right so you have

7417
05:11:17,240 --> 05:11:22,160
like 8.52 3 and then 8.6 it's like this

7418
05:11:20,798 --> 05:11:25,440
is like nothing this won't actually

7419
05:11:22,160 --> 05:11:28,840
matter in production it's like if you if

7420
05:11:25,440 --> 05:11:31,718
you take the difference um if you see

7421
05:11:28,840 --> 05:11:35,040
how much faster um the naive cter

7422
05:11:31,718 --> 05:11:36,480
formula is it's like literally 1.3%

7423
05:11:35,040 --> 05:11:38,360
faster which you're not ever going to

7424
05:11:36,480 --> 05:11:39,840
notice in a real environment like that

7425
05:11:38,360 --> 05:11:41,840
it's just unnoticeable

7426
05:11:39,840 --> 05:11:44,280
so it doesn't actually matter that much

7427
05:11:41,840 --> 05:11:46,440
if you care like totally just like go go

7428
05:11:44,280 --> 05:11:47,760
ahead go and R your own kernels but that

7429
05:11:46,440 --> 05:11:51,000
that's that's a general idea it doesn't

7430
05:11:47,760 --> 05:11:53,480
really matter that much um for for

7431
05:11:51,000 --> 05:11:55,320
convolutions it will matter though um

7432
05:11:53,480 --> 05:11:57,638
then going down to this I wrote a a p

7433
05:11:55,320 --> 05:12:00,080
torch script to just kind of illustrate

7434
05:11:57,638 --> 05:12:02,600
things out manually so like how pytorch

7435
05:12:00,080 --> 05:12:04,280
handles um how pytorch handles the

7436
05:12:02,600 --> 05:12:06,878
custom tan like when you write it from

7437
05:12:04,280 --> 05:12:09,200
scratch on your own um versus when you

7438
05:12:06,878 --> 05:12:14,160
just use the built-in the torch the

7439
05:12:09,200 --> 05:12:16,600
torch. 10 where did it go yes torch. 10

7440
05:12:14,160 --> 05:12:20,878
um so just kind of comparing those side

7441
05:12:16,600 --> 05:12:20,878
by side and seeing how they perform

7442
05:12:23,480 --> 05:12:30,798
um so if we write these out um o it's

7443
05:12:27,400 --> 05:12:34,040
taking some time to do this yeah so

7444
05:12:30,798 --> 05:12:37,080
custom t h um that's going to take 21

7445
05:12:34,040 --> 05:12:39,638
milliseconds remember how CN beforehand

7446
05:12:37,080 --> 05:12:41,360
uh was taking about like point I don't

7447
05:12:39,638 --> 05:12:44,440
know what it was like

7448
05:12:41,360 --> 05:12:46,718
point it was it was a very small number

7449
05:12:44,440 --> 05:12:49,280
um the custom like built in 10 is still

7450
05:12:46,718 --> 05:12:53,320
very fast um so if we go back and we

7451
05:12:49,280 --> 05:12:55,760
just nbcc compile this and then you know

7452
05:12:53,320 --> 05:12:57,400
run and let's just like look at which

7453
05:12:55,760 --> 05:12:59,040
what the shapes of our numbers are real

7454
05:12:57,400 --> 05:13:04,480
quick

7455
05:12:59,040 --> 05:13:11,120
um if I do batch size by say or if I do

7456
05:13:04,480 --> 05:13:11,120
256 by 32 by 224 by 224

7457
05:13:15,400 --> 05:13:20,600
four Rand n and we wrap these in

7458
05:13:18,120 --> 05:13:20,600
Brackets

7459
05:13:20,718 --> 05:13:26,718
here we can we can check this one real

7460
05:13:23,558 --> 05:13:26,718
quick and see how fast this

7461
05:13:27,400 --> 05:13:32,920
runs and then we will compare the python

7462
05:13:30,958 --> 05:13:35,040
script to that directly without like

7463
05:13:32,920 --> 05:13:37,280
deleting the output so we can't see it

7464
05:13:35,040 --> 05:13:39,320
um just so we understand like how much

7465
05:13:37,280 --> 05:13:41,760
performance could be gained from say

7466
05:13:39,320 --> 05:13:45,480
using uh using like a custom written

7467
05:13:41,760 --> 05:13:49,040
Cuda kernel or custom written uh CNN

7468
05:13:45,480 --> 05:13:51,280
function so if we go python TCH compare

7469
05:13:49,040 --> 05:13:54,280
it's going to take a second oh yeah ran

7470
05:13:51,280 --> 05:13:57,280
out to memory awesome

7471
05:13:54,280 --> 05:13:57,280
so

7472
05:13:58,798 --> 05:14:07,000
um Rand yeah no that doesn't work I bump

7473
05:14:02,440 --> 05:14:07,000
this down to like 128 it might work

7474
05:14:11,760 --> 05:14:17,718
it's not very fast yeah so like Custom

7475
05:14:15,638 --> 05:14:21,320
Custom 10 is super slow and then the

7476
05:14:17,718 --> 05:14:25,798
built-in one 4.3 let's say 4.4

7477
05:14:21,320 --> 05:14:27,600
milliseconds right um and then we go up

7478
05:14:25,798 --> 05:14:29,400
and we can see that you know this is

7479
05:14:27,600 --> 05:14:32,080
this is actually

7480
05:14:29,400 --> 05:14:37,240
um this is about double that right so if

7481
05:14:32,080 --> 05:14:37,240
we do 4 point about 4.4 *

7482
05:14:37,480 --> 05:14:44,200
2 8.8 right so we go back up and it's

7483
05:14:41,280 --> 05:14:45,600
like this is less than that so if we

7484
05:14:44,200 --> 05:14:47,200
write a naive kud kernel like this is

7485
05:14:45,600 --> 05:14:48,718
just naive if you were to optimize this

7486
05:14:47,200 --> 05:14:50,798
and imp Implement something like Loop

7487
05:14:48,718 --> 05:14:53,120
unrolling which we'll go into later and

7488
05:14:50,798 --> 05:14:55,240
you optimize this even just something as

7489
05:14:53,120 --> 05:14:57,400
simple as an activation function you can

7490
05:14:55,240 --> 05:14:59,520
get this surprisingly faster than Pi

7491
05:14:57,400 --> 05:15:00,878
torch um and just like write everything

7492
05:14:59,520 --> 05:15:03,920
manually you can actually get

7493
05:15:00,878 --> 05:15:07,320
considerable speed UPS so uh that's just

7494
05:15:03,920 --> 05:15:08,760
to provide some context there um but

7495
05:15:07,320 --> 05:15:10,080
yeah activation functions don't worry

7496
05:15:08,760 --> 05:15:13,400
about do that too much it's not really

7497
05:15:10,080 --> 05:15:15,080
going to affect you um and then we go to

7498
05:15:13,400 --> 05:15:17,320
like convolutions right this is this is

7499
05:15:15,080 --> 05:15:22,040
an example of where we're actually using

7500
05:15:17,320 --> 05:15:23,760
convolutions um so you know we we we

7501
05:15:22,040 --> 05:15:26,120
have a lot more things to pay attention

7502
05:15:23,760 --> 05:15:27,680
to I'm going to dive into this right now

7503
05:15:26,120 --> 05:15:29,600
now we can jump into convolutions a

7504
05:15:27,680 --> 05:15:31,718
little bit um I've been hacking around

7505
05:15:29,600 --> 05:15:33,760
with some of these and uh let's just

7506
05:15:31,718 --> 05:15:35,878
going to show you how these how these

7507
05:15:33,760 --> 05:15:38,440
work so we're going to start off with a

7508
05:15:35,878 --> 05:15:40,920
visualization first I'm going to bring

7509
05:15:38,440 --> 05:15:40,920
over a

7510
05:15:41,200 --> 05:15:46,600
convolutions visualized just pop over to

7511
05:15:43,760 --> 05:15:48,600
here and um this is just what it looks

7512
05:15:46,600 --> 05:15:50,878
like so we can like you have these input

7513
05:15:48,600 --> 05:15:54,240
sizes kernel size padding dilation and

7514
05:15:50,878 --> 05:15:56,360
stride input size is this I can change

7515
05:15:54,240 --> 05:15:59,600
the input image the kernel size the

7516
05:15:56,360 --> 05:16:01,920
actual weight itself the the weight term

7517
05:15:59,600 --> 05:16:04,798
um the padding so how much black pixels

7518
05:16:01,920 --> 05:16:07,920
are put around the input the dilation of

7519
05:16:04,798 --> 05:16:10,558
it right

7520
05:16:07,920 --> 05:16:12,820
um and then stride as well so I can make

7521
05:16:10,558 --> 05:16:13,958
it stride by one each time or stride by

7522
05:16:12,820 --> 05:16:16,718
[Music]

7523
05:16:13,958 --> 05:16:19,798
two um and and that's that's pretty much

7524
05:16:16,718 --> 05:16:22,320
just uh that that you can mess around

7525
05:16:19,798 --> 05:16:24,600
with that in your own time um but I've

7526
05:16:22,320 --> 05:16:28,520
written two scripts

7527
05:16:24,600 --> 05:16:30,280
so one for p torch so the P torch and

7528
05:16:28,520 --> 05:16:32,920
this one use the exact same values in

7529
05:16:30,280 --> 05:16:35,558
the exact same order um the exact same

7530
05:16:32,920 --> 05:16:37,440
parameters here they're all the same and

7531
05:16:35,558 --> 05:16:39,280
I'm just doing a side by-side comparison

7532
05:16:37,440 --> 05:16:42,638
of them so that we can get desired

7533
05:16:39,280 --> 05:16:44,798
output so it's

7534
05:16:42,638 --> 05:16:48,440
essentially it's essentially like a like

7535
05:16:44,798 --> 05:16:52,638
a 4x4 the input is like a 4x4 image so 1

7536
05:16:48,440 --> 05:16:56,120
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 and

7537
05:16:52,638 --> 05:16:59,600
then the kernel itself is 1 2 3 4 5 6 7

7538
05:16:56,120 --> 05:16:59,600
8 n and it's just going

7539
05:16:59,760 --> 05:17:05,840
to right it's going to do that and we

7540
05:17:02,718 --> 05:17:09,000
put a padding over it uh so that this is

7541
05:17:05,840 --> 05:17:11,680
going to end up flooring to one so Cur

7542
05:17:09,000 --> 05:17:15,600
size is three and then divide that uh

7543
05:17:11,680 --> 05:17:18,680
floor by two so if we go python go three

7544
05:17:15,600 --> 05:17:20,920
two we get one

7545
05:17:18,680 --> 05:17:22,200
and that's just going to be our padding

7546
05:17:20,920 --> 05:17:25,320
and then torch is going to do that with

7547
05:17:22,200 --> 05:17:28,000
this functional Library here um and and

7548
05:17:25,320 --> 05:17:29,760
we get the output right so this should

7549
05:17:28,000 --> 05:17:31,718
be very self-explanatory very easy we're

7550
05:17:29,760 --> 05:17:35,798
just reshaping it and then you know

7551
05:17:31,718 --> 05:17:38,240
reshaping it here as well um but when we

7552
05:17:35,798 --> 05:17:41,280
look at this part this is actually where

7553
05:17:38,240 --> 05:17:44,680
the fun kicks in so we do our cian N.H

7554
05:17:41,280 --> 05:17:46,160
at the top I've written a naive Cuda

7555
05:17:44,680 --> 05:17:48,480
kernel for this for doing 2D

7556
05:17:46,160 --> 05:17:50,360
convolutions takes in an input a kernel

7557
05:17:48,480 --> 05:17:52,680
the output the width height in channels

7558
05:17:50,360 --> 05:17:55,680
out channels kernel size uh and batch

7559
05:17:52,680 --> 05:17:57,840
size so uh don't don't worry about how

7560
05:17:55,680 --> 05:17:59,558
this works internally just it just let

7561
05:17:57,840 --> 05:18:01,440
it be there as some like template code

7562
05:17:59,558 --> 05:18:04,878
that's going to do what we want um it's

7563
05:18:01,440 --> 05:18:07,360
modular so um and then we have all the

7564
05:18:04,878 --> 05:18:09,520
same settings as we do in our py script

7565
05:18:07,360 --> 05:18:12,558
at the top here so

7566
05:18:09,520 --> 05:18:14,000
um we Define like how big our inputs

7567
05:18:12,558 --> 05:18:16,760
outputs and kernel elements are going to

7568
05:18:14,000 --> 05:18:18,840
be we print this out um we do our

7569
05:18:16,760 --> 05:18:19,878
classic just malic we have our our

7570
05:18:18,840 --> 05:18:21,558
values

7571
05:18:19,878 --> 05:18:24,240
organized

7572
05:18:21,558 --> 05:18:26,120
um more kud M Copy stuff and then we

7573
05:18:24,240 --> 05:18:27,878
create the CNN handle so this is all

7574
05:18:26,120 --> 05:18:31,638
very similar to uh what we were doing

7575
05:18:27,878 --> 05:18:33,400
with CNN and our t t function um we

7576
05:18:31,638 --> 05:18:36,320
created we create an input and output

7577
05:18:33,400 --> 05:18:39,520
tensor um filter descriptor for the

7578
05:18:36,320 --> 05:18:40,878
kernel itself um

7579
05:18:39,520 --> 05:18:43,080
convolution descriptor for the

7580
05:18:40,878 --> 05:18:44,558
convolution operation we create all of

7581
05:18:43,080 --> 05:18:47,320
these we create the convolution

7582
05:18:44,558 --> 05:18:48,760
descriptor with that memory address um

7583
05:18:47,320 --> 05:18:49,840
we're doing going to use the 4D

7584
05:18:48,760 --> 05:18:51,878
descriptor because it's going to be

7585
05:18:49,840 --> 05:18:55,040
shaped batch size by Channel by height

7586
05:18:51,878 --> 05:18:57,600
by width um and then if we look at these

7587
05:18:55,040 --> 05:19:00,120
so we have the we have the tensor

7588
05:18:57,600 --> 05:19:01,878
descriptor the format so how which what

7589
05:19:00,120 --> 05:19:04,160
it was like the shape of it and then the

7590
05:19:01,878 --> 05:19:08,080
data type which is just a float 32 as we

7591
05:19:04,160 --> 05:19:10,718
have here um and then we have the N by C

7592
05:19:08,080 --> 05:19:12,638
by h by W right so batch size by

7593
05:19:10,718 --> 05:19:14,638
channels by height by width and we do

7594
05:19:12,638 --> 05:19:16,480
the same thing for all of these um the

7595
05:19:14,638 --> 05:19:17,798
output is going to be out channels

7596
05:19:16,480 --> 05:19:19,280
because it's not inch it's going to be

7597
05:19:17,798 --> 05:19:22,878
how many out channels do we get in the

7598
05:19:19,280 --> 05:19:24,680
end um and then the filter descriptor is

7599
05:19:22,878 --> 05:19:26,320
going to have a different it's going to

7600
05:19:24,680 --> 05:19:29,558
be organized a different way it's going

7601
05:19:26,320 --> 05:19:31,400
to be um out channels by in channels by

7602
05:19:29,558 --> 05:19:34,520
height by width so out channels in

7603
05:19:31,400 --> 05:19:37,520
channels height uh height by width right

7604
05:19:34,520 --> 05:19:39,440
for the for the kernel itself for the

7605
05:19:37,520 --> 05:19:41,360
convolution filter you I'm going to use

7606
05:19:39,440 --> 05:19:44,160
those interchangeably convolution kernel

7607
05:19:41,360 --> 05:19:46,680
and convolution filter same thing um and

7608
05:19:44,160 --> 05:19:48,200
then the actual descriptor itself um

7609
05:19:46,680 --> 05:19:49,480
this is a 2d descriptor so it's going to

7610
05:19:48,200 --> 05:19:52,798
be a 2d

7611
05:19:49,480 --> 05:19:57,240
convolution um and we just dump all

7612
05:19:52,798 --> 05:19:59,840
these in so the padding H padding W um u

7613
05:19:57,240 --> 05:20:02,680
and v and dilations we we're just we're

7614
05:19:59,840 --> 05:20:04,280
not even using dilations um convolution

7615
05:20:02,680 --> 05:20:06,558
mode so it's going to be cross

7616
05:20:04,280 --> 05:20:09,200
correlation and the data type is just

7617
05:20:06,558 --> 05:20:10,920
float um so I don't expect you to know

7618
05:20:09,200 --> 05:20:12,440
what what all like the convolution laws

7619
05:20:10,920 --> 05:20:13,840
are and everything we're just comparing

7620
05:20:12,440 --> 05:20:18,440
to P torch and making sure that

7621
05:20:13,840 --> 05:20:22,160
everything lines up um and then in terms

7622
05:20:18,440 --> 05:20:24,040
of the um in terms of the algorithm

7623
05:20:22,160 --> 05:20:26,120
itself um we have a little thing that

7624
05:20:24,040 --> 05:20:27,798
searches through stuff here uh I might

7625
05:20:26,120 --> 05:20:29,840
change this later this doesn't it's not

7626
05:20:27,798 --> 05:20:32,160
very beautiful to look at um but what

7627
05:20:29,840 --> 05:20:35,280
you what you can do is actually just

7628
05:20:32,160 --> 05:20:38,080
like literally this uh where is it this

7629
05:20:35,280 --> 05:20:40,360
algo here when you do the when you give

7630
05:20:38,080 --> 05:20:42,760
uh when you give CNN like a workspace

7631
05:20:40,360 --> 05:20:45,840
size to do the operation in there's this

7632
05:20:42,760 --> 05:20:49,400
CNN convolution forward algo type which

7633
05:20:45,840 --> 05:20:52,480
is right here um forward algo type and

7634
05:20:49,400 --> 05:20:53,760
there's a bunch of them in here so uh by

7635
05:20:52,480 --> 05:20:55,240
default what that's supposed to do is

7636
05:20:53,760 --> 05:20:57,878
cycle through them and find which one is

7637
05:20:55,240 --> 05:20:59,320
the best but I find that it might be a

7638
05:20:57,878 --> 05:21:02,280
little bit better to just cycle through

7639
05:20:59,320 --> 05:21:04,200
these one one by one on your own so try

7640
05:21:02,280 --> 05:21:06,600
out implicit gem Tri out pre-comp gem

7641
05:21:04,200 --> 05:21:09,600
triy out gem tried out direct fft fft

7642
05:21:06,600 --> 05:21:12,040
tiling WI noat nonfused count right so

7643
05:21:09,600 --> 05:21:13,760
try all these out and see how they work

7644
05:21:12,040 --> 05:21:15,080
um which I which I have in this uh

7645
05:21:13,760 --> 05:21:18,520
comparison script we're going to do

7646
05:21:15,080 --> 05:21:20,480
later on but uh yeah don't don't don't

7647
05:21:18,520 --> 05:21:23,160
worry too much about those just you can

7648
05:21:20,480 --> 05:21:24,320
kind of just run the script as is um but

7649
05:21:23,160 --> 05:21:26,440
we're just trying to find the best

7650
05:21:24,320 --> 05:21:28,558
convolution algorithm for our problem at

7651
05:21:26,440 --> 05:21:31,680
hand right so when you have a a smaller

7652
05:21:28,558 --> 05:21:34,040
kernel and a small image a certain like

7653
05:21:31,680 --> 05:21:36,320
maybe an implicit gem might be faster

7654
05:21:34,040 --> 05:21:37,878
than say an fft tiling uh because of the

7655
05:21:36,320 --> 05:21:39,680
overhead right so you have to just

7656
05:21:37,878 --> 05:21:42,718
consider things like that your problem

7657
05:21:39,680 --> 05:21:45,558
size all that stuff um the workspace

7658
05:21:42,718 --> 05:21:48,160
size is just how big how big you

7659
05:21:45,558 --> 05:21:49,360
actually um return the minimum size of

7660
05:21:48,160 --> 05:21:52,040
the workspace to be pass to the

7661
05:21:49,360 --> 05:21:53,958
convolution given an algorithm right so

7662
05:21:52,040 --> 05:21:56,160
you're essentially just saying how much

7663
05:21:53,958 --> 05:21:58,320
do you get to work with here um and this

7664
05:21:56,160 --> 05:22:00,280
is defined by a bunch of things that it

7665
05:21:58,320 --> 05:22:02,040
it just it just decides this right so we

7666
05:22:00,280 --> 05:22:04,718
give it a bunch of descriptions and it's

7667
05:22:02,040 --> 05:22:10,040
going to use that context to decide um

7668
05:22:04,718 --> 05:22:10,040
what the workspace size should be um

7669
05:22:11,280 --> 05:22:16,840
now now we can do our Benchmark uh

7670
05:22:14,520 --> 05:22:18,798
warmup and Benchmark runs so we have

7671
05:22:16,840 --> 05:22:21,080
this um you know just skipping Alpha and

7672
05:22:18,798 --> 05:22:22,280
beta we have this convolution forward

7673
05:22:21,080 --> 05:22:25,360
function which I'll show you in the

7674
05:22:22,280 --> 05:22:28,280
Nvidia docs in a second here um this

7675
05:22:25,360 --> 05:22:30,718
consists of the handle Alpha tensor

7676
05:22:28,280 --> 05:22:33,520
descriptor for the input the input

7677
05:22:30,718 --> 05:22:36,120
itself which is just a you just any just

7678
05:22:33,520 --> 05:22:40,200
a void pointer right

7679
05:22:36,120 --> 05:22:43,320
um and then the the filter descriptor

7680
05:22:40,200 --> 05:22:47,760
the kernel uh convolution kernel so a

7681
05:22:43,320 --> 05:22:51,718
point again a pointer to to an array uh

7682
05:22:47,760 --> 05:22:54,958
the convolution descriptor so actual uh

7683
05:22:51,718 --> 05:22:57,240
operation algorithm itself the algorithm

7684
05:22:54,958 --> 05:22:59,360
um the workspace which we just find in

7685
05:22:57,240 --> 05:23:03,040
workspace size and bytes the workspace

7686
05:22:59,360 --> 05:23:05,600
size and bytes um we pass in this we we

7687
05:23:03,040 --> 05:23:08,878
do this size T which is like a size type

7688
05:23:05,600 --> 05:23:10,638
for like storing large values and we put

7689
05:23:08,878 --> 05:23:13,000
this into here and then this value

7690
05:23:10,638 --> 05:23:14,958
changes based on these settings right so

7691
05:23:13,000 --> 05:23:17,080
when we put this back in here it's going

7692
05:23:14,958 --> 05:23:18,558
to decide um when it's actually running

7693
05:23:17,080 --> 05:23:20,280
this how much do we need and and what

7694
05:23:18,558 --> 05:23:22,440
are the resources requirements it's pred

7695
05:23:20,280 --> 05:23:25,320
decided right

7696
05:23:22,440 --> 05:23:27,558
um and then we do um we just we just

7697
05:23:25,320 --> 05:23:30,558
enter the output description and then

7698
05:23:27,558 --> 05:23:32,638
the output uh the output device array

7699
05:23:30,558 --> 05:23:35,320
right and then we do the same thing but

7700
05:23:32,638 --> 05:23:37,440
with our uh con 2D except there's like

7701
05:23:35,320 --> 05:23:39,360
it's less complex to filter through and

7702
05:23:37,440 --> 05:23:41,760
then we synchronize all of our threads

7703
05:23:39,360 --> 05:23:44,000
and blocks in the GPU with Cuda device

7704
05:23:41,760 --> 05:23:45,558
synchronized very simple and we do the

7705
05:23:44,000 --> 05:23:47,798
same thing for benchmarks runs except we

7706
05:23:45,558 --> 05:23:51,400
just add a time and an event recording

7707
05:23:47,798 --> 05:23:53,680
as well right um so fairly simple

7708
05:23:51,400 --> 05:23:55,798
Concepts happening here just timing and

7709
05:23:53,680 --> 05:23:57,600
benchmarking and mainly just filtering

7710
05:23:55,798 --> 05:23:59,400
through what the heck a function takes

7711
05:23:57,600 --> 05:24:01,040
in and what are all these types doing

7712
05:23:59,400 --> 05:24:03,200
right that's that's really the mess you

7713
05:24:01,040 --> 05:24:06,400
have to dig through

7714
05:24:03,200 --> 05:24:08,798
um now if we go down um we can actually

7715
05:24:06,400 --> 05:24:11,840
we actually print out the CNN output and

7716
05:24:08,798 --> 05:24:13,280
the naive kernel output so and then the

7717
05:24:11,840 --> 05:24:15,000
flattened one as well so that we can

7718
05:24:13,280 --> 05:24:17,000
compare back to Pi torch element by

7719
05:24:15,000 --> 05:24:18,878
element um we just destroy all the

7720
05:24:17,000 --> 05:24:21,638
context afterwards same thing with the

7721
05:24:18,878 --> 05:24:27,360
tan uh same thing with the tan Cuda

7722
05:24:21,638 --> 05:24:30,958
script um go and run this so out

7723
05:24:27,360 --> 05:24:34,958
01 just like that link

7724
05:24:30,958 --> 05:24:38,440
CN we end we run this

7725
05:24:34,958 --> 05:24:40,360
um all of these are uh as expected and

7726
05:24:38,440 --> 05:24:42,120
so it's it's going to just yeah it's

7727
05:24:40,360 --> 05:24:43,600
going to select an algorithm these these

7728
05:24:42,120 --> 05:24:45,638
are all messed up I might change these

7729
05:24:43,600 --> 05:24:50,280
later but

7730
05:24:45,638 --> 05:24:51,920
um we notice that qnn is slower than the

7731
05:24:50,280 --> 05:24:54,240
naive kernel and that's because it's

7732
05:24:51,920 --> 05:24:56,798
just really small right it's very small

7733
05:24:54,240 --> 05:24:58,440
um qn probably has more to organize it's

7734
05:24:56,798 --> 05:25:01,360
got these Alpha Beta terms everywhere

7735
05:24:58,440 --> 05:25:02,680
it's got to take care of and um yeah

7736
05:25:01,360 --> 05:25:06,080
there there might just be some extra

7737
05:25:02,680 --> 05:25:08,680
overhead there so it is it is indeed um

7738
05:25:06,080 --> 05:25:10,760
you know like three four four times

7739
05:25:08,680 --> 05:25:13,120
slower than this just just because it's

7740
05:25:10,760 --> 05:25:16,718
smaller and it's not a big problem size

7741
05:25:13,120 --> 05:25:21,040
so if we look at the output here we see1

7742
05:25:16,718 --> 05:25:26,840
178 217 and then the end is like 274 275

7743
05:25:21,040 --> 05:25:26,840
175 now if I go and run um torch

7744
05:25:27,120 --> 05:25:32,638
compare it's going to do the exact same

7745
05:25:29,320 --> 05:25:39,040
thing notice how we get 111 178 217 and

7746
05:25:32,638 --> 05:25:43,360
then 295 1 uh or sorry 274 299 295 175

7747
05:25:39,040 --> 05:25:44,958
so go back here 274 295 175 perfect so

7748
05:25:43,360 --> 05:25:49,200
everything we can look through these

7749
05:25:44,958 --> 05:25:52,840
these all line up 1 2 3 4 5 6 7 8 9 10

7750
05:25:49,200 --> 05:25:55,160
11 12 13 14 15 16 elements uh and this

7751
05:25:52,840 --> 05:25:58,600
one has you know 16 elements as we print

7752
05:25:55,160 --> 05:26:00,120
out um at the bottom here just the

7753
05:25:58,600 --> 05:26:02,920
length of

7754
05:26:00,120 --> 05:26:05,480
that so I try to be like fa quick with

7755
05:26:02,920 --> 05:26:07,638
that it's very you know kind of just a

7756
05:26:05,480 --> 05:26:11,400
bunch of boiler plate code that we we

7757
05:26:07,638 --> 05:26:13,200
run through um but now uh now that we

7758
05:26:11,400 --> 05:26:15,280
know that this works and it's outputting

7759
05:26:13,200 --> 05:26:17,718
what we want it to we can actually go

7760
05:26:15,280 --> 05:26:20,840
take a look at the comparison script so

7761
05:26:17,718 --> 05:26:22,080
the comparison script um actually real

7762
05:26:20,840 --> 05:26:24,280
quick before I go to this comparison

7763
05:26:22,080 --> 05:26:27,958
script I'm going to bring up the uh the

7764
05:26:24,280 --> 05:26:31,718
cian and docs here just to kind of show

7765
05:26:27,958 --> 05:26:33,798
you the

7766
05:26:31,718 --> 05:26:37,400
uh go to

7767
05:26:33,798 --> 05:26:40,200
CNN so we're doing this convolution to

7768
05:26:37,400 --> 05:26:42,400
CN convolution forward right so we look

7769
05:26:40,200 --> 05:26:46,920
at this um and there's a bunch of things

7770
05:26:42,400 --> 05:26:46,920
in it so like this descriptor type

7771
05:26:49,280 --> 05:26:53,280
um handle Alpha Beta so it just

7772
05:26:51,878 --> 05:26:55,440
describes everything that we need to

7773
05:26:53,280 --> 05:26:57,600
know right so if something I said didn't

7774
05:26:55,440 --> 05:26:59,878
make sense maybe just like look at here

7775
05:26:57,600 --> 05:27:02,400
and it might make better sense to you

7776
05:26:59,878 --> 05:27:04,440
that way um but if we want to say look

7777
05:27:02,400 --> 05:27:06,120
at something like the forward algo type

7778
05:27:04,440 --> 05:27:07,680
we go here click on it and there's a

7779
05:27:06,120 --> 05:27:10,040
bunch of there's a bunch of values so

7780
05:27:07,680 --> 05:27:11,440
implicit um expresses as a matrix

7781
05:27:10,040 --> 05:27:12,878
product without actually explicitly

7782
05:27:11,440 --> 05:27:14,280
forming the Matrix that holds the input

7783
05:27:12,878 --> 05:27:15,558
tensor in so there's a bunch of

7784
05:27:14,280 --> 05:27:18,520
descriptions here about like different

7785
05:27:15,558 --> 05:27:20,718
algorithms you can use um and just like

7786
05:27:18,520 --> 05:27:22,360
when you might want to use it there's

7787
05:27:20,718 --> 05:27:24,040
also other articles on like which ones

7788
05:27:22,360 --> 05:27:26,280
are good for different cases um

7789
05:27:24,040 --> 05:27:27,920
convolutions are very very well covered

7790
05:27:26,280 --> 05:27:31,040
uh in the whole deep learning space so

7791
05:27:27,920 --> 05:27:33,040
shouldn't be too hard to navigate but

7792
05:27:31,040 --> 05:27:35,718
these are these are going to be your

7793
05:27:33,040 --> 05:27:38,878
forward forward pass

7794
05:27:35,718 --> 05:27:42,200
algorithms now if we

7795
05:27:38,878 --> 05:27:44,360
go back to um we go back to this

7796
05:27:42,200 --> 05:27:48,638
comparison script I'm essentially doing

7797
05:27:44,360 --> 05:27:51,638
the exact same thing except I set the um

7798
05:27:48,638 --> 05:27:55,160
I set the algorithm used to implicit gem

7799
05:27:51,638 --> 05:27:59,520
so that's the um that's this one right

7800
05:27:55,160 --> 05:28:04,320
here so I just set that manually and uh

7801
05:27:59,520 --> 05:28:08,638
yeah that's that's that's pretty much it

7802
05:28:04,320 --> 05:28:10,200
um now we we we set this to algo and

7803
05:28:08,638 --> 05:28:12,080
then we just plug an algo in there so

7804
05:28:10,200 --> 05:28:13,680
that's that's like the main difference

7805
05:28:12,080 --> 05:28:15,480
and then the other one is just how we

7806
05:28:13,680 --> 05:28:18,360
initialize our data everything else is

7807
05:28:15,480 --> 05:28:20,718
the same as this initial like uh just

7808
05:28:18,360 --> 05:28:26,360
the the convolution uh compare like

7809
05:28:20,718 --> 05:28:30,798
between P torch um so we where is

7810
05:28:26,360 --> 05:28:33,200
it yes so we initialize uh on the CPU

7811
05:28:30,798 --> 05:28:34,840
with just a bunch of random values um

7812
05:28:33,200 --> 05:28:38,360
and then we just do an operation with

7813
05:28:34,840 --> 05:28:40,200
those so I make the I make the element a

7814
05:28:38,360 --> 05:28:46,440
lot make this whole thing a lot bigger

7815
05:28:40,200 --> 05:28:51,000
so it's 224 by 224 by 11 by 32 by

7816
05:28:46,440 --> 05:28:52,718
uh 11 11 32 64 it's not it's not times

7817
05:28:51,000 --> 05:28:56,440
all of these but the input is going to

7818
05:28:52,718 --> 05:29:01,760
be of size width by height uh it's going

7819
05:28:56,440 --> 05:29:05,920
to be width by height by by in channels

7820
05:29:01,760 --> 05:29:08,200
by batch size so n chw as we did before

7821
05:29:05,920 --> 05:29:09,680
it's going to take up a lot of space um

7822
05:29:08,200 --> 05:29:12,240
but we're just going to Benchmark this

7823
05:29:09,680 --> 05:29:12,240
and see how it

7824
05:29:15,760 --> 05:29:21,638
goes so we go and run this get a batch

7825
05:29:18,280 --> 05:29:25,240
size of four and we notice CN average

7826
05:29:21,638 --> 05:29:26,958
time is 14.8 millisecs and the naive

7827
05:29:25,240 --> 05:29:30,520
kernel average time is about 82

7828
05:29:26,958 --> 05:29:33,200
milliseconds so if we do that if we do

7829
05:29:30,520 --> 05:29:33,200
that division

7830
05:29:34,718 --> 05:29:42,240
there we notice that we get a 5.5x speed

7831
05:29:40,040 --> 05:29:44,760
up by using 2D andm how awesome is that

7832
05:29:42,240 --> 05:29:46,400
right I mean if you optimized this naive

7833
05:29:44,760 --> 05:29:48,718
kernel up here and made it like more

7834
05:29:46,400 --> 05:29:50,520
specific to your your specific use case

7835
05:29:48,718 --> 05:29:52,840
rather than calculating a bunch of stuff

7836
05:29:50,520 --> 05:29:55,200
it would be faster um but this is kind

7837
05:29:52,840 --> 05:29:57,558
of just like for demonstration purposes

7838
05:29:55,200 --> 05:29:59,600
um CNN is still wildly fast and it would

7839
05:29:57,558 --> 05:30:02,520
take a while to get something that is

7840
05:29:59,600 --> 05:30:04,040
actually um more performant than this um

7841
05:30:02,520 --> 05:30:07,638
like significantly more performant it

7842
05:30:04,040 --> 05:30:09,480
would take a lot to do that um but

7843
05:30:07,638 --> 05:30:11,760
that's that's the idea right uh this is

7844
05:30:09,480 --> 05:30:13,000
why you use things like CNN and P torch

7845
05:30:11,760 --> 05:30:15,958
is because they're they're just like

7846
05:30:13,000 --> 05:30:18,120
super fast they just it just did a

7847
05:30:15,958 --> 05:30:21,080
massive convolution operation of like an

7848
05:30:18,120 --> 05:30:25,878
image with 32 channels of batch size 4

7849
05:30:21,080 --> 05:30:27,240
and a kernel size with like 11 by 11 um

7850
05:30:25,878 --> 05:30:28,680
it it's just like insane the amount of

7851
05:30:27,240 --> 05:30:30,840
operations it does in such a little

7852
05:30:28,680 --> 05:30:33,958
amount of time so that's what we're

7853
05:30:30,840 --> 05:30:36,040
working with um that's that's uh that's

7854
05:30:33,958 --> 05:30:38,878
that's C and in for

7855
05:30:36,040 --> 05:30:41,120
you now when you're working with big

7856
05:30:38,878 --> 05:30:42,840
massive data centers and GPU clusters

7857
05:30:41,120 --> 05:30:45,400
even if it's your own local rig that's

7858
05:30:42,840 --> 05:30:47,798
just on the side and you have 8 409s or

7859
05:30:45,400 --> 05:30:50,240
309s connected to it um this part might

7860
05:30:47,798 --> 05:30:54,200
come in a little bit of Handy right so

7861
05:30:50,240 --> 05:30:56,920
larger rigs or data centers um let me

7862
05:30:54,200 --> 05:31:00,040
just change that how that looks um so

7863
05:30:56,920 --> 05:31:02,080
you have kublos MP versus nccl versus

7864
05:31:00,040 --> 05:31:03,240
Mig uh now these are all different I'm

7865
05:31:02,080 --> 05:31:05,240
going to start with Mig because it's the

7866
05:31:03,240 --> 05:31:08,920
easiest one to explain essentially think

7867
05:31:05,240 --> 05:31:11,080
of it as your like your Amazon your AWS

7868
05:31:08,920 --> 05:31:13,160
and you have a you have a giant you know

7869
05:31:11,080 --> 05:31:15,480
GPU inside of your inside of your

7870
05:31:13,160 --> 05:31:17,558
facility your data center and so

7871
05:31:15,480 --> 05:31:19,520
typically with this type of GPU most

7872
05:31:17,558 --> 05:31:21,200
people aren't going to uh use the

7873
05:31:19,520 --> 05:31:22,760
entirety of it just like a small chunk

7874
05:31:21,200 --> 05:31:24,958
they just want the parallel processing

7875
05:31:22,760 --> 05:31:27,080
aspect for like some some whatever

7876
05:31:24,958 --> 05:31:28,840
signal processing I don't know and so

7877
05:31:27,080 --> 05:31:32,400
what you can do is you can actually

7878
05:31:28,840 --> 05:31:34,958
split that node um you split that node

7879
05:31:32,400 --> 05:31:37,120
into a bunch of a bunch of smaller gpus

7880
05:31:34,958 --> 05:31:39,680
it's multi-instance GPU you can have

7881
05:31:37,120 --> 05:31:42,600
multiple inst es connected to the same

7882
05:31:39,680 --> 05:31:45,280
card and you can split workloads evenly

7883
05:31:42,600 --> 05:31:47,440
and securely across those and so that's

7884
05:31:45,280 --> 05:31:49,520
what that's what Mig does it's used in

7885
05:31:47,440 --> 05:31:54,000
uh data center environments use cases

7886
05:31:49,520 --> 05:31:56,920
right um and then you have nccl so nccl

7887
05:31:54,000 --> 05:31:59,920
is actually really really important for

7888
05:31:56,920 --> 05:32:01,920
um distributed cluster Computing so this

7889
05:31:59,920 --> 05:32:04,480
is essentially going

7890
05:32:01,920 --> 05:32:07,080
to it's exactly how it sounds right it's

7891
05:32:04,480 --> 05:32:09,760
not going to it's not going to do

7892
05:32:07,080 --> 05:32:12,000
operations across but it's going to help

7893
05:32:09,760 --> 05:32:14,280
manage and communicate uh different

7894
05:32:12,000 --> 05:32:16,240
things across a cluster so use for

7895
05:32:14,280 --> 05:32:18,200
Distributing information collecting it

7896
05:32:16,240 --> 05:32:22,558
acting as a general cluster level

7897
05:32:18,200 --> 05:32:24,680
Communicator whereas kublos MP up here

7898
05:32:22,558 --> 05:32:26,638
is going to be doing the grunt work of

7899
05:32:24,680 --> 05:32:28,878
doing like say giant Matrix

7900
05:32:26,638 --> 05:32:30,400
multiplications across like a note of

7901
05:32:28,878 --> 05:32:33,760
eight 8

7902
05:32:30,400 --> 05:32:36,160
8100s um and then nccl is going to is

7903
05:32:33,760 --> 05:32:38,440
going to uh like run this in batches so

7904
05:32:36,160 --> 05:32:41,360
remember Collective Communications uh

7905
05:32:38,440 --> 05:32:43,600
these the operations within that um and

7906
05:32:41,360 --> 05:32:45,520
and there's more resources on this uh

7907
05:32:43,600 --> 05:32:48,840
would be like all reduce broadcast

7908
05:32:45,520 --> 05:32:51,120
gather scatter across right so not like

7909
05:32:48,840 --> 05:32:53,320
there's no like multiply or like fused

7910
05:32:51,120 --> 05:32:55,400
operations in here it's it's doing it

7911
05:32:53,320 --> 05:32:59,480
across a cluster and communicating uh

7912
05:32:55,400 --> 05:33:02,280
communicating data right um so just for

7913
05:32:59,480 --> 05:33:05,480
reference in pytorch you would use

7914
05:33:02,280 --> 05:33:07,798
distributed data parallel um for this

7915
05:33:05,480 --> 05:33:09,440
distributed cluster level Computing so

7916
05:33:07,798 --> 05:33:11,360
distributed data parallel data

7917
05:33:09,440 --> 05:33:13,000
parallelism at the module level which

7918
05:33:11,360 --> 05:33:15,440
can run across multiple machines so

7919
05:33:13,000 --> 05:33:17,760
module would be like say a function in P

7920
05:33:15,440 --> 05:33:20,280
torch right um which can run across

7921
05:33:17,760 --> 05:33:22,120
multiple machines um should spawn

7922
05:33:20,280 --> 05:33:23,920
multiple processes and create a single

7923
05:33:22,120 --> 05:33:25,638
data distributed parallel instance per

7924
05:33:23,920 --> 05:33:27,638
process uh and there's there's a bunch

7925
05:33:25,638 --> 05:33:29,558
of more stuff you can read about this

7926
05:33:27,638 --> 05:33:32,558
this is used a lot actually in pytorch

7927
05:33:29,558 --> 05:33:34,638
it's fairly simple to set up um but this

7928
05:33:32,558 --> 05:33:38,440
is this is like kind of all of that

7929
05:33:34,638 --> 05:33:41,240
simplified into one usable thing um

7930
05:33:38,440 --> 05:33:43,600
now going back um there's I do have more

7931
05:33:41,240 --> 05:33:45,160
resources on this too um I'm not going

7932
05:33:43,600 --> 05:33:46,840
to cover all of this since I don't

7933
05:33:45,160 --> 05:33:48,798
actually have a cluster in my house to

7934
05:33:46,840 --> 05:33:50,638
experiment it on uh but there are some

7935
05:33:48,798 --> 05:33:52,120
links and resources that you could you

7936
05:33:50,638 --> 05:33:53,840
know you could find yourself going down

7937
05:33:52,120 --> 05:33:59,040
a rabbit hole with these which is what

7938
05:33:53,840 --> 05:34:01,200
might be quite fun um but kuas MP is

7939
05:33:59,040 --> 05:34:04,280
actually going to it's literally

7940
05:34:01,200 --> 05:34:06,520
designed to do distributed basic dense

7941
05:34:04,280 --> 05:34:08,160
linear algebra so if you're doing like a

7942
05:34:06,520 --> 05:34:12,200
multi-gpu

7943
05:34:08,160 --> 05:34:15,160
um tensor tensor operation um this kualo

7944
05:34:12,200 --> 05:34:17,718
MP would handle that so if we go back

7945
05:34:15,160 --> 05:34:22,280
here and we go to

7946
05:34:17,718 --> 05:34:24,360
kuas uh kuas MP high performance Cuda

7947
05:34:22,280 --> 05:34:29,040
library for distributed dens linear

7948
05:34:24,360 --> 05:34:32,920
algebra um getting started um for

7949
05:34:29,040 --> 05:34:35,480
example like code samples um

7950
05:34:32,920 --> 05:34:38,360
right you you have these giant like

7951
05:34:35,480 --> 05:34:39,798
grids and stuff that you handle um

7952
05:34:38,360 --> 05:34:43,080
how to use

7953
05:34:39,798 --> 05:34:44,558
it um yeah maybe like how to use it for

7954
05:34:43,080 --> 05:34:47,000
example cap

7955
05:34:44,558 --> 05:34:48,798
API um and there's a bunch of

7956
05:34:47,000 --> 05:34:51,040
interesting resources on here as to how

7957
05:34:48,798 --> 05:34:53,280
you would do things maybe there's like

7958
05:34:51,040 --> 05:34:53,280
an

7959
05:34:57,280 --> 05:35:02,080
operation I don't know um that's this is

7960
05:35:00,000 --> 05:35:04,000
up for you to navigate it's optional you

7961
05:35:02,080 --> 05:35:05,600
don't actually have to know kublos MP

7962
05:35:04,000 --> 05:35:06,920
because pytorch does a lot of that for

7963
05:35:05,600 --> 05:35:08,958
you if you're just working with those

7964
05:35:06,920 --> 05:35:10,080
workloads um but if you're going to be

7965
05:35:08,958 --> 05:35:11,680
working on like data center

7966
05:35:10,080 --> 05:35:15,760
infrastructure this is something you

7967
05:35:11,680 --> 05:35:18,320
want to learn like Koss MP and nccl

7968
05:35:15,760 --> 05:35:23,000
so um hopefully that helps to provide

7969
05:35:18,320 --> 05:35:23,000
some context on uh like larger larger

7970
05:35:23,160 --> 05:35:27,638
setups give yourself a pad on the back

7971
05:35:25,520 --> 05:35:30,240
if you made it this far this has been a

7972
05:35:27,638 --> 05:35:31,958
lot so far and we've covered we've

7973
05:35:30,240 --> 05:35:33,360
covered actually an insane amount and

7974
05:35:31,958 --> 05:35:36,120
now we're going to cover one of the more

7975
05:35:33,360 --> 05:35:37,798
technical parts of the course called uh

7976
05:35:36,120 --> 05:35:41,240
matrix multiplication and how we we

7977
05:35:37,798 --> 05:35:44,400
optimize it so this is going to be one

7978
05:35:41,240 --> 05:35:47,000
of the most technical Parts uh mainly

7979
05:35:44,400 --> 05:35:49,240
because we're looking at uh like

7980
05:35:47,000 --> 05:35:51,920
lowlevel optimizations how do we

7981
05:35:49,240 --> 05:35:53,638
actually speed this up on the hardware

7982
05:35:51,920 --> 05:35:55,000
and so it's no longer just like the

7983
05:35:53,638 --> 05:35:56,760
general idea of how it works we're

7984
05:35:55,000 --> 05:35:58,160
actually using our knowledge and

7985
05:35:56,760 --> 05:36:00,718
additional knowledge that I'm going to

7986
05:35:58,160 --> 05:36:02,558
share with you on how we can makes uh on

7987
05:36:00,718 --> 05:36:05,200
how we can make the fundamental matrix

7988
05:36:02,558 --> 05:36:08,360
multiplication or the mat Mill algorithm

7989
05:36:05,200 --> 05:36:10,440
uh really really fast so this algorithm

7990
05:36:08,360 --> 05:36:13,200
is proprietary in deep learning it is

7991
05:36:10,440 --> 05:36:15,120
everywhere uh and so I figured the best

7992
05:36:13,200 --> 05:36:18,040
way to teach you how to optimize kernels

7993
05:36:15,120 --> 05:36:22,840
would be to use this as an example and

7994
05:36:18,040 --> 05:36:24,798
luckily uh we have we have a repo by uh

7995
05:36:22,840 --> 05:36:26,760
Simon bohim I think I think that's how

7996
05:36:24,798 --> 05:36:28,558
you pronounce it this guy's a this guy's

7997
05:36:26,760 --> 05:36:31,480
a performance or Colonel engineer at

7998
05:36:28,558 --> 05:36:33,878
anthropic so he probably knows what he's

7999
05:36:31,480 --> 05:36:36,680
doing and uh he made this really cool

8000
05:36:33,878 --> 05:36:38,040
repo called sjem cuda as well as a blog

8001
05:36:36,680 --> 05:36:39,840
post to go along with

8002
05:36:38,040 --> 05:36:41,760
so I'm just going to be following this

8003
05:36:39,840 --> 05:36:43,840
um and I was kind of lazy and not going

8004
05:36:41,760 --> 05:36:45,798
to like write all of this from scratch

8005
05:36:43,840 --> 05:36:48,798
on my own so I kind of just went along

8006
05:36:45,798 --> 05:36:50,360
with this and I want to explain to you

8007
05:36:48,798 --> 05:36:53,040
uh the steps of going through this and

8008
05:36:50,360 --> 05:36:54,798
how we actually how we Pro how we

8009
05:36:53,040 --> 05:36:57,638
progress through these different steps

8010
05:36:54,798 --> 05:37:00,080
getting up to uh pretty close to kublos

8011
05:36:57,638 --> 05:37:01,558
Performance and gigaflops uh or perhaps

8012
05:37:00,080 --> 05:37:03,958
even surpassing it depending on which

8013
05:37:01,558 --> 05:37:05,480
Hardware you have but uh this this is

8014
05:37:03,958 --> 05:37:07,600
going to be the goal so you might have

8015
05:37:05,480 --> 05:37:09,760
come across this article already uh but

8016
05:37:07,600 --> 05:37:11,360
but in case you haven't or in case you

8017
05:37:09,760 --> 05:37:13,718
have maybe this was too hard I'm just

8018
05:37:11,360 --> 05:37:15,958
going to go over this and uh we're we're

8019
05:37:13,718 --> 05:37:17,360
going to we're going to go superow level

8020
05:37:15,958 --> 05:37:19,558
things are going to be super clear after

8021
05:37:17,360 --> 05:37:21,400
you finish this part um you're going to

8022
05:37:19,558 --> 05:37:23,920
understand how to optimize uh Cuda

8023
05:37:21,400 --> 05:37:25,840
kernels so let's get started we're not

8024
05:37:23,920 --> 05:37:27,638
actually going to occupy the cud course

8025
05:37:25,840 --> 05:37:29,320
repo with this I'm going to link it in

8026
05:37:27,638 --> 05:37:30,920
the read me file so you can follow along

8027
05:37:29,320 --> 05:37:32,000
in case you're just going through the in

8028
05:37:30,920 --> 05:37:34,280
case you're going through this this

8029
05:37:32,000 --> 05:37:36,520
course repo but I'm actually going to

8030
05:37:34,280 --> 05:37:38,558
take some steps back here and I'm going

8031
05:37:36,520 --> 05:37:42,920
to clone it into my CA directory here so

8032
05:37:38,558 --> 05:37:42,920
I'm just going to delete the old version

8033
05:37:42,958 --> 05:37:47,840
um and we're going to get clone the

8034
05:37:45,360 --> 05:37:50,878
other repo in so it's just that

8035
05:37:47,840 --> 05:37:53,558
literally just uh pop in here copy copy

8036
05:37:50,878 --> 05:37:56,480
paste this in um go Ahad and get clone

8037
05:37:53,558 --> 05:37:59,958
that uh and then we're going to go ahead

8038
05:37:56,480 --> 05:38:02,558
and open this up and open this in vs

8039
05:37:59,958 --> 05:38:05,600
code so I'm going to drag that to my

8040
05:38:02,558 --> 05:38:10,000
side monitor here going to close

8041
05:38:05,600 --> 05:38:13,240
this and open this one up

8042
05:38:10,000 --> 05:38:15,920
again now inside of here we're going to

8043
05:38:13,240 --> 05:38:18,040
look at the read me first for setup um

8044
05:38:15,920 --> 05:38:20,840
so we can see that it uh in in the in

8045
05:38:18,040 --> 05:38:22,240
the build instructions we have to do um

8046
05:38:20,840 --> 05:38:28,280
we have to

8047
05:38:22,240 --> 05:38:31,280
do make directory build um CD into

8048
05:38:28,280 --> 05:38:31,280
it

8049
05:38:31,320 --> 05:38:37,558
oh uh and then we're going to

8050
05:38:34,600 --> 05:38:41,280
cake it's going to take take a second

8051
05:38:37,558 --> 05:38:45,040
there and then we go D- build period and

8052
05:38:41,280 --> 05:38:48,200
it's going to build everything for us um

8053
05:38:45,040 --> 05:38:49,798
now the idea here is to uh build

8054
05:38:48,200 --> 05:38:53,920
everything show all the different

8055
05:38:49,798 --> 05:38:57,000
benchmarks uh after after we actually uh

8056
05:38:53,920 --> 05:39:01,280
go through uh each optimization so we're

8057
05:38:57,000 --> 05:39:03,040
going to essentially print out the kuas

8058
05:39:01,280 --> 05:39:04,920
performance and then we're going to

8059
05:39:03,040 --> 05:39:06,478
print out naive and then we're going to

8060
05:39:04,920 --> 05:39:07,638
print out the next optimization and then

8061
05:39:06,478 --> 05:39:09,718
we're going to print the next one X One

8062
05:39:07,638 --> 05:39:11,040
XX until we get to the end then we're

8063
05:39:09,718 --> 05:39:13,600
going to compare all of them and see

8064
05:39:11,040 --> 05:39:17,160
which one is the fastest

8065
05:39:13,600 --> 05:39:18,280
so we can go ahead and start off with

8066
05:39:17,160 --> 05:39:21,680
just going

8067
05:39:18,280 --> 05:39:24,040
into just going into here so have a

8068
05:39:21,680 --> 05:39:26,600
bunch of different kernels we have naive

8069
05:39:24,040 --> 05:39:29,120
we have a global global memory colest so

8070
05:39:26,600 --> 05:39:30,920
if we actually go back to the uh blog

8071
05:39:29,120 --> 05:39:32,400
post which is what you know I kind of

8072
05:39:30,920 --> 05:39:34,798
expect you guys to follow there's not

8073
05:39:32,400 --> 05:39:38,478
too much here it's mainly just the code

8074
05:39:34,798 --> 05:39:40,320
so if we go back to the blog post um

8075
05:39:38,478 --> 05:39:41,840
we're doing we're we're we're

8076
05:39:40,320 --> 05:39:43,440
essentially just going in this order and

8077
05:39:41,840 --> 05:39:45,000
he does it in this order too so the

8078
05:39:43,440 --> 05:39:48,080
night of implantation I mean we've

8079
05:39:45,000 --> 05:39:50,840
already done this right um I can

8080
05:39:48,080 --> 05:39:52,680
actually go back and if I like make this

8081
05:39:50,840 --> 05:39:56,040
full

8082
05:39:52,680 --> 05:39:59,920
screen and I go out of this

8083
05:39:56,040 --> 05:40:05,958
um and I go into the Cuda course writing

8084
05:39:59,920 --> 05:40:05,958
your first kernels and then matm notice

8085
05:40:08,160 --> 05:40:11,280
I'm going to copy this for a

8086
05:40:14,400 --> 05:40:20,040
second and

8087
05:40:16,798 --> 05:40:23,200
then we're just going to paste it uh

8088
05:40:20,040 --> 05:40:23,200
inside of here just for

8089
05:40:24,958 --> 05:40:29,080
reference so we have these two kernels

8090
05:40:28,040 --> 05:40:30,440
and I just want to and I just want to

8091
05:40:29,080 --> 05:40:31,958
make sure that we're all like caught up

8092
05:40:30,440 --> 05:40:33,878
here the one I taught you before is the

8093
05:40:31,958 --> 05:40:39,400
exact same one that we're showing here

8094
05:40:33,878 --> 05:40:41,680
so this takes in um a b and C this one

8095
05:40:39,400 --> 05:40:43,280
takes an A B and C it it takes an alpha

8096
05:40:41,680 --> 05:40:44,638
and a beta as well I mean it's doing

8097
05:40:43,280 --> 05:40:46,840
it's doing a different it's slightly

8098
05:40:44,638 --> 05:40:48,878
different operation so it's not exactly

8099
05:40:46,840 --> 05:40:51,600
matrix multiplication but it's it's

8100
05:40:48,878 --> 05:40:53,920
essentially doing this uh this

8101
05:40:51,600 --> 05:40:55,080
essentially the the CU loss map that I

8102
05:40:53,920 --> 05:40:57,958
showed you before where it does the

8103
05:40:55,080 --> 05:41:01,080
alpha term uh like times every single

8104
05:40:57,958 --> 05:41:03,638
element in in the in the Matrix that we

8105
05:41:01,080 --> 05:41:09,240
calculate in the actual matl output and

8106
05:41:03,638 --> 05:41:12,160
then it adds um beta Plus plus a c

8107
05:41:09,240 --> 05:41:14,120
Matrix and then assign C to that new one

8108
05:41:12,160 --> 05:41:16,120
so it's a slightly different it's a

8109
05:41:14,120 --> 05:41:17,798
slightly different operation that we do

8110
05:41:16,120 --> 05:41:19,680
but we're mainly going to worry about

8111
05:41:17,798 --> 05:41:23,040
the matrix multiplication mechanics that

8112
05:41:19,680 --> 05:41:25,920
get get us to this temp variable so when

8113
05:41:23,040 --> 05:41:29,200
we look at this we have M K and N so

8114
05:41:25,920 --> 05:41:30,920
it's a matrix of shape so a is shape M

8115
05:41:29,200 --> 05:41:32,360
so it's like M vertical that's like the

8116
05:41:30,920 --> 05:41:35,000
batch size you could say and then it's

8117
05:41:32,360 --> 05:41:37,718
like K as like the length number of

8118
05:41:35,000 --> 05:41:41,400
columns right and then the B Matrix is

8119
05:41:37,718 --> 05:41:44,400
going to be K vertically and N long

8120
05:41:41,400 --> 05:41:45,920
right so we we essentially pass it in

8121
05:41:44,400 --> 05:41:48,280
the same way and we index them the same

8122
05:41:45,920 --> 05:41:51,080
way but in this example in in uh in

8123
05:41:48,280 --> 05:41:53,320
Simon's example we just we just pass

8124
05:41:51,080 --> 05:41:55,360
these in differently so it's like m and

8125
05:41:53,320 --> 05:41:56,680
n as like the edges and then K is the

8126
05:41:55,360 --> 05:41:58,558
middle one that we want to pay attention

8127
05:41:56,680 --> 05:42:00,240
to so we save that for the third one I

8128
05:41:58,558 --> 05:42:02,638
guess maybe that's that's thought the

8129
05:42:00,240 --> 05:42:05,840
thought process there but anyways we

8130
05:42:02,638 --> 05:42:09,120
have the we have X and Y so row and

8131
05:42:05,840 --> 05:42:12,440
column um the Y is the same as the row

8132
05:42:09,120 --> 05:42:14,878
here so uh which which y index is it at

8133
05:42:12,440 --> 05:42:16,200
because a row like row is vertical it

8134
05:42:14,878 --> 05:42:18,440
could be this row this row this it's

8135
05:42:16,200 --> 05:42:22,360
like a vertical scale right and then the

8136
05:42:18,440 --> 05:42:24,760
x is uh is column so which which column

8137
05:42:22,360 --> 05:42:27,240
is it at this is this is

8138
05:42:24,760 --> 05:42:29,000
horizontal and so we make sure it's not

8139
05:42:27,240 --> 05:42:30,440
out of bounds and then we continue with

8140
05:42:29,000 --> 05:42:32,718
what's inside of that little chunk of

8141
05:42:30,440 --> 05:42:35,558
memory and and we we proceed right so we

8142
05:42:32,718 --> 05:42:37,718
have this accumulator sum uh and then we

8143
05:42:35,558 --> 05:42:40,798
have this L term

8144
05:42:37,718 --> 05:42:43,080
uh we we could say that's the length and

8145
05:42:40,798 --> 05:42:45,360
uh and then we have this K which is

8146
05:42:43,080 --> 05:42:49,280
which is the length of a and then the

8147
05:42:45,360 --> 05:42:50,958
height of uh the the height of B right

8148
05:42:49,280 --> 05:42:54,558
so when you're do producting you're

8149
05:42:50,958 --> 05:42:57,360
you're taking the uh the row uh the a

8150
05:42:54,558 --> 05:42:59,680
row of a and a column of B so you're

8151
05:42:57,360 --> 05:43:02,160
iterating over you're iterating over k

8152
05:42:59,680 --> 05:43:04,040
and a and you're iterating over K and in

8153
05:43:02,160 --> 05:43:06,440
B right so that that's where that that

8154
05:43:04,040 --> 05:43:07,638
comes from that K stuff and so you're

8155
05:43:06,440 --> 05:43:09,920
just iterating

8156
05:43:07,638 --> 05:43:11,558
uh like in a row you're going through a

8157
05:43:09,920 --> 05:43:14,400
like this you're going through each

8158
05:43:11,558 --> 05:43:15,718
piece like each number and then in B

8159
05:43:14,400 --> 05:43:18,160
you're going through each number

8160
05:43:15,718 --> 05:43:20,760
vertically right and then we just plus

8161
05:43:18,160 --> 05:43:23,958
plus each time and then this sum output

8162
05:43:20,760 --> 05:43:27,600
this accumulator uh is just going to be

8163
05:43:23,958 --> 05:43:29,600
essentially the so a it we're looking to

8164
05:43:27,600 --> 05:43:32,040
uh multiply the first the first element

8165
05:43:29,600 --> 05:43:33,558
here by the first element here and then

8166
05:43:32,040 --> 05:43:35,000
and then advance and then advance and

8167
05:43:33,558 --> 05:43:37,478
then advance and then Advance right

8168
05:43:35,000 --> 05:43:39,680
that's what we're trying to do

8169
05:43:37,478 --> 05:43:41,080
you could also think of it as like a a

8170
05:43:39,680 --> 05:43:44,520
nice way I like to visualize this is

8171
05:43:41,080 --> 05:43:47,478
like a a 2X two tile so you have you

8172
05:43:44,520 --> 05:43:49,840
have like a 2X two tile you

8173
05:43:47,478 --> 05:43:53,200
have I'm try to visualize this from your

8174
05:43:49,840 --> 05:43:56,400
perspective you have like a up here and

8175
05:43:53,200 --> 05:44:01,000
then you have B down here and it's like

8176
05:43:56,400 --> 05:44:03,638
is that is that correct maybe it's maybe

8177
05:44:01,000 --> 05:44:06,240
it's yeah it's a here and then B here

8178
05:44:03,638 --> 05:44:10,080
and so when a has like a row that row is

8179
05:44:06,240 --> 05:44:12,320
going to like point to uh like the the

8180
05:44:10,080 --> 05:44:14,638
uh the y-coordinate in C and B is going

8181
05:44:12,320 --> 05:44:17,080
to point to the x coordinate in C so

8182
05:44:14,638 --> 05:44:19,200
when they like when they like intersect

8183
05:44:17,080 --> 05:44:20,600
it's going to find the the index in C

8184
05:44:19,200 --> 05:44:22,360
that you're going to calculate that dot

8185
05:44:20,600 --> 05:44:26,600
product result from that's a cool way I

8186
05:44:22,360 --> 05:44:28,878
like to visualize it for um but anyways

8187
05:44:26,600 --> 05:44:32,360
getting back to the point uh so we have

8188
05:44:28,878 --> 05:44:34,520
this row times uh times K so like which

8189
05:44:32,360 --> 05:44:37,320
row are we at the length of the row

8190
05:44:34,520 --> 05:44:39,600
right or sorry which which row are you

8191
05:44:37,320 --> 05:44:42,360
at so which row relative to you know the

8192
05:44:39,600 --> 05:44:43,478
the Cuda architecture itself and then K

8193
05:44:42,360 --> 05:44:45,320
which is the length of it so you're

8194
05:44:43,478 --> 05:44:47,360
going down you're essentially like

8195
05:44:45,320 --> 05:44:48,878
wrapping you're striding around and

8196
05:44:47,360 --> 05:44:51,240
you're doing this as many times as you

8197
05:44:48,878 --> 05:44:54,440
want to as like depending on which row

8198
05:44:51,240 --> 05:44:56,000
you're at and then you add the the K

8199
05:44:54,440 --> 05:44:58,320
offset to it so you might not be all the

8200
05:44:56,000 --> 05:45:00,440
way through the the length of it and so

8201
05:44:58,320 --> 05:45:01,520
you stop and that's where that plus

8202
05:45:00,440 --> 05:45:04,240
comes

8203
05:45:01,520 --> 05:45:08,000
from and then same thing for here we

8204
05:45:04,240 --> 05:45:09,638
have this L term which is K so how um

8205
05:45:08,000 --> 05:45:11,478
essentially which column are you at

8206
05:45:09,638 --> 05:45:15,520
right so here we have rows and then here

8207
05:45:11,478 --> 05:45:17,840
we have columns so it's uh L which is

8208
05:45:15,520 --> 05:45:20,240
essentially the the length of of that

8209
05:45:17,840 --> 05:45:23,718
vertical the length of a column and then

8210
05:45:20,240 --> 05:45:27,000
you iterate over uh you iterate Over N

8211
05:45:23,718 --> 05:45:28,600
times right so n is the n is the length

8212
05:45:27,000 --> 05:45:31,520
there and

8213
05:45:28,600 --> 05:45:34,478
so you essentially you essentially

8214
05:45:31,520 --> 05:45:36,280
Advance as many times you need to to the

8215
05:45:34,478 --> 05:45:38,558
to the I guess you could say to the

8216
05:45:36,280 --> 05:45:40,600
right and then you offset at whichever

8217
05:45:38,558 --> 05:45:44,000
column index it is so so same idea we're

8218
05:45:40,600 --> 05:45:47,320
just we're just advancing uh instead of

8219
05:45:44,000 --> 05:45:49,638
instead of going rows we're going

8220
05:45:47,320 --> 05:45:52,440
columns right and then we just

8221
05:45:49,638 --> 05:45:56,000
essentially assign uh

8222
05:45:52,440 --> 05:45:59,638
whichever in C we we index it like we go

8223
05:45:56,000 --> 05:46:02,400
uh row time uh time n so it's an it's an

8224
05:45:59,638 --> 05:46:08,240
M it's

8225
05:46:02,400 --> 05:46:10,718
an it's an M by n so it's going to be

8226
05:46:08,240 --> 05:46:12,680
like m is here and N is here so it's

8227
05:46:10,718 --> 05:46:15,760
going to go

8228
05:46:12,680 --> 05:46:18,478
uh row number times n so it's going to

8229
05:46:15,760 --> 05:46:20,200
stride n every time it wraps and then

8230
05:46:18,478 --> 05:46:23,440
it's going to do plus the column index

8231
05:46:20,200 --> 05:46:24,798
which is that X component right and

8232
05:46:23,440 --> 05:46:26,040
that's how you do the naive again just

8233
05:46:24,798 --> 05:46:28,718
just to give you a little refresher

8234
05:46:26,040 --> 05:46:31,080
there that was a while back we did it um

8235
05:46:28,718 --> 05:46:33,878
and then the same idea here so you

8236
05:46:31,080 --> 05:46:36,440
have m&n

8237
05:46:33,878 --> 05:46:38,120
m&n you have the accumulator we have

8238
05:46:36,440 --> 05:46:39,360
this I term that we're iterating through

8239
05:46:38,120 --> 05:46:42,200
and we have

8240
05:46:39,360 --> 05:46:46,360
K uh and then we go we have this temp

8241
05:46:42,200 --> 05:46:51,558
term we go X so X is the same as row so

8242
05:46:46,360 --> 05:46:53,958
notice how row is assigned to Y and X is

8243
05:46:51,558 --> 05:46:57,600
um well actually got a little bit stuck

8244
05:46:53,958 --> 05:47:00,400
there I was looking at the uh the block

8245
05:46:57,600 --> 05:47:02,440
and thread uh indexing scheme here and

8246
05:47:00,400 --> 05:47:04,760
it was kind of misleading so like notice

8247
05:47:02,440 --> 05:47:06,920
here how we have rows and those are by

8248
05:47:04,760 --> 05:47:08,718
the the Y index so whichever y position

8249
05:47:06,920 --> 05:47:10,718
I it's at that's like the row that it's

8250
05:47:08,718 --> 05:47:12,638
going to pluck out or the the column

8251
05:47:10,718 --> 05:47:15,718
it's like X so it's going to pluck out a

8252
05:47:12,638 --> 05:47:19,638
row or sorry a column in this in this

8253
05:47:15,718 --> 05:47:24,520
example we do um like X so that refers

8254
05:47:19,638 --> 05:47:25,840
to like right here um is is picking out

8255
05:47:24,520 --> 05:47:27,760
like it it kind of makes sense right

8256
05:47:25,840 --> 05:47:31,638
like X matches up with X and Y matches

8257
05:47:27,760 --> 05:47:33,520
up with Y but when we look in here um

8258
05:47:31,638 --> 05:47:36,280
like in comparison to this it's like the

8259
05:47:33,520 --> 05:47:39,478
row times the stride of K so we're going

8260
05:47:36,280 --> 05:47:40,878
to stride the the K length over and then

8261
05:47:39,478 --> 05:47:42,080
then come back to the next one and then

8262
05:47:40,878 --> 05:47:45,840
offset with

8263
05:47:42,080 --> 05:47:48,400
L in this one we have X which is like a

8264
05:47:45,840 --> 05:47:50,280
column Index right so it's like what why

8265
05:47:48,400 --> 05:47:53,120
would you do that we want to we want a

8266
05:47:50,280 --> 05:47:55,000
row index but this actually works and we

8267
05:47:53,120 --> 05:47:58,320
don't have to worry too much because

8268
05:47:55,000 --> 05:48:01,520
this is a square Matrix so because these

8269
05:47:58,320 --> 05:48:04,440
values are actually the same because uh

8270
05:48:01,520 --> 05:48:07,320
the grid and the and the thread index in

8271
05:48:04,440 --> 05:48:08,558
both the the X and Y dimensions are equ

8272
05:48:07,320 --> 05:48:10,120
we don't actually have to worry about

8273
05:48:08,558 --> 05:48:12,760
that so this is something youd want to

8274
05:48:10,120 --> 05:48:14,000
pay attention to in rectangular matrices

8275
05:48:12,760 --> 05:48:16,600
but we don't have to worry about this

8276
05:48:14,000 --> 05:48:19,120
right now so just kind of assume that uh

8277
05:48:16,600 --> 05:48:21,040
we can kind of just say that this is uh

8278
05:48:19,120 --> 05:48:22,600
like why and treat it that way but I'm

8279
05:48:21,040 --> 05:48:24,920
not going to edit this because we might

8280
05:48:22,600 --> 05:48:28,160
have to deal with this later on in uh in

8281
05:48:24,920 --> 05:48:30,240
future kernels so you kind of get the

8282
05:48:28,160 --> 05:48:33,558
idea though this is very similar to what

8283
05:48:30,240 --> 05:48:34,798
we were doing before um don't pretty

8284
05:48:33,558 --> 05:48:37,600
much just don't worry about the indexing

8285
05:48:34,798 --> 05:48:40,478
scheme it's it's going to be fine

8286
05:48:37,600 --> 05:48:43,878
um and then we yeah literally the only

8287
05:48:40,478 --> 05:48:47,240
change here is that we write out uh

8288
05:48:43,878 --> 05:48:49,958
using Alpha Beta and C right so that's

8289
05:48:47,240 --> 05:48:52,160
really the only difference there so

8290
05:48:49,958 --> 05:48:57,000
let's go ahead and actually run this

8291
05:48:52,160 --> 05:48:58,400
now so we pop into uh SJ Cuda and then

8292
05:48:57,000 --> 05:49:02,360
we go into

8293
05:48:58,400 --> 05:49:04,878
build now we can go uh sjem and then we

8294
05:49:02,360 --> 05:49:08,200
go number one so this is the naive

8295
05:49:04,878 --> 05:49:10,920
kernel that we run and we can see that

8296
05:49:08,200 --> 05:49:14,080
we're going to do a Max size so

8297
05:49:10,920 --> 05:49:16,200
Dimensions M = Nal K right so these

8298
05:49:14,080 --> 05:49:20,080
are okay I might have lagged there for a

8299
05:49:16,200 --> 05:49:22,520
second but yeah I mean as we can see um

8300
05:49:20,080 --> 05:49:24,360
Dimensions m equal nals K right so these

8301
05:49:22,520 --> 05:49:26,840
are all the same it's just like 128

8302
05:49:24,360 --> 05:49:27,760
essentially and then 256 512 1024 all

8303
05:49:26,840 --> 05:49:29,638
the way up to

8304
05:49:27,760 --> 05:49:31,958
496 and we can actually see the

8305
05:49:29,638 --> 05:49:35,200
throughput and G A flops per second so

8306
05:49:31,958 --> 05:49:39,080
what this means is how many billion Giga

8307
05:49:35,200 --> 05:49:42,040
right Giga is uh * 10 9 so billion and

8308
05:49:39,080 --> 05:49:43,240
then flops is floating Point operations

8309
05:49:42,040 --> 05:49:45,878
per

8310
05:49:43,240 --> 05:49:50,000
second and this is on a given size right

8311
05:49:45,878 --> 05:49:52,400
so on size 128 we get on average 46.2

8312
05:49:50,000 --> 05:49:56,520
gig uh billion floating Point operations

8313
05:49:52,400 --> 05:49:58,160
per second and on 496 we get about 166

8314
05:49:56,520 --> 05:49:59,958
billion floating Point operations per

8315
05:49:58,160 --> 05:50:01,920
second which sounds like a lot that

8316
05:49:59,958 --> 05:50:04,798
sounds like a lot of operations right

8317
05:50:01,920 --> 05:50:07,280
166 billion per second wow that that's

8318
05:50:04,798 --> 05:50:08,878
really high but the answer is that's

8319
05:50:07,280 --> 05:50:11,558
actually not that high it's going to get

8320
05:50:08,878 --> 05:50:13,280
a lot higher than this so high actually

8321
05:50:11,558 --> 05:50:14,958
that it it's going to seem like this is

8322
05:50:13,280 --> 05:50:18,120
minus kill this is going to be seem very

8323
05:50:14,958 --> 05:50:22,280
very small and actually slow so notice

8324
05:50:18,120 --> 05:50:26,878
how this took um this took uh eight

8325
05:50:22,280 --> 05:50:31,600
about is 83 seconds to do 50 runs right

8326
05:50:26,878 --> 05:50:35,000
so very um very slow or no per per run

8327
05:50:31,600 --> 05:50:37,240
sorry so not for 50 runs but for for

8328
05:50:35,000 --> 05:50:40,520
each run that it did which there were 50

8329
05:50:37,240 --> 05:50:43,958
of it took about 83 seconds to do that

8330
05:50:40,520 --> 05:50:45,280
on 496 with a naive kernel so a few

8331
05:50:43,958 --> 05:50:48,920
other points before we actually jump

8332
05:50:45,280 --> 05:50:51,240
into this um I want to First Look at the

8333
05:50:48,920 --> 05:50:54,400
uh I want to First Look at the blog post

8334
05:50:51,240 --> 05:50:56,638
here so when we're calculating the

8335
05:50:54,400 --> 05:50:58,718
output in the KN implementation I mean

8336
05:50:56,638 --> 05:51:01,080
like even just like looking at this uh

8337
05:50:58,718 --> 05:51:02,958
it's like really intuitive uh I I love

8338
05:51:01,080 --> 05:51:06,000
this example but

8339
05:51:02,958 --> 05:51:10,280
anyways uh when we actually look at the

8340
05:51:06,000 --> 05:51:11,400
the simple naive kernel um essentially

8341
05:51:10,280 --> 05:51:13,600
we're trying

8342
05:51:11,400 --> 05:51:16,040
to

8343
05:51:13,600 --> 05:51:19,878
uh we're trying to

8344
05:51:16,040 --> 05:51:23,558
find uh a certain part inside

8345
05:51:19,878 --> 05:51:24,878
of we are trying to find a certain index

8346
05:51:23,558 --> 05:51:27,798
inside of C that is going to be the

8347
05:51:24,878 --> 05:51:30,320
output so we're saying we want to do the

8348
05:51:27,798 --> 05:51:34,080
fastest possible calculation to get say

8349
05:51:30,320 --> 05:51:37,400
this number uh this index calculated in

8350
05:51:34,080 --> 05:51:40,600
the C output right in C

8351
05:51:37,400 --> 05:51:43,520
uh and so right now the way to do that

8352
05:51:40,600 --> 05:51:45,280
is to load in a row and a column and

8353
05:51:43,520 --> 05:51:47,040
then just just calculate that that's

8354
05:51:45,280 --> 05:51:48,958
that's what we found out naively so far

8355
05:51:47,040 --> 05:51:51,080
and it takes very few lines of code to

8356
05:51:48,958 --> 05:51:52,798
do that and so we just kind of iterate

8357
05:51:51,080 --> 05:51:54,320
through like I was saying before how you

8358
05:51:52,798 --> 05:51:56,360
put a on the side here and you let it

8359
05:51:54,320 --> 05:51:57,680
you let them sort of act as coordinates

8360
05:51:56,360 --> 05:52:03,000
that's that's what I was referring to

8361
05:51:57,680 --> 05:52:04,360
there um but anyways uh that's that's

8362
05:52:03,000 --> 05:52:05,600
kind of one of the one of the goals we

8363
05:52:04,360 --> 05:52:07,160
want to keep in mind is how do we

8364
05:52:05,600 --> 05:52:09,200
calculate the output in index it'll help

8365
05:52:07,160 --> 05:52:10,878
you it'll help provide some context on

8366
05:52:09,200 --> 05:52:13,000
how we actually get there cuz when we

8367
05:52:10,878 --> 05:52:14,920
deal with more complex kernels you'll

8368
05:52:13,000 --> 05:52:16,840
actually see there's a lot of steps to

8369
05:52:14,920 --> 05:52:18,718
actually get to a certain place and so

8370
05:52:16,840 --> 05:52:20,520
it helps when you're able to keep in a

8371
05:52:18,718 --> 05:52:22,080
consistent uh frame of thought where

8372
05:52:20,520 --> 05:52:23,360
it's like okay how are we actually

8373
05:52:22,080 --> 05:52:25,280
ending up at this result and then you

8374
05:52:23,360 --> 05:52:28,320
can sort of backtrack through and see

8375
05:52:25,280 --> 05:52:29,920
what's happening um so instead of just

8376
05:52:28,320 --> 05:52:31,558
like going and like reading like a novel

8377
05:52:29,920 --> 05:52:33,320
from from the start of the konal to the

8378
05:52:31,558 --> 05:52:34,760
end and just seeing like oh I guess like

8379
05:52:33,320 --> 05:52:36,160
we we'll see what we stumble into it's

8380
05:52:34,760 --> 05:52:37,798
like you actually want to see what

8381
05:52:36,160 --> 05:52:39,840
you're trying to calculate in the end

8382
05:52:37,798 --> 05:52:42,240
and that helps and and this blog post

8383
05:52:39,840 --> 05:52:43,798
refat a lot of context on that so

8384
05:52:42,240 --> 05:52:45,400
another little note I wanted to add is

8385
05:52:43,798 --> 05:52:47,680
like don't worry about 3D structures too

8386
05:52:45,400 --> 05:52:50,718
much so when we have the like the dim 3

8387
05:52:47,680 --> 05:52:52,360
type and we have the the YX and Zed uh

8388
05:52:50,718 --> 05:52:54,680
Dimensions all populated with numbers

8389
05:52:52,360 --> 05:52:56,798
greater than one it's like four 2 and

8390
05:52:54,680 --> 05:52:58,400
three it's like don't worry about that

8391
05:52:56,798 --> 05:52:59,638
we're not going to be dealing with 3D

8392
05:52:58,400 --> 05:53:01,840
stuff it's not going to be that

8393
05:52:59,638 --> 05:53:03,878
complicated it's going to be more so

8394
05:53:01,840 --> 05:53:07,400
like how do you transform uh

8395
05:53:03,878 --> 05:53:12,360
onedimensional and two-dimensional um

8396
05:53:07,400 --> 05:53:13,600
um dimensions and index efficiently so

8397
05:53:12,360 --> 05:53:17,760
don't worry about 3D stuff we're not

8398
05:53:13,600 --> 05:53:22,040
going to do any of that um and then the

8399
05:53:17,760 --> 05:53:25,760
actual uh indexing scheme here is giving

8400
05:53:22,040 --> 05:53:26,878
us coest memory access so I'll jump into

8401
05:53:25,760 --> 05:53:28,558
this in the next current a little bit

8402
05:53:26,878 --> 05:53:31,798
more in depth but essentially what's

8403
05:53:28,558 --> 05:53:34,680
happening is uh when we when we're doing

8404
05:53:31,798 --> 05:53:36,558
this uh like row calculation for example

8405
05:53:34,680 --> 05:53:39,520
um what it's like when it goes through

8406
05:53:36,558 --> 05:53:41,520
through um each row essentially what's

8407
05:53:39,520 --> 05:53:43,840
happening is uh we have this we have

8408
05:53:41,520 --> 05:53:47,240
this like X term from here and we put

8409
05:53:43,840 --> 05:53:49,000
this x term in all of the like in Cuda

8410
05:53:47,240 --> 05:53:50,760
when you have adjacent meaning like in

8411
05:53:49,000 --> 05:53:53,000
the X dimension in like in like the

8412
05:53:50,760 --> 05:53:55,878
length part horizontal when they are

8413
05:53:53,000 --> 05:54:00,080
next to each other you actually get CEST

8414
05:53:55,878 --> 05:54:04,280
memory accesses so when you're accessing

8415
05:54:00,080 --> 05:54:05,840
um a you actually you can actually uh in

8416
05:54:04,280 --> 05:54:08,280
in assembly it's actually going to group

8417
05:54:05,840 --> 05:54:11,080
multiple of the together into one so

8418
05:54:08,280 --> 05:54:13,520
when you have like a when you have a a

8419
05:54:11,080 --> 05:54:17,440
block inside of that you have a a warp

8420
05:54:13,520 --> 05:54:22,280
and inside of the warps you have um you

8421
05:54:17,440 --> 05:54:25,200
have uh 32 threads per warp right and so

8422
05:54:22,280 --> 05:54:28,120
we can like in inside of the actual warp

8423
05:54:25,200 --> 05:54:30,400
itself it's going to uh it's going to

8424
05:54:28,120 --> 05:54:32,200
call us memory access if possible and

8425
05:54:30,400 --> 05:54:34,280
when things are adjacent it actually

8426
05:54:32,200 --> 05:54:35,478
makes that possible so that's kind of

8427
05:54:34,280 --> 05:54:37,520
why we're seeing the weird indexing

8428
05:54:35,478 --> 05:54:39,558
scheme here again doesn't work for

8429
05:54:37,520 --> 05:54:41,878
rectangular matrices but in this case

8430
05:54:39,558 --> 05:54:45,680
it's kind of a an efficiency Improvement

8431
05:54:41,878 --> 05:54:47,760
for for indexing um the a matrix so

8432
05:54:45,680 --> 05:54:49,400
before we pop over to the global memory

8433
05:54:47,760 --> 05:54:52,240
colas kernel I thought I should probably

8434
05:54:49,400 --> 05:54:53,558
highlight something so this is important

8435
05:54:52,240 --> 05:54:55,440
to know for all kernels and even the

8436
05:54:53,558 --> 05:54:58,080
previous ones too but this is just kind

8437
05:54:55,440 --> 05:55:01,718
of like how memory is laid out right so

8438
05:54:58,080 --> 05:55:04,080
when we have a 2 X2 Matrix 1 2 34 um in

8439
05:55:01,718 --> 05:55:07,160
memory this is going to be laid out as

8440
05:55:04,080 --> 05:55:10,360
literally just a vector 1 2 3 4

8441
05:55:07,160 --> 05:55:11,760
so when we want to like for example in a

8442
05:55:10,360 --> 05:55:14,000
when we want to go to the next

8443
05:55:11,760 --> 05:55:16,840
essentially the next row and then do an

8444
05:55:14,000 --> 05:55:21,840
offset what we're doing is we're going a

8445
05:55:16,840 --> 05:55:23,878
current row times K which is this

8446
05:55:21,840 --> 05:55:26,798
Dimension here the the the horizontal

8447
05:55:23,878 --> 05:55:28,798
one so we're doing let's just say we

8448
05:55:26,798 --> 05:55:30,520
want to get to the number four here

8449
05:55:28,798 --> 05:55:33,160
right so if we want to get to number

8450
05:55:30,520 --> 05:55:35,040
four it's going to be current row well

8451
05:55:33,160 --> 05:55:37,520
the current row is going to be zero and

8452
05:55:35,040 --> 05:55:40,280
one right so current is going to be one

8453
05:55:37,520 --> 05:55:44,400
and then K is twoo long so it's going to

8454
05:55:40,280 --> 05:55:46,878
be uh 1 * 2 which is going to give us

8455
05:55:44,400 --> 05:55:49,958
this index so uh like the array at index

8456
05:55:46,878 --> 05:55:53,040
two is this so it's like 0 1 and two and

8457
05:55:49,958 --> 05:55:54,958
then the offset from that is going to be

8458
05:55:53,040 --> 05:55:57,920
the the the column index that's going to

8459
05:55:54,958 --> 05:56:00,000
be one so it's going to be uh 2 + 1 and

8460
05:55:57,920 --> 05:56:01,400
it's going to give us array at index 3

8461
05:56:00,000 --> 05:56:04,000
so that's just kind of what I mean by

8462
05:56:01,400 --> 05:56:05,680
strides it's like how how you go and you

8463
05:56:04,000 --> 05:56:08,280
like jump across the whole row that's

8464
05:56:05,680 --> 05:56:11,200
kind of what I mean there but going into

8465
05:56:08,280 --> 05:56:13,080
this uh glob memory Cass kernel scroll

8466
05:56:11,200 --> 05:56:15,520
down

8467
05:56:13,080 --> 05:56:17,200
and uh and I'm just going to I'm kind of

8468
05:56:15,520 --> 05:56:20,718
going to like skip this part and and

8469
05:56:17,200 --> 05:56:22,760
just lay it out for you but the whole

8470
05:56:20,718 --> 05:56:26,120
idea here and this is the critical

8471
05:56:22,760 --> 05:56:28,160
concept so the Matrix memory layout as I

8472
05:56:26,120 --> 05:56:29,878
just highlighted it's going to be it's

8473
05:56:28,160 --> 05:56:30,840
going to be consecutive memory like this

8474
05:56:29,878 --> 05:56:33,760
what going to

8475
05:56:30,840 --> 05:56:34,920
be it's going to be laid out like that

8476
05:56:33,760 --> 05:56:36,798
um and this is not going to be

8477
05:56:34,920 --> 05:56:39,400
consecutive memory right

8478
05:56:36,798 --> 05:56:42,638
so when we do do product of this and

8479
05:56:39,400 --> 05:56:43,958
this it's going to go to the third uh

8480
05:56:42,638 --> 05:56:45,558
third row and this is going to go to the

8481
05:56:43,958 --> 05:56:48,400
third column and we get this value right

8482
05:56:45,558 --> 05:56:51,040
this is a consecutive this is not in the

8483
05:56:48,400 --> 05:56:53,920
naive kernel we jump through these and

8484
05:56:51,040 --> 05:56:56,040
we iterate through uh we iterate through

8485
05:56:53,920 --> 05:56:57,160
a so we start off this like B column and

8486
05:56:56,040 --> 05:57:01,440
then we

8487
05:56:57,160 --> 05:57:01,440
go and then we we go to the next

8488
05:57:02,280 --> 05:57:07,718
one right we we we index in that fashion

8489
05:57:06,200 --> 05:57:10,000
we advance through the arrays in that

8490
05:57:07,718 --> 05:57:12,840
fashion and what we end up with in the

8491
05:57:10,000 --> 05:57:14,878
output is we get this like we get this

8492
05:57:12,840 --> 05:57:16,040
stack of blocks right and this and this

8493
05:57:14,878 --> 05:57:17,920
is what it looks like when we write to

8494
05:57:16,040 --> 05:57:21,400
the output it's going to be a stack of

8495
05:57:17,920 --> 05:57:24,080
blocks because we write um vertically as

8496
05:57:21,400 --> 05:57:28,000
as the rows advance in a that's what we

8497
05:57:24,080 --> 05:57:30,718
prioritize however if we uh

8498
05:57:28,000 --> 05:57:34,080
instead cess the memory

8499
05:57:30,718 --> 05:57:37,878
accesses uh we can get we can get these

8500
05:57:34,080 --> 05:57:40,160
laid out in this fashion so essentially

8501
05:57:37,878 --> 05:57:43,600
what we're doing is we're changing the

8502
05:57:40,160 --> 05:57:46,240
indexing scheme here um all of this

8503
05:57:43,600 --> 05:57:48,558
essentially Remains the Same except we

8504
05:57:46,240 --> 05:57:51,120
change the way that this is indexed and

8505
05:57:48,558 --> 05:57:53,958
we ensure that we're using thread idx

8506
05:57:51,120 --> 05:57:56,760
right so remember when we did uh when I

8507
05:57:53,958 --> 05:57:59,718
I was previously talking about how um

8508
05:57:56,760 --> 05:58:03,080
all of the essentially all of the all of

8509
05:57:59,718 --> 05:58:05,320
the the X like the thread idxx component

8510
05:58:03,080 --> 05:58:07,680
those are grouped together in a warp so

8511
05:58:05,320 --> 05:58:10,400
if you have like for example block size

8512
05:58:07,680 --> 05:58:13,840
32 um there's going to be 32 threads in

8513
05:58:10,400 --> 05:58:16,320
a warp that that's the maximum and so in

8514
05:58:13,840 --> 05:58:19,478
a in like blocks if you have this like

8515
05:58:16,320 --> 05:58:22,320
this this Square Block it's like 32 by

8516
05:58:19,478 --> 05:58:23,718
32 and you get the X Dimension you're

8517
05:58:22,320 --> 05:58:25,440
going to get the maximum number of

8518
05:58:23,718 --> 05:58:27,040
threads in a single warp if each element

8519
05:58:25,440 --> 05:58:29,920
there is dedicated to a different thread

8520
05:58:27,040 --> 05:58:33,040
right and so this way you're maximizing

8521
05:58:29,920 --> 05:58:35,080
the memory accesses because you can put

8522
05:58:33,040 --> 05:58:37,120
all of those together and you can make

8523
05:58:35,080 --> 05:58:39,520
that load uh you can make that data

8524
05:58:37,120 --> 05:58:41,520
transfer operation much more efficient

8525
05:58:39,520 --> 05:58:44,080
when you let an entire warp take care of

8526
05:58:41,520 --> 05:58:45,958
it it can do all the values at once or

8527
05:58:44,080 --> 05:58:48,798
or or group them together and make them

8528
05:58:45,958 --> 05:58:51,680
like way faster as opposed to going

8529
05:58:48,798 --> 05:58:54,558
through each individual uh each

8530
05:58:51,680 --> 05:58:58,080
individual uh like y component right so

8531
05:58:54,558 --> 05:59:00,360
when it goes like thread idx doy it's

8532
05:58:58,080 --> 05:59:03,440
actually it's actually not as efficient

8533
05:59:00,360 --> 05:59:06,240
right um and so we kind of just Chang

8534
05:59:03,440 --> 05:59:08,160
the indexing scheme here with these to

8535
05:59:06,240 --> 05:59:10,878
sort of illustrate the previous point I

8536
05:59:08,160 --> 05:59:12,920
wrote a little table for uh like what's

8537
05:59:10,878 --> 05:59:14,558
in the brackets here so this division of

8538
05:59:12,920 --> 05:59:17,520
thread idx and block size then the

8539
05:59:14,558 --> 05:59:19,520
modulo of or the modulus of that um so I

8540
05:59:17,520 --> 05:59:20,760
just kind of wrote a table here of what

8541
05:59:19,520 --> 05:59:23,240
these would actually look like in

8542
05:59:20,760 --> 05:59:24,718
practicality so if we just assume block

8543
05:59:23,240 --> 05:59:26,718
size is four which mean it's not in this

8544
05:59:24,718 --> 05:59:28,320
case but we can just simplify and

8545
05:59:26,718 --> 05:59:29,558
understand what's going on that way I

8546
05:59:28,320 --> 05:59:32,440
don't have to write out like a bunch of

8547
05:59:29,558 --> 05:59:34,360
numbers uh we assume block size is four

8548
05:59:32,440 --> 05:59:36,240
and so because block the because block

8549
05:59:34,360 --> 05:59:39,920
size that the size of an individual

8550
05:59:36,240 --> 05:59:42,280
block is four that means the thread idx

8551
05:59:39,920 --> 05:59:44,280
is going to have four indices uh in it

8552
05:59:42,280 --> 05:59:46,400
so it's going to be thread idx Z and

8553
05:59:44,280 --> 05:59:49,360
then one two and three it's going to

8554
05:59:46,400 --> 05:59:51,638
have four inside of it right so when we

8555
05:59:49,360 --> 05:59:53,520
divide we're going to floor the

8556
05:59:51,638 --> 05:59:54,798
operation uh that that's just what's

8557
05:59:53,520 --> 05:59:56,798
going to happen naturally is this is

8558
05:59:54,798 --> 05:59:58,440
going to get floored it's going to it's

8559
05:59:56,798 --> 06:00:00,680
going to truncate the end of it because

8560
05:59:58,440 --> 06:00:06,080
we're doing integer

8561
06:00:00,680 --> 06:00:09,200
Division and we're going to get 0 0 0 0

8562
06:00:06,080 --> 06:00:11,320
right 3 / 4 is 75 it's going to trunk it

8563
06:00:09,200 --> 06:00:14,160
75 and you're still going to have zero

8564
06:00:11,320 --> 06:00:16,878
um and then we jump up to when like this

8565
06:00:14,160 --> 06:00:18,798
advances then it's going to be uh well

8566
06:00:16,878 --> 06:00:21,200
the block size is four and when the idx

8567
06:00:18,798 --> 06:00:23,600
jumps to one then it's going to it's

8568
06:00:21,200 --> 06:00:26,160
going to be 1 Time 4 is four and then

8569
06:00:23,600 --> 06:00:28,600
plus Z and it wrap it kind of just like

8570
06:00:26,160 --> 06:00:30,400
resets right except it's plus one so we

8571
06:00:28,600 --> 06:00:33,840
have that going on then we have the

8572
06:00:30,400 --> 06:00:36,478
modulus as well so modulus is uh you

8573
06:00:33,840 --> 06:00:38,040
divide so 0 divided by four

8574
06:00:36,478 --> 06:00:41,520
um like integer and then what's the

8575
06:00:38,040 --> 06:00:42,760
remainder of that so if we do 1 / 4 um

8576
06:00:41,520 --> 06:00:44,160
that doesn't actually equal a whole

8577
06:00:42,760 --> 06:00:45,718
number so you end up with a remainder of

8578
06:00:44,160 --> 06:00:49,200
one and then you do that for the rest of

8579
06:00:45,718 --> 06:00:51,558
them so like 3id 4 is or three mod mod

8580
06:00:49,200 --> 06:00:53,320
four is three and then four mod four

8581
06:00:51,558 --> 06:00:54,798
since it just equals one there's no

8582
06:00:53,320 --> 06:00:56,320
remainder left so which just is zero

8583
06:00:54,798 --> 06:01:02,200
right and you get this thing where it's

8584
06:00:56,320 --> 06:01:06,680
like 0 0 uh 0 0000 0 1111 one and then

8585
06:01:02,200 --> 06:01:11,200
here it's like 0 1 2 3 0 1 2 3 right so

8586
06:01:06,680 --> 06:01:14,760
when we actually look at um this example

8587
06:01:11,200 --> 06:01:17,440
here inside of

8588
06:01:14,760 --> 06:01:21,958
the where is it no not this one inside

8589
06:01:17,440 --> 06:01:23,958
of the Coles kernel um notice how in I'm

8590
06:01:21,958 --> 06:01:27,080
going to show you a second in our code

8591
06:01:23,958 --> 06:01:29,240
how this row does not actually change

8592
06:01:27,080 --> 06:01:31,878
and what we're doing is we're just

8593
06:01:29,240 --> 06:01:33,600
indexing very carefully these values so

8594
06:01:31,878 --> 06:01:35,320
when we're when we have different

8595
06:01:33,600 --> 06:01:37,760
threads that are like because each

8596
06:01:35,320 --> 06:01:41,400
thread is going to calculate its own uh

8597
06:01:37,760 --> 06:01:43,360
dot product right um like this thread

8598
06:01:41,400 --> 06:01:46,080
and this thread adjacent to each other

8599
06:01:43,360 --> 06:01:48,478
in the same warp they're going to access

8600
06:01:46,080 --> 06:01:50,120
um adjacent values so when they're

8601
06:01:48,478 --> 06:01:52,040
accessing adjacent values in the same

8602
06:01:50,120 --> 06:01:54,878
warp you can actually group all these

8603
06:01:52,040 --> 06:01:56,760
together whereas instead if you just uh

8604
06:01:54,878 --> 06:01:58,400
did this one if you did this thread and

8605
06:01:56,760 --> 06:02:00,440
then this thread and then this thread

8606
06:01:58,400 --> 06:02:03,558
that means that the first index of all

8607
06:02:00,440 --> 06:02:05,520
those uh all those threads um you you

8608
06:02:03,558 --> 06:02:07,798
you cannot actually you cannot actually

8609
06:02:05,520 --> 06:02:08,920
col that because you have to do like a

8610
06:02:07,798 --> 06:02:12,440
stride and they're not they're not

8611
06:02:08,920 --> 06:02:14,040
adjacent right so that's essentially

8612
06:02:12,440 --> 06:02:15,760
what we're doing here um and then we end

8613
06:02:14,040 --> 06:02:17,638
up with this like instead of a stacking

8614
06:02:15,760 --> 06:02:21,040
like blocks we end up with this with

8615
06:02:17,638 --> 06:02:23,600
this horizontal layout um so when we go

8616
06:02:21,040 --> 06:02:25,958
here we can see that c row so this is

8617
06:02:23,600 --> 06:02:28,478
only actually going to change every

8618
06:02:25,958 --> 06:02:30,320
every um every time we advance right so

8619
06:02:28,478 --> 06:02:32,440
this is going to stay at zero which

8620
06:02:30,320 --> 06:02:34,760
means that c row that's going to stay at

8621
06:02:32,440 --> 06:02:36,478
zero and then the plus I part that's

8622
06:02:34,760 --> 06:02:40,600
going to advance with with the dot

8623
06:02:36,478 --> 06:02:42,040
product itself um and then here I is

8624
06:02:40,600 --> 06:02:44,558
automatically going to advance so that

8625
06:02:42,040 --> 06:02:47,280
means it's going to uh it's it's going

8626
06:02:44,558 --> 06:02:49,000
to advance a column each time while the

8627
06:02:47,280 --> 06:02:52,680
row is going to stay at the same place

8628
06:02:49,000 --> 06:02:53,760
because c row is staying at zero right

8629
06:02:52,680 --> 06:02:56,400
and so you can kind of see how this

8630
06:02:53,760 --> 06:02:58,520
works out we have we have C column or or

8631
06:02:56,400 --> 06:03:00,280
or current column and this is actually

8632
06:02:58,520 --> 06:03:02,600
going to change over the threads so it's

8633
06:03:00,280 --> 06:03:04,638
going to go zero it's going to go 0 1 2

8634
06:03:02,600 --> 06:03:07,040
3 4 and then it's going to jump to the

8635
06:03:04,638 --> 06:03:09,558
next block block idea right and so what

8636
06:03:07,040 --> 06:03:14,558
you end up with is literally what I just

8637
06:03:09,558 --> 06:03:16,920
demonstrated um you end up by

8638
06:03:14,558 --> 06:03:18,920
essentially each each thread within that

8639
06:03:16,920 --> 06:03:21,120
warp is is accessing an adjacent value

8640
06:03:18,920 --> 06:03:24,320
and so you can group those and and cess

8641
06:03:21,120 --> 06:03:26,080
or combine the memory aises together and

8642
06:03:24,320 --> 06:03:28,240
we get more performance efficiency with

8643
06:03:26,080 --> 06:03:30,920
this so that's kind of like what this

8644
06:03:28,240 --> 06:03:32,360
article that this section talked about

8645
06:03:30,920 --> 06:03:35,920
um and if

8646
06:03:32,360 --> 06:03:39,680
we if we bump back bump back to here and

8647
06:03:35,920 --> 06:03:42,760
actually run this so kernel number

8648
06:03:39,680 --> 06:03:46,280
two we can see um we're actually getting

8649
06:03:42,760 --> 06:03:50,240
a lot higher Giga flops on this

8650
06:03:46,280 --> 06:03:53,200
one uh and then we can see that we get

8651
06:03:50,240 --> 06:03:55,240
about 1183 gig of flops here so that's

8652
06:03:53,200 --> 06:03:57,878
actually a pretty big increase of

8653
06:03:55,240 --> 06:04:01,160
performance I think previously it was in

8654
06:03:57,878 --> 06:04:03,280
the it was about 10 and uh what was it

8655
06:04:01,160 --> 06:04:05,920
like 180 or something or

8656
06:04:03,280 --> 06:04:07,520
like 160 I can't remember but it was

8657
06:04:05,920 --> 06:04:09,558
very low so this is actually like

8658
06:04:07,520 --> 06:04:12,080
significantly it's like 10 almost 10x

8659
06:04:09,558 --> 06:04:14,718
higher this is like maybe 5 8 10x higher

8660
06:04:12,080 --> 06:04:16,920
than what we had before on the 496

8661
06:04:14,718 --> 06:04:18,638
Square Matrix right so that's actually a

8662
06:04:16,920 --> 06:04:23,920
crazy performance Improvement we were

8663
06:04:18,638 --> 06:04:26,120
previously at like 83 seconds uh per uh

8664
06:04:23,920 --> 06:04:30,000
per

8665
06:04:26,120 --> 06:04:32,600
um per run and now this is at point

8666
06:04:30,000 --> 06:04:38,280
point about2 so if you if you actually

8667
06:04:32,600 --> 06:04:40,120
do the math there um 83 over uh 12

8668
06:04:38,280 --> 06:04:42,760
that's about a 7x increase in

8669
06:04:40,120 --> 06:04:47,680
performance in throughput so uh that's

8670
06:04:42,760 --> 06:04:50,320
that's pretty good um now we can uh now

8671
06:04:47,680 --> 06:04:52,440
we can move on to Shared memory cache

8672
06:04:50,320 --> 06:04:55,160
blocking which introduces a different

8673
06:04:52,440 --> 06:04:57,240
concept still uses what we've currently

8674
06:04:55,160 --> 06:04:59,280
done but introduces a whole another

8675
06:04:57,240 --> 06:05:01,400
Paradigm that's going to uh really help

8676
06:04:59,280 --> 06:05:03,080
accelerate and speed things up so next

8677
06:05:01,400 --> 06:05:05,878
we jump into something called shared

8678
06:05:03,080 --> 06:05:07,400
memory or SRAM and this is abs abolutely

8679
06:05:05,878 --> 06:05:10,478
critical to take care of when we're

8680
06:05:07,400 --> 06:05:12,680
optimizing algorithms for performance so

8681
06:05:10,478 --> 06:05:18,240
let me just kind of explain what the

8682
06:05:12,680 --> 06:05:22,160
deals with this so right now we're using

8683
06:05:18,240 --> 06:05:25,200
uh Global memory right uh the the host

8684
06:05:22,160 --> 06:05:26,798
is just our little Ram slots uh going to

8685
06:05:25,200 --> 06:05:28,680
the CPU and that's like really slow

8686
06:05:26,798 --> 06:05:33,200
that's about 5 gigabytes per second

8687
06:05:28,680 --> 06:05:35,798
still fast but very slow compared to um

8688
06:05:33,200 --> 06:05:39,320
this 200 GB per second that we get with

8689
06:05:35,798 --> 06:05:41,360
our vram this is what we're using now or

8690
06:05:39,320 --> 06:05:43,920
you can get even faster and use shared

8691
06:05:41,360 --> 06:05:46,558
memory which is around 1.5 tab per

8692
06:05:43,920 --> 06:05:48,200
second of memory bandwidth or registers

8693
06:05:46,558 --> 06:05:49,840
which is about 8 terabytes per second of

8694
06:05:48,200 --> 06:05:52,400
memory bandwidth we're just going to

8695
06:05:49,840 --> 06:05:53,718
focus on registers right now or or sorry

8696
06:05:52,400 --> 06:05:59,478
shared memory right

8697
06:05:53,718 --> 06:06:01,840
now um now in this blog post he had uh

8698
06:05:59,478 --> 06:06:03,718
about 700 gigabyt of glove memory band

8699
06:06:01,840 --> 06:06:06,160
which is really fast compared to this

8700
06:06:03,718 --> 06:06:08,798
and then about uh

8701
06:06:06,160 --> 06:06:11,040
12 terabytes of or 12.1 terabytes of

8702
06:06:08,798 --> 06:06:14,320
shared memory bandwidth

8703
06:06:11,040 --> 06:06:18,280
so uh or sorry not 12 terabytes 1 Point

8704
06:06:14,320 --> 06:06:19,958
1 Point 1.2 terabytes memory bandwidth

8705
06:06:18,280 --> 06:06:21,680
um terabytes per

8706
06:06:19,958 --> 06:06:24,160
second

8707
06:06:21,680 --> 06:06:26,878
now how do we capitalize on that how do

8708
06:06:24,160 --> 06:06:29,478
we actually use shared memory well it's

8709
06:06:26,878 --> 06:06:33,958
actually easy

8710
06:06:29,478 --> 06:06:36,478
um you use this little keyword called uh

8711
06:06:33,958 --> 06:06:39,840
it's not here but

8712
06:06:36,478 --> 06:06:42,840
I have it it's called Uh shared so

8713
06:06:39,840 --> 06:06:45,160
shared memory is literally just how you

8714
06:06:42,840 --> 06:06:47,760
use the that Shar that little L1 cache

8715
06:06:45,160 --> 06:06:50,680
so when we look up at the actual

8716
06:06:47,760 --> 06:06:54,638
architecture of this um I could open

8717
06:06:50,680 --> 06:06:56,718
image and new tab so you have your

8718
06:06:54,638 --> 06:06:58,400
Global memory so like the the big chunk

8719
06:06:56,718 --> 06:07:01,240
of memory that you have that's about two

8720
06:06:58,400 --> 06:07:03,400
200 gigas 200 gigb a second then the L2

8721
06:07:01,240 --> 06:07:06,320
cache for like a transfer medium and

8722
06:07:03,400 --> 06:07:09,400
then each little SM or streaming

8723
06:07:06,320 --> 06:07:12,558
multiprocessor um these have their own

8724
06:07:09,400 --> 06:07:14,478
little L1 cache or the shared memory and

8725
06:07:12,558 --> 06:07:16,600
this is very small compared to these two

8726
06:07:14,478 --> 06:07:18,840
right um but they are extremely fast and

8727
06:07:16,600 --> 06:07:20,520
they connect directly with registers and

8728
06:07:18,840 --> 06:07:23,520
the and the cores on your

8729
06:07:20,520 --> 06:07:25,080
GPU so when we can utilize these there's

8730
06:07:23,520 --> 06:07:26,840
actually far less travel distance you

8731
06:07:25,080 --> 06:07:28,638
have to go so instead of like every time

8732
06:07:26,840 --> 06:07:32,718
you need to access a float you go all

8733
06:07:28,638 --> 06:07:34,958
the way through um like s m or shared

8734
06:07:32,718 --> 06:07:37,240
and then to L2 and then to Global you

8735
06:07:34,958 --> 06:07:39,520
literally just just uh store a bunch of

8736
06:07:37,240 --> 06:07:41,920
them temporarily in s and in shared

8737
06:07:39,520 --> 06:07:45,000
memory have all of the threads use them

8738
06:07:41,920 --> 06:07:47,558
for like like essentially do a ton of

8739
06:07:45,000 --> 06:07:49,638
work with the memory that it has and

8740
06:07:47,558 --> 06:07:52,920
then once you're finished with that you

8741
06:07:49,638 --> 06:07:56,400
can replace it uh you can you can write

8742
06:07:52,920 --> 06:07:58,000
new values from Global so instead of uh

8743
06:07:56,400 --> 06:08:01,920
writing from Global every time you need

8744
06:07:58,000 --> 06:08:03,718
to access something you instead load a

8745
06:08:01,920 --> 06:08:05,878
you preemptively load a bunch into

8746
06:08:03,718 --> 06:08:08,280
shared memory and then you use them for

8747
06:08:05,878 --> 06:08:09,798
a bunch of work and then you then you

8748
06:08:08,280 --> 06:08:11,040
replace them with a new one once you

8749
06:08:09,798 --> 06:08:14,760
advance

8750
06:08:11,040 --> 06:08:17,718
right and sort of the goal here is just

8751
06:08:14,760 --> 06:08:20,718
making sure that we do this properly so

8752
06:08:17,718 --> 06:08:20,718
if I just go out

8753
06:08:21,558 --> 06:08:26,120
again shared memory is located on chip

8754
06:08:24,200 --> 06:08:30,200
much lower latency higher higher memory

8755
06:08:26,120 --> 06:08:32,400
bandwidth um heed this on a voltage GPU

8756
06:08:30,200 --> 06:08:33,840
so to go over how exactly we'll be using

8757
06:08:32,400 --> 06:08:35,558
shared memory it's actually a bit of a

8758
06:08:33,840 --> 06:08:37,240
different philosophy now I'm not not

8759
06:08:35,558 --> 06:08:39,798
going to like do like write this out and

8760
06:08:37,240 --> 06:08:42,600
everything because it can get like quite

8761
06:08:39,798 --> 06:08:44,920
uh it can get quite intensive when we

8762
06:08:42,600 --> 06:08:46,920
write stuff out

8763
06:08:44,920 --> 06:08:48,600
but this is essentially what we're doing

8764
06:08:46,920 --> 06:08:50,958
we're doing a little thing called tiling

8765
06:08:48,600 --> 06:08:54,240
which I demonstrated earlier where you

8766
06:08:50,958 --> 06:08:56,478
have uh little tiles that you do Matrix

8767
06:08:54,240 --> 06:08:58,120
multiplies for in a in a bigger Matrix

8768
06:08:56,478 --> 06:09:00,840
multiply

8769
06:08:58,120 --> 06:09:04,558
so instead of doing instead of doing

8770
06:09:00,840 --> 06:09:06,040
rows and columns we actually go and do

8771
06:09:04,558 --> 06:09:08,400
something a bit different and instead

8772
06:09:06,040 --> 06:09:10,878
what that is is is say we have this this

8773
06:09:08,400 --> 06:09:13,360
Chunk in C here and and so what you

8774
06:09:10,878 --> 06:09:14,958
would do is you would essentially like

8775
06:09:13,360 --> 06:09:17,878
if you have

8776
06:09:14,958 --> 06:09:21,478
um say

8777
06:09:17,878 --> 06:09:25,440
a let me try to use the current vs code

8778
06:09:21,478 --> 06:09:29,080
thing as an example so we have this uh

8779
06:09:25,440 --> 06:09:31,360
we have a we have a a c right now going

8780
06:09:29,080 --> 06:09:31,360
back

8781
06:09:31,520 --> 06:09:36,000
to going back to this one I'm going to

8782
06:09:33,958 --> 06:09:39,120
try to exp this is this have to explain

8783
06:09:36,000 --> 06:09:41,520
so bear with me um we're essentially

8784
06:09:39,120 --> 06:09:43,520
going to load in tiles and we have this

8785
06:09:41,520 --> 06:09:44,600
little C like the coordinates here and

8786
06:09:43,520 --> 06:09:48,520
all we're going to do is we're just

8787
06:09:44,600 --> 06:09:50,718
going to multiply multiply these two um

8788
06:09:48,520 --> 06:09:52,840
together and then multiply these two

8789
06:09:50,718 --> 06:09:54,400
together and multiply these two together

8790
06:09:52,840 --> 06:09:58,040
and then like we just essentially

8791
06:09:54,400 --> 06:10:01,240
multiply every Matrix um and then that

8792
06:09:58,040 --> 06:10:04,440
that that like coordinates up to uh this

8793
06:10:01,240 --> 06:10:06,360
this final thing here so we go we go

8794
06:10:04,440 --> 06:10:08,440
like through this way way and we sort of

8795
06:10:06,360 --> 06:10:12,400
we start at the very like the the very

8796
06:10:08,440 --> 06:10:14,478
top of B and the Very left of of a and

8797
06:10:12,400 --> 06:10:16,958
we multiply those together and then we

8798
06:10:14,478 --> 06:10:18,760
add it to the next ones we go in and in

8799
06:10:16,958 --> 06:10:20,878
and in and in and in until we like maybe

8800
06:10:18,760 --> 06:10:24,080
Cross or something and then that like

8801
06:10:20,878 --> 06:10:27,680
intersection Point

8802
06:10:24,080 --> 06:10:30,200
um that intersection point right here

8803
06:10:27,680 --> 06:10:32,478
that's where C is right and so when when

8804
06:10:30,200 --> 06:10:34,080
you have a bunch of these smaller um

8805
06:10:32,478 --> 06:10:36,000
less intensive map moles that can be

8806
06:10:34,080 --> 06:10:38,000
actually done on blocks

8807
06:10:36,000 --> 06:10:39,920
uh on on thread blocks then it actually

8808
06:10:38,000 --> 06:10:42,000
makes the job a lot easier because what

8809
06:10:39,920 --> 06:10:43,440
you can do is you can be smart about it

8810
06:10:42,000 --> 06:10:45,718
and actually store these blocks in

8811
06:10:43,440 --> 06:10:47,440
shared memory right uh if you're doing

8812
06:10:45,718 --> 06:10:49,440
individual rows or columns I mean sure

8813
06:10:47,440 --> 06:10:51,718
you could do that but it actually allows

8814
06:10:49,440 --> 06:10:54,760
us to uh distribute some of the work

8815
06:10:51,718 --> 06:10:57,798
more when we're when we're using blocks

8816
06:10:54,760 --> 06:11:01,558
when we're using literal tiles of the

8817
06:10:57,798 --> 06:11:03,920
Matrix right uh so let's go ahead and

8818
06:11:01,558 --> 06:11:05,440
dig into how this actually works under

8819
06:11:03,920 --> 06:11:07,360
the hood you're going to understand ort

8820
06:11:05,440 --> 06:11:09,320
of how tiling Works more once I explain

8821
06:11:07,360 --> 06:11:13,520
it and how I explain how everything

8822
06:11:09,320 --> 06:11:16,440
advances as as we dig more into detail

8823
06:11:13,520 --> 06:11:18,400
but uh this this is the idea here we

8824
06:11:16,440 --> 06:11:19,878
just we tile and we we we store these

8825
06:11:18,400 --> 06:11:22,200
blocks temporarily in shared memory and

8826
06:11:19,878 --> 06:11:25,040
do as much work with them as possible

8827
06:11:22,200 --> 06:11:28,958
okay so this is the code for a shared uh

8828
06:11:25,040 --> 06:11:32,520
a shared memory cach or or tiled mapal

8829
06:11:28,958 --> 06:11:34,280
you could say and uh pretty much a lot

8830
06:11:32,520 --> 06:11:36,478
of it uh well not a lot of it but the

8831
06:11:34,280 --> 06:11:40,120
start is pretty close to or actually the

8832
06:11:36,478 --> 06:11:43,680
exact same as as the last C we wrote so

8833
06:11:40,120 --> 06:11:47,200
we have this uh c row maps to block idx

8834
06:11:43,680 --> 06:11:52,200
dox c row maps to block idx dox and C

8835
06:11:47,200 --> 06:11:54,798
column isy C column isy now we have this

8836
06:11:52,200 --> 06:11:58,320
thread column uh

8837
06:11:54,798 --> 06:12:01,878
is uh maps to the mod operator and then

8838
06:11:58,320 --> 06:12:05,558
row maps to division operator right so

8839
06:12:01,878 --> 06:12:09,000
row maps to division and column maps to

8840
06:12:05,558 --> 06:12:12,280
uh mod right so these are these are

8841
06:12:09,000 --> 06:12:14,240
the the we essentially use the the same

8842
06:12:12,280 --> 06:12:16,478
idea as before and then we add this

8843
06:12:14,240 --> 06:12:18,718
additional piece in which is going to

8844
06:12:16,478 --> 06:12:20,558
allocate uh some some space in the

8845
06:12:18,718 --> 06:12:24,160
shared memory which is going to be of

8846
06:12:20,558 --> 06:12:26,040
size uh block size by block size right

8847
06:12:24,160 --> 06:12:27,680
so it's just giant thing you could say

8848
06:12:26,040 --> 06:12:29,520
that like each of these little rows just

8849
06:12:27,680 --> 06:12:32,200
like wraps around and you have this like

8850
06:12:29,520 --> 06:12:33,638
super long thing laid out in memory but

8851
06:12:32,200 --> 06:12:36,798
we're going to treat it as an actual

8852
06:12:33,638 --> 06:12:39,600
block like a square um and so why we use

8853
06:12:36,798 --> 06:12:41,080
block size by block size is because um

8854
06:12:39,600 --> 06:12:42,718
if we just say I mean we're going to

8855
06:12:41,080 --> 06:12:45,080
lower we're going to lower what what we

8856
06:12:42,718 --> 06:12:47,280
interpret block size to be in in the

8857
06:12:45,080 --> 06:12:49,958
examples just for intuition purposes but

8858
06:12:47,280 --> 06:12:52,320
in practicality this would be 32 right

8859
06:12:49,958 --> 06:12:55,638
block size will be 32 you have 32

8860
06:12:52,320 --> 06:12:58,440
threads that fit in the warp and maximum

8861
06:12:55,638 --> 06:13:00,718
1,24 threads per block so if you

8862
06:12:58,440 --> 06:13:00,718
actually

8863
06:13:00,958 --> 06:13:08,280
divide um if you divide

8864
06:13:04,120 --> 06:13:11,080
1,24 by 32 you get 32 so what we end up

8865
06:13:08,280 --> 06:13:12,478
doing is we have a warp a warp takes

8866
06:13:11,080 --> 06:13:14,040
care of these warp takes care of these

8867
06:13:12,478 --> 06:13:15,320
warp takes care of these and we have

8868
06:13:14,040 --> 06:13:17,440
like we're literally taking up the

8869
06:13:15,320 --> 06:13:21,920
maximum amount everywhere uh just by

8870
06:13:17,440 --> 06:13:23,840
using block size uh or a shared a shared

8871
06:13:21,920 --> 06:13:26,878
allocation of block size by by block

8872
06:13:23,840 --> 06:13:29,558
size right that that's the idea there um

8873
06:13:26,878 --> 06:13:33,920
and so we go down and I'm just going to

8874
06:13:29,558 --> 06:13:35,878
pull this up on the side here just for

8875
06:13:33,920 --> 06:13:37,840
reference um

8876
06:13:35,878 --> 06:13:41,360
so in

8877
06:13:37,840 --> 06:13:42,878
a what we do is we say uh we're going to

8878
06:13:41,360 --> 06:13:45,280
advance the pointers to the starting

8879
06:13:42,878 --> 06:13:49,120
positions right

8880
06:13:45,280 --> 06:13:53,400
so essentially we're going to

8881
06:13:49,120 --> 06:13:57,840
multiply uh c row we're going to do c

8882
06:13:53,400 --> 06:14:02,240
the the the current row times uh times

8883
06:13:57,840 --> 06:14:04,080
oh can zoom out the current row times K

8884
06:14:02,240 --> 06:14:06,680
right so K is going to be this Dimension

8885
06:14:04,080 --> 06:14:09,478
here this this this long one the the

8886
06:14:06,680 --> 06:14:11,320
sort the horizontal one and so if we

8887
06:14:09,478 --> 06:14:14,840
multiply the current row by K let's just

8888
06:14:11,320 --> 06:14:19,160
say we have like a current row

8889
06:14:14,840 --> 06:14:21,520
of uh if we have a current row of if we

8890
06:14:19,160 --> 06:14:23,920
want to do current row of one right so

8891
06:14:21,520 --> 06:14:25,558
we want to do current row time K which

8892
06:14:23,920 --> 06:14:27,760
is this length so it's going to jump

8893
06:14:25,558 --> 06:14:29,520
down to this one and then we want to do

8894
06:14:27,760 --> 06:14:32,040
times block size which in this case

8895
06:14:29,520 --> 06:14:33,680
since we split it into a bunch of tiles

8896
06:14:32,040 --> 06:14:35,840
is going to be two right it's going to

8897
06:14:33,680 --> 06:14:37,798
be 2 by two

8898
06:14:35,840 --> 06:14:40,440
uh and so we end up jumping two instead

8899
06:14:37,798 --> 06:14:41,920
of that so we go um the current row is

8900
06:14:40,440 --> 06:14:45,280
going to be one so we're going to do

8901
06:14:41,920 --> 06:14:46,680
this this one here so it's going to jump

8902
06:14:45,280 --> 06:14:49,040
uh the length of

8903
06:14:46,680 --> 06:14:51,000
that times the number we want to do

8904
06:14:49,040 --> 06:14:52,680
which is essentially one so we don't

8905
06:14:51,000 --> 06:14:54,200
jump down one and then that doubles

8906
06:14:52,680 --> 06:14:55,958
because we have block size equal to two

8907
06:14:54,200 --> 06:14:57,798
right so it's going downum two rows and

8908
06:14:55,958 --> 06:15:01,360
then we end up exactly where we want we

8909
06:14:57,798 --> 06:15:03,440
want to start at the first number um on

8910
06:15:01,360 --> 06:15:05,760
the first on the first uh like

8911
06:15:03,440 --> 06:15:09,638
essentially tile of this row

8912
06:15:05,760 --> 06:15:12,760
right uh and then B we we advance that

8913
06:15:09,638 --> 06:15:14,920
pointer to uh the current column times

8914
06:15:12,760 --> 06:15:18,120
block size so in this case b let's say

8915
06:15:14,920 --> 06:15:23,520
we want B to be uh like two right so the

8916
06:15:18,120 --> 06:15:28,320
current column is uh is is 2 so we do 2

8917
06:15:23,520 --> 06:15:32,000
* 2 which is four so we go 0 1 2 3 4 and

8918
06:15:28,320 --> 06:15:34,280
we end up there right so very intuitive

8919
06:15:32,000 --> 06:15:37,120
uh and then we have C which essentially

8920
06:15:34,280 --> 06:15:38,680
combines the to so uh in this case we

8921
06:15:37,120 --> 06:15:43,440
want to do c so it's going to it's going

8922
06:15:38,680 --> 06:15:45,000
to be um like it's going to be this row

8923
06:15:43,440 --> 06:15:48,840
and this column here so we're going to

8924
06:15:45,000 --> 06:15:51,000
end up it's going to be um the the first

8925
06:15:48,840 --> 06:15:53,520
row and then and then this column so

8926
06:15:51,000 --> 06:15:55,400
it's going to be this tile uh that we

8927
06:15:53,520 --> 06:15:58,680
want to take care of so it's going to

8928
06:15:55,400 --> 06:16:00,638
jump it's going to jump over to uh this

8929
06:15:58,680 --> 06:16:05,080
one from

8930
06:16:00,638 --> 06:16:06,840
the uh from the uh what's it called from

8931
06:16:05,080 --> 06:16:10,160
from the a matrix we're going to jump

8932
06:16:06,840 --> 06:16:12,200
all the way to or what was it no no no

8933
06:16:10,160 --> 06:16:14,798
in B we're going to jump all the way

8934
06:16:12,200 --> 06:16:17,400
here and then in a uh we're we're

8935
06:16:14,798 --> 06:16:19,798
essentially just going to add the we're

8936
06:16:17,400 --> 06:16:24,240
going to add the offset right so we want

8937
06:16:19,798 --> 06:16:26,280
to we want to jump down to because this

8938
06:16:24,240 --> 06:16:31,240
is the end Dimension so instead of

8939
06:16:26,280 --> 06:16:31,240
multiplying by K we would do

8940
06:16:32,200 --> 06:16:38,160
uh we would do uh we would do times n so

8941
06:16:35,400 --> 06:16:40,638
it ends up being like essentially this

8942
06:16:38,160 --> 06:16:43,360
we essentially this plus this right

8943
06:16:40,638 --> 06:16:44,878
except instead of uh instead of using K

8944
06:16:43,360 --> 06:16:46,440
we use n because that's that's the

8945
06:16:44,878 --> 06:16:49,840
length of C right so that that's the

8946
06:16:46,440 --> 06:16:54,000
idea there um and then we end up at uh I

8947
06:16:49,840 --> 06:16:57,200
believe it was this this totle 24 25 34

8948
06:16:54,000 --> 06:16:58,718
35 um so we continue to go down and this

8949
06:16:57,200 --> 06:17:01,840
is where we Define our accumulator right

8950
06:16:58,718 --> 06:17:03,840
so the temporary accumulator we have uh

8951
06:17:01,840 --> 06:17:05,680
and then stuff really gets interesting

8952
06:17:03,840 --> 06:17:07,840
once we once we get inside of this for

8953
06:17:05,680 --> 06:17:11,120
Loop this is where the magic happens um

8954
06:17:07,840 --> 06:17:13,160
so we have this term uh block idx and

8955
06:17:11,120 --> 06:17:17,040
we're going to iterate over K right so K

8956
06:17:13,160 --> 06:17:18,798
is that K is that uh the row the the

8957
06:17:17,040 --> 06:17:21,320
sorry the column The Columns number of

8958
06:17:18,798 --> 06:17:23,000
columns in a and the number of rows in B

8959
06:17:21,320 --> 06:17:25,718
right so we're going to advance block

8960
06:17:23,000 --> 06:17:28,360
size each time uh or sorry going to

8961
06:17:25,718 --> 06:17:32,360
advance advance block idx by block size

8962
06:17:28,360 --> 06:17:35,680
each time um and so inside of here

8963
06:17:32,360 --> 06:17:37,958
initially we want to store uh the stuff

8964
06:17:35,680 --> 06:17:40,638
in SRAM right or or shared memory so we

8965
06:17:37,958 --> 06:17:44,638
want to store it in here and literally

8966
06:17:40,638 --> 06:17:47,520
all we do is we look at uh the index

8967
06:17:44,638 --> 06:17:50,478
inside of here so thread row which row

8968
06:17:47,520 --> 06:17:51,840
is it times the block size so block size

8969
06:17:50,478 --> 06:17:53,680
is going to be that stride or that

8970
06:17:51,840 --> 06:17:55,280
wrapper and then plus the thread column

8971
06:17:53,680 --> 06:17:56,520
so which offset do we want to be at

8972
06:17:55,280 --> 06:18:01,760
right it's going to pick out a certain

8973
06:17:56,520 --> 06:18:03,760
spot in there um and luckily enough uh I

8974
06:18:01,760 --> 06:18:06,680
picked a block size of two so it's going

8975
06:18:03,760 --> 06:18:08,360
to be 2 by two and that actually makes

8976
06:18:06,680 --> 06:18:09,798
our job a lot more simpler to understand

8977
06:18:08,360 --> 06:18:12,520
I mean you can abstract it up to like

8978
06:18:09,798 --> 06:18:14,878
four or 8 or even 32 but we're going to

8979
06:18:12,520 --> 06:18:16,958
stick with block size of two for now and

8980
06:18:14,878 --> 06:18:19,680
this means we're just going to have um

8981
06:18:16,958 --> 06:18:21,680
we're just going to have two a thread

8982
06:18:19,680 --> 06:18:26,200
thread index thread

8983
06:18:21,680 --> 06:18:28,920
idxx of zero and one right so very very

8984
06:18:26,200 --> 06:18:31,000
basic threads to work with here and so

8985
06:18:28,920 --> 06:18:32,920
we're essentially just going to load uh

8986
06:18:31,000 --> 06:18:35,320
into this this shared memory which we

8987
06:18:32,920 --> 06:18:38,080
defined up here uh and we're just going

8988
06:18:35,320 --> 06:18:40,798
to essentially that that little spot

8989
06:18:38,080 --> 06:18:43,718
inside of it we're going to we're going

8990
06:18:40,798 --> 06:18:46,680
to pick that out from A and B so in a

8991
06:18:43,718 --> 06:18:48,760
it's going to be the thread row times K

8992
06:18:46,680 --> 06:18:51,558
so K is going to be that that that

8993
06:18:48,760 --> 06:18:54,878
length right

8994
06:18:51,558 --> 06:18:57,600
uh and then it's going to be

8995
06:18:54,878 --> 06:19:00,440
uh plus the plus the thread column

8996
06:18:57,600 --> 06:19:02,240
offset right so just that that ex

8997
06:19:00,440 --> 06:19:05,760
essentially the the same idea as what

8998
06:19:02,240 --> 06:19:09,200
we're doing here um

8999
06:19:05,760 --> 06:19:13,760
and then we're going to have the uh same

9000
06:19:09,200 --> 06:19:16,080
idea for B which is going to be n so n

9001
06:19:13,760 --> 06:19:20,120
is that again n is

9002
06:19:16,080 --> 06:19:22,760
the n is the top one here um and then K

9003
06:19:20,120 --> 06:19:26,958
is the top one here so K corresponds to

9004
06:19:22,760 --> 06:19:28,240
A and N corresponds to B right um I hope

9005
06:19:26,958 --> 06:19:30,760
that kind of makes sense and then

9006
06:19:28,240 --> 06:19:32,520
afterwards we just sync up everything so

9007
06:19:30,760 --> 06:19:34,360
this part's a little weird because we're

9008
06:19:32,520 --> 06:19:36,478
doing like sync threads but this kernel

9009
06:19:34,360 --> 06:19:39,200
itself like everything in here up till

9010
06:19:36,478 --> 06:19:41,478
now is like a thread so what this means

9011
06:19:39,200 --> 06:19:42,958
is that in the entire Block it's going

9012
06:19:41,478 --> 06:19:44,040
to make sure that all the threads catch

9013
06:19:42,958 --> 06:19:45,440
up to this point it's going to make sure

9014
06:19:44,040 --> 06:19:47,360
that every it's going to put a barrier

9015
06:19:45,440 --> 06:19:48,638
and makes all make sure all the threads

9016
06:19:47,360 --> 06:19:51,000
have like put what they needed to in

9017
06:19:48,638 --> 06:19:53,240
memory or else if we start doing other

9018
06:19:51,000 --> 06:19:55,000
things then you might have like a zero

9019
06:19:53,240 --> 06:19:56,360
value there where there's like there's

9020
06:19:55,000 --> 06:19:58,878
nothing that exists at that place in

9021
06:19:56,360 --> 06:20:00,240
memory and you're using that to do

9022
06:19:58,878 --> 06:20:01,638
operations which is then going to make

9023
06:20:00,240 --> 06:20:03,638
your answer wrong so you want to make

9024
06:20:01,638 --> 06:20:05,600
sure that all of the threads within the

9025
06:20:03,638 --> 06:20:07,760
block are actually caught up to here so

9026
06:20:05,600 --> 06:20:09,878
like these are on the level of threads

9027
06:20:07,760 --> 06:20:11,638
but we're essentially telling Cuda that

9028
06:20:09,878 --> 06:20:12,878
we want all of the different threads

9029
06:20:11,638 --> 06:20:14,718
that are doing all these parallel

9030
06:20:12,878 --> 06:20:16,040
operations to catch up within the for

9031
06:20:14,718 --> 06:20:18,000
Loop that's what we're

9032
06:20:16,040 --> 06:20:23,120
doing

9033
06:20:18,000 --> 06:20:24,920
um then we uh Advance a uh then we

9034
06:20:23,120 --> 06:20:27,080
advance a by block size so this is just

9035
06:20:24,920 --> 06:20:29,200
like advancing it preemptively we

9036
06:20:27,080 --> 06:20:31,320
already have all of this stuff St uh

9037
06:20:29,200 --> 06:20:33,600
stored in shared memory so we can

9038
06:20:31,320 --> 06:20:35,240
actually just Advance a we can ADV we

9039
06:20:33,600 --> 06:20:37,878
can advance the a point because remember

9040
06:20:35,240 --> 06:20:39,400
a is a pointer right um we can only

9041
06:20:37,878 --> 06:20:42,000
actually use like the index to get the

9042
06:20:39,400 --> 06:20:44,080
values but a itself is a pointer so we

9043
06:20:42,000 --> 06:20:47,798
advance that in memory we advance that

9044
06:20:44,080 --> 06:20:51,120
in the memory space by block size so a

9045
06:20:47,798 --> 06:20:54,200
is uh so a is like this this side one

9046
06:20:51,120 --> 06:20:56,520
that's like going to point inwards to c

9047
06:20:54,200 --> 06:20:59,920
and then B is going to point downwards

9048
06:20:56,520 --> 06:21:02,680
so uh a is going to advance a single

9049
06:20:59,920 --> 06:21:05,120
block so if a is like here for example

9050
06:21:02,680 --> 06:21:09,280
um a is going to advance block side so

9051
06:21:05,120 --> 06:21:10,520
two it's going to jump to here right um

9052
06:21:09,280 --> 06:21:14,878
and then

9053
06:21:10,520 --> 06:21:16,558
B uh B is going to B is going to do the

9054
06:21:14,878 --> 06:21:19,000
same but it's going to jump so it's

9055
06:21:16,558 --> 06:21:19,000
going to

9056
06:21:20,120 --> 06:21:24,280
go it it's just going to it's going to

9057
06:21:22,200 --> 06:21:26,478
Jump N right so it's n is like this

9058
06:21:24,280 --> 06:21:28,240
length it's going to do block size times

9059
06:21:26,478 --> 06:21:30,440
n it's going to jump it's going to jump

9060
06:21:28,240 --> 06:21:31,638
down two right and it's it's going to do

9061
06:21:30,440 --> 06:21:34,040
exactly what we want so it's going to

9062
06:21:31,638 --> 06:21:37,200
advance the tiles in the directions that

9063
06:21:34,040 --> 06:21:39,040
we expect Ed them to you might have had

9064
06:21:37,200 --> 06:21:41,240
some confusion about how we're just

9065
06:21:39,040 --> 06:21:45,280
using like the thread columns and just a

9066
06:21:41,240 --> 06:21:47,360
reminder uh these this a term is already

9067
06:21:45,280 --> 06:21:49,878
advanced to the correct position right

9068
06:21:47,360 --> 06:21:52,958
so once we're Advanced to like this this

9069
06:21:49,878 --> 06:21:55,600
Tile For example then we can just then

9070
06:21:52,958 --> 06:21:57,520
we can pretty much just use threads we

9071
06:21:55,600 --> 06:21:59,718
can use the the the thread indexing

9072
06:21:57,520 --> 06:22:01,878
scheme and that'll give us exactly what

9073
06:21:59,718 --> 06:22:01,878
we

9074
06:22:02,320 --> 06:22:06,920
want so now notice inside of this for

9075
06:22:04,920 --> 06:22:09,478
Loop we have the same indexing scheme as

9076
06:22:06,920 --> 06:22:12,760
we did in the global memory colest uh

9077
06:22:09,478 --> 06:22:15,718
kernel so when we uh when when we're

9078
06:22:12,760 --> 06:22:18,120
just efficient about going through the

9079
06:22:15,718 --> 06:22:20,040
columns and having the C as like a as

9080
06:22:18,120 --> 06:22:22,360
like a horizontal layout instead of like

9081
06:22:20,040 --> 06:22:23,878
a vertical stack of blocks that we're

9082
06:22:22,360 --> 06:22:27,280
just doing the exact same thing here

9083
06:22:23,878 --> 06:22:31,400
right um and so you might be wondering

9084
06:22:27,280 --> 06:22:33,280
about this temp variable um so this temp

9085
06:22:31,400 --> 06:22:34,958
this temporary is just is just going to

9086
06:22:33,280 --> 06:22:37,840
start off as nothing

9087
06:22:34,958 --> 06:22:39,440
and all we're doing is each thread

9088
06:22:37,840 --> 06:22:41,478
essentially each thread has its own temp

9089
06:22:39,440 --> 06:22:43,760
variable right that's going to be stored

9090
06:22:41,478 --> 06:22:45,760
uh in the register each thre has its own

9091
06:22:43,760 --> 06:22:49,280
temp variable and it's going to

9092
06:22:45,760 --> 06:22:51,440
accumulate this temp variable for uh

9093
06:22:49,280 --> 06:22:52,840
it's going to accumulate a DOT product

9094
06:22:51,440 --> 06:22:54,760
so what this is going to

9095
06:22:52,840 --> 06:22:57,160
do is it's not actually going to

9096
06:22:54,760 --> 06:22:59,280
multiply matrices together instead what

9097
06:22:57,160 --> 06:23:01,680
it's going to do is it's going to just

9098
06:22:59,280 --> 06:23:03,958
accumulate as it goes through tiles

9099
06:23:01,680 --> 06:23:07,440
right so as it goes through it's going

9100
06:23:03,958 --> 06:23:11,000
to like accumulate each value in the

9101
06:23:07,440 --> 06:23:14,360
output of C as it as it goes along right

9102
06:23:11,000 --> 06:23:16,718
so based on the thread it's going to say

9103
06:23:14,360 --> 06:23:19,520
say one is going to do like the row of

9104
06:23:16,718 --> 06:23:21,440
this one and then the column of this one

9105
06:23:19,520 --> 06:23:23,200
right and so when they when they

9106
06:23:21,440 --> 06:23:24,520
interact together or when they when they

9107
06:23:23,200 --> 06:23:26,958
interact they're going to end up at like

9108
06:23:24,520 --> 06:23:28,320
this top left part maybe and the thread

9109
06:23:26,958 --> 06:23:30,958
holds the temporary value for that

9110
06:23:28,320 --> 06:23:32,638
specific part um and what it's going to

9111
06:23:30,958 --> 06:23:35,080
do as like as it goes down through the

9112
06:23:32,638 --> 06:23:36,958
tiles it's going to accumulate this dot

9113
06:23:35,080 --> 06:23:39,958
product right so you get the first you

9114
06:23:36,958 --> 06:23:41,680
get the first dot product and then you

9115
06:23:39,958 --> 06:23:42,798
add it to the next one from the next

9116
06:23:41,680 --> 06:23:44,520
tile right because you're you're

9117
06:23:42,798 --> 06:23:46,600
essentially just doing the normal naive

9118
06:23:44,520 --> 06:23:48,280
matrix multiplication but you're just

9119
06:23:46,600 --> 06:23:52,520
accumulating through the

9120
06:23:48,280 --> 06:23:55,878
tiles and so in the end uh you end up

9121
06:23:52,520 --> 06:23:59,320
with just this this accumulated temp

9122
06:23:55,878 --> 06:24:01,160
however this is only for a single dot

9123
06:23:59,320 --> 06:24:03,718
product operation this is only for one

9124
06:24:01,160 --> 06:24:05,958
tile and the reason why we have this for

9125
06:24:03,718 --> 06:24:08,440
Loop inside of this one we're being very

9126
06:24:05,958 --> 06:24:10,120
clever about this is so that we can

9127
06:24:08,440 --> 06:24:12,120
actually do this accumulation through

9128
06:24:10,120 --> 06:24:14,160
the tiles right so that way we can kind

9129
06:24:12,120 --> 06:24:16,920
of be we could just be clever about how

9130
06:24:14,160 --> 06:24:18,798
we uh go about doing that uh and then

9131
06:24:16,920 --> 06:24:21,040
after we're finished we can just go

9132
06:24:18,798 --> 06:24:22,360
ahead and uh you know sync up all the

9133
06:24:21,040 --> 06:24:24,718
threads make sure that they're all

9134
06:24:22,360 --> 06:24:27,920
caught up and before we actually write

9135
06:24:24,718 --> 06:24:30,160
this out to C so I guess just going a

9136
06:24:27,920 --> 06:24:33,040
little like iterating over this again

9137
06:24:30,160 --> 06:24:35,558
the thread row so that's like which

9138
06:24:33,040 --> 06:24:36,920
which row within the tile we want times

9139
06:24:35,558 --> 06:24:39,558
the block size so that's going to be our

9140
06:24:36,920 --> 06:24:41,920
wrapper and then the the idx is going to

9141
06:24:39,558 --> 06:24:44,160
be how we're iterating through it so a

9142
06:24:41,920 --> 06:24:45,558
is row so we're going to that's the idx

9143
06:24:44,160 --> 06:24:48,478
we're going to go through this way

9144
06:24:45,558 --> 06:24:52,320
that's going to be the the offset on the

9145
06:24:48,478 --> 06:24:56,600
horizontal part and then B is going to

9146
06:24:52,320 --> 06:24:58,878
be uh idx times block size uh and then

9147
06:24:56,600 --> 06:25:00,160
plus that offset as we were doing uh

9148
06:24:58,878 --> 06:25:03,638
before right so we're maintaining that

9149
06:25:00,160 --> 06:25:06,760
Global memory access uh the that access

9150
06:25:03,638 --> 06:25:08,760
pattern that we had before um and we're

9151
06:25:06,760 --> 06:25:11,478
and we're just simply writing out that

9152
06:25:08,760 --> 06:25:13,120
temporary variable as it accumulates

9153
06:25:11,478 --> 06:25:15,240
through the tiles right so that that's

9154
06:25:13,120 --> 06:25:18,000
the idea here is we're accumulating

9155
06:25:15,240 --> 06:25:19,320
through the tiles so now we can uh

9156
06:25:18,000 --> 06:25:21,080
hopefully that makes sense feel free to

9157
06:25:19,320 --> 06:25:23,320
rewatch some parts of that feel free to

9158
06:25:21,080 --> 06:25:25,478
plug it into something like chat GPT or

9159
06:25:23,320 --> 06:25:28,638
CLA son it or something like that and

9160
06:25:25,478 --> 06:25:31,638
try to try to visualize what's happening

9161
06:25:28,638 --> 06:25:33,798
I have I have a separate uh additional

9162
06:25:31,638 --> 06:25:36,240
like diagram here just of like what this

9163
06:25:33,798 --> 06:25:38,878
looks like laid out uh I decid to add

9164
06:25:36,240 --> 06:25:41,080
this to the to the the course assets

9165
06:25:38,878 --> 06:25:43,040
folder inside of the faster Mill section

9166
06:25:41,080 --> 06:25:46,478
so if you want to check this out I might

9167
06:25:43,040 --> 06:25:48,958
add other ones to it but uh this is yeah

9168
06:25:46,478 --> 06:25:50,920
this this is the uh shared memory

9169
06:25:48,958 --> 06:25:54,360
blocking uh cud

9170
06:25:50,920 --> 06:25:57,840
kernel so now we can actually go in and

9171
06:25:54,360 --> 06:26:01,400
uh and profile this thing so I up into

9172
06:25:57,840 --> 06:26:03,680
here and just go sjem number

9173
06:26:01,400 --> 06:26:05,920
three we run this for a second it's

9174
06:26:03,680 --> 06:26:08,240
going to be really fast and so if we

9175
06:26:05,920 --> 06:26:11,080
actually compare this give it a second

9176
06:26:08,240 --> 06:26:14,240
it's doing the the last one there so if

9177
06:26:11,080 --> 06:26:18,440
I actually compare this to number

9178
06:26:14,240 --> 06:26:18,440
two which wasn't

9179
06:26:19,638 --> 06:26:25,798
uh give it a

9180
06:26:21,718 --> 06:26:27,680
second yeah so our number two with just

9181
06:26:25,798 --> 06:26:30,878
the Cass memory access was achieving

9182
06:26:27,680 --> 06:26:33,280
about you know 1,200 and this one is

9183
06:26:30,878 --> 06:26:35,878
achieving about 1,600 so we have a

9184
06:26:33,280 --> 06:26:37,600
decent improvement from that right um

9185
06:26:35,878 --> 06:26:39,760
but I probably should have done this

9186
06:26:37,600 --> 06:26:42,200
earlier but just to like spoil just to

9187
06:26:39,760 --> 06:26:46,680
spoil the surprise kublos is actually a

9188
06:26:42,200 --> 06:26:51,000
lot fast we run the 0 kublos is uh about

9189
06:26:46,680 --> 06:26:52,958
11,400 gig flops or 11.5 Tera flops

9190
06:26:51,000 --> 06:26:58,520
which is really fast especially compared

9191
06:26:52,958 --> 06:27:01,120
to our previous uh our our previous

9192
06:26:58,520 --> 06:27:03,558
naive kernel right this is extremely

9193
06:27:01,120 --> 06:27:06,320
fast now just to iterate before we move

9194
06:27:03,558 --> 06:27:08,240
on uh this this uh shared memory kernel

9195
06:27:06,320 --> 06:27:10,478
this is not implementing like the full

9196
06:27:08,240 --> 06:27:12,840
version of tiling this is just

9197
06:27:10,478 --> 06:27:15,920
implementing like a partial dot product

9198
06:27:12,840 --> 06:27:18,240
version of tiling known as blocking so

9199
06:27:15,920 --> 06:27:20,160
when we when we accumulate like dot

9200
06:27:18,240 --> 06:27:22,000
products that's not actually like the

9201
06:27:20,160 --> 06:27:23,798
full like the what you would Inuit it as

9202
06:27:22,000 --> 06:27:25,160
tiling right tiling as I described

9203
06:27:23,798 --> 06:27:27,680
before is when you like take the

9204
06:27:25,160 --> 06:27:29,360
matrices you multiply them and then you

9205
06:27:27,680 --> 06:27:31,760
you advance and then you multiply again

9206
06:27:29,360 --> 06:27:35,400
and you like add them every time they

9207
06:27:31,760 --> 06:27:38,478
multiply uh element wise and this is not

9208
06:27:35,400 --> 06:27:40,958
what we did here what we did was a

9209
06:27:38,478 --> 06:27:42,920
partial dot product so keep that in mind

9210
06:27:40,958 --> 06:27:44,520
the next one is going to be a little bit

9211
06:27:42,920 --> 06:27:49,718
different though

9212
06:27:44,520 --> 06:27:52,120
so 1D uh 1D block tiling is a bit more

9213
06:27:49,718 --> 06:27:54,120
advanced um but we'll get through it so

9214
06:27:52,120 --> 06:27:56,798
just to help get tiling crystal clear in

9215
06:27:54,120 --> 06:27:58,520
your head I'm going to use two matrices

9216
06:27:56,798 --> 06:28:00,520
as an example just to show how this

9217
06:27:58,520 --> 06:28:02,680
intuition works and I've actually

9218
06:28:00,520 --> 06:28:04,360
written out I did a little I was testing

9219
06:28:02,680 --> 06:28:05,798
a little bit and I I did the math Wrong

9220
06:28:04,360 --> 06:28:09,240
by hand so we're just going to use use

9221
06:28:05,798 --> 06:28:13,240
the computer for that um but I wrote out

9222
06:28:09,240 --> 06:28:17,680
a matrix a here so 1 2 3 4 5 6 7 8 9 10

9223
06:28:13,240 --> 06:28:19,520
11 12 13 14 15 16 and then this one B it

9224
06:28:17,680 --> 06:28:23,120
counts by twos but it goes backwards so

9225
06:28:19,520 --> 06:28:25,920
2 4 6 8 10 12 14 16 and then and then

9226
06:28:23,120 --> 06:28:28,558
continuous up to 32 so I've written

9227
06:28:25,920 --> 06:28:29,958
these out here in the terminal um and if

9228
06:28:28,558 --> 06:28:32,520
we just do

9229
06:28:29,958 --> 06:28:35,000
a multiply

9230
06:28:32,520 --> 06:28:36,440
B we notice that we get this output

9231
06:28:35,000 --> 06:28:37,878
result and these are pretty I mean these

9232
06:28:36,440 --> 06:28:40,440
are pretty easy numbers to work with

9233
06:28:37,878 --> 06:28:42,920
right so what I want to do is work

9234
06:28:40,440 --> 06:28:46,878
specifically with this upper right tile

9235
06:28:42,920 --> 06:28:48,840
the 200 180 456 404 that's exactly what

9236
06:28:46,878 --> 06:28:51,240
I want to work with so how we're going

9237
06:28:48,840 --> 06:28:53,400
to do this is we're going to use the

9238
06:28:51,240 --> 06:28:55,080
we're going to use an idea from

9239
06:28:53,400 --> 06:28:58,638
here

9240
06:28:55,080 --> 06:29:02,000
um to get this top right piece we

9241
06:28:58,638 --> 06:29:03,600
essentially want to uh cross inward

9242
06:29:02,000 --> 06:29:08,638
right so we're going to start off with

9243
06:29:03,600 --> 06:29:09,958
multiply a this this a portion with uh

9244
06:29:08,638 --> 06:29:12,798
with this B portion we're going to

9245
06:29:09,958 --> 06:29:16,000
multiply those together a * not B * a a

9246
06:29:12,798 --> 06:29:18,320
* B and then we're going to add that

9247
06:29:16,000 --> 06:29:22,240
with the product the the matrix model

9248
06:29:18,320 --> 06:29:25,558
product of this piece and this piece

9249
06:29:22,240 --> 06:29:28,440
right so if we start out first by doing

9250
06:29:25,558 --> 06:29:31,840
1 two 5 six and

9251
06:29:28,440 --> 06:29:34,798
go say um

9252
06:29:31,840 --> 06:29:37,760
a uh temp

9253
06:29:34,798 --> 06:29:37,760
torch.

9254
06:29:38,360 --> 06:29:43,558
tensor and then inside of here we're

9255
06:29:40,478 --> 06:29:47,798
going to have a smaller ones so it's

9256
06:29:43,558 --> 06:29:47,798
first going to be uh 1

9257
06:29:50,440 --> 06:29:56,320
1256 uh and then we multiply that with

9258
06:29:53,360 --> 06:29:56,320
torch.

9259
06:30:00,760 --> 06:30:05,958
tensor tor. tenser 4 to 1210

9260
06:30:09,760 --> 06:30:15,798
right and if we print out a temp we get

9261
06:30:13,878 --> 06:30:20,600
this

9262
06:30:15,798 --> 06:30:20,600
result now we do uh B

9263
06:30:22,160 --> 06:30:28,200
temp tch. tensor and I'm just going to

9264
06:30:26,280 --> 06:30:30,160
print the uh actually I'll just I'll

9265
06:30:28,200 --> 06:30:31,798
just print the layout here we'll do a

9266
06:30:30,160 --> 06:30:33,040
we'll do the same idea for a temp but

9267
06:30:31,798 --> 06:30:34,558
I'm just going to remove these values

9268
06:30:33,040 --> 06:30:37,760
and we'll place this

9269
06:30:34,558 --> 06:30:40,958
with uh with B temp so this is going to

9270
06:30:37,760 --> 06:30:40,958
be uh

9271
06:30:46,240 --> 06:30:52,958
3478 multiply that with uh 2018 28

9272
06:31:02,920 --> 06:31:10,398
26 20 18 28 26 all right okay awesome

9273
06:31:08,080 --> 06:31:12,840
now we print out B

9274
06:31:10,398 --> 06:31:18,600
temp

9275
06:31:12,840 --> 06:31:20,840
oh and then we do a temp plus b

9276
06:31:18,600 --> 06:31:24,600
temp and we get the result that we were

9277
06:31:20,840 --> 06:31:30,080
expecting 200 180 456

9278
06:31:24,600 --> 06:31:31,360
404 um 2 180 45644 and that's a TOD M

9279
06:31:30,080 --> 06:31:32,638
that that's really all there is to it it

9280
06:31:31,360 --> 06:31:34,680
just helps when you're able to draw this

9281
06:31:32,638 --> 06:31:36,798
out by hand and understand what the

9282
06:31:34,680 --> 06:31:38,878
purpose is when we like advance for

9283
06:31:36,798 --> 06:31:41,040
example when we advance pointers by like

9284
06:31:38,878 --> 06:31:42,160
a a greater offset than like one or two

9285
06:31:41,040 --> 06:31:44,040
or three or four it's like when you're

9286
06:31:42,160 --> 06:31:46,360
skipping entire rows and you're going to

9287
06:31:44,040 --> 06:31:48,760
like a c when we when we when we do that

9288
06:31:46,360 --> 06:31:50,958
stuff this will help you understand why

9289
06:31:48,760 --> 06:31:54,360
we're doing it

9290
06:31:50,958 --> 06:31:55,840
um but uh yeah we can go ahead and begin

9291
06:31:54,360 --> 06:31:57,798
awesome so now let's go ahead and look

9292
06:31:55,840 --> 06:32:02,360
at the uh boiler plate code for this as

9293
06:31:57,798 --> 06:32:04,120
well as the runner script uh for this so

9294
06:32:02,360 --> 06:32:06,360
in the runner script we'll actually pop

9295
06:32:04,120 --> 06:32:08,080
down and you can see each uh each little

9296
06:32:06,360 --> 06:32:10,600
like function and how we call everything

9297
06:32:08,080 --> 06:32:11,958
in here so like the kuas function for

9298
06:32:10,600 --> 06:32:15,840
example there's like different ones for

9299
06:32:11,958 --> 06:32:17,840
like fp32 brain float 16 tensor float 32

9300
06:32:15,840 --> 06:32:20,000
right uh and then we have like the naive

9301
06:32:17,840 --> 06:32:21,798
the colest the shared me caching which

9302
06:32:20,000 --> 06:32:24,080
we just recently did and then we have

9303
06:32:21,798 --> 06:32:26,000
the 1D block tiling right so notice how

9304
06:32:24,080 --> 06:32:29,040
in the shared memory block we just have

9305
06:32:26,000 --> 06:32:31,478
32s everywhere we have 32 32 32 32 and

9306
06:32:29,040 --> 06:32:33,680
32 right everything's 32 because

9307
06:32:31,478 --> 06:32:36,440
everything is a square but in this one

9308
06:32:33,680 --> 06:32:38,638
we we actually mix it up a little bit so

9309
06:32:36,440 --> 06:32:42,280
we we have these essentially this this

9310
06:32:38,638 --> 06:32:46,040
block uh so M it's going to be this this

9311
06:32:42,280 --> 06:32:49,638
block on uh on a so it's going to

9312
06:32:46,040 --> 06:32:52,040
because it's specific to uh A and C

9313
06:32:49,638 --> 06:32:54,080
because when we do our our uh like our

9314
06:32:52,040 --> 06:32:56,558
matrix multiplication shapes we're going

9315
06:32:54,080 --> 06:32:59,200
to cancel out the inner ones uh K and

9316
06:32:56,558 --> 06:33:01,360
then you're going to be left with um

9317
06:32:59,200 --> 06:33:04,000
we're going to be left with M and N

9318
06:33:01,360 --> 06:33:05,760
right so

9319
06:33:04,000 --> 06:33:08,840
just pay attention to these shapes here

9320
06:33:05,760 --> 06:33:10,760
uh we also have the eights down below so

9321
06:33:08,840 --> 06:33:13,160
uh just just like be aware of this we

9322
06:33:10,760 --> 06:33:15,200
use we use different uh shapes here to

9323
06:33:13,160 --> 06:33:18,000
help speed up operations because we

9324
06:33:15,200 --> 06:33:21,680
introduce A New Concept um and that

9325
06:33:18,000 --> 06:33:24,760
concept is 1D block tiling for

9326
06:33:21,680 --> 06:33:28,080
calculating multiple results per thread

9327
06:33:24,760 --> 06:33:31,878
so if we actually go back into uh the

9328
06:33:28,080 --> 06:33:34,120
shared me uh blocking uh when we write

9329
06:33:31,878 --> 06:33:36,680
the output we have the specific spe ific

9330
06:33:34,120 --> 06:33:38,920
index per thread that we write out it's

9331
06:33:36,680 --> 06:33:41,638
just one we write out one output of that

9332
06:33:38,920 --> 06:33:43,360
block or or sorry of that tile and

9333
06:33:41,638 --> 06:33:45,000
that's it uh we just leave it at that

9334
06:33:43,360 --> 06:33:48,000
there's no iterations we're not we're

9335
06:33:45,000 --> 06:33:50,160
not wunning multiple per thread just one

9336
06:33:48,000 --> 06:33:52,200
now if we go to here we notice that

9337
06:33:50,160 --> 06:33:56,280
we're actually writing multiple so we go

9338
06:33:52,200 --> 06:33:59,398
over this res or this result idx um

9339
06:33:56,280 --> 06:34:01,200
that's essentially goes uh it iterates

9340
06:33:59,398 --> 06:34:03,760
through uh

9341
06:34:01,200 --> 06:34:07,840
TM which is this term that we found in

9342
06:34:03,760 --> 06:34:10,600
in here which is uh essentially threads

9343
06:34:07,840 --> 06:34:13,920
per M Dimension you could think of it

9344
06:34:10,600 --> 06:34:16,040
that way so in short we're going to be

9345
06:34:13,920 --> 06:34:17,760
writing out multiple results per thread

9346
06:34:16,040 --> 06:34:19,798
and that's going to speed up um

9347
06:34:17,760 --> 06:34:21,920
everything a lot so like imagine if you

9348
06:34:19,798 --> 06:34:23,200
had to do uh you had to issue a new

9349
06:34:21,920 --> 06:34:25,280
thread every time you're going to write

9350
06:34:23,200 --> 06:34:26,558
an output of this entire Matrix so if

9351
06:34:25,280 --> 06:34:28,440
you have a

9352
06:34:26,558 --> 06:34:31,360
496 uh by

9353
06:34:28,440 --> 06:34:34,080
496 that's going to be about 16

9354
06:34:31,360 --> 06:34:36,160
different threads that are writing out

9355
06:34:34,080 --> 06:34:37,520
right that that is a lot of threads you

9356
06:34:36,160 --> 06:34:41,520
have

9357
06:34:37,520 --> 06:34:42,840
to there's a lot going on there so uh

9358
06:34:41,520 --> 06:34:45,878
when

9359
06:34:42,840 --> 06:34:48,478
we um when when we use when we iterate

9360
06:34:45,878 --> 06:34:50,040
over we can get one thread calculating

9361
06:34:48,478 --> 06:34:51,878
uh multiple and make things more

9362
06:34:50,040 --> 06:34:54,080
efficient it does it does make the

9363
06:34:51,878 --> 06:34:55,558
indexing more complex and this is

9364
06:34:54,080 --> 06:34:57,398
probably one of the most intuitively

9365
06:34:55,558 --> 06:35:00,760
difficult kernels to understand but once

9366
06:34:57,398 --> 06:35:03,440
we get there uh it should be a breeze so

9367
06:35:00,760 --> 06:35:07,040
going up for our boiler play code we

9368
06:35:03,440 --> 06:35:10,280
have this c r is block ID x.y so we're

9369
06:35:07,040 --> 06:35:14,080
going to use blocks uh each individual

9370
06:35:10,280 --> 06:35:17,080
uh block of threads is going to

9371
06:35:14,080 --> 06:35:19,718
calculate uh a specific tile on the

9372
06:35:17,080 --> 06:35:23,080
output right so we have this block

9373
06:35:19,718 --> 06:35:25,320
essentially the current the current row

9374
06:35:23,080 --> 06:35:28,080
and the the current column so current

9375
06:35:25,320 --> 06:35:30,398
row is like it's like a like vertical

9376
06:35:28,080 --> 06:35:32,040
which row are we selecting and that's y

9377
06:35:30,398 --> 06:35:34,040
right so that's vertical and then the

9378
06:35:32,040 --> 06:35:38,120
column like we doing before it's you

9379
06:35:34,040 --> 06:35:42,120
know horizontal Dimension um now we go

9380
06:35:38,120 --> 06:35:45,478
to here which is essentially the the

9381
06:35:42,120 --> 06:35:48,760
thread uh the the lower level thread

9382
06:35:45,478 --> 06:35:51,160
column within that so we have this we

9383
06:35:48,760 --> 06:35:54,840
have this BN term that's used and this

9384
06:35:51,160 --> 06:35:57,760
BN term we remember back to here was 64

9385
06:35:54,840 --> 06:36:01,718
right so let me pop back to

9386
06:35:57,760 --> 06:36:04,520
this this BN term we see it in both the

9387
06:36:01,718 --> 06:36:07,360
B Matrix and and the C Matrix right this

9388
06:36:04,520 --> 06:36:10,120
is BN right it's that length right there

9389
06:36:07,360 --> 06:36:13,440
so when we actually

9390
06:36:10,120 --> 06:36:16,320
um when we do mod BN what that's going

9391
06:36:13,440 --> 06:36:19,080
to do is it's like if if BN is 64 it's

9392
06:36:16,320 --> 06:36:21,000
going to be like um thread idx like zero

9393
06:36:19,080 --> 06:36:22,878
divide by that it's just going to be

9394
06:36:21,000 --> 06:36:24,680
zero right and then it's going to go 1 2

9395
06:36:22,878 --> 06:36:27,638
3 4 5 because we just have this

9396
06:36:24,680 --> 06:36:29,478
remainder that's like not 64 right so

9397
06:36:27,638 --> 06:36:31,360
once we pass 64 then it then it's going

9398
06:36:29,478 --> 06:36:32,840
to Loop so it's going to it's going to

9399
06:36:31,360 --> 06:36:36,200
iterate through all the columns and it's

9400
06:36:32,840 --> 06:36:40,718
going to go up 0 to 63 and then once we

9401
06:36:36,200 --> 06:36:42,160
actually hit 64 it's going to um it's

9402
06:36:40,718 --> 06:36:44,000
going to it's going to divide right so

9403
06:36:42,160 --> 06:36:46,478
it's going to floor up it's going to

9404
06:36:44,000 --> 06:36:48,638
floor to zero for each of these indices

9405
06:36:46,478 --> 06:36:50,958
leading up to 64 and then once we

9406
06:36:48,638 --> 06:36:52,600
actually hit it it's going to divide and

9407
06:36:50,958 --> 06:36:54,398
it's it's going to go to one right

9408
06:36:52,600 --> 06:36:55,558
because it's going to floor down to one

9409
06:36:54,398 --> 06:36:57,398
and we're going to be it's going to be

9410
06:36:55,558 --> 06:36:59,080
like a bunch of zeros and then a one

9411
06:36:57,398 --> 06:37:03,000
right and we use this to pick out our

9412
06:36:59,080 --> 06:37:04,958
rows so the columns it's like 0 1 2 3 4

9413
06:37:03,000 --> 06:37:07,240
5 all the way up up to 64 and then the

9414
06:37:04,958 --> 06:37:09,600
rows it's like every time we stride this

9415
06:37:07,240 --> 06:37:11,680
many it's going to it's going to bump up

9416
06:37:09,600 --> 06:37:12,958
one so the division is going to go up to

9417
06:37:11,680 --> 06:37:15,878
like one and then two and then three

9418
06:37:12,958 --> 06:37:17,320
every time we stride that 64 length and

9419
06:37:15,878 --> 06:37:19,320
then it's going to increase the row

9420
06:37:17,320 --> 06:37:21,680
index so it's going to it's essentially

9421
06:37:19,320 --> 06:37:24,840
going to be like like I said a row index

9422
06:37:21,680 --> 06:37:27,360
it's going to move us downward right in

9423
06:37:24,840 --> 06:37:30,680
C uh and this is this this is kind of

9424
06:37:27,360 --> 06:37:33,638
why we we have this here so moving

9425
06:37:30,680 --> 06:37:35,120
further down we have the a shared and

9426
06:37:33,638 --> 06:37:37,360
the B shared so just the the normal

9427
06:37:35,120 --> 06:37:41,120
shared memory that we allocate um so

9428
06:37:37,360 --> 06:37:42,638
this going to be M by K and then K by n

9429
06:37:41,120 --> 06:37:47,718
right that's that's the space we're

9430
06:37:42,638 --> 06:37:49,320
storing and then uh in this specific uh

9431
06:37:47,718 --> 06:37:55,120
in this specific

9432
06:37:49,320 --> 06:37:58,558
thread we advance the we we advance the

9433
06:37:55,120 --> 06:38:01,080
block tile to the beginning of A's row

9434
06:37:58,558 --> 06:38:03,478
and B's column right so we did the same

9435
06:38:01,080 --> 06:38:06,638
thing in our past one where we had

9436
06:38:03,478 --> 06:38:07,840
Advanced everything uh forward right so

9437
06:38:06,638 --> 06:38:10,558
this is

9438
06:38:07,840 --> 06:38:13,840
literally uh this is literally the same

9439
06:38:10,558 --> 06:38:15,398
idea right so we're we're doing the

9440
06:38:13,840 --> 06:38:16,718
exact same thing there we're just using

9441
06:38:15,398 --> 06:38:19,320
a little bit different terms because

9442
06:38:16,718 --> 06:38:21,000
it's now rectangular tiles um these

9443
06:38:19,320 --> 06:38:23,000
assertions here are pretty much in place

9444
06:38:21,000 --> 06:38:26,000
to say uh we don't want to go out of the

9445
06:38:23,000 --> 06:38:27,478
block dimx range right it's just like a

9446
06:38:26,000 --> 06:38:29,798
essentially like a you could think of it

9447
06:38:27,478 --> 06:38:32,160
as a boundary Checker so when we're when

9448
06:38:29,798 --> 06:38:34,200
we iterate through BM or BK we don't

9449
06:38:32,160 --> 06:38:35,920
want to go out of range we want to make

9450
06:38:34,200 --> 06:38:38,520
sure that these kind of add up we want

9451
06:38:35,920 --> 06:38:40,360
to make sure that um when we have this

9452
06:38:38,520 --> 06:38:42,440
like 2D structure that when we flatten

9453
06:38:40,360 --> 06:38:44,638
it out it stretches the length of block

9454
06:38:42,440 --> 06:38:48,120
dim dox and that that's pretty much

9455
06:38:44,638 --> 06:38:51,000
what's happening there um we we assert

9456
06:38:48,120 --> 06:38:55,200
both of these so uh essentially when we

9457
06:38:51,000 --> 06:38:57,240
go back it's like um you know n and K

9458
06:38:55,200 --> 06:39:00,638
are the same and then M and K are also

9459
06:38:57,240 --> 06:39:02,760
the same so that kind of lines up there

9460
06:39:00,638 --> 06:39:05,478
and then same and then for these ones

9461
06:39:02,760 --> 06:39:08,878
the same IDE as what we were doing here

9462
06:39:05,478 --> 06:39:11,878
so the thread column so we do this thisx

9463
06:39:08,878 --> 06:39:14,160
divided by the uh the X Dimension there

9464
06:39:11,878 --> 06:39:16,160
which is BN that's the that's that's the

9465
06:39:14,160 --> 06:39:20,200
trailing dimension in C it's going to be

9466
06:39:16,160 --> 06:39:23,000
M by n and so in here we just do thread

9467
06:39:20,200 --> 06:39:25,520
idx divided or for the for the column

9468
06:39:23,000 --> 06:39:28,080
index of a the inner column index of k

9469
06:39:25,520 --> 06:39:32,000
or a sorry uh we're going to do thread

9470
06:39:28,080 --> 06:39:35,760
idx mod BK so BK is that that horizontal

9471
06:39:32,000 --> 06:39:37,440
dimension in a right cuz it's M by

9472
06:39:35,760 --> 06:39:39,958
K and

9473
06:39:37,440 --> 06:39:43,558
then the inner row in

9474
06:39:39,958 --> 06:39:45,398
a is going to be just the the division

9475
06:39:43,558 --> 06:39:47,200
of that so whenever we stride the length

9476
06:39:45,398 --> 06:39:48,558
of K it's going to notch up one and it's

9477
06:39:47,200 --> 06:39:50,520
going to tell us which row index we're

9478
06:39:48,558 --> 06:39:52,200
at right that's that's kind of how we

9479
06:39:50,520 --> 06:39:53,958
that's how we use the threads to decide

9480
06:39:52,200 --> 06:39:56,520
which index we're at and then same idea

9481
06:39:53,958 --> 06:39:58,718
for uh same idea for B here except we

9482
06:39:56,520 --> 06:40:03,080
use the trailing dimension in B which is

9483
06:39:58,718 --> 06:40:04,878
n uh instead of K in the a matrix right

9484
06:40:03,080 --> 06:40:07,280
and and then in in

9485
06:40:04,878 --> 06:40:09,878
uh we essentially here we we allocate

9486
06:40:07,280 --> 06:40:11,440
memory this is going to be very

9487
06:40:09,878 --> 06:40:14,040
important for when we write things out

9488
06:40:11,440 --> 06:40:17,200
later on so notice how we iterate over

9489
06:40:14,040 --> 06:40:20,200
this term TM we're going to make thread

9490
06:40:17,200 --> 06:40:23,040
results like a like an actual uh thread

9491
06:40:20,200 --> 06:40:25,680
local cache that has the size TM TM is

9492
06:40:23,040 --> 06:40:27,120
very small right so in here TM is TM is

9493
06:40:25,680 --> 06:40:30,360
actually eight so that can that can

9494
06:40:27,120 --> 06:40:32,160
easily fit in registers um and then we

9495
06:40:30,360 --> 06:40:33,440
just initialize this with a number just

9496
06:40:32,160 --> 06:40:35,320
say zero and then we're going to

9497
06:40:33,440 --> 06:40:36,920
populate that later on right so we just

9498
06:40:35,320 --> 06:40:40,478
initialize this beforehand and then

9499
06:40:36,920 --> 06:40:42,320
we're going to change it later um now we

9500
06:40:40,478 --> 06:40:46,558
actually jump into a little bit more

9501
06:40:42,320 --> 06:40:48,440
advanced stuff so this entire sorry this

9502
06:40:46,558 --> 06:40:51,760
entire Loop here is where a lot of the

9503
06:40:48,440 --> 06:40:53,920
magic actually happens so when we're

9504
06:40:51,760 --> 06:40:57,478
when we're in a single uh when we're in

9505
06:40:53,920 --> 06:40:58,798
a single um when we're in one when we're

9506
06:40:57,478 --> 06:41:02,080
in one of these

9507
06:40:58,798 --> 06:41:06,478
iterations we are trying to calculate

9508
06:41:02,080 --> 06:41:09,360
the uh compl complete tile for uh an

9509
06:41:06,478 --> 06:41:11,840
output in uh C right within a block

9510
06:41:09,360 --> 06:41:14,280
that's what we're trying to do a a block

9511
06:41:11,840 --> 06:41:17,360
a block in like a certain block idx

9512
06:41:14,280 --> 06:41:20,680
within the grid is going to calculate uh

9513
06:41:17,360 --> 06:41:23,558
an output tile in C that's the goal here

9514
06:41:20,680 --> 06:41:26,958
so we outer we we Loop over these block

9515
06:41:23,558 --> 06:41:31,398
tiles by iterating over K right K is

9516
06:41:26,958 --> 06:41:34,520
that um K is the uh horizontal dimension

9517
06:41:31,398 --> 06:41:37,718
in a and the vertical dimension in B

9518
06:41:34,520 --> 06:41:39,920
right so we we iterate over those and we

9519
06:41:37,718 --> 06:41:42,200
advance by this much each time we're not

9520
06:41:39,920 --> 06:41:44,360
actually going to use uh we're not

9521
06:41:42,200 --> 06:41:46,360
actually going to use block idx like as

9522
06:41:44,360 --> 06:41:48,040
you can see we only actually have it in

9523
06:41:46,360 --> 06:41:50,558
this one line here it doesn't show up

9524
06:41:48,040 --> 06:41:52,638
anywhere else so this is just for making

9525
06:41:50,558 --> 06:41:54,638
sure uh that we don't if we just like

9526
06:41:52,638 --> 06:41:55,958
iterate by one each time then it's going

9527
06:41:54,638 --> 06:41:58,080
to go out of range and we're going to do

9528
06:41:55,958 --> 06:42:01,000
this Loop way more than we need to so we

9529
06:41:58,080 --> 06:42:03,878
just want to do it for as many blocks or

9530
06:42:01,000 --> 06:42:07,440
for as many blocks uh as we need

9531
06:42:03,878 --> 06:42:11,558
right we then populate the uh shared

9532
06:42:07,440 --> 06:42:14,840
memory caches so this is within uh this

9533
06:42:11,558 --> 06:42:17,760
is within a single uh tile right so

9534
06:42:14,840 --> 06:42:19,878
notice how we use the inner row a times

9535
06:42:17,760 --> 06:42:24,398
K right so it's the that's the that's

9536
06:42:19,878 --> 06:42:27,440
the which row are we at inside of it um

9537
06:42:24,398 --> 06:42:30,040
and this is these are like um these are

9538
06:42:27,440 --> 06:42:31,520
like very small ranges of indices right

9539
06:42:30,040 --> 06:42:33,558
and that's going to in a it's going to

9540
06:42:31,520 --> 06:42:35,160
loop around K and then that EXT column

9541
06:42:33,558 --> 06:42:37,840
index is going to tell us which position

9542
06:42:35,160 --> 06:42:39,558
we're at right relative to uh the thread

9543
06:42:37,840 --> 06:42:41,320
of course so we can like parallelize the

9544
06:42:39,558 --> 06:42:44,120
ACT actual loading

9545
06:42:41,320 --> 06:42:45,680
part and then we do the same thing for B

9546
06:42:44,120 --> 06:42:48,718
all right so we have this they have this

9547
06:42:45,680 --> 06:42:52,160
row um and we're we're we have we have

9548
06:42:48,718 --> 06:42:54,798
like which row are we at and then uh we

9549
06:42:52,160 --> 06:42:56,360
want to essentially stride that number

9550
06:42:54,798 --> 06:42:59,160
based on n and then end up with that

9551
06:42:56,360 --> 06:43:02,320
offset column index um so that's what

9552
06:42:59,160 --> 06:43:04,680
this is Here We sync everything up so

9553
06:43:02,320 --> 06:43:07,000
this is again at the at the Block Level

9554
06:43:04,680 --> 06:43:09,478
so it so a kernel normally runs at the

9555
06:43:07,000 --> 06:43:11,080
level of threads but because we're doing

9556
06:43:09,478 --> 06:43:13,798
sync threads it's going to apply to all

9557
06:43:11,080 --> 06:43:15,120
of them so all of the threads uh within

9558
06:43:13,798 --> 06:43:16,478
this within this block are actually

9559
06:43:15,120 --> 06:43:17,760
going to line up they're all going we're

9560
06:43:16,478 --> 06:43:19,080
going to put a barrier and they're all

9561
06:43:17,760 --> 06:43:22,280
going to meet up and they're going to

9562
06:43:19,080 --> 06:43:24,240
synchronize at the same same spot right

9563
06:43:22,280 --> 06:43:27,200
um and then we just Advance the block

9564
06:43:24,240 --> 06:43:29,040
Tile For The Next Step so when we uh

9565
06:43:27,200 --> 06:43:31,080
when we need to do this load again this

9566
06:43:29,040 --> 06:43:32,440
is already ready and we don't want to we

9567
06:43:31,080 --> 06:43:33,600
don't want to like worry about this

9568
06:43:32,440 --> 06:43:35,878
anymore right

9569
06:43:33,600 --> 06:43:38,000
okay so remember A and B are just

9570
06:43:35,878 --> 06:43:41,840
pointers right when we scroll up we see

9571
06:43:38,000 --> 06:43:44,120
A and B are Pointers to uh float arrays

9572
06:43:41,840 --> 06:43:46,120
right so when these are laid out in

9573
06:43:44,120 --> 06:43:49,040
memory uh they're they're not like it's

9574
06:43:46,120 --> 06:43:50,760
not like an array of arrays um or like

9575
06:43:49,040 --> 06:43:52,478
an an array of pointers where each

9576
06:43:50,760 --> 06:43:54,160
pointer inside that array is a new array

9577
06:43:52,478 --> 06:43:56,798
like we did in the in the C and C++

9578
06:43:54,160 --> 06:43:58,680
review chapter it's not like that it's

9579
06:43:56,798 --> 06:44:01,280
it's literally just a it's literally

9580
06:43:58,680 --> 06:44:04,080
just a pointer and it's the pointer is

9581
06:44:01,280 --> 06:44:05,760
at the start of that thing that's start

9582
06:44:04,080 --> 06:44:07,040
of the array that's laid out in memory

9583
06:44:05,760 --> 06:44:08,878
so it's not the actual value it's just

9584
06:44:07,040 --> 06:44:10,878
the memory address so if we take that

9585
06:44:08,878 --> 06:44:12,638
memory address and we we plus one it'll

9586
06:44:10,878 --> 06:44:15,360
go to the next index next one next one

9587
06:44:12,638 --> 06:44:17,320
next one right um that's what we're

9588
06:44:15,360 --> 06:44:20,120
doing here so we already did this in the

9589
06:44:17,320 --> 06:44:23,240
last we already did this here um where

9590
06:44:20,120 --> 06:44:27,638
we or is it this part we just we just

9591
06:44:23,240 --> 06:44:30,520
Advance further um so a advances um

9592
06:44:27,638 --> 06:44:33,840
essentially plus an an entire block size

9593
06:44:30,520 --> 06:44:37,280
so we advance the we we just Advance

9594
06:44:33,840 --> 06:44:38,320
Plus uh whatever this value is to a so

9595
06:44:37,280 --> 06:44:40,478
it's going

9596
06:44:38,320 --> 06:44:42,798
to it's just going to increase that much

9597
06:44:40,478 --> 06:44:44,120
whatever we set that to and then this is

9598
06:44:42,798 --> 06:44:46,638
going to increase but it's going to have

9599
06:44:44,120 --> 06:44:50,360
that n stride right so the the trailing

9600
06:44:46,638 --> 06:44:52,478
dimension in B is n so it's like K byn

9601
06:44:50,360 --> 06:44:54,320
and so it's going to it's going to wrap

9602
06:44:52,478 --> 06:44:57,240
right it's going to wrap and it's just

9603
06:44:54,320 --> 06:45:00,558
going to find the sort of the the next

9604
06:44:57,240 --> 06:45:01,638
the next one in uh in in B right and

9605
06:45:00,558 --> 06:45:03,878
that's that that's really all we're

9606
06:45:01,638 --> 06:45:06,200
doing there

9607
06:45:03,878 --> 06:45:08,360
so then we go to this next part uh this

9608
06:45:06,200 --> 06:45:10,000
is where actually a lot of magic happens

9609
06:45:08,360 --> 06:45:11,920
and I'll do my best to explain this but

9610
06:45:10,000 --> 06:45:15,440
this part's like kind of intuitively

9611
06:45:11,920 --> 06:45:18,478
hard so we have multiple Forbes in here

9612
06:45:15,440 --> 06:45:21,718
we have this idx that we iterate over BK

9613
06:45:18,478 --> 06:45:25,280
with uh we have this float we have this

9614
06:45:21,718 --> 06:45:28,000
this specific float um this this this

9615
06:45:25,280 --> 06:45:30,000
sorry this this temp temporary variable

9616
06:45:28,000 --> 06:45:31,160
and then we have an inner loop here so

9617
06:45:30,000 --> 06:45:33,558
I'll try to explain this as best as

9618
06:45:31,160 --> 06:45:36,920
possible um

9619
06:45:33,558 --> 06:45:38,558
we jump back to here we notice how uh

9620
06:45:36,920 --> 06:45:40,000
like initially we have these two

9621
06:45:38,558 --> 06:45:43,080
matrices that we're trying to multiply

9622
06:45:40,000 --> 06:45:44,558
together so this is uh this is a and

9623
06:45:43,080 --> 06:45:46,280
this is B right and we have this this

9624
06:45:44,558 --> 06:45:50,520
tile intersection as we did on the

9625
06:45:46,280 --> 06:45:52,280
Whiteboard there um and so the really

9626
06:45:50,520 --> 06:45:55,040
the the magic happens is is like in

9627
06:45:52,280 --> 06:45:56,638
these Loops right we've already taken um

9628
06:45:55,040 --> 06:45:58,398
in this outer one we've already actually

9629
06:45:56,638 --> 06:45:59,878
taken the block tiles we already have

9630
06:45:58,398 --> 06:46:03,160
these in shared memory and now we have

9631
06:45:59,878 --> 06:46:04,718
to do fast operations with them so we go

9632
06:46:03,160 --> 06:46:06,240
down to here where we actually have

9633
06:46:04,718 --> 06:46:09,558
these in shared memory right this is a

9634
06:46:06,240 --> 06:46:11,558
tall and and not very wide this is uh

9635
06:46:09,558 --> 06:46:13,040
wide and a little bit tall right so it's

9636
06:46:11,558 --> 06:46:18,600
it kind of matches

9637
06:46:13,040 --> 06:46:23,520
up and then inside of here what we do is

9638
06:46:18,600 --> 06:46:26,000
we notice how we do we iterate over idx

9639
06:46:23,520 --> 06:46:28,000
and then in here we iterate uh through

9640
06:46:26,000 --> 06:46:31,160
through this this this TM right so we

9641
06:46:28,000 --> 06:46:35,440
have this idx and then res idx uh over

9642
06:46:31,160 --> 06:46:38,040
TM so if we actually pop back to here um

9643
06:46:35,440 --> 06:46:40,600
essentially what's happening is this at

9644
06:46:38,040 --> 06:46:44,958
the lowest level in the innermost Loop

9645
06:46:40,600 --> 06:46:46,920
res idx is going through uh like we we

9646
06:46:44,958 --> 06:46:48,398
have this jump here which I'll explain

9647
06:46:46,920 --> 06:46:52,000
that indexing in a second like how we

9648
06:46:48,398 --> 06:46:54,040
actually arve there but res idx is going

9649
06:46:52,000 --> 06:46:57,240
through these it's advancing uh

9650
06:46:54,040 --> 06:46:59,520
vertically downward and it's multiplying

9651
06:46:57,240 --> 06:47:01,840
with whatever value this is so that

9652
06:46:59,520 --> 06:47:03,558
little top left corner in B that top

9653
06:47:01,840 --> 06:47:06,000
left corner value in B is going to stay

9654
06:47:03,558 --> 06:47:08,478
the same and res idx is just going to

9655
06:47:06,000 --> 06:47:11,160
multiply with that value it's going to

9656
06:47:08,478 --> 06:47:12,958
go and it's going to compute a partial

9657
06:47:11,160 --> 06:47:16,680
dotproduct along this

9658
06:47:12,958 --> 06:47:19,718
column right now now when we do uh when

9659
06:47:16,680 --> 06:47:22,680
we iterate over uh idx what's going to

9660
06:47:19,718 --> 06:47:24,478
happen is uh idx is going to it's going

9661
06:47:22,680 --> 06:47:26,760
to go forward this way and it's going to

9662
06:47:24,478 --> 06:47:29,040
go down this way right so notice how

9663
06:47:26,760 --> 06:47:30,558
these these arrows are are colored very

9664
06:47:29,040 --> 06:47:33,638
similarly they're they're actually the

9665
06:47:30,558 --> 06:47:36,320
same color so that means we're GNA

9666
06:47:33,638 --> 06:47:38,958
idx is going to evolve downwards in B

9667
06:47:36,320 --> 06:47:43,920
and idx is going to evolve uh to the

9668
06:47:38,958 --> 06:47:45,718
right in a so whenever we evolve like

9669
06:47:43,920 --> 06:47:47,120
one it's going to like essentially res

9670
06:47:45,718 --> 06:47:50,000
idx is going to take this value it's

9671
06:47:47,120 --> 06:47:52,200
going to go and then we're going to uh

9672
06:47:50,000 --> 06:47:54,200
evolve one forward and then where IX is

9673
06:47:52,200 --> 06:47:56,040
going to reset and then it's going to do

9674
06:47:54,200 --> 06:47:57,920
a DOT a partial dot product for the next

9675
06:47:56,040 --> 06:47:59,958
column right it's going to do a partial

9676
06:47:57,920 --> 06:48:02,280
dot product uh for the next column and

9677
06:47:59,958 --> 06:48:05,280
it's going to do this all the way until

9678
06:48:02,280 --> 06:48:07,000
it is finished inside of inside of uh

9679
06:48:05,280 --> 06:48:09,360
this this entire section and then when

9680
06:48:07,000 --> 06:48:12,320
these evolve uh inwards right so when

9681
06:48:09,360 --> 06:48:14,360
this is when this is going forwards um

9682
06:48:12,320 --> 06:48:16,320
and like just not even considering res

9683
06:48:14,360 --> 06:48:17,718
idx just like think about like sure

9684
06:48:16,320 --> 06:48:20,040
these ones are all filled in this one

9685
06:48:17,718 --> 06:48:21,680
too but when this goes forward when this

9686
06:48:20,040 --> 06:48:23,478
goes forward and multiplies with this

9687
06:48:21,680 --> 06:48:26,558
and they are both inching in one at a

9688
06:48:23,478 --> 06:48:30,120
time as dot this is what's happening as

9689
06:48:26,558 --> 06:48:32,798
idx is is moving up um these are

9690
06:48:30,120 --> 06:48:34,878
actually acting as little um like

9691
06:48:32,798 --> 06:48:39,000
essentially little tiles right and so

9692
06:48:34,878 --> 06:48:41,440
you end up Computing this specific uh

9693
06:48:39,000 --> 06:48:44,638
you end up Computing the full dot

9694
06:48:41,440 --> 06:48:47,958
product of this so when when this one

9695
06:48:44,638 --> 06:48:50,718
moves like here it's like 1/4 of it is

9696
06:48:47,958 --> 06:48:54,200
done right 1/4 of it is done and then it

9697
06:48:50,718 --> 06:48:56,080
moves up half is done half is done 75 75

9698
06:48:54,200 --> 06:48:59,280
and then it's fully done once that

9699
06:48:56,080 --> 06:49:02,080
evolves all four steps this entire Index

9700
06:48:59,280 --> 06:49:03,958
right here is computed and so we notice

9701
06:49:02,080 --> 06:49:06,120
that when we do res idx and we go

9702
06:49:03,958 --> 06:49:09,760
through all of these we end up

9703
06:49:06,120 --> 06:49:11,718
completing the entire row so when we go

9704
06:49:09,760 --> 06:49:14,680
through this way and these this this

9705
06:49:11,718 --> 06:49:18,360
goes forward we complete one column at a

9706
06:49:14,680 --> 06:49:24,280
time right one column at a time is uh

9707
06:49:18,360 --> 06:49:27,160
completed per thread and so when we um

9708
06:49:24,280 --> 06:49:30,360
when we have other threads acting like

9709
06:49:27,160 --> 06:49:32,200
thread column and the thread row which

9710
06:49:30,360 --> 06:49:33,920
is acting as these little blocks that

9711
06:49:32,200 --> 06:49:36,920
are shiting downwards instead of like

9712
06:49:33,920 --> 06:49:38,200
little individual 1D columns um you

9713
06:49:36,920 --> 06:49:41,320
actually end up Computing the entire

9714
06:49:38,200 --> 06:49:42,600
thing so thread column is laid out this

9715
06:49:41,320 --> 06:49:45,878
way through like it's like essentially

9716
06:49:42,600 --> 06:49:48,718
all the column indices in in the B

9717
06:49:45,878 --> 06:49:52,000
tile and then you have the thread rows

9718
06:49:48,718 --> 06:49:54,120
which are here and so this is going to

9719
06:49:52,000 --> 06:49:56,200
compute all of the all of the columns in

9720
06:49:54,120 --> 06:49:58,920
C it's going to it's going to cover all

9721
06:49:56,200 --> 06:50:01,040
of them in the tiling aspect and then

9722
06:49:58,920 --> 06:50:02,398
this is also going to cover all of them

9723
06:50:01,040 --> 06:50:05,240
so what you actually end up with is you

9724
06:50:02,398 --> 06:50:08,280
can complete the entire tile by giving a

9725
06:50:05,240 --> 06:50:10,878
thread more operations to do so when we

9726
06:50:08,280 --> 06:50:13,878
actually jump into this

9727
06:50:10,878 --> 06:50:15,958
um we can see so first of all we have

9728
06:50:13,878 --> 06:50:18,520
this temp B right so this is coming from

9729
06:50:15,958 --> 06:50:19,878
the B shared memory this is so we just

9730
06:50:18,520 --> 06:50:22,160
say

9731
06:50:19,878 --> 06:50:23,958
idxx is zero this is the first iteration

9732
06:50:22,160 --> 06:50:26,718
it hasn't or the zero with iteration it

9733
06:50:23,958 --> 06:50:27,718
hasn't changed yet so this is zero and

9734
06:50:26,718 --> 06:50:30,760
it's going

9735
06:50:27,718 --> 06:50:35,280
to it's going to evolve you know across

9736
06:50:30,760 --> 06:50:36,878
BN zero number of times right so 0 *

9737
06:50:35,280 --> 06:50:40,000
that is z and then plus the thread

9738
06:50:36,878 --> 06:50:41,680
column if that is also if that is also

9739
06:50:40,000 --> 06:50:43,520
zero then it's just going to be here

9740
06:50:41,680 --> 06:50:47,680
right it's literally just going to be

9741
06:50:43,520 --> 06:50:49,440
there um and when idx moves up then it's

9742
06:50:47,680 --> 06:50:51,680
going to

9743
06:50:49,440 --> 06:50:53,320
um like this isn't it's going it's not

9744
06:50:51,680 --> 06:50:55,440
going to move this way that the this

9745
06:50:53,320 --> 06:50:57,440
offset here is from the thread itself

9746
06:50:55,440 --> 06:50:58,958
the thread index itself but we're

9747
06:50:57,440 --> 06:51:00,600
actually going to Traverse downward

9748
06:50:58,958 --> 06:51:04,120
right that's that's the do idx as I was

9749
06:51:00,600 --> 06:51:07,958
explaining before and then uh you have

9750
06:51:04,120 --> 06:51:10,160
the uh this res idx the result idx and

9751
06:51:07,958 --> 06:51:13,958
this goes through TM right so this is

9752
06:51:10,160 --> 06:51:15,600
this little block that we have here

9753
06:51:13,958 --> 06:51:18,120
um

9754
06:51:15,600 --> 06:51:20,920
and when we actually look at how this is

9755
06:51:18,120 --> 06:51:23,638
accumulating remember this is the size

9756
06:51:20,920 --> 06:51:26,000
of um this is the size of TM we're going

9757
06:51:23,638 --> 06:51:27,840
to iterate through TM with this red idx

9758
06:51:26,000 --> 06:51:29,680
or res idx that's going to be the the

9759
06:51:27,840 --> 06:51:31,718
POS the the amount through TM which

9760
06:51:29,680 --> 06:51:34,360
we've iterated through we're setting

9761
06:51:31,718 --> 06:51:36,878
that index to uh a place in shared

9762
06:51:34,360 --> 06:51:41,718
memory and this exact place is going to

9763
06:51:36,878 --> 06:51:45,718
be so the thread row times uh TM plus

9764
06:51:41,718 --> 06:51:47,878
res idx right so thread Row in this

9765
06:51:45,718 --> 06:51:50,240
case thread Row in this case is

9766
06:51:47,878 --> 06:51:52,360
whichever one of these it falls at right

9767
06:51:50,240 --> 06:51:55,600
so that that's a specific row that it

9768
06:51:52,360 --> 06:51:57,680
falls at considering that we evolve uh

9769
06:51:55,600 --> 06:52:00,200
like in TM blocks right that's that's

9770
06:51:57,680 --> 06:52:02,558
the that's the amount that we evolve we

9771
06:52:00,200 --> 06:52:07,200
we progress each time and then this res

9772
06:52:02,558 --> 06:52:09,478
idx part um res idx is how much are we

9773
06:52:07,200 --> 06:52:11,240
vertically offset so we we advance like

9774
06:52:09,478 --> 06:52:12,878
four or like eight or four or eight

9775
06:52:11,240 --> 06:52:15,920
downwards and then we have this

9776
06:52:12,878 --> 06:52:17,920
additional offset res idx um but we have

9777
06:52:15,920 --> 06:52:19,600
to make sure that we actually arrive at

9778
06:52:17,920 --> 06:52:21,718
that specific piece cuz it's it's going

9779
06:52:19,600 --> 06:52:23,160
to be laid out in memory like this right

9780
06:52:21,718 --> 06:52:25,080
so we have to make sure that we evolve

9781
06:52:23,160 --> 06:52:28,240
straight downwards and that we get to

9782
06:52:25,080 --> 06:52:28,240
that certain res ID ex

9783
06:52:28,680 --> 06:52:34,440
position now we multiply this by the k

9784
06:52:32,840 --> 06:52:35,680
which makes this a lot easier for us

9785
06:52:34,440 --> 06:52:37,840
right that's that essentially solves

9786
06:52:35,680 --> 06:52:40,520
that problem so however many we want to

9787
06:52:37,840 --> 06:52:43,280
go down this this thread whichever

9788
06:52:40,520 --> 06:52:46,080
thread row we're talking about times TM

9789
06:52:43,280 --> 06:52:49,718
which is you know that that block

9790
06:52:46,080 --> 06:52:51,920
space um plus res idx which is that

9791
06:52:49,718 --> 06:52:54,680
offset and then times that all of that

9792
06:52:51,920 --> 06:52:56,680
times BK right this this K Dimension

9793
06:52:54,680 --> 06:52:57,558
here and that's just going to times it's

9794
06:52:56,680 --> 06:53:02,200
going to

9795
06:52:57,558 --> 06:53:02,200
go right where we need to be

9796
06:53:02,638 --> 06:53:06,638
then we just add the idx offset which as

9797
06:53:05,280 --> 06:53:08,520
I highlighted

9798
06:53:06,638 --> 06:53:10,520
before is literally just going to

9799
06:53:08,520 --> 06:53:11,920
progress that way so it's going to

9800
06:53:10,520 --> 06:53:14,200
iterate all the way to the starting

9801
06:53:11,920 --> 06:53:16,558
position uh and then idx is going to

9802
06:53:14,200 --> 06:53:19,440
tell us how much has it gone to the

9803
06:53:16,558 --> 06:53:22,718
right uh and so you have this do

9804
06:53:19,440 --> 06:53:24,760
idx here that traverses downwards and

9805
06:53:22,718 --> 06:53:27,398
here it traverses to the

9806
06:53:24,760 --> 06:53:31,120
right now if we go back we're

9807
06:53:27,398 --> 06:53:33,840
multiplying this um by this one we're

9808
06:53:31,120 --> 06:53:37,320
multiplying it by uh the the temporary B

9809
06:53:33,840 --> 06:53:39,958
value keep in mind this is the only

9810
06:53:37,320 --> 06:53:41,280
thing controlling this is the idx which

9811
06:53:39,958 --> 06:53:43,200
we already highlighted this is going to

9812
06:53:41,280 --> 06:53:45,638
make it go downwards and then the thread

9813
06:53:43,200 --> 06:53:47,080
column so thread column as I mentioned

9814
06:53:45,638 --> 06:53:48,920
before again we're kind of just going

9815
06:53:47,080 --> 06:53:50,160
like starting with the visual example

9816
06:53:48,920 --> 06:53:53,200
and then going into the code and then

9817
06:53:50,160 --> 06:53:55,280
connecting that to our visual example so

9818
06:53:53,200 --> 06:53:57,878
thread column is that that horizontal

9819
06:53:55,280 --> 06:53:59,360
offset and that that like each thread is

9820
06:53:57,878 --> 06:54:01,040
just going to get a different horizontal

9821
06:53:59,360 --> 06:54:03,680
offset right so it depends on which

9822
06:54:01,040 --> 06:54:07,200
thread we're at um

9823
06:54:03,680 --> 06:54:09,240
and then we go back to here um and we

9824
06:54:07,200 --> 06:54:10,360
have this this thread row right so

9825
06:54:09,240 --> 06:54:12,798
that's just going to depend on which

9826
06:54:10,360 --> 06:54:15,240
block we're at so in this case we're not

9827
06:54:12,798 --> 06:54:17,440
at block uh we're not at sorry we're not

9828
06:54:15,240 --> 06:54:20,478
at thread row zero we're at thread Row

9829
06:54:17,440 --> 06:54:22,240
one so it's going to Traverse TM uh

9830
06:54:20,478 --> 06:54:24,000
layers down which in this case is like

9831
06:54:22,240 --> 06:54:26,280
maybe four or eight or whatever whatever

9832
06:54:24,000 --> 06:54:28,040
number we pick right um and then it's

9833
06:54:26,280 --> 06:54:29,920
going to end up there and then the rest

9834
06:54:28,040 --> 06:54:31,600
of the math is going to ensure that we

9835
06:54:29,920 --> 06:54:34,840
get to the correct position with respect

9836
06:54:31,600 --> 06:54:34,840
to resid and

9837
06:54:35,280 --> 06:54:40,680
idx some other things you want to pay

9838
06:54:37,200 --> 06:54:43,600
attention to here are how these actually

9839
06:54:40,680 --> 06:54:46,798
uh are coest in memory right so in

9840
06:54:43,600 --> 06:54:48,878
memory um keep in mind when we're

9841
06:54:46,798 --> 06:54:50,320
loading these columns in when we're when

9842
06:54:48,878 --> 06:54:51,878
we're loading these little column bits

9843
06:54:50,320 --> 06:54:54,478
in in uh in

9844
06:54:51,878 --> 06:54:55,878
memory we're loading them as if they're

9845
06:54:54,478 --> 06:54:58,080
like adjacent next to each other right

9846
06:54:55,878 --> 06:55:00,398
so this like thread zero is going to be

9847
06:54:58,080 --> 06:55:01,840
adjacent to to thread thread column one

9848
06:55:00,398 --> 06:55:03,680
thread column Z is adjacent to thread

9849
06:55:01,840 --> 06:55:05,840
column one they're next next to each

9850
06:55:03,680 --> 06:55:08,200
other thread column two three four five

9851
06:55:05,840 --> 06:55:10,840
right so when we actually load this on

9852
06:55:08,200 --> 06:55:12,240
the level of threads that memory access

9853
06:55:10,840 --> 06:55:14,320
is going to be CEST it's going to be

9854
06:55:12,240 --> 06:55:16,240
combined we're not going to have to you

9855
06:55:14,320 --> 06:55:18,080
know consider this stride and then like

9856
06:55:16,240 --> 06:55:19,878
oh we need to get two memory accesses to

9857
06:55:18,080 --> 06:55:22,958
get both of these it's like no you can

9858
06:55:19,878 --> 06:55:27,040
actually fit um like a bunch like

9859
06:55:22,958 --> 06:55:29,000
however many as you need into one right

9860
06:55:27,040 --> 06:55:32,360
so technically what you're going to have

9861
06:55:29,000 --> 06:55:34,718
here is you're going to have um

9862
06:55:32,360 --> 06:55:38,040
since BN is

9863
06:55:34,718 --> 06:55:40,280
64 BN is 64 so this this whole length

9864
06:55:38,040 --> 06:55:41,798
here it's going to have two warps so

9865
06:55:40,280 --> 06:55:44,320
it's going to literally going to be uh

9866
06:55:41,798 --> 06:55:48,600
like two memory accesses that we need to

9867
06:55:44,320 --> 06:55:50,320
do because um that like an entire warp

9868
06:55:48,600 --> 06:55:52,360
that can actually make memory accesses

9869
06:55:50,320 --> 06:55:54,120
really efficient when we have two that's

9870
06:55:52,360 --> 06:55:56,798
that's effectively just two memory

9871
06:55:54,120 --> 06:55:58,638
accesses uh that we have to worry about

9872
06:55:56,798 --> 06:56:00,878
so it's really awesome that we have

9873
06:55:58,638 --> 06:56:03,398
these that we have these colest right I

9874
06:56:00,878 --> 06:56:05,760
mean this itself this other one isn't

9875
06:56:03,398 --> 06:56:07,840
going to be coess but that's fine um

9876
06:56:05,760 --> 06:56:09,798
because we you know we're we're still

9877
06:56:07,840 --> 06:56:11,600
using shared memory what we do care

9878
06:56:09,798 --> 06:56:14,160
about though is that these are colest

9879
06:56:11,600 --> 06:56:15,240
right the column accesses are colest and

9880
06:56:14,160 --> 06:56:18,638
that's going to make things really

9881
06:56:15,240 --> 06:56:21,040
really fast so to iterate um we we

9882
06:56:18,638 --> 06:56:23,200
essentially iterate over and we complete

9883
06:56:21,040 --> 06:56:25,398
we let the threads complete uh The

9884
06:56:23,200 --> 06:56:27,000
Columns uh like partially and they they

9885
06:56:25,398 --> 06:56:29,200
Advance over and the and the dot

9886
06:56:27,000 --> 06:56:30,638
products are going or the the dot ID EXs

9887
06:56:29,200 --> 06:56:32,280
are going to evolve and they're they're

9888
06:56:30,638 --> 06:56:34,398
going to close inwards and then they're

9889
06:56:32,280 --> 06:56:35,958
going to complete that slowly right and

9890
06:56:34,398 --> 06:56:37,840
these these individual threads are going

9891
06:56:35,958 --> 06:56:38,878
to complete the columns right so they're

9892
06:56:37,840 --> 06:56:42,080
going to they're going to move through

9893
06:56:38,878 --> 06:56:44,360
the res idx and then the do ID XS are

9894
06:56:42,080 --> 06:56:45,878
going to evolve this way so you end up

9895
06:56:44,360 --> 06:56:47,638
just completing a whole column because

9896
06:56:45,878 --> 06:56:50,520
these line up and this one lines up and

9897
06:56:47,638 --> 06:56:53,680
so you get you get a bunch of

9898
06:56:50,520 --> 06:56:56,360
these and then one of these

9899
06:56:53,680 --> 06:56:57,600
right so then we sync up the threads we

9900
06:56:56,360 --> 06:56:58,558
make sure that they're all caught up so

9901
06:56:57,600 --> 06:57:00,120
that we can actually write out the

9902
06:56:58,558 --> 06:57:02,360
results safely right this is for a

9903
06:57:00,120 --> 06:57:03,718
specific block tile so when we have like

9904
06:57:02,360 --> 06:57:04,600
a bunch of them in the entire thing we

9905
06:57:03,718 --> 06:57:06,958
have a bunch of block tiles we're

9906
06:57:04,600 --> 06:57:08,240
worrying about um we want to make sure

9907
06:57:06,958 --> 06:57:09,638
that we've synced them up for this

9908
06:57:08,240 --> 06:57:10,920
current block tile just for safety

9909
06:57:09,638 --> 06:57:12,558
purposes right we don't want to mess

9910
06:57:10,920 --> 06:57:14,920
anything up so we're just going we're

9911
06:57:12,558 --> 06:57:17,080
going to be safe there and then when we

9912
06:57:14,920 --> 06:57:19,160
actually write out the results it's

9913
06:57:17,080 --> 06:57:22,160
going to be very similar to what we

9914
06:57:19,160 --> 06:57:25,798
actually did um up here right so we

9915
06:57:22,160 --> 06:57:27,558
iterate through this TM term again um

9916
06:57:25,798 --> 06:57:31,840
and we're going to have thread row time

9917
06:57:27,558 --> 06:57:34,320
DM plus red res idx and then times n

9918
06:57:31,840 --> 06:57:38,520
right so n is in the bigger picture of

9919
06:57:34,320 --> 06:57:40,840
that whole that whole C Matrix um and

9920
06:57:38,520 --> 06:57:43,360
then plus the actual uh thread column

9921
06:57:40,840 --> 06:57:45,478
itself which is going to be that offset

9922
06:57:43,360 --> 06:57:47,478
uh and that that's the actual uh index

9923
06:57:45,478 --> 06:57:51,160
that we write out and we're going to

9924
06:57:47,478 --> 06:57:52,718
iterate over TM or eight indices every

9925
06:57:51,160 --> 06:57:54,240
single thread so each thread is going to

9926
06:57:52,718 --> 06:57:56,080
write out eight elements right instead

9927
06:57:54,240 --> 06:57:59,240
of just one writing out eight

9928
06:57:56,080 --> 06:58:01,360
elements um and then we're we're just

9929
06:57:59,240 --> 06:58:02,958
going to keep in mind this you know this

9930
06:58:01,360 --> 06:58:05,040
this thread result results we're just

9931
06:58:02,958 --> 06:58:07,760
essentially each time we write out it's

9932
06:58:05,040 --> 06:58:09,240
just going to populate that index so

9933
06:58:07,760 --> 06:58:12,160
it's like a very easy way of just

9934
06:58:09,240 --> 06:58:13,840
keeping track of uh of like which ones

9935
06:58:12,160 --> 06:58:16,440
we write out at the lowest level of

9936
06:58:13,840 --> 06:58:18,440
Hardware like SM and registers so it's

9937
06:58:16,440 --> 06:58:20,320
just going to make our jobs easier on

9938
06:58:18,440 --> 06:58:22,360
that level so we could literally just

9939
06:58:20,320 --> 06:58:23,680
store them one by one it's like eight an

9940
06:58:22,360 --> 06:58:25,160
array of eight elements and then we

9941
06:58:23,680 --> 06:58:26,638
write out that array of eight elements

9942
06:58:25,160 --> 06:58:29,840
we don't have to worry about strides or

9943
06:58:26,638 --> 06:58:32,320
any of that stuff right um we multiply

9944
06:58:29,840 --> 06:58:34,120
Alpha by that um for each for each one

9945
06:58:32,320 --> 06:58:36,760
right for each index and then we do the

9946
06:58:34,120 --> 06:58:41,360
same with beta so beta is another term

9947
06:58:36,760 --> 06:58:43,760
and we have C which uh I mean c is just

9948
06:58:41,360 --> 06:58:45,680
we're essentially just element or we are

9949
06:58:43,760 --> 06:58:47,360
we are pointwise multiplying beta so

9950
06:58:45,680 --> 06:58:49,680
it's like whatever beta is maybe like

9951
06:58:47,360 --> 06:58:53,080
0.5 or three or whatever it is we're

9952
06:58:49,680 --> 06:58:55,558
just multiplying each each one uh and so

9953
06:58:53,080 --> 06:58:57,920
we consider the the strides and the

9954
06:58:55,558 --> 06:59:00,798
offset as as well for

9955
06:58:57,920 --> 06:59:03,000
that okay awesome so now we can actually

9956
06:59:00,798 --> 06:59:06,080
generate or we can actually see how this

9957
06:59:03,000 --> 06:59:08,160
performs uh so remember how bad our

9958
06:59:06,080 --> 06:59:11,680
initial kernel was I'm going to go ahead

9959
06:59:08,160 --> 06:59:13,040
and run SJ 00 just to show kuas um it's

9960
06:59:11,680 --> 06:59:16,520
going to iterate up and we're going to

9961
06:59:13,040 --> 06:59:18,320
get about 11.4 11.5 Tera flops of

9962
06:59:16,520 --> 06:59:20,760
performance and then if we go ahead and

9963
06:59:18,320 --> 06:59:24,360
run the block to one so kernel number

9964
06:59:20,760 --> 06:59:24,360
four go ahead and run

9965
06:59:24,520 --> 06:59:28,558
this we're going to get like actually

9966
06:59:26,840 --> 06:59:29,878
quite a bit faster than the previous

9967
06:59:28,558 --> 06:59:32,958
which was

9968
06:59:29,878 --> 06:59:36,360
03 and this one

9969
06:59:32,958 --> 06:59:36,360
uh give it a

9970
06:59:36,840 --> 06:59:42,600
second this one gave us about 1 1600 uh

9971
06:59:40,040 --> 06:59:44,240
1600 gig flops this one is about a 3X

9972
06:59:42,600 --> 06:59:46,920
speed up of what we previously had which

9973
06:59:44,240 --> 06:59:49,320
is really good so that pretty much just

9974
06:59:46,920 --> 06:59:51,680
shows uh that the memory access patterns

9975
06:59:49,320 --> 06:59:54,440
we use so that that coling of of memory

9976
06:59:51,680 --> 06:59:57,360
access was really really useful and even

9977
06:59:54,440 --> 06:59:58,920
more useful was using a single thread to

9978
06:59:57,360 --> 07:00:00,718
do multiple computations to compute

9979
06:59:58,920 --> 07:00:02,680
eight elements instead of just one right

9980
07:00:00,718 --> 07:00:06,360
so that really really SP up our

9981
07:00:02,680 --> 07:00:07,478
throughput and performance there so now

9982
07:00:06,360 --> 07:00:10,840
if

9983
07:00:07,478 --> 07:00:13,600
um I actually wrote a like off camera I

9984
07:00:10,840 --> 07:00:17,040
wrote a separate uh I wrote a separate

9985
07:00:13,600 --> 07:00:19,840
function here or not function file main.

9986
07:00:17,040 --> 07:00:22,398
cuu inside of the Kel's folder so so we

9987
07:00:19,840 --> 07:00:25,200
could easily like compile it um what I

9988
07:00:22,398 --> 07:00:27,958
pretty much did is I just uh imported

9989
07:00:25,200 --> 07:00:29,878
this so the the for essentially the

9990
07:00:27,958 --> 07:00:32,360
block tiling kernel the the header for

9991
07:00:29,878 --> 07:00:34,200
that right here uh I included the macro

9992
07:00:32,360 --> 07:00:37,558
which we had in the previous file I

9993
07:00:34,200 --> 07:00:40,878
initialized the majores so 1024 1024

9994
07:00:37,558 --> 07:00:44,398
1024 um and then our previous like 64 64

9995
07:00:40,878 --> 07:00:46,958
8 and 8 and then I just populated these

9996
07:00:44,398 --> 07:00:48,680
um could amalik manage just use like

9997
07:00:46,958 --> 07:00:49,798
this this unified memory which is like

9998
07:00:48,680 --> 07:00:52,160
going to just reduce a bunch of

9999
07:00:49,798 --> 07:00:53,798
boilerplate and make things uh a little

10000
07:00:52,160 --> 07:00:55,280
bit sped up for us but you're you're

10001
07:00:53,798 --> 07:00:57,440
going to see the the main thing that

10002
07:00:55,280 --> 07:00:58,840
we're looking for in a second um I

10003
07:00:57,440 --> 07:01:00,718
initialize everything properly and then

10004
07:00:58,840 --> 07:01:02,040
I call this kernel uh and we're just

10005
07:01:00,718 --> 07:01:05,958
trying to see like what is the actual

10006
07:01:02,040 --> 07:01:08,638
code look like under the hood here so if

10007
07:01:05,958 --> 07:01:12,558
uh if I go up to this command that I ran

10008
07:01:08,638 --> 07:01:14,680
recently nvcc D PTX that means parallel

10009
07:01:12,558 --> 07:01:17,120
thread execution parallel thread

10010
07:01:14,680 --> 07:01:19,638
execution is what Cuda compiles down to

10011
07:01:17,120 --> 07:01:22,040
it's the Assembly Language for parallel

10012
07:01:19,638 --> 07:01:23,840
processors uh and then you have just the

10013
07:01:22,040 --> 07:01:26,798
file that we're compiling and then we

10014
07:01:23,840 --> 07:01:30,920
output kernel. PTX

10015
07:01:26,798 --> 07:01:32,398
right or I have to get out of this um

10016
07:01:30,920 --> 07:01:36,920
and then go into

10017
07:01:32,398 --> 07:01:38,360
SRC kernels uh and then go and run that

10018
07:01:36,920 --> 07:01:40,878
this is going to give us this kernel.

10019
07:01:38,360 --> 07:01:42,680
PTX file which I'll just open here we

10020
07:01:40,878 --> 07:01:49,558
bring this

10021
07:01:42,680 --> 07:01:49,558
up we go into uh kernel.

10022
07:01:49,878 --> 07:01:56,958
PTX we notice uh there are a lot of

10023
07:01:53,638 --> 07:01:58,600
lines in this there are 308

10024
07:01:56,958 --> 07:02:01,558
lines

10025
07:01:58,600 --> 07:02:04,320
and it doesn't tell us anything super

10026
07:02:01,558 --> 07:02:07,760
specific specific right like we can see

10027
07:02:04,320 --> 07:02:12,040
um like for example like an ad an ad

10028
07:02:07,760 --> 07:02:15,760
operation or uh like a fuse multiply ad

10029
07:02:12,040 --> 07:02:18,558
with a with a floating Point 32 number

10030
07:02:15,760 --> 07:02:19,958
um it's like I think output and then

10031
07:02:18,558 --> 07:02:21,798
like multiply these and then add this

10032
07:02:19,958 --> 07:02:23,280
one I can't remember the exact order but

10033
07:02:21,798 --> 07:02:25,330
like fuse multiply add right you can

10034
07:02:23,280 --> 07:02:26,478
find all these instructions in here

10035
07:02:25,330 --> 07:02:32,638
[Music]

10036
07:02:26,478 --> 07:02:32,638
um multiply right um

10037
07:02:33,000 --> 07:02:38,360
yeah we have the the load instruction so

10038
07:02:35,718 --> 07:02:40,920
LD so it's going to load uh a floating

10039
07:02:38,360 --> 07:02:44,840
Point number into uh

10040
07:02:40,920 --> 07:02:44,840
shared into the into the

10041
07:02:45,558 --> 07:02:52,160
SRAM and then we leave this if I can

10042
07:02:48,478 --> 07:02:54,320
exit Vim uh I have a separate one that I

10043
07:02:52,160 --> 07:02:57,520
also outputed this is the Shader

10044
07:02:54,320 --> 07:03:00,280
assembly so initially nvcc is going to

10045
07:02:57,520 --> 07:03:01,920
compile everything down to PTX and then

10046
07:03:00,280 --> 07:03:03,638
PTX is going to further compile to

10047
07:03:01,920 --> 07:03:05,520
Shader assembly which is then actually

10048
07:03:03,638 --> 07:03:09,440
run on the GPU Shader assembly is what

10049
07:03:05,520 --> 07:03:12,040
it executes so if we compile uh you know

10050
07:03:09,440 --> 07:03:14,000
Cuda binary right this actual the actual

10051
07:03:12,040 --> 07:03:16,120
binary that we run and then we and then

10052
07:03:14,000 --> 07:03:18,958
we uh compose this back up into the

10053
07:03:16,120 --> 07:03:21,520
Shader Assembly Language that uh is

10054
07:03:18,958 --> 07:03:25,878
actually executing and then we output

10055
07:03:21,520 --> 07:03:25,878
that in the Cu uh the Cuda binary

10056
07:03:26,558 --> 07:03:32,280
format we

10057
07:03:28,280 --> 07:03:32,280
can uh

10058
07:03:33,080 --> 07:03:40,120
Oh wrong one let me let me check real

10059
07:03:37,120 --> 07:03:43,478
quick we go yes so we've compiled it

10060
07:03:40,120 --> 07:03:44,920
into this and now we we uh we look at it

10061
07:03:43,478 --> 07:03:47,920
again through the special command so

10062
07:03:44,920 --> 07:03:50,520
Cuda object dump uh dump Shader assembly

10063
07:03:47,920 --> 07:03:53,680
and then just open essentially open that

10064
07:03:50,520 --> 07:03:56,120
and it's going to give us the exact uh

10065
07:03:53,680 --> 07:03:58,120
assembly code or what we just what we

10066
07:03:56,120 --> 07:04:00,240
just uh compiled right that entire

10067
07:03:58,120 --> 07:04:03,360
script

10068
07:04:00,240 --> 07:04:05,680
so if we look look very carefully for

10069
07:04:03,360 --> 07:04:09,160
like load instructions right so if we

10070
07:04:05,680 --> 07:04:12,840
look for LD

10071
07:04:09,160 --> 07:04:15,840
um which I know is from the it's from

10072
07:04:12,840 --> 07:04:19,240
here so PTX compiled to Shader assembly

10073
07:04:15,840 --> 07:04:21,520
the S shared memory loads from from B

10074
07:04:19,240 --> 07:04:24,878
shared are vectorized right B shared was

10075
07:04:21,520 --> 07:04:26,280
the one where we had uh these individual

10076
07:04:24,878 --> 07:04:30,040
uh columns right so when that was in

10077
07:04:26,280 --> 07:04:31,718
shared memory the smm loads from shared

10078
07:04:30,040 --> 07:04:33,878
are vectorized remember when we had the

10079
07:04:31,718 --> 07:04:35,680
threads that were adjacent to each other

10080
07:04:33,878 --> 07:04:38,200
those are what we're looking for so if

10081
07:04:35,680 --> 07:04:38,200
we look for

10082
07:04:41,798 --> 07:04:50,920
LDS so we have

10083
07:04:44,280 --> 07:04:55,160
LDS um U 128 right LDS u32 right we have

10084
07:04:50,920 --> 07:04:57,798
all these ldss here um and this is when

10085
07:04:55,160 --> 07:05:00,760
they're not coest so this this LDS when

10086
07:04:57,798 --> 07:05:03,080
we have the 32 that means it's it's it's

10087
07:05:00,760 --> 07:05:06,080
not uh it's not cess together right

10088
07:05:03,080 --> 07:05:08,160
these these um when it's when it's not

10089
07:05:06,080 --> 07:05:10,840
accessing like four in a row so if you

10090
07:05:08,160 --> 07:05:14,600
do a a 32-bit floating Point number 32

10091
07:05:10,840 --> 07:05:16,920
bits * 4 is 128 so when we're accessing

10092
07:05:14,600 --> 07:05:19,600
four in a row it's going to be 128bit

10093
07:05:16,920 --> 07:05:21,200
load in uh Shader assembly so this is

10094
07:05:19,600 --> 07:05:24,000
what it actually looks like when we get

10095
07:05:21,200 --> 07:05:25,120
those cols memory accesses going and

10096
07:05:24,000 --> 07:05:27,600
this is what it looks like when we're

10097
07:05:25,120 --> 07:05:29,320
not right so we end up having to uh you

10098
07:05:27,600 --> 07:05:32,200
know maybe load more so we have like two

10099
07:05:29,320 --> 07:05:34,160
loads here uh but anyways that this this

10100
07:05:32,200 --> 07:05:37,120
is the entire Point here just to show

10101
07:05:34,160 --> 07:05:39,280
you what the actual loads look like in

10102
07:05:37,120 --> 07:05:41,080
binary format that this this is what

10103
07:05:39,280 --> 07:05:44,200
they look like uh and we're going to

10104
07:05:41,080 --> 07:05:47,200
further optimize uh these kernels to

10105
07:05:44,200 --> 07:05:50,558
perform better right fe uh I think this

10106
07:05:47,200 --> 07:05:52,478
is f ffma is like fuse multiply ad

10107
07:05:50,558 --> 07:05:55,080
floating Point fuse multiply ad I can't

10108
07:05:52,478 --> 07:06:02,920
remember we can actually search that up

10109
07:05:55,080 --> 07:06:02,920
uh to uh F FMA in uh Shader assembly

10110
07:06:09,240 --> 07:06:15,840
so yeah fused fused multiply ad there's

10111
07:06:14,320 --> 07:06:17,280
probably a manual somewhere which I'm

10112
07:06:15,840 --> 07:06:19,680
not going to look through right now you

10113
07:06:17,280 --> 07:06:21,280
can do that but uh these are the Shader

10114
07:06:19,680 --> 07:06:23,440
assembly instructions that we're working

10115
07:06:21,280 --> 07:06:27,280
with

10116
07:06:23,440 --> 07:06:29,680
um Now we move on to 2D block tiling

10117
07:06:27,280 --> 07:06:31,840
which Builds on what we've already done

10118
07:06:29,680 --> 07:06:34,120
but makes it even more efficient and

10119
07:06:31,840 --> 07:06:36,200
makes it even more

10120
07:06:34,120 --> 07:06:37,478
performant okay absolutely give yourself

10121
07:06:36,200 --> 07:06:39,520
pat on the back if you made it this far

10122
07:06:37,478 --> 07:06:40,958
this been quite challenging so uh you

10123
07:06:39,520 --> 07:06:43,398
know feel free to take take a coffee

10124
07:06:40,958 --> 07:06:45,878
break or whatever um get some

10125
07:06:43,398 --> 07:06:49,520
tea I have some tea with me right here

10126
07:06:45,878 --> 07:06:51,920
so you know it's it it is a grind um but

10127
07:06:49,520 --> 07:06:54,798
if we step back to just this blog post

10128
07:06:51,920 --> 07:06:56,600
here we go to Kernel number five so

10129
07:06:54,798 --> 07:06:58,320
increasing arithmetic intensity with 2D

10130
07:06:56,600 --> 07:06:59,718
block tiling before we were just

10131
07:06:58,320 --> 07:07:01,878
calculating columns right we were just

10132
07:06:59,718 --> 07:07:03,398
calculating columns and now this one is

10133
07:07:01,878 --> 07:07:04,680
going to calculate entire blocks it's

10134
07:07:03,398 --> 07:07:06,120
going to calculate like a mini block

10135
07:07:04,680 --> 07:07:08,638
inside of the big block tile right

10136
07:07:06,120 --> 07:07:11,958
that's the idea so before we just had

10137
07:07:08,638 --> 07:07:13,320
this idx in the in the in the the B tile

10138
07:07:11,958 --> 07:07:14,478
this was going to go downwards and we

10139
07:07:13,320 --> 07:07:16,240
just had a single thing that was

10140
07:07:14,478 --> 07:07:18,760
iterating down and calculating you know

10141
07:07:16,240 --> 07:07:20,080
a column so you have this you would just

10142
07:07:18,760 --> 07:07:22,160
kind of intersect and you get this

10143
07:07:20,080 --> 07:07:26,040
column filled

10144
07:07:22,160 --> 07:07:27,840
out but now we have this res idx uh n

10145
07:07:26,040 --> 07:07:29,360
component right so that's just that's

10146
07:07:27,840 --> 07:07:33,160
just going to go and make this

10147
07:07:29,360 --> 07:07:35,280
essentially cover um like like a square

10148
07:07:33,160 --> 07:07:36,600
or like a row and a column right so they

10149
07:07:35,280 --> 07:07:39,680
intersect and it's going to fill up like

10150
07:07:36,600 --> 07:07:40,958
a square area um so we have this we have

10151
07:07:39,680 --> 07:07:43,200
this term which we're going to iterate

10152
07:07:40,958 --> 07:07:45,878
over and then we have this new TN term

10153
07:07:43,200 --> 07:07:49,120
so TM is literally just this component

10154
07:07:45,878 --> 07:07:52,360
and then TM is this component underneath

10155
07:07:49,120 --> 07:07:54,398
right um so now like stepping into

10156
07:07:52,360 --> 07:07:56,558
this this is kind of just what it looks

10157
07:07:54,398 --> 07:07:58,760
like under the hood when we sort of

10158
07:07:56,558 --> 07:08:02,160
visualize how this is how this is being

10159
07:07:58,760 --> 07:08:04,360
calculated so we look at our um

10160
07:08:02,160 --> 07:08:06,600
we look at our our a tile and we look at

10161
07:08:04,360 --> 07:08:10,000
our B tile and it's literally just we're

10162
07:08:06,600 --> 07:08:12,280
storing a column um in reg M so that's

10163
07:08:10,000 --> 07:08:14,040
like an actual register memory in this

10164
07:08:12,280 --> 07:08:16,040
kernel we're actually using register

10165
07:08:14,040 --> 07:08:18,718
memory so we're we're occupying it a

10166
07:08:16,040 --> 07:08:21,200
little bit more we're uh as you can see

10167
07:08:18,718 --> 07:08:23,478
we literally populate it so reg m is

10168
07:08:21,200 --> 07:08:27,600
going to be of that size and then T reg

10169
07:08:23,478 --> 07:08:29,000
n is TN size right um and the total

10170
07:08:27,600 --> 07:08:31,600
results is going to be the surface area

10171
07:08:29,000 --> 07:08:33,120
of that total area there

10172
07:08:31,600 --> 07:08:35,520
um and we just we essentially just store

10173
07:08:33,120 --> 07:08:37,440
a column and a row and those will those

10174
07:08:35,520 --> 07:08:38,958
will dot product through this entire

10175
07:08:37,440 --> 07:08:41,600
thing and they will intersect and you'll

10176
07:08:38,958 --> 07:08:43,240
get this little square right here um and

10177
07:08:41,600 --> 07:08:44,680
you can we can see how that evolves at

10178
07:08:43,240 --> 07:08:49,160
each step

10179
07:08:44,680 --> 07:08:50,600
right so now there's my marker let's

10180
07:08:49,160 --> 07:08:55,000
actually step into this

10181
07:08:50,600 --> 07:08:57,320
kernel so we go back to here and we see

10182
07:08:55,000 --> 07:09:01,120
um you know run sjem 2D block tiling

10183
07:08:57,320 --> 07:09:04,440
right so we have this we we have a we

10184
07:09:01,120 --> 07:09:07,320
have this same BK term right so BK is

10185
07:09:04,440 --> 07:09:09,478
just going to be eight in this case um

10186
07:09:07,320 --> 07:09:11,120
and we have TM which is also the same

10187
07:09:09,478 --> 07:09:12,160
and we have this new TN term right so

10188
07:09:11,120 --> 07:09:14,840
we're essentially just going to

10189
07:09:12,160 --> 07:09:16,638
calculate um like this area that we fill

10190
07:09:14,840 --> 07:09:19,478
out is going to contain 64 elements it's

10191
07:09:16,638 --> 07:09:20,878
going to be 8 by 8 that's going to fill

10192
07:09:19,478 --> 07:09:22,718
that entire thing up and we're going to

10193
07:09:20,878 --> 07:09:24,798
have 64 in

10194
07:09:22,718 --> 07:09:27,558
there now we have this little hacky

10195
07:09:24,798 --> 07:09:29,878
situation for handling like the very

10196
07:09:27,558 --> 07:09:31,360
very low like the smaller matrices that

10197
07:09:29,878 --> 07:09:32,718
are going to be tested when we do a

10198
07:09:31,360 --> 07:09:34,958
Benchmark so this is why we have this

10199
07:09:32,718 --> 07:09:39,558
little out statement here

10200
07:09:34,958 --> 07:09:42,120
um so in case we you know decide to use

10201
07:09:39,558 --> 07:09:43,920
um in case we decide to use like a very

10202
07:09:42,120 --> 07:09:45,958
small Matrix this is able to deal with

10203
07:09:43,920 --> 07:09:46,878
that effectively but normally we're just

10204
07:09:45,958 --> 07:09:48,718
going to we're just going to pay

10205
07:09:46,878 --> 07:09:51,080
attention to this first if statement for

10206
07:09:48,718 --> 07:09:53,920
now um so if these are bigger than

10207
07:09:51,080 --> 07:09:56,000
bigger than or equal 128

10208
07:09:53,920 --> 07:09:59,280
right

10209
07:09:56,000 --> 07:10:03,320
um yeah so in

10210
07:09:59,280 --> 07:10:07,040
here pay attention to this we have a

10211
07:10:03,320 --> 07:10:11,440
grid dim which is going to have um an X

10212
07:10:07,040 --> 07:10:12,760
component right and then a a y component

10213
07:10:11,440 --> 07:10:15,000
right so that that's going to that's

10214
07:10:12,760 --> 07:10:16,878
going to stay the same and then this

10215
07:10:15,000 --> 07:10:18,440
block Dimension the the number of

10216
07:10:16,878 --> 07:10:20,200
threads within a block right it's the

10217
07:10:18,440 --> 07:10:22,240
that like the block itself the dimension

10218
07:10:20,200 --> 07:10:24,080
of that the X component there's just

10219
07:10:22,240 --> 07:10:26,958
going to be one value here and that's

10220
07:10:24,080 --> 07:10:29,760
going to be um essentially the the total

10221
07:10:26,958 --> 07:10:32,320
number of elements in the output tile so

10222
07:10:29,760 --> 07:10:34,718
you know we have this B and this BN and

10223
07:10:32,320 --> 07:10:36,760
then they then they they you sort of

10224
07:10:34,718 --> 07:10:39,718
just get this this filled out area of C

10225
07:10:36,760 --> 07:10:42,000
the C tile that is going to be of this

10226
07:10:39,718 --> 07:10:44,120
of this size right and then we divide

10227
07:10:42,000 --> 07:10:47,680
that by the total surface area covered

10228
07:10:44,120 --> 07:10:49,840
by um by just what a thread calculate so

10229
07:10:47,680 --> 07:10:51,600
a thread is going to go through those 64

10230
07:10:49,840 --> 07:10:52,760
elements it's going to be 8 by8 it's

10231
07:10:51,600 --> 07:10:54,280
going to go through those and they're

10232
07:10:52,760 --> 07:10:57,878
going to calculate a small little mini

10233
07:10:54,280 --> 07:11:00,520
grid for us um like per thread right um

10234
07:10:57,878 --> 07:11:04,638
and so if we we divide the total number

10235
07:11:00,520 --> 07:11:06,558
of output result results by the uh by

10236
07:11:04,638 --> 07:11:08,000
the space covered by a thread we

10237
07:11:06,558 --> 07:11:10,160
actually get the number of threads right

10238
07:11:08,000 --> 07:11:12,320
because each little piece here is a

10239
07:11:10,160 --> 07:11:13,520
thread and so if we divide this by this

10240
07:11:12,320 --> 07:11:16,680
we actually get the total number of

10241
07:11:13,520 --> 07:11:18,478
threads within that entire SE tile um so

10242
07:11:16,680 --> 07:11:22,360
if we do this math

10243
07:11:18,478 --> 07:11:22,360
here we're going to get so

10244
07:11:23,680 --> 07:11:26,680
128

10245
07:11:27,718 --> 07:11:32,840
times8 and then divide this by

10246
07:11:31,760 --> 07:11:37,840
8

10247
07:11:32,840 --> 07:11:40,360
by8 and this simplifies to well 128 is 2

10248
07:11:37,840 --> 07:11:42,878
to 7 right because remember like if you

10249
07:11:40,360 --> 07:11:44,798
if you know like int 8 Precision it's

10250
07:11:42,878 --> 07:11:47,958
it's like it's like an image essentially

10251
07:11:44,798 --> 07:11:51,680
like number of number of like uh RGB

10252
07:11:47,958 --> 07:11:54,638
values in a single U like in a single

10253
07:11:51,680 --> 07:11:56,718
Pixel so you have like RGB and each of

10254
07:11:54,638 --> 07:11:58,558
those is like Zer to 255 so that that's

10255
07:11:56,718 --> 07:12:00,878
like how you can go down to what like uh

10256
07:11:58,558 --> 07:12:02,798
128 is 2 to the 7 cuz it's like in 7

10257
07:12:00,878 --> 07:12:07,600
instead 8 that's how I made that

10258
07:12:02,798 --> 07:12:07,600
Association but we have this 2

10259
07:12:08,320 --> 07:12:16,520
7

10260
07:12:10,878 --> 07:12:20,280
7/ 2 3 * 2 3 and what you end up

10261
07:12:16,520 --> 07:12:20,280
with is 2

10262
07:12:22,718 --> 07:12:29,160
14

10263
07:12:24,878 --> 07:12:33,680
/ 2 6 and we do our exponent laws and we

10264
07:12:29,160 --> 07:12:36,680
get eight right so 14 - 6 is 8 so two

10265
07:12:33,680 --> 07:12:40,120
the8 that means we have

10266
07:12:36,680 --> 07:12:45,040
256 my marker works

10267
07:12:40,120 --> 07:12:45,040
properly 256 threads

10268
07:12:47,200 --> 07:12:53,958
right 256 threads my writing is messy

10269
07:12:50,718 --> 07:12:55,240
ignore that um awesome so we know the

10270
07:12:53,958 --> 07:12:56,280
number of threads that we're calculating

10271
07:12:55,240 --> 07:12:58,200
now this is going to help us when we

10272
07:12:56,280 --> 07:13:00,718
actually jump into this kernel here so

10273
07:12:58,200 --> 07:13:03,520
you know we have our our BM or BN BK TM

10274
07:13:00,718 --> 07:13:03,520
and TM again

10275
07:13:04,120 --> 07:13:10,398
right there's a lot happening here let's

10276
07:13:06,638 --> 07:13:12,840
break this down so to start we have our

10277
07:13:10,398 --> 07:13:14,398
our row which is going to be the the

10278
07:13:12,840 --> 07:13:17,160
typical y component we've already went

10279
07:13:14,398 --> 07:13:19,920
over this the total results in a block

10280
07:13:17,160 --> 07:13:22,680
tile are as we already said in the in

10281
07:13:19,920 --> 07:13:24,600
the runner file uh this is just

10282
07:13:22,680 --> 07:13:26,760
essentially the the the whole the whole

10283
07:13:24,600 --> 07:13:28,160
seed Matrix so you have the M Dimension

10284
07:13:26,760 --> 07:13:29,638
then you have the N Dimension you

10285
07:13:28,160 --> 07:13:30,798
multiply those and you get total area

10286
07:13:29,638 --> 07:13:32,680
right that's the number of results we're

10287
07:13:30,798 --> 07:13:36,320
going to get calculate and then we get

10288
07:13:32,680 --> 07:13:38,718
the number of threads per that entire

10289
07:13:36,320 --> 07:13:41,680
thing so how many individual squares are

10290
07:13:38,718 --> 07:13:43,360
there that a thread is occupying right

10291
07:13:41,680 --> 07:13:45,080
and so that the square surface area is

10292
07:13:43,360 --> 07:13:46,520
this which we already calculated and so

10293
07:13:45,080 --> 07:13:49,840
this number of threads per block tile

10294
07:13:46,520 --> 07:13:51,558
should equal 256 right and then we just

10295
07:13:49,840 --> 07:13:53,958
assert that down here so we want to make

10296
07:13:51,558 --> 07:13:55,280
sure that 256 value is equal to what we

10297
07:13:53,958 --> 07:13:57,240
just calculated which is going to hold

10298
07:13:55,280 --> 07:13:59,878
true of course and then it won't it'll

10299
07:13:57,240 --> 07:14:03,440
actually continue with executing right

10300
07:13:59,878 --> 07:14:05,638
um let go down a little bit further

10301
07:14:03,440 --> 07:14:06,920
um don't worry about these right now

10302
07:14:05,638 --> 07:14:10,360
we're going to get into those in a

10303
07:14:06,920 --> 07:14:12,680
second so we do the the classical just

10304
07:14:10,360 --> 07:14:15,558
allocate shared memory we advance the

10305
07:14:12,680 --> 07:14:17,840
block tiles based on um you know where

10306
07:14:15,558 --> 07:14:20,920
they're supposed to be at in

10307
07:14:17,840 --> 07:14:25,798
the uh in in the in the current block

10308
07:14:20,920 --> 07:14:25,798
right or or I guess in in in the current

10309
07:14:26,120 --> 07:14:30,600
thread this is where a lot of the magic

10310
07:14:28,120 --> 07:14:32,478
happens right here so this this part is

10311
07:14:30,600 --> 07:14:34,958
crucial pay attention to let's first go

10312
07:14:32,478 --> 07:14:36,478
over the easy stuff so thread results is

10313
07:14:34,958 --> 07:14:38,280
just like how big is that that little

10314
07:14:36,478 --> 07:14:40,080
square that thread is going to calculate

10315
07:14:38,280 --> 07:14:42,718
that's just going to be um this is going

10316
07:14:40,080 --> 07:14:45,520
to be 8 by 8

10317
07:14:42,718 --> 07:14:49,200
right we have this reg M term which we

10318
07:14:45,520 --> 07:14:51,360
saw in the blog post so reg m is this

10319
07:14:49,200 --> 07:14:55,478
and reg n is

10320
07:14:51,360 --> 07:14:59,478
that TM and TN right that's just going

10321
07:14:55,478 --> 07:15:01,240
to be the the little the I guess the the

10322
07:14:59,478 --> 07:15:04,320
iterations of the dot product so each

10323
07:15:01,240 --> 07:15:06,718
dot at each iteration of the of idx it's

10324
07:15:04,320 --> 07:15:07,840
going to store those in registers and

10325
07:15:06,718 --> 07:15:09,600
it's going to calculate them really

10326
07:15:07,840 --> 07:15:10,878
really fast right it's going to be like

10327
07:15:09,600 --> 07:15:12,920
it's going to be like eight it's going

10328
07:15:10,878 --> 07:15:16,040
to be like 8 by one and then a 1 by 8

10329
07:15:12,920 --> 07:15:16,040
and it's just going to evolve that

10330
07:15:16,360 --> 07:15:20,840
way now we look up at

10331
07:15:19,040 --> 07:15:23,798
these

10332
07:15:20,840 --> 07:15:26,200
so in a row a so we just essentially

10333
07:15:23,798 --> 07:15:27,718
have the thread index and we divide that

10334
07:15:26,200 --> 07:15:30,478
by BK

10335
07:15:27,718 --> 07:15:32,160
right in a row same idea but we use the

10336
07:15:30,478 --> 07:15:33,398
mod oper

10337
07:15:32,160 --> 07:15:35,080
then we have this new term called a

10338
07:15:33,398 --> 07:15:36,478
stride right and now we're actually

10339
07:15:35,080 --> 07:15:38,160
going to do the math we're going to do

10340
07:15:36,478 --> 07:15:42,240
the math for a stride

10341
07:15:38,160 --> 07:15:42,240
here so so stride

10342
07:15:50,798 --> 07:15:58,718
a stride a is going to be number of

10343
07:15:53,958 --> 07:15:58,718
threads per block tile which is 256

10344
07:16:01,638 --> 07:16:07,200
divid by BK which in this case

10345
07:16:08,360 --> 07:16:14,120
is8 so here if you divide this out you

10346
07:16:11,478 --> 07:16:14,120
end up getting

10347
07:16:14,958 --> 07:16:20,958
32 so our stride a is going to be 32

10348
07:16:21,120 --> 07:16:27,000
right my marker is a little weird let's

10349
07:16:24,080 --> 07:16:29,120
pay attention to that term

10350
07:16:27,000 --> 07:16:32,080
um and then we have this other stride

10351
07:16:29,120 --> 07:16:34,160
down here stride B is the same thing but

10352
07:16:32,080 --> 07:16:36,920
instead of dividing by BK we divide by

10353
07:16:34,160 --> 07:16:41,680
BN so BN is actually

10354
07:16:36,920 --> 07:16:43,398
128 so if we divide 256

10355
07:16:41,680 --> 07:16:46,958
right

10356
07:16:43,398 --> 07:16:46,958
128 the answer is

10357
07:16:47,040 --> 07:16:50,840
two now these are going to be important

10358
07:16:49,520 --> 07:16:53,000
these are going to be very important as

10359
07:16:50,840 --> 07:16:55,120
we sort of Step through this so let's

10360
07:16:53,000 --> 07:16:58,440
jump down to this actual uh loop here

10361
07:16:55,120 --> 07:17:00,520
now so inside of a shared look at how we

10362
07:16:58,440 --> 07:17:02,398
Index this start starts here and ends

10363
07:17:00,520 --> 07:17:05,680
there we have this first thing in the

10364
07:17:02,398 --> 07:17:06,878
brackets which is uh inner row a so it's

10365
07:17:05,680 --> 07:17:10,200
going to be a certain row that we're

10366
07:17:06,878 --> 07:17:13,600
looking at plus a load offset right so

10367
07:17:10,200 --> 07:17:16,080
the load offset is going to iterate up

10368
07:17:13,600 --> 07:17:19,440
to BM which in this case is

10369
07:17:16,080 --> 07:17:21,360
128 so you can think of it as us having

10370
07:17:19,440 --> 07:17:25,240
this

10371
07:17:21,360 --> 07:17:25,240
um this is the a

10372
07:17:26,600 --> 07:17:32,680
tile

10373
07:17:28,398 --> 07:17:36,240
right and so this is going to

10374
07:17:32,680 --> 07:17:36,240
be this going to be

10375
07:17:36,718 --> 07:17:42,600
BM and that's going to be BK right so

10376
07:17:40,520 --> 07:17:45,600
inside of here we're going to iterate up

10377
07:17:42,600 --> 07:17:50,558
to BM in strides of stride a now stride

10378
07:17:45,600 --> 07:17:52,600
a is size 32 right stri a is 32 so if we

10379
07:17:50,558 --> 07:17:53,798
actually look at how many times it's

10380
07:17:52,600 --> 07:17:55,680
going to stride through this until it

10381
07:17:53,798 --> 07:17:59,398
reaches the end it's going to start at

10382
07:17:55,680 --> 07:18:02,760
zero and then it's going to go down to

10383
07:17:59,398 --> 07:18:02,760
32 I can

10384
07:18:04,360 --> 07:18:07,398
and then it's going to go down to

10385
07:18:08,920 --> 07:18:14,718
64 and then it's going to go down to uh

10386
07:18:12,280 --> 07:18:14,718
I believe

10387
07:18:17,200 --> 07:18:20,558
96 and then it's going to stop right

10388
07:18:19,280 --> 07:18:22,080
it's going to it's it's not going to

10389
07:18:20,558 --> 07:18:24,040
actually hit this it's going to stop

10390
07:18:22,080 --> 07:18:26,120
there so we're going to have this this

10391
07:18:24,040 --> 07:18:27,680
initial offet of zero and then it's

10392
07:18:26,120 --> 07:18:29,360
going to bump up to 32 in the next

10393
07:18:27,680 --> 07:18:33,840
iteration and it's going to jump to 64

10394
07:18:29,360 --> 07:18:35,320
and then 96 so notice the actual area

10395
07:18:33,840 --> 07:18:37,040
that we have to fill in here right

10396
07:18:35,320 --> 07:18:38,478
notice the actual area we have to fill

10397
07:18:37,040 --> 07:18:41,478
in so this

10398
07:18:38,478 --> 07:18:41,478
area

10399
07:18:44,160 --> 07:18:49,798
is

10400
07:18:46,878 --> 07:18:52,200
32 and then this BK we already know is

10401
07:18:49,798 --> 07:18:52,200
eight

10402
07:18:54,240 --> 07:18:58,680
right and it's going to be the same for

10403
07:18:56,280 --> 07:19:00,920
this and same for that and same for

10404
07:18:58,680 --> 07:19:04,040
that so this is actually where where

10405
07:19:00,920 --> 07:19:07,680
some really cool stuff comes in now this

10406
07:19:04,040 --> 07:19:11,040
this inner row a here this inner row a

10407
07:19:07,680 --> 07:19:13,040
is calculated as the thread idx divided

10408
07:19:11,040 --> 07:19:16,200
BK

10409
07:19:13,040 --> 07:19:21,240
right so this value if we do the math

10410
07:19:16,200 --> 07:19:21,240
here idx is say Max is out at

10411
07:19:22,878 --> 07:19:32,638
256 my marker I need to get

10412
07:19:26,080 --> 07:19:32,638
a 256 ID BK which is a right

10413
07:19:32,878 --> 07:19:37,798
now this number is going to max out at

10414
07:19:35,360 --> 07:19:40,240
32 and that's going to be our row right

10415
07:19:37,798 --> 07:19:43,520
so remember this if it's

10416
07:19:40,240 --> 07:19:45,840
32 this number this this inner row which

10417
07:19:43,520 --> 07:19:48,878
is based off of the thread itself is

10418
07:19:45,840 --> 07:19:51,320
going to max out at 32 now this inner

10419
07:19:48,878 --> 07:19:54,958
column on the other hand is going to do

10420
07:19:51,320 --> 07:19:56,600
mod BK right so when we do this it's

10421
07:19:54,958 --> 07:19:59,000
essentially going to it's going to go

10422
07:19:56,600 --> 07:20:00,760
like 0 1 2 3 all the way up to eight and

10423
07:19:59,000 --> 07:20:01,958
then once it hits eight again it's it's

10424
07:20:00,760 --> 07:20:04,360
going to jump rack zero and it's going

10425
07:20:01,958 --> 07:20:06,200
to reset so it's going to be like 0

10426
07:20:04,360 --> 07:20:08,040
through 8 and then reset 0 through 8 and

10427
07:20:06,200 --> 07:20:11,080
reset right and then we're going to have

10428
07:20:08,040 --> 07:20:13,320
these these rows uh the row index that

10429
07:20:11,080 --> 07:20:15,320
goes from 0 to 32 right so every time we

10430
07:20:13,320 --> 07:20:17,280
hit the next eight it's going to it's

10431
07:20:15,320 --> 07:20:19,320
going to bump up one

10432
07:20:17,280 --> 07:20:22,160
right and so this actually makes a lot

10433
07:20:19,320 --> 07:20:24,280
of sense so inside of here we have this

10434
07:20:22,160 --> 07:20:27,600
load offset which is going to be strided

10435
07:20:24,280 --> 07:20:30,320
by this much this 32 here and then we

10436
07:20:27,600 --> 07:20:32,280
add whatever this row offset is so we

10437
07:20:30,320 --> 07:20:34,718
have the row offset plus whatever that

10438
07:20:32,280 --> 07:20:37,040
number is we can effectively populate

10439
07:20:34,718 --> 07:20:39,160
this entire area here so it's like the

10440
07:20:37,040 --> 07:20:42,000
zero or the or say it's like I don't

10441
07:20:39,160 --> 07:20:44,558
know 64 and then plus whichever row it's

10442
07:20:42,000 --> 07:20:48,320
at from the thread index itself which we

10443
07:20:44,558 --> 07:20:52,320
which we calculate up here

10444
07:20:48,320 --> 07:20:53,600
um it's going to be that times the K

10445
07:20:52,320 --> 07:20:55,878
Dimension so it's that's why it's going

10446
07:20:53,600 --> 07:20:57,638
to stride over as many rows as it needs

10447
07:20:55,878 --> 07:21:00,080
to and it's going to end up at some

10448
07:20:57,638 --> 07:21:02,000
certain index and then once we multiply

10449
07:21:00,080 --> 07:21:04,680
that we know where it's at vertically

10450
07:21:02,000 --> 07:21:07,638
and then we simply add the inner column

10451
07:21:04,680 --> 07:21:09,040
index to that so the inner column index

10452
07:21:07,638 --> 07:21:11,958
like I said before is going to cap out

10453
07:21:09,040 --> 07:21:14,398
at 8 right so it's going to cap out at 8

10454
07:21:11,958 --> 07:21:17,280
and it's going to land somewhere here so

10455
07:21:14,398 --> 07:21:21,760
by doing this we only actually iterate

10456
07:21:17,280 --> 07:21:24,760
over this four times and each thread

10457
07:21:21,760 --> 07:21:28,200
spanning you know 32 * 8 CU if you

10458
07:21:24,760 --> 07:21:32,520
actually do 32 * 8 that's

10459
07:21:28,200 --> 07:21:32,520
like 2 5

10460
07:21:33,680 --> 07:21:42,200
2 5 is 32 and then uh two uh 2 the 3 is

10461
07:21:38,680 --> 07:21:46,638
8 so 5 + 3 with our exponent laws that's

10462
07:21:42,200 --> 07:21:48,760
2 to the 8 which is int 8 maps to 256

10463
07:21:46,638 --> 07:21:49,878
right so that that's how I kind of make

10464
07:21:48,760 --> 07:21:52,878
those associations you don't have to

10465
07:21:49,878 --> 07:21:55,840
follow along but 8 * 32 is

10466
07:21:52,878 --> 07:21:57,798
256 look how awesome that is each thread

10467
07:21:55,840 --> 07:22:00,280
is going to occupy an individual spot in

10468
07:21:57,798 --> 07:22:02,200
here so we're going to load this Chunk

10469
07:22:00,280 --> 07:22:05,080
in each thread is going to take a little

10470
07:22:02,200 --> 07:22:06,600
spot in there 32 * 8 and then we load in

10471
07:22:05,080 --> 07:22:09,600
the next one each thread takes its own

10472
07:22:06,600 --> 07:22:11,000
spot so it's like one operation for

10473
07:22:09,600 --> 07:22:12,478
every single iteration right so we don't

10474
07:22:11,000 --> 07:22:16,360
have to go sequentially through it each

10475
07:22:12,478 --> 07:22:18,320
thread is just boom done it's one

10476
07:22:16,360 --> 07:22:20,120
instruction same for this same for this

10477
07:22:18,320 --> 07:22:22,558
same for

10478
07:22:20,120 --> 07:22:24,080
this and then we just index accordingly

10479
07:22:22,558 --> 07:22:26,558
we just kind of adjust it so instead of

10480
07:22:24,080 --> 07:22:28,638
BK being the being the stride over you

10481
07:22:26,558 --> 07:22:30,160
know we do we times K so it's able to

10482
07:22:28,638 --> 07:22:32,878
stride over the entire thing when we're

10483
07:22:30,160 --> 07:22:34,320
actually loading from the GPU vram um

10484
07:22:32,878 --> 07:22:36,440
because it's it's actually bigger right

10485
07:22:34,320 --> 07:22:39,080
so the these matrices are much bigger

10486
07:22:36,440 --> 07:22:40,958
when we're when they aren't um tiles yet

10487
07:22:39,080 --> 07:22:43,080
so we have to stride like the whole K

10488
07:22:40,958 --> 07:22:45,680
distance instead of just the the the the

10489
07:22:43,080 --> 07:22:48,958
tile K

10490
07:22:45,680 --> 07:22:53,240
distance and then same idea applies to

10491
07:22:48,958 --> 07:22:55,240
um BS right the the the not BS

10492
07:22:53,240 --> 07:22:59,558
bshar

10493
07:22:55,240 --> 07:22:59,558
not not that

10494
07:23:01,878 --> 07:23:06,318
if I get up for a

10495
07:23:03,440 --> 07:23:08,958
second and look at what BS looks like

10496
07:23:06,318 --> 07:23:08,958
it's literally

10497
07:23:10,840 --> 07:23:15,878
this let me actually get a different

10498
07:23:12,878 --> 07:23:15,878
marker

10499
07:23:17,478 --> 07:23:23,760
terrible it's going to be like this go

10500
07:23:20,638 --> 07:23:27,280
down this way right that's what that's

10501
07:23:23,760 --> 07:23:30,040
what BS is going to look like now if we

10502
07:23:27,280 --> 07:23:32,398
look at if we look at these values

10503
07:23:30,040 --> 07:23:36,680
inside we have this load offset which is

10504
07:23:32,398 --> 07:23:37,920
going to iterate up to um up to up to BK

10505
07:23:36,680 --> 07:23:39,958
right and

10506
07:23:37,920 --> 07:23:43,318
BK is

10507
07:23:39,958 --> 07:23:46,318
this remember it's not it's not this

10508
07:23:43,318 --> 07:23:49,840
part anymore it's it's the side and then

10509
07:23:46,318 --> 07:23:52,318
this part is BN right because it's K byn

10510
07:23:49,840 --> 07:23:55,240
as the B Matrix

10511
07:23:52,318 --> 07:23:57,440
so when we when we iterate in these

10512
07:23:55,240 --> 07:24:02,000
strides it's actually going to end up

10513
07:23:57,440 --> 07:24:04,680
being um well BN in this case is 128 so

10514
07:24:02,000 --> 07:24:08,120
it's going to be this uh this stride

10515
07:24:04,680 --> 07:24:12,240
value is going to be um or stride B

10516
07:24:08,120 --> 07:24:16,240
sorry is going to be 256 / 128 which is

10517
07:24:12,240 --> 07:24:20,680
two right as we calculated um down

10518
07:24:16,240 --> 07:24:23,360
here so it's actually going to stride um

10519
07:24:20,680 --> 07:24:23,360
it's actually going to

10520
07:24:23,440 --> 07:24:30,558
stride it's going to stride two right

10521
07:24:26,318 --> 07:24:32,558
and two times the length of this is

10522
07:24:30,558 --> 07:24:34,440
actually a quarter right so if you have

10523
07:24:32,558 --> 07:24:37,718
eight values you have eight different

10524
07:24:34,440 --> 07:24:40,600
eight different rows you split it

10525
07:24:37,718 --> 07:24:42,638
once now you're in half and there's four

10526
07:24:40,600 --> 07:24:45,558
rows here and four rows there now if you

10527
07:24:42,638 --> 07:24:45,558
split it in half

10528
07:24:46,398 --> 07:24:50,718
again then you have two and two and two

10529
07:24:49,240 --> 07:24:53,040
and two all that that up adds up to

10530
07:24:50,718 --> 07:24:55,040
eight so when we actually stride we're

10531
07:24:53,040 --> 07:24:56,760
just going a quarter of the way down

10532
07:24:55,040 --> 07:24:59,558
same thing in here we go a quarter of

10533
07:24:56,760 --> 07:25:01,958
the way quarter quarter right same idea

10534
07:24:59,558 --> 07:25:03,958
applies here so whatever this stride

10535
07:25:01,958 --> 07:25:05,520
value is it's going to start at zero and

10536
07:25:03,958 --> 07:25:09,120
it's going to go up to six because it's

10537
07:25:05,520 --> 07:25:12,318
going to it's going to cap out at

10538
07:25:09,120 --> 07:25:15,360
um it's going to cap up at B whatever BK

10539
07:25:12,318 --> 07:25:18,558
was in this case is eight and it's just

10540
07:25:15,360 --> 07:25:22,000
going to stop so it's going to go 0 0 2

10541
07:25:18,558 --> 07:25:26,318
4 6 and it's going to stop right then we

10542
07:25:22,000 --> 07:25:29,318
have this inner row b which is from here

10543
07:25:26,318 --> 07:25:32,920
and this is just the same idea so inside

10544
07:25:29,318 --> 07:25:35,280
of this we have the thread idx um in

10545
07:25:32,920 --> 07:25:37,440
this case it it's just going to max out

10546
07:25:35,280 --> 07:25:41,318
at

10547
07:25:37,440 --> 07:25:41,318
256 and we divide that

10548
07:25:41,718 --> 07:25:47,240
number we divide that number by BN which

10549
07:25:44,638 --> 07:25:47,240
in this case is

10550
07:25:49,318 --> 07:25:55,318
128 and the column is the same idea it's

10551
07:25:52,318 --> 07:25:56,638
going to go zero all the way up to 128

10552
07:25:55,318 --> 07:25:57,958
and then it's going to reset right so

10553
07:25:56,638 --> 07:26:01,558
it's not just going to iterate every

10554
07:25:57,958 --> 07:26:04,360
time we stri 128 it's going to to uh

10555
07:26:01,558 --> 07:26:09,280
it's going to go up it's going to

10556
07:26:04,360 --> 07:26:12,120
essentially one mod 128 is one 127 28 is

10557
07:26:09,280 --> 07:26:15,600
127 right so that's that's literally all

10558
07:26:12,120 --> 07:26:15,600
it's going to do here

10559
07:26:15,798 --> 07:26:20,080
um in case you haven't noticed already

10560
07:26:18,478 --> 07:26:21,878
this is the same idea as here except

10561
07:26:20,080 --> 07:26:23,958
we're just dealing with kind of like a a

10562
07:26:21,878 --> 07:26:25,840
more like very stretched out thing

10563
07:26:23,958 --> 07:26:29,478
instead of instead of like these nice to

10564
07:26:25,840 --> 07:26:31,680
look at I candy looking blocks right so

10565
07:26:29,478 --> 07:26:33,760
the idea here is is that we just go

10566
07:26:31,680 --> 07:26:37,040
however much we need to in here CU this

10567
07:26:33,760 --> 07:26:39,398
this total thing is like 128 and 128

10568
07:26:37,040 --> 07:26:40,878
that totals up to 256 so it's

10569
07:26:39,398 --> 07:26:43,160
essentially each thread gets its own

10570
07:26:40,878 --> 07:26:45,040
little space in there one thread two

10571
07:26:43,160 --> 07:26:46,958
thread three thread four

10572
07:26:45,040 --> 07:26:48,718
thread right they all get their own

10573
07:26:46,958 --> 07:26:51,160
little piece in there and they're able

10574
07:26:48,718 --> 07:26:52,160
to load in quarters downwards right that

10575
07:26:51,160 --> 07:26:54,878
that's that's essentially how we're

10576
07:26:52,160 --> 07:26:57,840
loading into Shar memory so just given

10577
07:26:54,878 --> 07:26:59,878
this new context we're just loading um

10578
07:26:57,840 --> 07:27:01,318
you know we're just being clever about

10579
07:26:59,878 --> 07:27:04,240
how we load

10580
07:27:01,318 --> 07:27:05,360
things so given that we understand how

10581
07:27:04,240 --> 07:27:06,920
it's everything is loaded into share

10582
07:27:05,360 --> 07:27:08,200
memory we can go and jump into the next

10583
07:27:06,920 --> 07:27:09,680
part here and this part is like where

10584
07:27:08,200 --> 07:27:11,360
things get a little it's a little funny

10585
07:27:09,680 --> 07:27:13,958
it's not actually as as intuitive and

10586
07:27:11,360 --> 07:27:15,798
bad as this part um but it is still a

10587
07:27:13,958 --> 07:27:20,040
little bit weird with the indexing part

10588
07:27:15,798 --> 07:27:23,000
so if we jump into this it's we iterate

10589
07:27:20,040 --> 07:27:25,080
over this idx right and this idx going

10590
07:27:23,000 --> 07:27:28,000
back to this is literally just it's it's

10591
07:27:25,080 --> 07:27:29,160
going to evolve right so idx index0 1 2

10592
07:27:28,000 --> 07:27:30,718
and three right they're going they're

10593
07:27:29,160 --> 07:27:32,120
going inwards like this

10594
07:27:30,718 --> 07:27:34,680
uh one thread is in charge of this

10595
07:27:32,120 --> 07:27:36,840
little block at the center um and the

10596
07:27:34,680 --> 07:27:40,040
threat the each thread is is responsible

10597
07:27:36,840 --> 07:27:44,080
for uh loading in a column and a row

10598
07:27:40,040 --> 07:27:47,718
right one thread takes care of that

10599
07:27:44,080 --> 07:27:49,200
um now if we look inside of here we load

10600
07:27:47,718 --> 07:27:51,120
into the registers right so the

10601
07:27:49,200 --> 07:27:52,478
registers are the extremely fast pieces

10602
07:27:51,120 --> 07:27:56,558
of storage that are literally right next

10603
07:27:52,478 --> 07:27:58,280
to the core in in the in the GPU and so

10604
07:27:56,558 --> 07:28:01,120
we have this reg M right which is going

10605
07:27:58,280 --> 07:28:03,478
to load that column in so how do we load

10606
07:28:01,120 --> 07:28:06,318
this we have to look at this thread row

10607
07:28:03,478 --> 07:28:08,318
right and to understand everything else

10608
07:28:06,318 --> 07:28:10,760
let's go look at thread row here so

10609
07:28:08,318 --> 07:28:14,080
where is this thread row so it's the

10610
07:28:10,760 --> 07:28:18,000
thread ID x.x so this could be a maximum

10611
07:28:14,080 --> 07:28:23,280
of you know

10612
07:28:18,000 --> 07:28:29,160
256 and then we divide that by BN / TN

10613
07:28:23,280 --> 07:28:35,240
right so BN is 128 and TN is 8 so you

10614
07:28:29,160 --> 07:28:39,798
divide 128 by 8 and that's 16 so we have

10615
07:28:35,240 --> 07:28:41,398
256 divided or 0 to 255 and then

10616
07:28:39,798 --> 07:28:47,200
whichever one of those it is divide that

10617
07:28:41,398 --> 07:28:49,840
by 16 right so inside of this we end up

10618
07:28:47,200 --> 07:28:51,280
having 16 different numbers that could

10619
07:28:49,840 --> 07:28:54,600
we could be through right so if we look

10620
07:28:51,280 --> 07:28:56,520
at this original atile Here There are 16

10621
07:28:54,600 --> 07:28:59,760
different

10622
07:28:56,520 --> 07:29:01,798
um There are 16 different rows we could

10623
07:28:59,760 --> 07:29:03,718
be right and these are just offsets keep

10624
07:29:01,798 --> 07:29:06,718
in mind we're loading in columns that

10625
07:29:03,718 --> 07:29:10,200
are eight elements long so we have like

10626
07:29:06,718 --> 07:29:11,280
16 16 16 all the way down uh we have

10627
07:29:10,200 --> 07:29:13,638
this eight

10628
07:29:11,280 --> 07:29:17,798
times um

10629
07:29:13,638 --> 07:29:21,760
and each of these or no sorry 16 times

10630
07:29:17,798 --> 07:29:23,760
um each each each little column here

10631
07:29:21,760 --> 07:29:26,360
this little like colored in area you

10632
07:29:23,760 --> 07:29:28,798
could say that is going to be eight long

10633
07:29:26,360 --> 07:29:32,120
right so we essentially are loading in

10634
07:29:28,798 --> 07:29:36,718
like we go eight that's like that's like

10635
07:29:32,120 --> 07:29:38,520
a single um that is a single column and

10636
07:29:36,718 --> 07:29:39,920
then whichever one comes next it's like

10637
07:29:38,520 --> 07:29:42,638
this is another eight and we go all the

10638
07:29:39,920 --> 07:29:46,478
way down 16 times and that multiplies up

10639
07:29:42,638 --> 07:29:49,680
to the total length of M which is 128 so

10640
07:29:46,478 --> 07:29:51,080
the math is mathing um the same thing

10641
07:29:49,680 --> 07:29:52,798
applies to thread column right I mean

10642
07:29:51,080 --> 07:29:54,280
these are these are square matrices so

10643
07:29:52,798 --> 07:29:58,080
it's not actually too bad to deal with

10644
07:29:54,280 --> 07:30:00,360
this um so same idea here um and we end

10645
07:29:58,080 --> 07:30:02,840
up with some range between 0 and 50

10646
07:30:00,360 --> 07:30:08,718
right so if we go back

10647
07:30:02,840 --> 07:30:13,760
down thread row time TM so TM is

10648
07:30:08,718 --> 07:30:16,160
uh TM is which uh like TM is essentially

10649
07:30:13,760 --> 07:30:18,080
eight right so it's going to be

10650
07:30:16,160 --> 07:30:21,160
whichever thread row we're at so which

10651
07:30:18,080 --> 07:30:22,878
whichever one of these out of um out of

10652
07:30:21,160 --> 07:30:28,040
out of 16 are we

10653
07:30:22,878 --> 07:30:31,080
at and then times um times

10654
07:30:28,040 --> 07:30:33,718
TM plus I and I is going to be that

10655
07:30:31,080 --> 07:30:35,360
little iterator here that goes up to TM

10656
07:30:33,718 --> 07:30:37,958
right TM is like that the length of that

10657
07:30:35,360 --> 07:30:39,280
column or the like yeah you could say

10658
07:30:37,958 --> 07:30:41,878
the height or the the length of that

10659
07:30:39,280 --> 07:30:43,360
column it's going to iterate up to I and

10660
07:30:41,878 --> 07:30:46,318
it's going to put that into the register

10661
07:30:43,360 --> 07:30:48,440
M right we multiply this whole thing by

10662
07:30:46,318 --> 07:30:50,360
BK so that we get this offset going

10663
07:30:48,440 --> 07:30:52,440
because we have to go through this k

10664
07:30:50,360 --> 07:30:53,760
thing and reset every single time we

10665
07:30:52,440 --> 07:30:56,200
want to get a new column you can't just

10666
07:30:53,760 --> 07:30:58,558
go directly down you have to stride one

10667
07:30:56,200 --> 07:31:01,478
entire length over to get to the next

10668
07:30:58,558 --> 07:31:03,240
one and then we add idx to this so This

10669
07:31:01,478 --> 07:31:06,600
actually becomes very intuitive when we

10670
07:31:03,240 --> 07:31:08,200
look at it from a glance so hopefully

10671
07:31:06,600 --> 07:31:12,000
this is sort of hopefully this is sort

10672
07:31:08,200 --> 07:31:13,680
of making sense in your head um but this

10673
07:31:12,000 --> 07:31:16,080
idx is just going to evolve us it's

10674
07:31:13,680 --> 07:31:17,440
going to be that that horizontal offset

10675
07:31:16,080 --> 07:31:18,878
and then this is the vertical offset

10676
07:31:17,440 --> 07:31:21,040
right this whole portion here that's the

10677
07:31:18,878 --> 07:31:24,478
vertical offset

10678
07:31:21,040 --> 07:31:26,200
um and then we apply the same concept to

10679
07:31:24,478 --> 07:31:28,600
loading into register n right so let

10680
07:31:26,200 --> 07:31:30,360
register n isn't as bad um because we're

10681
07:31:28,600 --> 07:31:33,120
actually loading uh we're actually

10682
07:31:30,360 --> 07:31:35,798
loading horizontally so it's going to be

10683
07:31:33,120 --> 07:31:39,520
the idx which by the way idx is going

10684
07:31:35,798 --> 07:31:41,558
downwards now so idx times BN BN is the

10685
07:31:39,520 --> 07:31:46,280
length of it right so you're going to go

10686
07:31:41,558 --> 07:31:48,958
down um whichever idx you're at

10687
07:31:46,280 --> 07:31:51,840
um you're going to go down that many

10688
07:31:48,958 --> 07:31:54,280
layers and then you're going to do uh

10689
07:31:51,840 --> 07:31:56,160
whichever whichever thread column you're

10690
07:31:54,280 --> 07:31:58,120
at so each thread is going to have its

10691
07:31:56,160 --> 07:31:59,680
own uh it's going to have its own little

10692
07:31:58,120 --> 07:32:01,280
section of that right it's going to have

10693
07:31:59,680 --> 07:32:03,680
it own job cuz the thread is like

10694
07:32:01,280 --> 07:32:06,600
loading a specific a specific Square in

10695
07:32:03,680 --> 07:32:10,200
that tile so thread here thread here

10696
07:32:06,600 --> 07:32:12,398
thread here thread here it's like

10697
07:32:10,200 --> 07:32:13,680
yeah each thread has its own thing so

10698
07:32:12,398 --> 07:32:16,040
we're just worrying about a single

10699
07:32:13,680 --> 07:32:19,200
thread

10700
07:32:16,040 --> 07:32:22,600
um thread thread is like you know thread

10701
07:32:19,200 --> 07:32:24,080
one is like here and then thread 255 is

10702
07:32:22,600 --> 07:32:25,840
or thread zero is here and then thread

10703
07:32:24,080 --> 07:32:27,558
255 is here because it goes to like the

10704
07:32:25,840 --> 07:32:30,160
very edge and then the very edge and

10705
07:32:27,558 --> 07:32:31,200
then they intersect at 256 right that

10706
07:32:30,160 --> 07:32:33,680
that's how that's how I'm sort of

10707
07:32:31,200 --> 07:32:34,840
visualizing this now we have this extra

10708
07:32:33,680 --> 07:32:36,680
I term which is literally just the

10709
07:32:34,840 --> 07:32:38,398
horizontal offset so when you when

10710
07:32:36,680 --> 07:32:40,680
you've like looped around how many

10711
07:32:38,398 --> 07:32:42,760
whatever amount of do ID EXs you need

10712
07:32:40,680 --> 07:32:44,080
then you have that additional I which is

10713
07:32:42,760 --> 07:32:45,600
going to be the offset so you're going

10714
07:32:44,080 --> 07:32:46,920
to load in the first first element in

10715
07:32:45,600 --> 07:32:48,280
the row then the second then the third

10716
07:32:46,920 --> 07:32:49,680
then the fourth and then the same

10717
07:32:48,280 --> 07:32:51,520
applies for this column up here which we

10718
07:32:49,680 --> 07:32:53,520
already did it's just going to do 1 2 3

10719
07:32:51,520 --> 07:32:55,080
4 all the way down to eight right and

10720
07:32:53,520 --> 07:32:57,558
it's going to store this out in like a

10721
07:32:55,080 --> 07:32:59,398
register memory like a line and this

10722
07:32:57,558 --> 07:33:02,718
allows us to easily do product that

10723
07:32:59,398 --> 07:33:05,478
right so when we drop down to here um

10724
07:33:02,718 --> 07:33:08,798
this is the entire Loop yellow to Yellow

10725
07:33:05,478 --> 07:33:11,120
um we start off with the the M component

10726
07:33:08,798 --> 07:33:13,440
right so thread m is this part and then

10727
07:33:11,120 --> 07:33:16,600
thread n is this part so this this n

10728
07:33:13,440 --> 07:33:19,000
component is actually inside of it now

10729
07:33:16,600 --> 07:33:22,160
the thread result is calculated as res

10730
07:33:19,000 --> 07:33:26,200
idx M so whichever however much it is

10731
07:33:22,160 --> 07:33:28,200
through uh TM right that times TN which

10732
07:33:26,200 --> 07:33:29,920
is the which is the horizontal stride

10733
07:33:28,200 --> 07:33:32,760
that you need to do to get to the next

10734
07:33:29,920 --> 07:33:35,680
the next um the next row

10735
07:33:32,760 --> 07:33:40,440
right and then you have this TN as well

10736
07:33:35,680 --> 07:33:40,440
so TN is how how far along you are so

10737
07:33:41,240 --> 07:33:45,440
it's or sorry res idx is how far along

10738
07:33:43,798 --> 07:33:48,000
you are uh so this is going to be your

10739
07:33:45,440 --> 07:33:50,160
your vertical stride and then this is

10740
07:33:48,000 --> 07:33:51,558
going to be the horizontal offset right

10741
07:33:50,160 --> 07:33:53,520
so that's how that's we mean we're

10742
07:33:51,558 --> 07:33:55,360
storing it in linear memory but this is

10743
07:33:53,520 --> 07:34:01,718
this is how we're going to index into it

10744
07:33:55,360 --> 07:34:01,718
right and then we do whatever that is um

10745
07:34:02,080 --> 07:34:06,200
and this keep in mind this is an

10746
07:34:03,520 --> 07:34:08,318
individual um this an individual grid

10747
07:34:06,200 --> 07:34:11,440
right say you're you're looking at a

10748
07:34:08,318 --> 07:34:13,760
point within that grid you're doing um

10749
07:34:11,440 --> 07:34:15,840
essentially a DOT product across those

10750
07:34:13,760 --> 07:34:18,638
so you have to iterate through you know

10751
07:34:15,840 --> 07:34:19,760
eight and then eight again so it's 64

10752
07:34:18,638 --> 07:34:22,798
iterations you have to go through

10753
07:34:19,760 --> 07:34:24,398
filling up that entire tile um and then

10754
07:34:22,798 --> 07:34:27,398
you just use you know as you would

10755
07:34:24,398 --> 07:34:29,440
expect that that value whatever it is so

10756
07:34:27,398 --> 07:34:31,478
you're just essentially just crossing

10757
07:34:29,440 --> 07:34:33,878
them you're finding where they intersect

10758
07:34:31,478 --> 07:34:36,280
and then you're setting whatever this is

10759
07:34:33,878 --> 07:34:39,398
in that so it end is just becoming this

10760
07:34:36,280 --> 07:34:42,200
entire uh Matrix light out it's like

10761
07:34:39,398 --> 07:34:43,798
instead of being uh loaded like this you

10762
07:34:42,200 --> 07:34:46,040
just take this row and you attach it to

10763
07:34:43,798 --> 07:34:47,680
the end and then this attach it to there

10764
07:34:46,040 --> 07:34:50,398
and then this one it's even further

10765
07:34:47,680 --> 07:34:51,878
right that's all we're doing there so

10766
07:34:50,398 --> 07:34:54,840
it's like literally as you would imagine

10767
07:34:51,878 --> 07:34:56,240
in your head this is how it's working um

10768
07:34:54,840 --> 07:34:57,680
it's just it's just important to

10769
07:34:56,240 --> 07:34:59,080
actually highlight like what the

10770
07:34:57,680 --> 07:35:01,080
indexing is actually doing instead of

10771
07:34:59,080 --> 07:35:02,520
just trusting that it works it's really

10772
07:35:01,080 --> 07:35:04,440
important to actually dig deep into what

10773
07:35:02,520 --> 07:35:05,920
this is doing under the hood so I

10774
07:35:04,440 --> 07:35:07,840
encourage you if this doesn't entirely

10775
07:35:05,920 --> 07:35:09,120
make sense it's very intuitive I

10776
07:35:07,840 --> 07:35:11,040
encourage you to test it with your own

10777
07:35:09,120 --> 07:35:12,680
examples so even just like get a piece

10778
07:35:11,040 --> 07:35:14,680
of paper write down on a whiteboard

10779
07:35:12,680 --> 07:35:15,958
whatever you need to do um and just and

10780
07:35:14,680 --> 07:35:17,920
just sort of write this out and try to

10781
07:35:15,958 --> 07:35:20,080
visualize it through each step right so

10782
07:35:17,920 --> 07:35:21,878
you can even set for example TM to and

10783
07:35:20,080 --> 07:35:23,440
TN to four right you can make it much

10784
07:35:21,878 --> 07:35:25,398
easier on yourself you don't have to go

10785
07:35:23,440 --> 07:35:26,958
to the full extent that we're using with

10786
07:35:25,398 --> 07:35:28,760
you know our parameters like eight and

10787
07:35:26,958 --> 07:35:30,638
two 128 here you don't have to even go

10788
07:35:28,760 --> 07:35:32,958
that far you can be very simple about

10789
07:35:30,638 --> 07:35:35,920
how you exercise that

10790
07:35:32,958 --> 07:35:38,920
um but yeah so this is actually how we

10791
07:35:35,920 --> 07:35:41,478
calculate the the individual thread tile

10792
07:35:38,920 --> 07:35:43,718
so the This Thread tile this little 2D

10793
07:35:41,478 --> 07:35:45,318
thing inside of the bigger block tile

10794
07:35:43,718 --> 07:35:48,760
and we calculate one of those per thread

10795
07:35:45,318 --> 07:35:53,520
right so 256 threads are laid out um so

10796
07:35:48,760 --> 07:35:56,840
it's it's like 0 to 16 0 to I think

10797
07:35:53,520 --> 07:35:58,440
240 and then over here it's 256 that

10798
07:35:56,840 --> 07:36:00,080
that it might be like shifted based on

10799
07:35:58,440 --> 07:36:02,398
how you're seeing that the picture in my

10800
07:36:00,080 --> 07:36:05,040
hand waving but that's the idea is you

10801
07:36:02,398 --> 07:36:08,440
go from from0

10802
07:36:05,040 --> 07:36:10,600
to 256 right let me just make sure

10803
07:36:08,440 --> 07:36:12,878
everything is uh synced up you know we

10804
07:36:10,600 --> 07:36:14,920
make sure that all these are done um you

10805
07:36:12,878 --> 07:36:16,840
know as we're iterating through all

10806
07:36:14,920 --> 07:36:18,798
these all of these block tiles right we

10807
07:36:16,840 --> 07:36:21,638
have to go like on the on the bigger

10808
07:36:18,798 --> 07:36:24,280
matrices A and B we actually have to

10809
07:36:21,638 --> 07:36:26,120
take these tiles and we have to uh move

10810
07:36:24,280 --> 07:36:27,920
them closer together right so this is

10811
07:36:26,120 --> 07:36:29,680
what this whole Loop is doing we want to

10812
07:36:27,920 --> 07:36:34,080
make sure everything is synced up both

10813
07:36:29,680 --> 07:36:36,280
after the uh shared memory um population

10814
07:36:34,080 --> 07:36:38,360
so after we we we populate those we want

10815
07:36:36,280 --> 07:36:39,920
to sync everything and then once we've

10816
07:36:38,360 --> 07:36:42,000
written once we've written all the

10817
07:36:39,920 --> 07:36:43,240
results here we also want to sync

10818
07:36:42,000 --> 07:36:45,398
everything up make sure all the threads

10819
07:36:43,240 --> 07:36:48,958
are caught up before we you know evolve

10820
07:36:45,398 --> 07:36:51,200
to the next one and mess with stuff um

10821
07:36:48,958 --> 07:36:52,680
so then we have this write out um and

10822
07:36:51,200 --> 07:36:56,920
this isn't actually too bad this part's

10823
07:36:52,680 --> 07:36:58,600
pretty good so inside of here we iterate

10824
07:36:56,920 --> 07:37:00,600
over the same things that we did here

10825
07:36:58,600 --> 07:37:02,600
when we were actually calculating uh

10826
07:37:00,600 --> 07:37:05,398
when we were we were multiplying those

10827
07:37:02,600 --> 07:37:07,440
those thread rows and columns those

10828
07:37:05,398 --> 07:37:11,080
little thread

10829
07:37:07,440 --> 07:37:12,600
tiles and if we just step through this

10830
07:37:11,080 --> 07:37:14,878
this is actually going to seem a little

10831
07:37:12,600 --> 07:37:17,558
bit weird at first but if we scroll up

10832
07:37:14,878 --> 07:37:20,680
remember that we advance everything to

10833
07:37:17,558 --> 07:37:23,318
the starting position given this thread

10834
07:37:20,680 --> 07:37:24,878
right or given this block rather so we

10835
07:37:23,318 --> 07:37:27,798
have these initial terms which are the

10836
07:37:24,878 --> 07:37:30,040
blocks these are which tiles we which

10837
07:37:27,798 --> 07:37:31,318
tile we actually care about within C

10838
07:37:30,040 --> 07:37:32,798
and this is already stored here so we

10839
07:37:31,318 --> 07:37:34,760
already know which tile we actually want

10840
07:37:32,798 --> 07:37:36,520
to worry about and the the memory

10841
07:37:34,760 --> 07:37:38,360
address has gone through it's skipped a

10842
07:37:36,520 --> 07:37:41,120
bunch of

10843
07:37:38,360 --> 07:37:42,440
spaces through just like integer

10844
07:37:41,120 --> 07:37:43,798
operations it's been multiplied and

10845
07:37:42,440 --> 07:37:45,558
added up to the point where it's where

10846
07:37:43,798 --> 07:37:47,798
we want it and then we just do

10847
07:37:45,558 --> 07:37:50,160
everything from there so we can if we

10848
07:37:47,798 --> 07:37:51,718
stride like an entire length K it'll go

10849
07:37:50,160 --> 07:37:53,878
here and then the remainder of it and

10850
07:37:51,718 --> 07:37:56,718
it'll end up back to where it is but

10851
07:37:53,878 --> 07:37:57,798
just like one one element lower right so

10852
07:37:56,718 --> 07:38:00,160
that that's really all we're working

10853
07:37:57,798 --> 07:38:02,680
with here now if we scroll back down

10854
07:38:00,160 --> 07:38:05,520
it's literally just like this end term

10855
07:38:02,680 --> 07:38:07,200
that's that's like the end term um that

10856
07:38:05,520 --> 07:38:08,120
that's the that's the horizontal part

10857
07:38:07,200 --> 07:38:11,280
right so that's the part that we're

10858
07:38:08,120 --> 07:38:12,680
actually striding over so if we have um

10859
07:38:11,280 --> 07:38:17,200
we're looking at the C

10860
07:38:12,680 --> 07:38:20,520
Matrix we have thread row times thread M

10861
07:38:17,200 --> 07:38:23,360
right or or TM which is

10862
07:38:20,520 --> 07:38:26,398
8 plus the res index remember we it's

10863
07:38:23,360 --> 07:38:27,440
the same same idea as as uh as what we

10864
07:38:26,398 --> 07:38:30,360
did up

10865
07:38:27,440 --> 07:38:32,718
there and so

10866
07:38:30,360 --> 07:38:35,120
when we are

10867
07:38:32,718 --> 07:38:38,398
um it's essentially the same as this

10868
07:38:35,120 --> 07:38:41,760
except we're doing I instead so you know

10869
07:38:38,398 --> 07:38:44,520
I is you could think of i as like up to

10870
07:38:41,760 --> 07:38:45,920
um you know TM it's like the same idea

10871
07:38:44,520 --> 07:38:49,160
we're just iterating through that that's

10872
07:38:45,920 --> 07:38:52,920
going to be the um offset inside of that

10873
07:38:49,160 --> 07:38:56,318
tile right so like relative to

10874
07:38:52,920 --> 07:38:58,200
um relative to the actual SE tow that

10875
07:38:56,318 --> 07:39:01,240
we're working on this is going to be

10876
07:38:58,200 --> 07:39:02,680
like the relative off set right um so

10877
07:39:01,240 --> 07:39:05,000
that's going to be

10878
07:39:02,680 --> 07:39:09,000
downwards time n right so that's going

10879
07:39:05,000 --> 07:39:11,638
to give us our our downwards um our

10880
07:39:09,000 --> 07:39:15,478
downwards movement and then to progress

10881
07:39:11,638 --> 07:39:18,798
sideways we have the red column time TN

10882
07:39:15,478 --> 07:39:21,160
right so an individual thread um

10883
07:39:18,798 --> 07:39:24,718
individual thread

10884
07:39:21,160 --> 07:39:26,718
times times that uh that TN length of

10885
07:39:24,718 --> 07:39:29,760
eight right so essentially we're going

10886
07:39:26,718 --> 07:39:32,160
to have a bunch of uh a bunch of threads

10887
07:39:29,760 --> 07:39:34,160
occupying like a square and those

10888
07:39:32,160 --> 07:39:37,600
threads are going

10889
07:39:34,160 --> 07:39:38,718
to um or no not not the threads

10890
07:39:37,600 --> 07:39:40,760
occupying a square but they're going to

10891
07:39:38,718 --> 07:39:42,160
occupy the whole thing and then threads

10892
07:39:40,760 --> 07:39:45,080
are each thread is going to iterate

10893
07:39:42,160 --> 07:39:46,958
through that TM and TN right and so we

10894
07:39:45,080 --> 07:39:49,558
have this this vertical offset we have

10895
07:39:46,958 --> 07:39:51,718
the horizontal offset and then plus that

10896
07:39:49,558 --> 07:39:53,000
additional little kick to the right

10897
07:39:51,718 --> 07:39:54,600
we're going to iter this is how much we

10898
07:39:53,000 --> 07:39:57,638
need to actually like stride over like

10899
07:39:54,600 --> 07:39:59,600
how big steps we we take um and then the

10900
07:39:57,638 --> 07:40:01,080
initial just like inside of that inside

10901
07:39:59,600 --> 07:40:02,840
of like one of these steps it's like how

10902
07:40:01,080 --> 07:40:05,360
much do you actually go forward right

10903
07:40:02,840 --> 07:40:07,360
how much do you do you add to

10904
07:40:05,360 --> 07:40:10,280
it and then we just use what we've

10905
07:40:07,360 --> 07:40:12,398
already computed so thread results um

10906
07:40:10,280 --> 07:40:15,840
same indexing scheme here this should be

10907
07:40:12,398 --> 07:40:17,920
fairly intuitive um we just multiply

10908
07:40:15,840 --> 07:40:19,840
this element wise know for each

10909
07:40:17,920 --> 07:40:22,000
iteration in the loop and then we have

10910
07:40:19,840 --> 07:40:24,080
our beta times you know element wise

10911
07:40:22,000 --> 07:40:27,600
this which is literally the same same

10912
07:40:24,080 --> 07:40:29,478
indexing scheme that we use here so

10913
07:40:27,600 --> 07:40:31,760
hopefully the the block tent kernel

10914
07:40:29,478 --> 07:40:33,878
sense now we're going to jump into well

10915
07:40:31,760 --> 07:40:36,638
actually before we actually jump into

10916
07:40:33,878 --> 07:40:41,558
the uh vectorized kernel I want to run

10917
07:40:36,638 --> 07:40:44,478
this so if we go sjem we go sjem 04

10918
07:40:41,558 --> 07:40:48,160
right this is this is block ping

10919
07:40:44,478 --> 07:40:50,520
normal this is regular 1D block tiling

10920
07:40:48,160 --> 07:40:55,440
right then if we step this up to number

10921
07:40:50,520 --> 07:40:58,120
five look at this 4800 gig flops on 496

10922
07:40:55,440 --> 07:41:00,760
right that's decent but if we step it up

10923
07:40:58,120 --> 07:41:03,200
to five

10924
07:41:00,760 --> 07:41:09,798
we double

10925
07:41:03,200 --> 07:41:14,120
it so if we do uh python we go at

10926
07:41:09,798 --> 07:41:14,120
9162 /

10927
07:41:14,718 --> 07:41:21,520
4873 got about 1.9x speed up right which

10928
07:41:17,920 --> 07:41:23,080
is really good now we compare to the

10929
07:41:21,520 --> 07:41:24,878
results

10930
07:41:23,080 --> 07:41:30,360
here

10931
07:41:24,878 --> 07:41:32,520
um so about about this time 1.9 roughly

10932
07:41:30,360 --> 07:41:33,760
is it's pretty close to 70 right so I

10933
07:41:32,520 --> 07:41:36,200
mean we're we're essentially getting the

10934
07:41:33,760 --> 07:41:39,280
same results here uh so everything is

10935
07:41:36,200 --> 07:41:42,160
working out we just doubled the speed by

10936
07:41:39,280 --> 07:41:44,360
using 2D block tiling instead of 1D and

10937
07:41:42,160 --> 07:41:46,718
now we're already about 2/3 of the way

10938
07:41:44,360 --> 07:41:49,920
to kuo Performance we're actually really

10939
07:41:46,718 --> 07:41:51,160
really high up there so now let's go

10940
07:41:49,920 --> 07:41:52,600
ahead and continue with vectorized

10941
07:41:51,160 --> 07:41:55,160
memory access which is going to give us

10942
07:41:52,600 --> 07:41:58,360
an additional little performance boost

10943
07:41:55,160 --> 07:41:59,760
okay awesome so now we have this new

10944
07:41:58,360 --> 07:42:02,878
colel to worry about

10945
07:41:59,760 --> 07:42:07,440
number six on vectorizing the shared

10946
07:42:02,878 --> 07:42:10,558
memory loads so if we look inside of

10947
07:42:07,440 --> 07:42:12,398
here essentially we do this we

10948
07:42:10,558 --> 07:42:16,318
everything in here Remains the

10949
07:42:12,398 --> 07:42:19,040
Same except we have this new float 4

10950
07:42:16,318 --> 07:42:20,638
type right so notice how we have this

10951
07:42:19,040 --> 07:42:22,520
this float 4 I'm going to just highlight

10952
07:42:20,638 --> 07:42:25,398
that and see where it shows up so it

10953
07:42:22,520 --> 07:42:28,040
shows up here when we're loading

10954
07:42:25,398 --> 07:42:30,318
into uh when we are loading into shared

10955
07:42:28,040 --> 07:42:32,760
memory right so a shared and B shared

10956
07:42:30,318 --> 07:42:35,160
right this is where it comes up as well

10957
07:42:32,760 --> 07:42:37,878
as when we're writing out the results so

10958
07:42:35,160 --> 07:42:40,440
when we're going in from Global vram

10959
07:42:37,878 --> 07:42:42,478
into shared memory we use float for

10960
07:42:40,440 --> 07:42:46,160
loads and then when we're writing out

10961
07:42:42,478 --> 07:42:47,958
from registers we load uh we load with

10962
07:42:46,160 --> 07:42:50,600
float FL as well right this is what's

10963
07:42:47,958 --> 07:42:52,318
happening here now before diving in

10964
07:42:50,600 --> 07:42:53,440
right into this I want to go over and

10965
07:42:52,318 --> 07:42:55,040
review just like what the heck this

10966
07:42:53,440 --> 07:42:56,958
float board does there's a lot of terms

10967
07:42:55,040 --> 07:42:58,638
like reinterpret cast and all this all

10968
07:42:56,958 --> 07:43:00,120
this weird symbols and stuff so let's

10969
07:42:58,638 --> 07:43:02,360
just like go and clarify what this all

10970
07:43:00,120 --> 07:43:04,160
means I wrote a separate file here

10971
07:43:02,360 --> 07:43:05,440
called float for. cuu you can write the

10972
07:43:04,160 --> 07:43:08,200
same thing but I'm just going to go over

10973
07:43:05,440 --> 07:43:12,520
this kind of Step by Step so we have an

10974
07:43:08,200 --> 07:43:15,840
array length n right 1 3 4 all

10975
07:43:12,520 --> 07:43:18,398
floats we have a host input host output

10976
07:43:15,840 --> 07:43:22,318
we initialize device input and output we

10977
07:43:18,398 --> 07:43:24,558
K like those with n * size of float we

10978
07:43:22,318 --> 07:43:26,520
kud M Copy that from host to

10979
07:43:24,558 --> 07:43:29,920
device host to

10980
07:43:26,520 --> 07:43:31,558
device we we you launch this with a

10981
07:43:29,920 --> 07:43:33,318
uh with a grid size of one there's a

10982
07:43:31,558 --> 07:43:34,840
single block and within that block

10983
07:43:33,318 --> 07:43:37,398
there's a single thread so it's just one

10984
07:43:34,840 --> 07:43:39,760
thread that's actually being used here

10985
07:43:37,398 --> 07:43:41,360
now we run this and then we copy back

10986
07:43:39,760 --> 07:43:45,398
and then we display these based on their

10987
07:43:41,360 --> 07:43:47,478
indices right so 0 1 2 3 then 0 1 2 3

10988
07:43:45,398 --> 07:43:49,080
the inputs and the outputs and now what

10989
07:43:47,478 --> 07:43:51,120
actually happens inside of here is what

10990
07:43:49,080 --> 07:43:52,840
we want to pay attention to so we pass

10991
07:43:51,120 --> 07:43:55,360
this this device input and the device

10992
07:43:52,840 --> 07:43:59,920
output in right these are Pointers two

10993
07:43:55,360 --> 07:44:01,478
arrays um output as well now we have idx

10994
07:43:59,920 --> 07:44:03,558
which thread index in this case is going

10995
07:44:01,478 --> 07:44:06,318
to be zero right it goes from like zero

10996
07:44:03,558 --> 07:44:09,000
up to whatever the length is minus one

10997
07:44:06,318 --> 07:44:12,120
and so this is just going to be zero so

10998
07:44:09,000 --> 07:44:13,478
when we actually look at this this idx

10999
07:44:12,120 --> 07:44:16,878
pay attention don't don't worry about

11000
07:44:13,478 --> 07:44:21,080
this yet this idx is going to be Time 4

11001
07:44:16,878 --> 07:44:24,558
so 0 * 4 0 * 4 0 * 4 0 * 4 it's just

11002
07:44:21,080 --> 07:44:27,558
going to be 0 1 2 and three

11003
07:44:24,558 --> 07:44:29,958
right these are going to be our x y z

11004
07:44:27,558 --> 07:44:31,840
and W components now let's actually look

11005
07:44:29,958 --> 07:44:33,840
at what's happening here so this new

11006
07:44:31,840 --> 07:44:35,680
float 4 type it's part of the it's part

11007
07:44:33,840 --> 07:44:37,000
of the Cuda runtime it's part of the

11008
07:44:35,680 --> 07:44:39,798
well it's not part of the Cuda runtime

11009
07:44:37,000 --> 07:44:43,558
if we actually look at this it's part of

11010
07:44:39,798 --> 07:44:45,000
um x86 right Vector types we have this

11011
07:44:43,558 --> 07:44:46,398
float 4 we have a bunch of different

11012
07:44:45,000 --> 07:44:49,600
other Vector types that are device

11013
07:44:46,398 --> 07:44:52,080
built-ins so we can't actually um we

11014
07:44:49,600 --> 07:44:53,398
cannot actually uh see what these are

11015
07:44:52,080 --> 07:44:56,000
under the hood they just kind of work

11016
07:44:53,398 --> 07:44:58,318
for us U if I try to click on these

11017
07:44:56,000 --> 07:45:00,920
right it's just like that's really all

11018
07:44:58,318 --> 07:45:03,398
it is

11019
07:45:00,920 --> 07:45:06,280
so there's Parts in here a lot of this

11020
07:45:03,398 --> 07:45:08,958
is built in handled by the compiler Etc

11021
07:45:06,280 --> 07:45:11,760
and so when we break down what's

11022
07:45:08,958 --> 07:45:13,718
happening here um we notice that we have

11023
07:45:11,760 --> 07:45:16,360
a few parts we have this reinterpret

11024
07:45:13,718 --> 07:45:21,360
cast and Lally all this means is that

11025
07:45:16,360 --> 07:45:23,680
we're going to reinterpret as this uh in

11026
07:45:21,360 --> 07:45:25,240
in the actual instructions right so this

11027
07:45:23,680 --> 07:45:27,398
isn't going to manipulate memory or do

11028
07:45:25,240 --> 07:45:29,638
any data Transformations it's literally

11029
07:45:27,398 --> 07:45:32,160
just going to uh

11030
07:45:29,638 --> 07:45:33,680
reinterpret as a float 4 that's that's

11031
07:45:32,160 --> 07:45:35,840
what the it's going to tell the compiler

11032
07:45:33,680 --> 07:45:39,600
what to do right and this is going to be

11033
07:45:35,840 --> 07:45:42,318
a pointer to float 4 right so we're

11034
07:45:39,600 --> 07:45:45,398
we're transforming uh this essentially

11035
07:45:42,318 --> 07:45:48,080
so this is a memory address this ENT is

11036
07:45:45,398 --> 07:45:49,878
a memory address to whichever index

11037
07:45:48,080 --> 07:45:53,558
we're looking at so in this

11038
07:45:49,878 --> 07:45:57,040
case it's going to be you know idx * 4

11039
07:45:53,558 --> 07:45:59,120
in this case this is just going to be um

11040
07:45:57,040 --> 07:46:02,638
this is just going to be like idx is z

11041
07:45:59,120 --> 07:46:04,398
so it's going to be 0 uh 0 * 4 that's

11042
07:46:02,638 --> 07:46:08,040
just the beginning element right that's

11043
07:46:04,398 --> 07:46:10,920
literally all all this is is just um

11044
07:46:08,040 --> 07:46:13,318
we're we're doing input at index zero

11045
07:46:10,920 --> 07:46:14,680
and then we're uh we're getting the

11046
07:46:13,318 --> 07:46:16,520
memory address for that so it's the

11047
07:46:14,680 --> 07:46:18,040
memory address so the first element in

11048
07:46:16,520 --> 07:46:20,280
that entire array and then there's the

11049
07:46:18,040 --> 07:46:25,478
following memory addresses for for the

11050
07:46:20,280 --> 07:46:28,318
extra ones and then we uh we we we we

11051
07:46:25,478 --> 07:46:30,040
reinterpret this as a float for pointer

11052
07:46:28,318 --> 07:46:32,000
so we got the pointer the the memory

11053
07:46:30,040 --> 07:46:33,638
address here and we're reinterpreting

11054
07:46:32,000 --> 07:46:35,798
this as a float 4 pointer so we're just

11055
07:46:33,638 --> 07:46:38,840
going from pointer as a float to pointer

11056
07:46:35,798 --> 07:46:41,040
as a float 4 and the float 4 uh is just

11057
07:46:38,840 --> 07:46:43,958
going to contain essentially the

11058
07:46:41,040 --> 07:46:45,798
starting index plus an additional uh

11059
07:46:43,958 --> 07:46:47,958
three afterwards

11060
07:46:45,798 --> 07:46:52,280
right um and then we just have this

11061
07:46:47,958 --> 07:46:53,958
stored as as uh index zero uh for this

11062
07:46:52,280 --> 07:46:55,558
specific data type we don't want to be

11063
07:46:53,958 --> 07:46:57,200
you know redundant and take up extra

11064
07:46:55,558 --> 07:46:58,520
space so we're just going to index zero

11065
07:46:57,200 --> 07:47:00,920
and then the compiler is going to know

11066
07:46:58,520 --> 07:47:03,000
what to do with that later on um it's

11067
07:47:00,920 --> 07:47:05,080
just literally just going to be thex

11068
07:47:03,000 --> 07:47:08,120
component is going to be index0 Y is

11069
07:47:05,080 --> 07:47:09,878
going to be index one Z is two and then

11070
07:47:08,120 --> 07:47:11,718
W is three right so that's that's

11071
07:47:09,878 --> 07:47:14,000
literally how we have it laid out how we

11072
07:47:11,718 --> 07:47:15,600
have this thing laid out here and that's

11073
07:47:14,000 --> 07:47:17,958
literally all the all the float 4 data

11074
07:47:15,600 --> 07:47:20,240
type is so hope that that kind of makes

11075
07:47:17,958 --> 07:47:22,798
sense to you um you know when you read

11076
07:47:20,240 --> 07:47:24,520
complex uh when you read kind of like

11077
07:47:22,798 --> 07:47:26,120
complicated expressions like this it's

11078
07:47:24,520 --> 07:47:27,680
good to just break it down step by step

11079
07:47:26,120 --> 07:47:29,160
right so you have this like you have

11080
07:47:27,680 --> 07:47:30,440
this thing with its open and and it's

11081
07:47:29,160 --> 07:47:31,840
closed and then you have this thing with

11082
07:47:30,440 --> 07:47:33,558
it's open and it's closed then this

11083
07:47:31,840 --> 07:47:35,440
thing with it's open and it's closed you

11084
07:47:33,558 --> 07:47:37,440
just kind of see like you do like your

11085
07:47:35,440 --> 07:47:39,478
order of operations or simplified

11086
07:47:37,440 --> 07:47:41,878
however you want but yeah hopefully that

11087
07:47:39,478 --> 07:47:43,840
makes sense and if we can uh I'm

11088
07:47:41,878 --> 07:47:46,840
actually going

11089
07:47:43,840 --> 07:47:46,840
to

11090
07:47:48,000 --> 07:47:57,318
uh oh sjm CD into Source kernels and

11091
07:47:53,440 --> 07:48:00,200
then if we go and compile that into

11092
07:47:57,318 --> 07:48:03,718
float 4 we notice that we get everything

11093
07:48:00,200 --> 07:48:05,520
as expected right so 1 2 3 4 that is the

11094
07:48:03,718 --> 07:48:08,718
poost input so that's how we initialized

11095
07:48:05,520 --> 07:48:10,798
it here um and then the host output

11096
07:48:08,718 --> 07:48:13,440
which is um exactly how we want

11097
07:48:10,798 --> 07:48:14,280
everything to be stored um that is all

11098
07:48:13,440 --> 07:48:17,520
checking

11099
07:48:14,280 --> 07:48:19,360
out now this next kernel is really fun

11100
07:48:17,520 --> 07:48:20,878
uh it it plays around with some things

11101
07:48:19,360 --> 07:48:22,360
that are usually played around with and

11102
07:48:20,878 --> 07:48:25,440
toyed around with when you write really

11103
07:48:22,360 --> 07:48:29,040
really uh performance optimal crud

11104
07:48:25,440 --> 07:48:31,280
kernels right so the idea here is like

11105
07:48:29,040 --> 07:48:36,638
remember how when I showed you before

11106
07:48:31,280 --> 07:48:38,638
those uh we go back to here and uh I'm

11107
07:48:36,638 --> 07:48:42,360
going to step

11108
07:48:38,638 --> 07:48:47,680
out and go to SRC kernels and then we

11109
07:48:42,360 --> 07:48:47,680
did the we did this one and we saw

11110
07:48:55,958 --> 07:49:03,520
um we go up we saw the these um like

11111
07:49:00,398 --> 07:49:06,680
this load this this load instruction the

11112
07:49:03,520 --> 07:49:09,440
load. E and then there was like some

11113
07:49:06,680 --> 07:49:12,120
load uh 128 so like load 32s and then

11114
07:49:09,440 --> 07:49:15,000
load 128s right so that that's like the

11115
07:49:12,120 --> 07:49:17,680
whole that's the whole deal here is

11116
07:49:15,000 --> 07:49:18,760
we're going to try to make more of these

11117
07:49:17,680 --> 07:49:21,280
load

11118
07:49:18,760 --> 07:49:26,160
128s a single floating Point number is

11119
07:49:21,280 --> 07:49:29,680
32 bits and so if we um if we if we make

11120
07:49:26,160 --> 07:49:33,000
this like a vector type meaning

11121
07:49:29,680 --> 07:49:35,520
you know we just put multiple numbers

11122
07:49:33,000 --> 07:49:37,318
with each other in the same type then we

11123
07:49:35,520 --> 07:49:38,920
can have more and we can load more

11124
07:49:37,318 --> 07:49:41,440
things

11125
07:49:38,920 --> 07:49:43,840
so let's go ahead and actually jump back

11126
07:49:41,440 --> 07:49:45,240
to uh this part here and let's explain

11127
07:49:43,840 --> 07:49:47,000
like what the heck we're

11128
07:49:45,240 --> 07:49:50,760
doing

11129
07:49:47,000 --> 07:49:54,200
so in this one we are

11130
07:49:50,760 --> 07:49:56,240
essentially uh we're essentially taking

11131
07:49:54,200 --> 07:49:59,040
the a tile so it's like normally

11132
07:49:56,240 --> 07:50:00,600
vertical so it would be like tilted

11133
07:49:59,040 --> 07:50:03,200
downwards like the bottom would go here

11134
07:50:00,600 --> 07:50:05,240
and then this part would go right there

11135
07:50:03,200 --> 07:50:08,080
we're just transposing it

11136
07:50:05,240 --> 07:50:10,200
right and this transposing is going to

11137
07:50:08,080 --> 07:50:12,040
let us cheat a little bit um we still

11138
07:50:10,200 --> 07:50:13,520
get the same amount of memory in that

11139
07:50:12,040 --> 07:50:15,360
shared block except we index it

11140
07:50:13,520 --> 07:50:19,398
differently and this will allow us to

11141
07:50:15,360 --> 07:50:23,240
coass memory accesses right so normally

11142
07:50:19,398 --> 07:50:25,360
we would um if we go back up we would be

11143
07:50:23,240 --> 07:50:29,638
taking these and we would be advancing

11144
07:50:25,360 --> 07:50:31,920
to the right here but notice how

11145
07:50:29,638 --> 07:50:34,520
um for example like when we're actually

11146
07:50:31,920 --> 07:50:36,878
loading this in uh our threads are going

11147
07:50:34,520 --> 07:50:38,000
to be loading in um from like top to

11148
07:50:36,878 --> 07:50:40,920
bottom right so we're going to have like

11149
07:50:38,000 --> 07:50:42,798
a like threads essentially organized in

11150
07:50:40,920 --> 07:50:45,200
in different pieces and we're going to

11151
07:50:42,798 --> 07:50:46,798
be iterating downwards to populate this

11152
07:50:45,200 --> 07:50:51,080
right if you remember how we populated

11153
07:50:46,798 --> 07:50:55,200
it before um these axises were not CEST

11154
07:50:51,080 --> 07:51:00,000
but uh the B the B tile the B shared

11155
07:50:55,200 --> 07:51:03,200
tile um was CEST because our our threads

11156
07:51:00,000 --> 07:51:05,160
were uh loading horizontally so when our

11157
07:51:03,200 --> 07:51:07,120
threads we need like thread one thread 2

11158
07:51:05,160 --> 07:51:08,878
thread three thread four those are all

11159
07:51:07,120 --> 07:51:11,600
next to each other so Cuda is going to

11160
07:51:08,878 --> 07:51:13,398
go ahead and load those in um as a like

11161
07:51:11,600 --> 07:51:14,600
if you have four of them adjacent to

11162
07:51:13,398 --> 07:51:16,638
each other it's going to load all those

11163
07:51:14,600 --> 07:51:18,558
in as a single load operation instead of

11164
07:51:16,638 --> 07:51:21,120
four separate ones so that's what we're

11165
07:51:18,558 --> 07:51:23,558
aiming for here and the idea here is

11166
07:51:21,120 --> 07:51:25,280
instead of advancing instead of having

11167
07:51:23,558 --> 07:51:28,798
this vertical tile and then taking this

11168
07:51:25,280 --> 07:51:32,000
and advancing to the and like having idx

11169
07:51:28,798 --> 07:51:33,478
to the right it's going to be flipped

11170
07:51:32,000 --> 07:51:35,000
and we're going to iterate downward so

11171
07:51:33,478 --> 07:51:37,680
our threads are going to be paired like

11172
07:51:35,000 --> 07:51:39,798
this next to each other and it's going

11173
07:51:37,680 --> 07:51:41,760
to iterate downward right that's the

11174
07:51:39,798 --> 07:51:44,280
whole idea there um but what's even more

11175
07:51:41,760 --> 07:51:45,718
important isn't even how uh they iterate

11176
07:51:44,280 --> 07:51:49,040
downward that's just what the graphic

11177
07:51:45,718 --> 07:51:52,920
looks like what it's more so about is

11178
07:51:49,040 --> 07:51:55,398
how we actually um are able to load from

11179
07:51:52,920 --> 07:51:57,718
Global into shared right so how do we

11180
07:51:55,398 --> 07:51:59,478
take the a how do we take the a tile

11181
07:51:57,718 --> 07:52:02,280
just on its own and then load that into

11182
07:51:59,478 --> 07:52:06,240
shared and then populate it like this

11183
07:52:02,280 --> 07:52:08,558
right so we pop back to here we notice a

11184
07:52:06,240 --> 07:52:11,840
few things so first of all this is the

11185
07:52:08,558 --> 07:52:14,920
same this is the same this is the same

11186
07:52:11,840 --> 07:52:17,120
but these and these are different right

11187
07:52:14,920 --> 07:52:19,200
so if we pop down here just pay

11188
07:52:17,120 --> 07:52:22,080
attention to the inner rows and columns

11189
07:52:19,200 --> 07:52:23,798
for now so notice here how that this

11190
07:52:22,080 --> 07:52:25,398
this is actually very familiar but

11191
07:52:23,798 --> 07:52:27,558
notice how we don't actually have that

11192
07:52:25,398 --> 07:52:30,040
Loop so if we go back to uh block tiling

11193
07:52:27,558 --> 07:52:32,920
for example this two block tiling we

11194
07:52:30,040 --> 07:52:36,160
were loading uh in four Loops so we had

11195
07:52:32,920 --> 07:52:38,040
um BM was 128 long and then this load

11196
07:52:36,160 --> 07:52:40,160
offset would increase an increments of

11197
07:52:38,040 --> 07:52:42,398
32 and it would do four different

11198
07:52:40,160 --> 07:52:44,840
iterations of the four Loops totaling

11199
07:52:42,398 --> 07:52:46,798
totaling eight different iterations per

11200
07:52:44,840 --> 07:52:49,160
thread so each thread was doing eight

11201
07:52:46,798 --> 07:52:50,680
different instructions or I guess at the

11202
07:52:49,160 --> 07:52:53,080
high level eight different instructions

11203
07:52:50,680 --> 07:52:55,760
for uh for moving data

11204
07:52:53,080 --> 07:52:57,280
around and that's like kind of a

11205
07:52:55,760 --> 07:52:59,760
bottleneck a little bit so we can

11206
07:52:57,280 --> 07:53:01,160
actually speed that up and notice here

11207
07:52:59,760 --> 07:53:02,558
we don't actually have any Loops it's

11208
07:53:01,160 --> 07:53:04,920
literally just we store this temp

11209
07:53:02,558 --> 07:53:07,200
variable um this is doing this is done

11210
07:53:04,920 --> 07:53:09,878
once per thread by the way so this is

11211
07:53:07,200 --> 07:53:13,638
like once per thread um but we

11212
07:53:09,878 --> 07:53:16,398
essentially we go into we go into a we

11213
07:53:13,638 --> 07:53:20,360
get this inner row and we essentially

11214
07:53:16,398 --> 07:53:24,360
just we essentially just find um where

11215
07:53:20,360 --> 07:53:26,398
exactly uh where exactly this is at and

11216
07:53:24,360 --> 07:53:29,240
we're going to load this into shared

11217
07:53:26,398 --> 07:53:30,478
memory as if it's transposed right

11218
07:53:29,240 --> 07:53:32,638
so for

11219
07:53:30,478 --> 07:53:35,240
example I have this thing written on the

11220
07:53:32,638 --> 07:53:37,920
board here this is the a tile and I drew

11221
07:53:35,240 --> 07:53:39,558
a little section at the bottom with 1 2

11222
07:53:37,920 --> 07:53:41,520
3 and four these are four different

11223
07:53:39,558 --> 07:53:44,000
values that we're going to count as a

11224
07:53:41,520 --> 07:53:46,680
float four all right so when we actually

11225
07:53:44,000 --> 07:53:49,040
transpose this when we do a tile then

11226
07:53:46,680 --> 07:53:52,398
say t you might not be able to see all

11227
07:53:49,040 --> 07:53:54,200
those just don't worry about it um we

11228
07:53:52,398 --> 07:53:55,638
transposes and essentially we're like

11229
07:53:54,200 --> 07:53:59,318
flipping

11230
07:53:55,638 --> 07:54:01,120
over over this this dotted line here so

11231
07:53:59,318 --> 07:54:04,638
these are going to flip over they're

11232
07:54:01,120 --> 07:54:08,718
going to go from this

11233
07:54:04,638 --> 07:54:10,840
to this right those are going to flip

11234
07:54:08,718 --> 07:54:12,920
across that

11235
07:54:10,840 --> 07:54:17,558
line and they're going to end up like

11236
07:54:12,920 --> 07:54:17,558
that what you're going to have

11237
07:54:19,200 --> 07:54:24,200
is you're going to have when these when

11238
07:54:21,440 --> 07:54:29,000
these flip over they're going to be

11239
07:54:24,200 --> 07:54:29,000
ordered one two

11240
07:54:29,798 --> 07:54:37,200
I'm sorry bottom bottom I one two three

11241
07:54:39,760 --> 07:54:44,558
four

11242
07:54:41,600 --> 07:54:47,040
now don't worry too much about how these

11243
07:54:44,558 --> 07:54:49,600
are in uh how these are in like a column

11244
07:54:47,040 --> 07:54:52,040
format what we do care about is just

11245
07:54:49,600 --> 07:54:55,080
that we're loading these in a coest

11246
07:54:52,040 --> 07:54:57,558
manner right so notice how I've kind of

11247
07:54:55,080 --> 07:54:59,798
Taken like half of this tile here and

11248
07:54:57,558 --> 07:55:03,160
I've actually done this for a reason so

11249
07:54:59,798 --> 07:55:05,638
if we if we go ahead and look back at

11250
07:55:03,160 --> 07:55:08,120
what these inner rows and all this stuff

11251
07:55:05,638 --> 07:55:11,878
is saying it's essentially the same as

11252
07:55:08,120 --> 07:55:14,920
what we had um over over here in 2D

11253
07:55:11,878 --> 07:55:18,478
block tiling except instead of just BN

11254
07:55:14,920 --> 07:55:20,680
or BK it's that and then divide by four

11255
07:55:18,478 --> 07:55:23,558
right so I mean it's it's four for a

11256
07:55:20,680 --> 07:55:26,000
reason it's literally because we're just

11257
07:55:23,558 --> 07:55:29,200
we're just um we're using the float 4

11258
07:55:26,000 --> 07:55:31,638
type so if we're going to do uh thread

11259
07:55:29,200 --> 07:55:33,080
idx which in this case is going to max

11260
07:55:31,638 --> 07:55:38,160
out at

11261
07:55:33,080 --> 07:55:44,040
20 it's going to max out at um say

11262
07:55:38,160 --> 07:55:50,718
255 and then BK is 8 so it's going to be

11263
07:55:44,040 --> 07:55:52,478
8 / 4 which is 255 / 2 which gives us

11264
07:55:50,718 --> 07:55:57,440
the indices 0 to

11265
07:55:52,478 --> 07:55:57,440
127 now this one on the other hand

11266
07:55:58,760 --> 07:56:05,760
is going to be um same idea except we're

11267
07:56:01,558 --> 07:56:07,520
doing the mod right so 255 mod 2 that

11268
07:56:05,760 --> 07:56:08,958
means every two times it's going to

11269
07:56:07,520 --> 07:56:12,160
reset at zero again so we're just going

11270
07:56:08,958 --> 07:56:14,638
to have the two numbers zero and

11271
07:56:12,160 --> 07:56:17,200
one essentially this indexing allows us

11272
07:56:14,638 --> 07:56:19,398
to treat this as a group of four so when

11273
07:56:17,200 --> 07:56:21,718
we divide that means it's shrinking to a

11274
07:56:19,398 --> 07:56:23,440
fourth of its length and then when we go

11275
07:56:21,718 --> 07:56:24,680
down here and multiply by four again

11276
07:56:23,440 --> 07:56:26,638
which you'll kind of see the intuition

11277
07:56:24,680 --> 07:56:28,360
for in a second and when we multiply by

11278
07:56:26,638 --> 07:56:29,600
four again it's just going to stretch

11279
07:56:28,360 --> 07:56:32,520
that back out to what it normally used

11280
07:56:29,600 --> 07:56:34,120
to be so this is just considering uh

11281
07:56:32,520 --> 07:56:36,958
this little Flo float for indexing

11282
07:56:34,120 --> 07:56:40,000
scheme we have now if we pop down to

11283
07:56:36,958 --> 07:56:43,040
this here uh we can see that this I'm

11284
07:56:40,000 --> 07:56:46,080
actually purposely making this um match

11285
07:56:43,040 --> 07:56:49,080
our a tile here so notice how we have

11286
07:56:46,080 --> 07:56:53,920
this inner row a right and inner row a

11287
07:56:49,080 --> 07:56:57,718
is between 0 and 127 so the row for this

11288
07:56:53,920 --> 07:57:00,160
tile is between zero let me put this

11289
07:56:57,718 --> 07:57:00,160
here for now

11290
07:57:02,240 --> 07:57:08,680
Z and 127 right so you might not be able

11291
07:57:05,280 --> 07:57:11,558
to see that but 0 127 this is very long

11292
07:57:08,680 --> 07:57:14,760
this is like the longer side um and then

11293
07:57:11,558 --> 07:57:20,240
we have 0 to one so two values for the

11294
07:57:14,760 --> 07:57:26,318
for the column right column A so 0 to

11295
07:57:20,240 --> 07:57:29,958
one or or one rather and all this allows

11296
07:57:26,318 --> 07:57:33,760
us to do is just sort things more easily

11297
07:57:29,958 --> 07:57:38,200
so notice how we have like uh we have

11298
07:57:33,760 --> 07:57:40,240
this thing that's normally of size

11299
07:57:38,200 --> 07:57:43,958
128 by

11300
07:57:40,240 --> 07:57:47,878
8 and 128 by 8 if we actually do the

11301
07:57:43,958 --> 07:57:47,878
multiplication for that

11302
07:57:48,160 --> 07:57:52,558
um that's 1,24

11303
07:57:54,000 --> 07:58:00,478
right so

11304
07:57:56,920 --> 07:58:03,798
1,24 and and then reduce this number we

11305
07:58:00,478 --> 07:58:06,240
divide this one by four

11306
07:58:03,798 --> 07:58:10,120
right that actually reduces the whole

11307
07:58:06,240 --> 07:58:10,120
thing from 1024 to

11308
07:58:10,240 --> 07:58:18,280
256 guess how many threads we have 256

11309
07:58:14,680 --> 07:58:21,680
it works out perfectly so each thread is

11310
07:58:18,280 --> 07:58:24,240
essentially taking its own little one of

11311
07:58:21,680 --> 07:58:26,878
these right so this is going this is

11312
07:58:24,240 --> 07:58:29,080
spanning down 127 0 to 127 and this is

11313
07:58:26,878 --> 07:58:31,478
going 01 and which just like split in

11314
07:58:29,080 --> 07:58:36,240
half right so you have you have like 1 2

11315
07:58:31,478 --> 07:58:37,638
3 4 5 6 7 right eight and we're just

11316
07:58:36,240 --> 07:58:39,600
like splitting this in half and then

11317
07:58:37,638 --> 07:58:42,360
it's it's spanning the length uh

11318
07:58:39,600 --> 07:58:45,878
spanning spanning the height downwards

11319
07:58:42,360 --> 07:58:47,760
right and all we have to do is just

11320
07:58:45,878 --> 07:58:49,878
store this as a float so this is what we

11321
07:58:47,760 --> 07:58:52,478
do here we we literally uh take the

11322
07:58:49,878 --> 07:58:54,558
inner row we do that times case we need

11323
07:58:52,478 --> 07:58:56,840
to you know stride around and and get

11324
07:58:54,558 --> 07:58:59,398
back to the same to the same uh to the

11325
07:58:56,840 --> 07:59:01,638
same column again and then we do plus

11326
07:58:59,398 --> 07:59:04,280
the inner column A offset this is all in

11327
07:59:01,638 --> 07:59:06,398
context of like this a tile being

11328
07:59:04,280 --> 07:59:08,360
Advanced where it needs to be so we're

11329
07:59:06,398 --> 07:59:09,760
literally just adding K considering that

11330
07:59:08,360 --> 07:59:11,878
we're in this bigger Matrix right that's

11331
07:59:09,760 --> 07:59:13,718
all we need k for everything else is it

11332
07:59:11,878 --> 07:59:15,718
it just works because we're in the we're

11333
07:59:13,718 --> 07:59:18,280
just dealing relative to this specific

11334
07:59:15,718 --> 07:59:21,558
tile as a part of the whole bigger

11335
07:59:18,280 --> 07:59:23,200
Matrix now inner column we're actually

11336
07:59:21,558 --> 07:59:27,120
going to stretch this out back to four

11337
07:59:23,200 --> 07:59:28,680
again because uh because we previously

11338
07:59:27,120 --> 07:59:30,718
shrunk it right so we just need to

11339
07:59:28,680 --> 07:59:33,920
expand that back

11340
07:59:30,718 --> 07:59:35,760
again and this is just going to index um

11341
07:59:33,920 --> 07:59:39,160
in the old fashion style right where we

11342
07:59:35,760 --> 07:59:42,958
where we find the the vertical offset

11343
07:59:39,160 --> 07:59:46,360
meaning uh meaning here and then we add

11344
07:59:42,958 --> 07:59:49,360
that so vertical offset and then we plus

11345
07:59:46,360 --> 07:59:52,398
to the so we like we find which row we

11346
07:59:49,360 --> 07:59:54,040
want we multiply by the by the K term

11347
07:59:52,398 --> 07:59:56,520
and then we get where we want to get and

11348
07:59:54,040 --> 07:59:58,920
then we Plus for the horizontal offset

11349
07:59:56,520 --> 08:00:01,798
right and we ex we have to multiply by

11350
07:59:58,920 --> 08:00:03,478
four to like to actually uh span that

11351
08:00:01,798 --> 08:00:07,360
entire length right so if you wanted to

11352
08:00:03,478 --> 08:00:08,878
go to the very end of The Matrix um if

11353
08:00:07,360 --> 08:00:10,840
you want to go to the very end of The

11354
08:00:08,878 --> 08:00:13,000
Matrix here you'd actually have to take

11355
08:00:10,840 --> 08:00:14,318
this two term or whatever that is times

11356
08:00:13,000 --> 08:00:16,240
four and it would it would get you all

11357
08:00:14,318 --> 08:00:18,878
the way to the end right that's that's

11358
08:00:16,240 --> 08:00:21,760
the whole idea there um but if we go

11359
08:00:18,878 --> 08:00:25,040
into this look at the way we store these

11360
08:00:21,760 --> 08:00:26,840
right this isn't actually too bad so in

11361
08:00:25,040 --> 08:00:28,240
a shared we have these four different

11362
08:00:26,840 --> 08:00:30,920
they have these four different stor

11363
08:00:28,240 --> 08:00:33,920
which is each four different component

11364
08:00:30,920 --> 08:00:36,920
of the float 4 variable so we have this

11365
08:00:33,920 --> 08:00:40,680
x y z and W term this is the first one

11366
08:00:36,920 --> 08:00:44,520
index zero and then index one index 2

11367
08:00:40,680 --> 08:00:46,798
Etc right so all we're doing there is

11368
08:00:44,520 --> 08:00:48,440
we're looking at the inner column

11369
08:00:46,798 --> 08:00:51,000
whichever column that is keep in mind we

11370
08:00:48,440 --> 08:00:52,478
have this new tile of this shape we're

11371
08:00:51,000 --> 08:00:54,600
doing it relative to this because this

11372
08:00:52,478 --> 08:00:58,040
is how we've this is how we're storing

11373
08:00:54,600 --> 08:00:59,080
everything so we're going to go um the

11374
08:00:58,040 --> 08:01:02,520
inner

11375
08:00:59,080 --> 08:01:05,000
column whichever that is so in this case

11376
08:01:02,520 --> 08:01:07,240
the the column we have to keep in

11377
08:01:05,000 --> 08:01:10,280
context how we stored column for this

11378
08:01:07,240 --> 08:01:14,360
one the column in this case is which one

11379
08:01:10,280 --> 08:01:17,680
of these but since we

11380
08:01:14,360 --> 08:01:21,040
um since we transposed it we have to

11381
08:01:17,680 --> 08:01:23,558
consider the column in the context of

11382
08:01:21,040 --> 08:01:26,558
here right so it's actually a little bit

11383
08:01:23,558 --> 08:01:28,120
different so instead of having row as

11384
08:01:26,558 --> 08:01:29,920
this it's actually column because we

11385
08:01:28,120 --> 08:01:30,920
interpreted column from this originally

11386
08:01:29,920 --> 08:01:32,040
that that's kind of what's going on

11387
08:01:30,920 --> 08:01:34,360
there hopefully that's hopefully that's

11388
08:01:32,040 --> 08:01:37,280
easy to understand

11389
08:01:34,360 --> 08:01:38,798
um and then we have this times 4 which

11390
08:01:37,280 --> 08:01:41,680
is obviously we're going to stretch it

11391
08:01:38,798 --> 08:01:44,080
out as we need it to and then whichever

11392
08:01:41,680 --> 08:01:47,280
index we need to plus right we're

11393
08:01:44,080 --> 08:01:48,798
storing this as like a as like these

11394
08:01:47,280 --> 08:01:50,440
these vertical values right so instead

11395
08:01:48,798 --> 08:01:52,600
of horizontally laying them out we're

11396
08:01:50,440 --> 08:01:54,718
storing them vertically and this is why

11397
08:01:52,600 --> 08:01:56,440
you have this extra index here this one

11398
08:01:54,718 --> 08:01:58,638
this is going to be like the the the

11399
08:01:56,440 --> 08:02:01,200
column Offset you could say

11400
08:01:58,638 --> 08:02:03,520
um like which which sorry which yeah

11401
08:02:01,200 --> 08:02:07,600
like which row are you at rather and

11402
08:02:03,520 --> 08:02:10,080
then the the BM term that is just this

11403
08:02:07,600 --> 08:02:12,398
and then we move it over here so it's

11404
08:02:10,080 --> 08:02:15,040
going to stride across as many times as

11405
08:02:12,398 --> 08:02:18,160
it needs to and then we can add the just

11406
08:02:15,040 --> 08:02:19,638
inner row uh a part right so so the row

11407
08:02:18,160 --> 08:02:21,558
originally came from this part and now

11408
08:02:19,638 --> 08:02:23,478
we're just flipping it over to this side

11409
08:02:21,558 --> 08:02:24,638
so we get the actual offset there and

11410
08:02:23,478 --> 08:02:26,920
that's literally that's literally how we

11411
08:02:24,638 --> 08:02:29,840
store it it's like literally that easy

11412
08:02:26,920 --> 08:02:29,840
um

11413
08:02:30,718 --> 08:02:35,920
so then we go further and we look at b b

11414
08:02:34,478 --> 08:02:36,958
shouldn't actually be too hard I'm not

11415
08:02:35,920 --> 08:02:39,478
even going to explain this it's

11416
08:02:36,958 --> 08:02:42,160
literally like be we're just explicitly

11417
08:02:39,478 --> 08:02:46,600
adding memory cols uh memory coling here

11418
08:02:42,160 --> 08:02:48,718
like if I go back to this um what's it

11419
08:02:46,600 --> 08:02:51,040
called shouldn't the compiler just be

11420
08:02:48,718 --> 08:02:53,958
able to colest the second version and

11421
08:02:51,040 --> 08:02:55,638
generate the 128bit loads um the and

11422
08:02:53,958 --> 08:02:57,398
then the reason is that the compiler has

11423
08:02:55,638 --> 08:03:00,240
no way to verify that the float B

11424
08:02:57,398 --> 08:03:02,478
pointer is pass to the kernel um as as

11425
08:03:00,240 --> 08:03:04,840
128 bit aligned right which would be a

11426
08:03:02,478 --> 08:03:06,440
requirement for this so essentially

11427
08:03:04,840 --> 08:03:08,200
we're just saying like like in the in

11428
08:03:06,440 --> 08:03:10,000
the previous example where I showed all

11429
08:03:08,200 --> 08:03:11,718
the Shader assembly there might have

11430
08:03:10,000 --> 08:03:13,638
been some instances where it did not

11431
08:03:11,718 --> 08:03:16,798
actually know that it's okay to store as

11432
08:03:13,638 --> 08:03:20,280
a 128 bit right is it like a float Fort

11433
08:03:16,798 --> 08:03:22,398
like an implicitly 128bit align type um

11434
08:03:20,280 --> 08:03:25,398
but in this case we're explicitly

11435
08:03:22,398 --> 08:03:27,398
passing reinterpret uh cast and we're

11436
08:03:25,398 --> 08:03:29,160
we're explicitly setting it to to float

11437
08:03:27,398 --> 08:03:31,638
for or

11438
08:03:29,160 --> 08:03:33,160
128bit and that's promising the compiler

11439
08:03:31,638 --> 08:03:35,280
that this is aligned right this is

11440
08:03:33,160 --> 08:03:36,558
telling the compiler you can you can do

11441
08:03:35,280 --> 08:03:38,958
it what you need to with this we've set

11442
08:03:36,558 --> 08:03:40,920
this up properly work your

11443
08:03:38,958 --> 08:03:42,398
magic and we're just helping out the

11444
08:03:40,920 --> 08:03:44,440
compiler that way so that that's what's

11445
08:03:42,398 --> 08:03:47,440
happening here same like it's literally

11446
08:03:44,440 --> 08:03:49,398
this this identical indexing scheme um

11447
08:03:47,440 --> 08:03:51,840
not really much to to talk about here

11448
08:03:49,398 --> 08:03:53,638
but uh after this is all done keep in

11449
08:03:51,840 --> 08:03:56,840
mind this is done once per thread right

11450
08:03:53,638 --> 08:03:58,398
so we do we store and then we write

11451
08:03:56,840 --> 08:04:01,958
right right right

11452
08:03:58,398 --> 08:04:04,600
and it's just super fast right so don't

11453
08:04:01,958 --> 08:04:06,478
don't worry about the uh don't worry

11454
08:04:04,600 --> 08:04:07,878
about how we're writing this we don't

11455
08:04:06,478 --> 08:04:10,000
actually have to make the rights coess

11456
08:04:07,878 --> 08:04:12,318
so then the rights don't have to be um

11457
08:04:10,000 --> 08:04:14,760
you know adjacent like this they can be

11458
08:04:12,318 --> 08:04:17,558
uh separated like by

11459
08:04:14,760 --> 08:04:20,080
rows uh and then as long as the reads

11460
08:04:17,558 --> 08:04:22,958
are coess that's fine so the reads

11461
08:04:20,080 --> 08:04:24,760
coming from here and then we're just

11462
08:04:22,958 --> 08:04:26,398
we're just flipping it over and it's

11463
08:04:24,760 --> 08:04:28,878
Landing there which is

11464
08:04:26,398 --> 08:04:30,878
perfect and then we move on to actually

11465
08:04:28,878 --> 08:04:33,080
calculating the results of that little

11466
08:04:30,878 --> 08:04:34,840
of that little thread block the the

11467
08:04:33,080 --> 08:04:38,718
essentially the thread mini tile within

11468
08:04:34,840 --> 08:04:41,920
the bigger one and we iterate through

11469
08:04:38,718 --> 08:04:43,360
with normal. idx right s similar scheme

11470
08:04:41,920 --> 08:04:44,600
to what we had before except the

11471
08:04:43,360 --> 08:04:48,080
indexing scheme is a little bit

11472
08:04:44,600 --> 08:04:50,040
different um we remember how we we have

11473
08:04:48,080 --> 08:04:52,080
b as like this this rectangle where it's

11474
08:04:50,040 --> 08:04:54,840
like this is short and this is this is

11475
08:04:52,080 --> 08:04:56,878
long right a is the same way now now

11476
08:04:54,840 --> 08:05:00,120
that we've transposed it a is also like

11477
08:04:56,878 --> 08:05:03,520
this so instead of a being uh like this

11478
08:05:00,120 --> 08:05:05,318
it's now flipped over like this right

11479
08:05:03,520 --> 08:05:08,000
and so all we have to do is just change

11480
08:05:05,318 --> 08:05:09,798
up how we index it and that'll work so

11481
08:05:08,000 --> 08:05:12,798
I'm not even going to go over b b is

11482
08:05:09,798 --> 08:05:15,000
like very obvious understand but uh

11483
08:05:12,798 --> 08:05:19,760
going over a right how do we iterate

11484
08:05:15,000 --> 08:05:24,280
through this so idx right when we look

11485
08:05:19,760 --> 08:05:26,398
at this we go um so whichever whichever

11486
08:05:24,280 --> 08:05:29,120
index we're at we'll just disregard that

11487
08:05:26,398 --> 08:05:32,040
for now um we'll just say that's zero

11488
08:05:29,120 --> 08:05:33,398
maybe um and then this since this is

11489
08:05:32,040 --> 08:05:36,360
multiplying by zero that's going to

11490
08:05:33,398 --> 08:05:41,360
simplify to zero so it's going to

11491
08:05:36,360 --> 08:05:42,718
be uh zero plus and then we'll say uh

11492
08:05:41,360 --> 08:05:47,440
maybe

11493
08:05:42,718 --> 08:05:49,318
the whichever thread row it's at um

11494
08:05:47,440 --> 08:05:51,878
whichever thread row it's at is going

11495
08:05:49,318 --> 08:05:55,000
it's going to multiply by TM which is

11496
08:05:51,878 --> 08:05:59,558
eight so it's going to stride however

11497
08:05:55,000 --> 08:05:59,558
much it needs to right um

11498
08:06:00,840 --> 08:06:05,600
and then we simply add I to that right

11499
08:06:03,040 --> 08:06:07,600
so thread row instead of being here

11500
08:06:05,600 --> 08:06:09,920
thread row is actually here because it

11501
08:06:07,600 --> 08:06:11,318
was previously right you can see how

11502
08:06:09,920 --> 08:06:13,478
that translation works this was thread

11503
08:06:11,318 --> 08:06:15,160
row this was long this is thread row now

11504
08:06:13,478 --> 08:06:17,318
now this is long right so it's it's the

11505
08:06:15,160 --> 08:06:19,200
same idea it's we're just we're just

11506
08:06:17,318 --> 08:06:20,440
making the naming a little bit confusing

11507
08:06:19,200 --> 08:06:22,680
there that that's really all it is we're

11508
08:06:20,440 --> 08:06:24,520
just stay consistent with whatever this

11509
08:06:22,680 --> 08:06:28,280
term means because we have to transpose

11510
08:06:24,520 --> 08:06:30,718
it right um and then we just iterate

11511
08:06:28,280 --> 08:06:32,760
through through um through I as we need

11512
08:06:30,718 --> 08:06:35,080
to right

11513
08:06:32,760 --> 08:06:36,478
so this should be intuitive I don't

11514
08:06:35,080 --> 08:06:37,920
really think I need to explain that too

11515
08:06:36,478 --> 08:06:40,760
much it's just like mainly paying

11516
08:06:37,920 --> 08:06:42,638
attention to the uh the naming

11517
08:06:40,760 --> 08:06:44,440
convention so thread Row versus thread

11518
08:06:42,638 --> 08:06:48,200
column like why are those different um

11519
08:06:44,440 --> 08:06:50,240
that's because you you're uh

11520
08:06:48,200 --> 08:06:55,120
transposing and then you do the typical

11521
08:06:50,240 --> 08:06:58,600
write out um so if we go back to this uh

11522
08:06:55,120 --> 08:07:00,160
up this is all it's doing right right so

11523
08:06:58,600 --> 08:07:02,680
revolving idx

11524
08:07:00,160 --> 08:07:04,160
downward all of these all of these

11525
08:07:02,680 --> 08:07:06,280
threads are sort of next to each other

11526
08:07:04,160 --> 08:07:10,478
in memory that's why we're striding um

11527
08:07:06,280 --> 08:07:13,718
TM right TM is that

11528
08:07:10,478 --> 08:07:16,000
um this is TM so whichever thread row we

11529
08:07:13,718 --> 08:07:17,080
are at um we're going to stride TM

11530
08:07:16,000 --> 08:07:21,600
because each thread is going to take

11531
08:07:17,080 --> 08:07:21,600
care of multiple of those values

11532
08:07:23,120 --> 08:07:28,360
um and yeah we end up uh we end up just

11533
08:07:26,798 --> 08:07:31,718
just sort of looking this a bit

11534
08:07:28,360 --> 08:07:31,718
differently uh and

11535
08:07:32,798 --> 08:07:37,080
then yeah just just some more visual

11536
08:07:35,080 --> 08:07:39,080
examples instead of having it as like

11537
08:07:37,080 --> 08:07:44,318
here and then these these like inch

11538
08:07:39,080 --> 08:07:47,240
these like uh inch forward

11539
08:07:44,318 --> 08:07:48,520
it's right but this is actually switched

11540
08:07:47,240 --> 08:07:50,600
over so it's just kind of the

11541
08:07:48,520 --> 08:07:52,760
transposing spatial reasoning you have

11542
08:07:50,600 --> 08:07:56,200
to have to get through and then this all

11543
08:07:52,760 --> 08:07:58,520
like is pretty straightforward um but

11544
08:07:56,200 --> 08:08:01,080
yeah now we can actually pop over to the

11545
08:07:58,520 --> 08:08:02,840
the write out section awesome so we're

11546
08:08:01,080 --> 08:08:04,240
actually almost done we just have to

11547
08:08:02,840 --> 08:08:07,558
write out the results now and we have to

11548
08:08:04,240 --> 08:08:10,240
make sure that these our these are also

11549
08:08:07,558 --> 08:08:15,478
uh in this Flo float for types so also

11550
08:08:10,240 --> 08:08:17,840
uh colest into uh the the uh global view

11551
08:08:15,478 --> 08:08:20,840
Ram right so we have the the normal um

11552
08:08:17,840 --> 08:08:22,440
iterate over TM and iterate over TN

11553
08:08:20,840 --> 08:08:25,760
except we change the iterators a bit

11554
08:08:22,440 --> 08:08:27,760
differently so in TM the rows we're

11555
08:08:25,760 --> 08:08:29,520
iterating over we're we're doing we're

11556
08:08:27,760 --> 08:08:32,200
iterating up one each time so it's going

11557
08:08:29,520 --> 08:08:36,040
to go one one one one one right and then

11558
08:08:32,200 --> 08:08:40,840
for TN we're going plus 4 each time so

11559
08:08:36,040 --> 08:08:42,920
we have eight values here and we have um

11560
08:08:40,840 --> 08:08:45,160
we have two values here right so when

11561
08:08:42,920 --> 08:08:47,318
you have this total 8 by8 thing you

11562
08:08:45,160 --> 08:08:49,040
would normally have to do 64 values but

11563
08:08:47,318 --> 08:08:53,398
because we're doing float 4 We're

11564
08:08:49,040 --> 08:08:54,878
storing four of those times 16 total

11565
08:08:53,398 --> 08:08:57,760
right so you can kind of do the math

11566
08:08:54,878 --> 08:09:01,120
it's like four values time 16 different

11567
08:08:57,760 --> 08:09:03,000
uh rights um so it's going to do four

11568
08:09:01,120 --> 08:09:04,958
values each right for 16 of them and

11569
08:09:03,000 --> 08:09:08,798
we're going to pop full that full 64

11570
08:09:04,958 --> 08:09:14,318
thing across the TM and and TN tile

11571
08:09:08,798 --> 08:09:18,478
right um so we we set this temporary

11572
08:09:14,318 --> 08:09:21,000
variable as the current uh C the C the

11573
08:09:18,478 --> 08:09:24,360
current C value right so this whatever

11574
08:09:21,000 --> 08:09:26,080
uh whatever the the the current C is

11575
08:09:24,360 --> 08:09:28,120
that we care about relative to how we've

11576
08:09:26,080 --> 08:09:29,840
indexed through these right and this is

11577
08:09:28,120 --> 08:09:33,760
all keeping in mind to where we are

11578
08:09:29,840 --> 08:09:36,638
actually at in the entire in the entire

11579
08:09:33,760 --> 08:09:39,798
C Matrix the entire C Matrix right so

11580
08:09:36,638 --> 08:09:43,040
the the thread row offset times this

11581
08:09:39,798 --> 08:09:45,638
plus the res idx um M offset and then

11582
08:09:43,040 --> 08:09:48,280
all of that times n which is the stride

11583
08:09:45,638 --> 08:09:50,760
of that and then plus our our columns

11584
08:09:48,280 --> 08:09:52,360
time TN and then our res idx n this

11585
08:09:50,760 --> 08:09:54,920
should all make sense this indexing

11586
08:09:52,360 --> 08:09:57,000
should be like pretty much Crystal Clear

11587
08:09:54,920 --> 08:09:58,958
um we've we've done very similar to this

11588
08:09:57,000 --> 08:10:02,000
already

11589
08:09:58,958 --> 08:10:03,760
um and a lot of the syntax is also

11590
08:10:02,000 --> 08:10:04,878
similar to this float 4 we did here so

11591
08:10:03,760 --> 08:10:06,398
if something doesn't make sense just

11592
08:10:04,878 --> 08:10:09,040
look back at this and break things down

11593
08:10:06,398 --> 08:10:11,638
again that's the easiest way to do it um

11594
08:10:09,040 --> 08:10:15,040
but in here we have this we have this

11595
08:10:11,638 --> 08:10:18,718
temp type right and now temp keep in

11596
08:10:15,040 --> 08:10:22,080
mind this is what C originally was so we

11597
08:10:18,718 --> 08:10:25,040
can actually store uh things from the

11598
08:10:22,080 --> 08:10:26,718
existing uh TMP or the temp variable

11599
08:10:25,040 --> 08:10:29,080
inside of it against we can actually do

11600
08:10:26,718 --> 08:10:30,718
more to and then right back inside of it

11601
08:10:29,080 --> 08:10:32,120
cuz we don't care about the old C result

11602
08:10:30,718 --> 08:10:34,878
we just want to calculate the new one

11603
08:10:32,120 --> 08:10:36,798
right so we have this thisx part which

11604
08:10:34,878 --> 08:10:39,040
is the index zero so we're not going to

11605
08:10:36,798 --> 08:10:43,080
add anything and then index one and then

11606
08:10:39,040 --> 08:10:45,120
two and then three these are all x y z w

11607
08:10:43,080 --> 08:10:47,958
respectively uh so we have this Alpha

11608
08:10:45,120 --> 08:10:50,440
term the thread results which is with

11609
08:10:47,958 --> 08:10:52,638
with respect to the thread results uh

11610
08:10:50,440 --> 08:10:55,878
variable in the register so it's going

11611
08:10:52,638 --> 08:11:00,280
to be the res index M so which which row

11612
08:10:55,878 --> 08:11:02,638
are you at right and then the um and

11613
08:11:00,280 --> 08:11:04,440
then TN which is which is how that spans

11614
08:11:02,638 --> 08:11:05,798
so you're going to stride over and get

11615
08:11:04,440 --> 08:11:08,680
to whichever row you want then you're

11616
08:11:05,798 --> 08:11:11,040
going to offsite offset by this and then

11617
08:11:08,680 --> 08:11:13,600
based on which Val which index you're at

11618
08:11:11,040 --> 08:11:16,478
within the float 4 array within that

11619
08:11:13,600 --> 08:11:18,878
specific the the float 4 window right

11620
08:11:16,478 --> 08:11:21,280
you're going to add uh these to the

11621
08:11:18,878 --> 08:11:22,920
actual uh memory addresses or the or the

11622
08:11:21,280 --> 08:11:26,958
index themselves right store it as like

11623
08:11:22,920 --> 08:11:29,200
kind of a vector type so uh this way we

11624
08:11:26,958 --> 08:11:31,200
can actually get the the value that we

11625
08:11:29,200 --> 08:11:33,160
want and then we simply just add that to

11626
08:11:31,200 --> 08:11:35,240
the beta scaler and then multiply that

11627
08:11:33,160 --> 08:11:37,478
by the existing C value as we got from

11628
08:11:35,240 --> 08:11:39,638
up here right so that should be very

11629
08:11:37,478 --> 08:11:41,680
straightforward um luckily this is only

11630
08:11:39,638 --> 08:11:43,558
in register so the indexing isn't too

11631
08:11:41,680 --> 08:11:46,958
complicated uh and then we simply just

11632
08:11:43,558 --> 08:11:49,040
write back right so using the same idea

11633
08:11:46,958 --> 08:11:53,000
that we did back here the same indexing

11634
08:11:49,040 --> 08:11:57,200
scheme uh and we simply uh write write

11635
08:11:53,000 --> 08:11:58,200
out c um on the level of threads that's

11636
08:11:57,200 --> 08:12:02,000
Factor memory

11637
08:11:58,200 --> 08:12:04,478
coing okay awesome so now uh we can

11638
08:12:02,000 --> 08:12:06,318
actually just print out how well these

11639
08:12:04,478 --> 08:12:11,000
do right so we just finished up the

11640
08:12:06,318 --> 08:12:13,520
vector the V col colest memory access uh

11641
08:12:11,000 --> 08:12:15,000
with vectorization kernel let's go ahead

11642
08:12:13,520 --> 08:12:17,478
and print that out how well does that do

11643
08:12:15,000 --> 08:12:19,638
so the last one was uh 2D block tiling

11644
08:12:17,478 --> 08:12:23,638
kernel number five we go and print this

11645
08:12:19,638 --> 08:12:26,680
out and we get a peak of about um 9,000

11646
08:12:23,638 --> 08:12:29,120
so about about 9100 gig flops and this

11647
08:12:26,680 --> 08:12:29,120
one

11648
08:12:30,600 --> 08:12:37,200
about 10,800 gigaflops which is quite a

11649
08:12:33,760 --> 08:12:38,160
big increase from before so that's about

11650
08:12:37,200 --> 08:12:40,200
what

11651
08:12:38,160 --> 08:12:42,638
roughly uh

11652
08:12:40,200 --> 08:12:45,638
1,600 more Giga flops than before which

11653
08:12:42,638 --> 08:12:48,760
is pretty solid right it's like a 15% 16

11654
08:12:45,638 --> 08:12:50,478
whatever perc increase in performance um

11655
08:12:48,760 --> 08:12:52,958
so now let's actually go ahead and print

11656
08:12:50,478 --> 08:12:54,878
like some more right uh I know the the

11657
08:12:52,958 --> 08:12:57,878
autot tuning one was good so we go print

11658
08:12:54,878 --> 08:12:57,878
09

11659
08:12:58,478 --> 08:13:03,318
this one did about 11,000 so relative to

11660
08:13:01,318 --> 08:13:05,200
this one it did you know moderately

11661
08:13:03,318 --> 08:13:08,440
better and then we can print out the

11662
08:13:05,200 --> 08:13:10,638
last one which was kublos so kublos was

11663
08:13:08,440 --> 08:13:14,638
supposed to be the fastest we can

11664
08:13:10,638 --> 08:13:17,398
actually see that this

11665
08:13:14,638 --> 08:13:19,160
11,496 is very close to the autotuned

11666
08:13:17,398 --> 08:13:20,680
kernel right so if we actually look back

11667
08:13:19,160 --> 08:13:23,360
to what we previously did which was

11668
08:13:20,680 --> 08:13:26,240
number six we are very close to coup

11669
08:13:23,360 --> 08:13:29,040
loss so 10,000 we'll just do 10,800

11670
08:13:26,240 --> 08:13:29,040
divided by

11671
08:13:31,240 --> 08:13:36,718
um 10,800 ID 11 uh

11672
08:13:37,760 --> 08:13:45,040
11,496 so we get about

11673
08:13:40,120 --> 08:13:48,040
94% uh kuo performance just using the uh

11674
08:13:45,040 --> 08:13:49,760
vectorized uh uh like the float 4

11675
08:13:48,040 --> 08:13:52,040
loading right so that's ridiculously

11676
08:13:49,760 --> 08:13:54,878
good and we can still optimize further

11677
08:13:52,040 --> 08:13:56,478
with these ones right so consider that

11678
08:13:54,878 --> 08:13:59,160
if we were to optimize these further and

11679
08:13:56,478 --> 08:14:00,718
maybe use um some additional tricks that

11680
08:13:59,160 --> 08:14:02,080
you'd find in like research papers like

11681
08:14:00,718 --> 08:14:03,958
this could actually get really really

11682
08:14:02,080 --> 08:14:07,638
fast right

11683
08:14:03,958 --> 08:14:09,040
so that's uh it's pretty cool that we

11684
08:14:07,638 --> 08:14:10,280
can do that just on our own hardware and

11685
08:14:09,040 --> 08:14:13,080
we can actually see it from start to

11686
08:14:10,280 --> 08:14:14,398
finish and understand it intuitively um

11687
08:14:13,080 --> 08:14:17,200
the reason I'm not going to go over

11688
08:14:14,398 --> 08:14:18,958
anymore is because it's just more and

11689
08:14:17,200 --> 08:14:20,958
more complexity to dig through as you go

11690
08:14:18,958 --> 08:14:23,120
through each one um and I want to save

11691
08:14:20,958 --> 08:14:24,318
some time for the last final project in

11692
08:14:23,120 --> 08:14:26,760
the course which we're going to do

11693
08:14:24,318 --> 08:14:28,840
shortly um but yeah I'm not going to

11694
08:14:26,760 --> 08:14:32,000
cover all of these I just kind of cover

11695
08:14:28,840 --> 08:14:33,718
the main ones so uh cols memory access

11696
08:14:32,000 --> 08:14:36,160
from from Global just like a bump up

11697
08:14:33,718 --> 08:14:38,240
from naive and then uh all the different

11698
08:14:36,160 --> 08:14:40,680
tiling variants and then how we can how

11699
08:14:38,240 --> 08:14:43,878
we can vectorize memory access

11700
08:14:40,680 --> 08:14:48,318
right so given that um let's actually go

11701
08:14:43,878 --> 08:14:51,120
ahead and print out the uh the assembly

11702
08:14:48,318 --> 08:14:55,160
instructions for this specific kernel so

11703
08:14:51,120 --> 08:14:58,920
if I pop out of here and go into Source

11704
08:14:55,160 --> 08:15:03,440
SL kernels do

11705
08:14:58,920 --> 08:15:06,760
nvcc PTX and then we

11706
08:15:03,440 --> 08:15:06,760
go number

11707
08:15:07,000 --> 08:15:14,160
six out and we can just go

11708
08:15:11,318 --> 08:15:16,878
Um number six.

11709
08:15:14,160 --> 08:15:18,318
PTX okay so instead of that uh I just

11710
08:15:16,878 --> 08:15:20,440
remember that we had this main file here

11711
08:15:18,318 --> 08:15:22,600
which we can reference so if I just go

11712
08:15:20,440 --> 08:15:24,638
ahead and save this if I like take the

11713
08:15:22,600 --> 08:15:27,878
the title of

11714
08:15:24,638 --> 08:15:30,798
this and I replace

11715
08:15:27,878 --> 08:15:33,600
I repl that in

11716
08:15:30,798 --> 08:15:36,440
here uh and then we go ahead and compile

11717
08:15:33,600 --> 08:15:38,360
we're going to get our actual uh PTX

11718
08:15:36,440 --> 08:15:42,360
instructions here

11719
08:15:38,360 --> 08:15:45,558
so uh it seems like we got some errors

11720
08:15:42,360 --> 08:15:50,638
which is because if I actually go in

11721
08:15:45,558 --> 08:15:50,638
here we have to copy this copy it

11722
08:15:50,760 --> 08:15:55,478
back oh we're getting

11723
08:15:55,680 --> 08:16:00,238
vim and

11724
08:15:57,920 --> 08:16:02,238
and so pretty much what happened there

11725
08:16:00,238 --> 08:16:04,200
was I had it imported properly but we

11726
08:16:02,238 --> 08:16:07,000
just weren't including the TN at the end

11727
08:16:04,200 --> 08:16:09,440
here so I just added that um and now it

11728
08:16:07,000 --> 08:16:12,920
now it's successfully uh compiled so we

11729
08:16:09,440 --> 08:16:14,920
get kernel. PTX we open this um and we

11730
08:16:12,920 --> 08:16:18,000
can go ahead and see um let's see if we

11731
08:16:14,920 --> 08:16:20,878
can find any the 128's aren't in here so

11732
08:16:18,000 --> 08:16:23,958
I actually have to go in and uh and

11733
08:16:20,878 --> 08:16:27,638
change this to Shader assembly we can go

11734
08:16:23,958 --> 08:16:32,318
ahead and do um

11735
08:16:27,638 --> 08:16:36,318
n BCC Das will do Arch is

11736
08:16:32,318 --> 08:16:39,440
SM oh Arch equals sore

11737
08:16:36,318 --> 08:16:43,160
86 and then we'll do Q

11738
08:16:39,440 --> 08:16:43,160
binary um main

11739
08:16:43,200 --> 08:16:47,600
dosc um and then out and we'll do it

11740
08:16:46,280 --> 08:16:51,000
main

11741
08:16:47,600 --> 08:16:56,398
main. Cuda binary and then we'll go

11742
08:16:51,000 --> 08:16:56,398
ahead and C object object dump

11743
08:16:58,318 --> 08:17:03,120
and we'll do main dot key binary just

11744
08:17:01,040 --> 08:17:06,958
like that we go ahead and move this

11745
08:17:03,120 --> 08:17:13,958
up and we can see that um if we look for

11746
08:17:06,958 --> 08:17:15,558
the specific ldg doe instruction ldg

11747
08:17:13,958 --> 08:17:18,600
do

11748
08:17:15,558 --> 08:17:20,680
128 128

11749
08:17:18,600 --> 08:17:23,558
128

11750
08:17:20,680 --> 08:17:28,440
1288 128

11751
08:17:23,558 --> 08:17:31,120
128 and so on and so forth so how cool

11752
08:17:28,440 --> 08:17:33,280
is that we can actually verify that all

11753
08:17:31,120 --> 08:17:36,120
of these

11754
08:17:33,280 --> 08:17:40,558
um may we can find like a 32 in here

11755
08:17:36,120 --> 08:17:40,558
somewhere oh that's just going to be in

11756
08:17:41,080 --> 08:17:47,280
uh that's a

11757
08:17:44,280 --> 08:17:49,638
register that's a register as well

11758
08:17:47,280 --> 08:17:49,638
that's a

11759
08:17:50,200 --> 08:17:55,920
register so we actually do not have any

11760
08:17:52,680 --> 08:17:58,638
32-bit loads any 32-bit transfers at all

11761
08:17:55,920 --> 08:18:02,238
everything is perfect

11762
08:17:58,638 --> 08:18:02,238
um everything is

11763
08:18:03,638 --> 08:18:09,120
great how

11764
08:18:06,040 --> 08:18:11,040
wonderful so now uh now that that kind

11765
08:18:09,120 --> 08:18:13,958
of makes sense that we are able to

11766
08:18:11,040 --> 08:18:16,478
literally see what the uh instructions

11767
08:18:13,958 --> 08:18:19,878
are looking like when we provide optim

11768
08:18:16,478 --> 08:18:21,718
optimizations um you know we can we can

11769
08:18:19,878 --> 08:18:23,080
actually we actually feel a lot more

11770
08:18:21,718 --> 08:18:24,600
confident in our ability it's not just

11771
08:18:23,080 --> 08:18:26,040
like me telling you that this works and

11772
08:18:24,600 --> 08:18:28,200
trusting it it's like you can actually

11773
08:18:26,040 --> 08:18:30,200
see it this is what is being uh this is

11774
08:18:28,200 --> 08:18:31,878
what's being run on the gpus controllers

11775
08:18:30,200 --> 08:18:34,238
the little microcontrollers inside that

11776
08:18:31,878 --> 08:18:38,040
issue the instructions

11777
08:18:34,238 --> 08:18:39,318
right and so if we pop back to here um

11778
08:18:38,040 --> 08:18:41,398
there's one little thing I wanted to

11779
08:18:39,318 --> 08:18:44,958
cover it's an extra optimization which

11780
08:18:41,398 --> 08:18:48,360
is using tensor cores so we go to

11781
08:18:44,958 --> 08:18:50,000
programming tensor cores um you

11782
08:18:48,360 --> 08:18:53,520
literally just search it's on the Nvidia

11783
08:18:50,000 --> 08:18:57,318
blog programming tensor cores in Cuda 9

11784
08:18:53,520 --> 08:19:02,478
so Cuda 9 um I mean we already actually

11785
08:18:57,318 --> 08:19:04,398
have I go here Nvidia SMI we're on Cuda

11786
08:19:02,478 --> 08:19:07,238
12.5 so that's fine we don't have to

11787
08:19:04,398 --> 08:19:09,398
worry about incompatibility issues um

11788
08:19:07,238 --> 08:19:09,398
but

11789
08:19:09,920 --> 08:19:15,000
essentially this gives the whole

11790
08:19:12,398 --> 08:19:17,718
instruction sets on how to use tensor

11791
08:19:15,000 --> 08:19:21,798
cores right so

11792
08:19:17,718 --> 08:19:23,440
um ca9 provides these new features these

11793
08:19:21,798 --> 08:19:25,798
tensor cores will essentially like for

11794
08:19:23,440 --> 08:19:27,798
example on the on the Volta architecture

11795
08:19:25,798 --> 08:19:30,398
you can you can do this where it's like

11796
08:19:27,798 --> 08:19:32,680
you multiply these two fp16 matrices and

11797
08:19:30,398 --> 08:19:35,040
then you add another one right the the

11798
08:19:32,680 --> 08:19:38,760
fuse multiply and add operation we saw

11799
08:19:35,040 --> 08:19:41,398
previously um except on the level of 4x4

11800
08:19:38,760 --> 08:19:45,280
tensors and these are supported in the

11801
08:19:41,398 --> 08:19:46,840
in the CU loss Library um so two tensor

11802
08:19:45,280 --> 08:19:48,718
two Cuda libraries that use tensor cores

11803
08:19:46,840 --> 08:19:50,680
are kublos and CNN right the ones we

11804
08:19:48,718 --> 08:19:53,280
ones we covered earlier kublos use

11805
08:19:50,680 --> 08:19:56,718
tensor course to speed up gem Matrix

11806
08:19:53,280 --> 08:20:00,920
multiply uh operations and CNN uses it

11807
08:19:56,718 --> 08:20:02,600
to speed up convolutions and rnns so

11808
08:20:00,920 --> 08:20:04,120
that's cool but what if we want to write

11809
08:20:02,600 --> 08:20:05,920
our own what if we want to write our own

11810
08:20:04,120 --> 08:20:07,558
code that isn't dependent on CU loss

11811
08:20:05,920 --> 08:20:08,840
right because CU loss we could just call

11812
08:20:07,558 --> 08:20:11,040
like a separate function but it might

11813
08:20:08,840 --> 08:20:13,718
not be as slow or we can't like fuse the

11814
08:20:11,040 --> 08:20:15,680
actual tensor core instructions with a

11815
08:20:13,718 --> 08:20:17,718
specific kernel that we want to do like

11816
08:20:15,680 --> 08:20:19,798
for example um flash attention right if

11817
08:20:17,718 --> 08:20:21,238
we wanted to write that kublos might not

11818
08:20:19,798 --> 08:20:23,000
let us do that because it's sort of

11819
08:20:21,238 --> 08:20:27,360
independent we can't include those calls

11820
08:20:23,000 --> 08:20:30,238
inside of kernels um so if we wanted to

11821
08:20:27,360 --> 08:20:31,280
actually look at how to use that um

11822
08:20:30,238 --> 08:20:34,478
where did it

11823
08:20:31,280 --> 08:20:36,000
go scroll down a little bit tensor

11824
08:20:34,478 --> 08:20:39,920
course and

11825
08:20:36,000 --> 08:20:42,200
CNN and then um programmatic access to

11826
08:20:39,920 --> 08:20:44,920
tensor cores and that's where ca 9.0

11827
08:20:42,200 --> 08:20:48,680
comes in so we're good um essentially

11828
08:20:44,920 --> 08:20:52,040
there's this new thing called a um the

11829
08:20:48,680 --> 08:20:55,280
complete namespace is NV and then in C++

11830
08:20:52,040 --> 08:20:57,398
it's like the the W the W

11831
08:20:55,280 --> 08:20:59,398
MMA um

11832
08:20:57,398 --> 08:21:00,920
this means warp Matrix multiply

11833
08:20:59,398 --> 08:21:02,798
accumulate which I'm not going to go

11834
08:21:00,920 --> 08:21:04,280
over like exactly what that means nor do

11835
08:21:02,798 --> 08:21:09,718
I entirely know what's happening under

11836
08:21:04,280 --> 08:21:12,318
the hood there but um that's that is the

11837
08:21:09,718 --> 08:21:14,600
tensor core uh operations that we can

11838
08:21:12,318 --> 08:21:16,440
call right so there's a bunch of

11839
08:21:14,600 --> 08:21:18,680
examples here I'm not going to go over

11840
08:21:16,440 --> 08:21:22,478
this part but uh this is just kind of

11841
08:21:18,680 --> 08:21:23,878
like a nice little um you know dock for

11842
08:21:22,478 --> 08:21:26,600
understanding how the heck to use tensor

11843
08:21:23,878 --> 08:21:28,000
core operations and you might you might

11844
08:21:26,600 --> 08:21:29,600
might even want to implement those

11845
08:21:28,000 --> 08:21:31,558
inside of the block tiling kernel right

11846
08:21:29,600 --> 08:21:34,600
so the ones that we just wrote you might

11847
08:21:31,558 --> 08:21:37,238
want to say take like a single thread

11848
08:21:34,600 --> 08:21:38,680
and have like a thread or a piece of it

11849
08:21:37,238 --> 08:21:40,440
and try to like block these actual

11850
08:21:38,680 --> 08:21:42,040
tensor core operations right and make

11851
08:21:40,440 --> 08:21:44,280
these really really fast so that way

11852
08:21:42,040 --> 08:21:45,478
it's not just so that way it's not

11853
08:21:44,280 --> 08:21:46,840
iterating through eight and then

11854
08:21:45,478 --> 08:21:49,760
iterating through eight more and having

11855
08:21:46,840 --> 08:21:52,040
64 total operations to do but rather

11856
08:21:49,760 --> 08:21:53,840
just having one right so these are

11857
08:21:52,040 --> 08:21:56,360
literally going to be organized in a 3D

11858
08:21:53,840 --> 08:22:00,040
structure inside of the tensor cores on

11859
08:21:56,360 --> 08:22:03,120
the the GPU um and it's it's literally

11860
08:22:00,040 --> 08:22:04,680
just like a a 3D thing or a 2d thing

11861
08:22:03,120 --> 08:22:07,160
depending on how big Dimensions you're

11862
08:22:04,680 --> 08:22:10,040
using um because you can do like batch

11863
08:22:07,160 --> 08:22:13,238
by uh Time by Channel if you're using

11864
08:22:10,040 --> 08:22:16,878
Transformers or you can do

11865
08:22:13,238 --> 08:22:19,160
um yeah just there there is there is a

11866
08:22:16,878 --> 08:22:21,840
lot going on in the hardware but you can

11867
08:22:19,160 --> 08:22:23,520
actually you can capitalize on those so

11868
08:22:21,840 --> 08:22:26,080
anyways I'm not going to ramble on this

11869
08:22:23,520 --> 08:22:28,638
is this is tensor cores uh you have

11870
08:22:26,080 --> 08:22:30,398
permission have fun with this um I

11871
08:22:28,638 --> 08:22:31,798
encourage you to do so I might add some

11872
08:22:30,398 --> 08:22:33,878
more to this course later on in the

11873
08:22:31,798 --> 08:22:35,798
GitHub repo that'll you know sort of

11874
08:22:33,878 --> 08:22:37,760
talk more about tensor course but this

11875
08:22:35,798 --> 08:22:40,200
is

11876
08:22:37,760 --> 08:22:42,840
it okay so we can finally take a

11877
08:22:40,200 --> 08:22:45,318
breather now matrix multiplication is

11878
08:22:42,840 --> 08:22:47,120
pretty much finished um there's there's

11879
08:22:45,318 --> 08:22:49,238
a little bit more that we'll do later on

11880
08:22:47,120 --> 08:22:50,680
but for now you can consider matrix

11881
08:22:49,238 --> 08:22:54,878
multiplication finished for the next

11882
08:22:50,680 --> 08:22:58,280
section or two um now we go into Triton

11883
08:22:54,878 --> 08:23:00,798
which essentially takes the previous uh

11884
08:22:58,280 --> 08:23:02,478
takes the previous chapter and says

11885
08:23:00,798 --> 08:23:06,478
let's abstract that and make it easier

11886
08:23:02,478 --> 08:23:08,520
to use um so so taking uh you know

11887
08:23:06,478 --> 08:23:10,360
matrix multiplication or like tiled

11888
08:23:08,520 --> 08:23:12,280
optimizations where you're tiling things

11889
08:23:10,360 --> 08:23:13,520
into blocks and then you know

11890
08:23:12,280 --> 08:23:15,958
multiplying them together more

11891
08:23:13,520 --> 08:23:18,080
efficiently we can actually take that

11892
08:23:15,958 --> 08:23:22,520
and do it in Python with much simpler

11893
08:23:18,080 --> 08:23:22,520
syntax so this is Triton

11894
08:23:22,760 --> 08:23:28,798
um Triton is a bit different than Cuda

11895
08:23:25,718 --> 08:23:30,318
all right so before I actually go into

11896
08:23:28,798 --> 08:23:32,238
the the whole design here and what

11897
08:23:30,318 --> 08:23:34,638
Triton is about I want you to pay

11898
08:23:32,238 --> 08:23:36,040
attention to something so if I go pip

11899
08:23:34,638 --> 08:23:40,000
install

11900
08:23:36,040 --> 08:23:42,478
torch you'll see that we get all of

11901
08:23:40,000 --> 08:23:44,718
these all these all this Nvidia stuff

11902
08:23:42,478 --> 08:23:49,638
which we've seen before and then we get

11903
08:23:44,718 --> 08:23:52,958
this Triton Triton 3.0 right and Triton

11904
08:23:49,638 --> 08:23:54,798
is used by pie torch under the hood or

11905
08:23:52,958 --> 08:23:57,558
accelerating and making things faster

11906
08:23:54,798 --> 08:24:00,160
with python syntax right Tron is also

11907
08:23:57,558 --> 08:24:02,238
fast just like Cuda and so there there

11908
08:24:00,160 --> 08:24:06,318
are some differences between them which

11909
08:24:02,238 --> 08:24:10,398
I thought I should highlight um so if we

11910
08:24:06,318 --> 08:24:12,638
pop over to the uh pop over to the

11911
08:24:10,398 --> 08:24:14,600
Triton website search up Triton doc or

11912
08:24:12,638 --> 08:24:17,520
Trident website or whatever uh this will

11913
08:24:14,600 --> 08:24:21,040
come up they also have a GitHub as well

11914
08:24:17,520 --> 08:24:25,360
so the GitHub has you know a lot of uh

11915
08:24:21,040 --> 08:24:26,798
you know useful stuff um yeah just

11916
08:24:25,360 --> 08:24:29,280
playing around with it so setting up

11917
08:24:26,798 --> 08:24:31,080
configuring tridon doing custom things

11918
08:24:29,280 --> 08:24:33,798
uh but you can just PP install tridon

11919
08:24:31,080 --> 08:24:36,280
like that and it'll work the same way um

11920
08:24:33,798 --> 08:24:38,718
but if you do look at at the Triton

11921
08:24:36,280 --> 08:24:41,440
website you'll see a bunch of sections

11922
08:24:38,718 --> 08:24:43,600
so like how do you install it um

11923
08:24:41,440 --> 08:24:46,080
tutorials on how to use Triton so

11924
08:24:43,600 --> 08:24:47,760
there's like a bunch here which I don't

11925
08:24:46,080 --> 08:24:51,120
necessarily cover in this course but you

11926
08:24:47,760 --> 08:24:54,760
can you can go over more of these uh if

11927
08:24:51,120 --> 08:24:56,558
you want to um and then there's the

11928
08:24:54,760 --> 08:24:59,080
important ones that we care about so

11929
08:24:56,558 --> 08:25:02,318
Triton um what are like the Triton docs

11930
08:24:59,080 --> 08:25:04,200
there's like jit autotune heris all that

11931
08:25:02,318 --> 08:25:07,558
um just going

11932
08:25:04,200 --> 08:25:10,360
through going through

11933
08:25:07,558 --> 08:25:11,680
functions um and then we get into Triton

11934
08:25:10,360 --> 08:25:14,760
language which is the most important

11935
08:25:11,680 --> 08:25:16,680
part of all um we have a bunch of

11936
08:25:14,760 --> 08:25:18,000
different operations here with tridon

11937
08:25:16,680 --> 08:25:19,958
and I'll go into the whole design of

11938
08:25:18,000 --> 08:25:22,120
Tron in a second here but this it just

11939
08:25:19,958 --> 08:25:25,318
like makes it really easy to see that

11940
08:25:22,120 --> 08:25:27,878
all of the operations are right here so

11941
08:25:25,318 --> 08:25:32,200
um like the programming model uh

11942
08:25:27,878 --> 08:25:34,440
operations to know creation Ops um so if

11943
08:25:32,200 --> 08:25:36,080
you have like a if you have an X tensor

11944
08:25:34,440 --> 08:25:37,798
and you want to make a y but you don't

11945
08:25:36,080 --> 08:25:39,080
want to populate it with anything or you

11946
08:25:37,798 --> 08:25:42,040
just want to make it zeros you can say

11947
08:25:39,080 --> 08:25:44,600
zeros like right as a creation um shape

11948
08:25:42,040 --> 08:25:46,840
manipulation linear algebra so dot

11949
08:25:44,600 --> 08:25:50,920
product

11950
08:25:46,840 --> 08:25:52,398
um pointer Ops uh memory pointer Ops

11951
08:25:50,920 --> 08:25:55,200
bunch of things math there's a lot of

11952
08:25:52,398 --> 08:25:56,798
math Ops reduction Ops so like maximum

11953
08:25:55,200 --> 08:25:58,878
or you have an array and you're trying

11954
08:25:56,798 --> 08:26:00,840
to find the maximum of it and you reduce

11955
08:25:58,878 --> 08:26:04,040
it to just returning a single

11956
08:26:00,840 --> 08:26:06,280
value uh scan and swort operations

11957
08:26:04,040 --> 08:26:08,398
atomics like we went on previously

11958
08:26:06,280 --> 08:26:10,238
random number generation and Etc right

11959
08:26:08,398 --> 08:26:12,798
there's there's a lot here even even uh

11960
08:26:10,238 --> 08:26:14,000
debugging and printing too so try and

11961
08:26:12,798 --> 08:26:16,160
this is this is where all of the

11962
08:26:14,000 --> 08:26:18,318
operations are that you're going to find

11963
08:26:16,160 --> 08:26:23,160
um which makes this really easy to go

11964
08:26:18,318 --> 08:26:27,238
through um but if we go through uh where

11965
08:26:23,160 --> 08:26:31,958
is it introduction yes so the whole idea

11966
08:26:27,238 --> 08:26:36,558
here and if I go to uh Triton

11967
08:26:31,958 --> 08:26:39,160
paper yes this one so I'll look at this

11968
08:26:36,558 --> 08:26:41,120
in a second here but this is essentially

11969
08:26:39,160 --> 08:26:43,398
what Triton was inspired by is this this

11970
08:26:41,120 --> 08:26:45,000
paper here for tille neural net

11971
08:26:43,398 --> 08:26:48,160
computations like we went on in the

11972
08:26:45,000 --> 08:26:50,638
previous chapter right um and and this

11973
08:26:48,160 --> 08:26:53,600
this is the whole idea here is you have

11974
08:26:50,638 --> 08:26:55,280
scalar Pro Cuda has a scalar program and

11975
08:26:53,600 --> 08:26:57,238
block threads versus Trent which has a

11976
08:26:55,280 --> 08:26:58,718
block program and scal threads it's like

11977
08:26:57,238 --> 08:27:01,760
okay what does this mean this is super

11978
08:26:58,718 --> 08:27:03,920
weird um there's a lot of like geometry

11979
08:27:01,760 --> 08:27:06,718
happening here so like how do we

11980
08:27:03,920 --> 08:27:08,958
actually break this down

11981
08:27:06,718 --> 08:27:12,878
um

11982
08:27:08,958 --> 08:27:15,440
now to clarify the reason why Cuda is a

11983
08:27:12,878 --> 08:27:17,798
scale scalar program with block threads

11984
08:27:15,440 --> 08:27:19,878
is because you write a kernel to operate

11985
08:27:17,798 --> 08:27:21,638
at the level of threads or scalers in

11986
08:27:19,878 --> 08:27:23,638
this case so that's why it's a scaler

11987
08:27:21,638 --> 08:27:27,600
program each individual kernel runs on a

11988
08:27:23,638 --> 08:27:29,600
thread um but you need to be aware you

11989
08:27:27,600 --> 08:27:31,558
need to be implicitly aware that those

11990
08:27:29,600 --> 08:27:35,160
kernels also operate on the level of

11991
08:27:31,558 --> 08:27:36,680
groups too so when you have uh like for

11992
08:27:35,160 --> 08:27:39,478
example shared memory like that's

11993
08:27:36,680 --> 08:27:42,638
something you need to worry about right

11994
08:27:39,478 --> 08:27:45,760
um Cuda

11995
08:27:42,638 --> 08:27:47,478
has Cuda has blocked threads so scalar

11996
08:27:45,760 --> 08:27:49,398
program it's like the actual kernel that

11997
08:27:47,478 --> 08:27:51,520
you write and then block threads with

11998
08:27:49,398 --> 08:27:53,160
the idea of when you actually write the

11999
08:27:51,520 --> 08:27:54,558
the kernel itself which runs on a thread

12000
08:27:53,160 --> 08:27:55,798
it has to be aware of of all the other

12001
08:27:54,558 --> 08:27:57,878
threads too that that's kind of what I'm

12002
08:27:55,798 --> 08:28:01,638
getting out there and then Triton is

12003
08:27:57,878 --> 08:28:04,840
abstracted up to thread blocks so

12004
08:28:01,638 --> 08:28:06,280
um this this essentially means compiler

12005
08:28:04,840 --> 08:28:09,478
takes care of all these thread level

12006
08:28:06,280 --> 08:28:11,520
operations uh for us so when you write

12007
08:28:09,478 --> 08:28:13,238
something in Triton instead of having it

12008
08:28:11,520 --> 08:28:15,200
run on each individual thread and have

12009
08:28:13,238 --> 08:28:16,478
them communicate and be aware of that

12010
08:28:15,200 --> 08:28:19,200
it's going to write on the level of

12011
08:28:16,478 --> 08:28:21,040
blocks and all of those like thread

12012
08:28:19,200 --> 08:28:22,558
level instructions and optimizations are

12013
08:28:21,040 --> 08:28:24,040
going to be handled by the compiler for

12014
08:28:22,558 --> 08:28:26,600
you so you don't have to actually worry

12015
08:28:24,040 --> 08:28:30,280
about those um

12016
08:28:26,600 --> 08:28:32,520
and then when we talk about um when we

12017
08:28:30,280 --> 08:28:32,520
talk

12018
08:28:32,680 --> 08:28:38,638
about when we talk about scalar threads

12019
08:28:36,200 --> 08:28:41,638
it means you don't have to be uh as

12020
08:28:38,638 --> 08:28:43,760
worried about them talking inter uh like

12021
08:28:41,638 --> 08:28:45,920
interconnected with each other you don't

12022
08:28:43,760 --> 08:28:47,600
have to be aware because Cuda actually

12023
08:28:45,920 --> 08:28:49,760
handles that part so since you're

12024
08:28:47,600 --> 08:28:51,478
writing on the level of blocks you don't

12025
08:28:49,760 --> 08:28:53,000
actually have to worry about interthread

12026
08:28:51,478 --> 08:28:55,478
Communications you just have to worry

12027
08:28:53,000 --> 08:28:57,878
about what exactly the block is doing um

12028
08:28:55,478 --> 08:28:59,638
write the oper out clearly for that and

12029
08:28:57,878 --> 08:29:03,398
then TR will actually handle that under

12030
08:28:59,638 --> 08:29:05,160
the hood so you saw how um there's

12031
08:29:03,398 --> 08:29:07,478
there's very little operations in the

12032
08:29:05,160 --> 08:29:09,280
tron do language section this is because

12033
08:29:07,478 --> 08:29:11,558
the compiler is able to handle a lot of

12034
08:29:09,280 --> 08:29:13,840
the additional operations that that

12035
08:29:11,558 --> 08:29:16,398
allow for for performance increases

12036
08:29:13,840 --> 08:29:18,920
right um so so that's that's the whole

12037
08:29:16,398 --> 08:29:21,680
idea behind um that's the whole idea

12038
08:29:18,920 --> 08:29:25,280
behind Triton and that whole uh blocked

12039
08:29:21,680 --> 08:29:27,040
versus scalar uh blocked versus blocked

12040
08:29:25,280 --> 08:29:30,040
and scalar thread

12041
08:29:27,040 --> 08:29:30,040
philosophy

12042
08:29:30,238 --> 08:29:35,600
so why can't we just skip Cuda and go

12043
08:29:32,718 --> 08:29:38,160
straight to Triton why can't we do this

12044
08:29:35,600 --> 08:29:42,080
well Tron is an abstraction on top of

12045
08:29:38,160 --> 08:29:43,798
Cuda so you have lower level stuff that

12046
08:29:42,080 --> 08:29:46,680
offers optimizations and that you can be

12047
08:29:43,798 --> 08:29:47,558
explicit about to the Nvidia compiler

12048
08:29:46,680 --> 08:29:50,558
the

12049
08:29:47,558 --> 08:29:53,160
nbcc um and Triton takes advantage of

12050
08:29:50,558 --> 08:29:54,638
those and bumps it up a few layers uh to

12051
08:29:53,160 --> 08:29:56,238
something that you can sort of

12052
08:29:54,638 --> 08:29:58,798
understand easily and and write less

12053
08:29:56,238 --> 08:30:00,520
boiler plate code for so we still need

12054
08:29:58,798 --> 08:30:01,520
to understand how Cuda Works to ensure

12055
08:30:00,520 --> 08:30:03,520
that we're applying the right

12056
08:30:01,520 --> 08:30:05,600
optimizations you can't naively write

12057
08:30:03,520 --> 08:30:07,238
Triton code without knowing uh what's

12058
08:30:05,600 --> 08:30:11,160
going on on a low level it just helps

12059
08:30:07,238 --> 08:30:13,478
helps you prevent boiler plate

12060
08:30:11,160 --> 08:30:16,200
um you also may want to optimize your

12061
08:30:13,478 --> 08:30:17,440
own kernels in Cuda right so going back

12062
08:30:16,200 --> 08:30:19,958
to what I said before you kind of need

12063
08:30:17,440 --> 08:30:21,520
to know the low LEL operations uh in

12064
08:30:19,958 --> 08:30:24,760
order to make sure that everything is

12065
08:30:21,520 --> 08:30:27,000
working as intended right um and if you

12066
08:30:24,760 --> 08:30:30,200
want to build on top of Tri or build

12067
08:30:27,000 --> 08:30:31,920
things like it or abstract above Cuda

12068
08:30:30,200 --> 08:30:33,398
which is shr you you need to learn Cuda

12069
08:30:31,920 --> 08:30:35,680
you need to understand what is Cuda

12070
08:30:33,398 --> 08:30:37,280
doing so that you can build on top of it

12071
08:30:35,680 --> 08:30:40,280
so you can Leverage What it contains

12072
08:30:37,280 --> 08:30:42,760
already right

12073
08:30:40,280 --> 08:30:45,280
um and so if we go to the Triton paper

12074
08:30:42,760 --> 08:30:47,318
here Intermediate Language and compiler

12075
08:30:45,280 --> 08:30:49,280
for tile neuronet

12076
08:30:47,318 --> 08:30:51,958
computations that this is It's

12077
08:30:49,280 --> 08:30:54,440
essentially just

12078
08:30:51,958 --> 08:30:57,958
um

12079
08:30:54,440 --> 08:31:02,120
it it does everything that kublos and

12080
08:30:57,958 --> 08:31:05,000
and CNN does but does it um roughly as

12081
08:31:02,120 --> 08:31:06,360
fast and without a ton of boiler plate

12082
08:31:05,000 --> 08:31:08,160
so that that's like the whole idea is

12083
08:31:06,360 --> 08:31:10,440
you have these tiled computations that

12084
08:31:08,160 --> 08:31:12,840
you have to do in kublos and cdnn and it

12085
08:31:10,440 --> 08:31:14,840
takes care of those so I'm not going to

12086
08:31:12,840 --> 08:31:17,238
go through this um this is like this is

12087
08:31:14,840 --> 08:31:20,398
a separate course going through all of

12088
08:31:17,238 --> 08:31:22,360
Triton but we're we're going to go over

12089
08:31:20,398 --> 08:31:24,638
like how how do the basics work right so

12090
08:31:22,360 --> 08:31:27,200
that you understand um how Triton can be

12091
08:31:24,638 --> 08:31:30,958
applied um and maybe it'll give you some

12092
08:31:27,200 --> 08:31:33,680
ideas as to what to do later on okay so

12093
08:31:30,958 --> 08:31:36,318
now we actually go into an example of a

12094
08:31:33,680 --> 08:31:37,840
vector Edition kernel in Trine all right

12095
08:31:36,318 --> 08:31:40,120
so I'm going to try to break this down

12096
08:31:37,840 --> 08:31:41,360
as kind of as as efficiently as possible

12097
08:31:40,120 --> 08:31:43,040
uh this stuff isn't too hard it's

12098
08:31:41,360 --> 08:31:44,600
supposed to be kind of a break from

12099
08:31:43,040 --> 08:31:47,238
doing uh things like matrix

12100
08:31:44,600 --> 08:31:48,798
multiplication so uh you you'll you'll

12101
08:31:47,238 --> 08:31:51,200
probably find this you'll probably find

12102
08:31:48,798 --> 08:31:52,318
this is a end up end up being you'll

12103
08:31:51,200 --> 08:31:54,798
probably find this ends up being a

12104
08:31:52,318 --> 08:31:57,520
breeze uh so we start off you know we

12105
08:31:54,798 --> 08:31:59,600
import torch so Triton is very closely

12106
08:31:57,520 --> 08:32:02,120
linked with torch uh so we we end up

12107
08:31:59,600 --> 08:32:04,238
using that to to handle some other stuff

12108
08:32:02,120 --> 08:32:07,160
like initialize arrays and matrices and

12109
08:32:04,238 --> 08:32:08,878
stuff we import Triton the Triton

12110
08:32:07,160 --> 08:32:12,558
language and then shorthand version of

12111
08:32:08,878 --> 08:32:15,000
that and then we go down further um we

12112
08:32:12,558 --> 08:32:16,600
initialize uh a seed so that the results

12113
08:32:15,000 --> 08:32:19,000
are reproducible when we when you

12114
08:32:16,600 --> 08:32:20,238
randomly initialize stuff um to ensure

12115
08:32:19,000 --> 08:32:22,478
that you know we don't get errors or

12116
08:32:20,238 --> 08:32:25,638
whatever it's a good practice and then

12117
08:32:22,478 --> 08:32:28,958
we have a size of 2 to the 25 so if we

12118
08:32:25,638 --> 08:32:32,478
go into here we go

12119
08:32:28,958 --> 08:32:35,798
uh this is 33 million elements long

12120
08:32:32,478 --> 08:32:35,798
right so if we go

12121
08:32:37,558 --> 08:32:44,600
this 33.5 million elements

12122
08:32:41,478 --> 08:32:46,600
long um and then we just initialize our

12123
08:32:44,600 --> 08:32:49,000
X and Y the two two that we're going to

12124
08:32:46,600 --> 08:32:50,718
add on device that are randomly randomly

12125
08:32:49,000 --> 08:32:52,638
initialized uh and then we just have

12126
08:32:50,718 --> 08:32:54,120
some benchmarking stuff down here don't

12127
08:32:52,638 --> 08:32:56,440
worry about this this is just for

12128
08:32:54,120 --> 08:32:58,760
testing and and seeing how perform which

12129
08:32:56,440 --> 08:33:00,920
we which we'll see in a second here um

12130
08:32:58,760 --> 08:33:02,680
but going up when we're actually adding

12131
08:33:00,920 --> 08:33:04,760
them together this is when you when you

12132
08:33:02,680 --> 08:33:06,958
do um when you're actually adding them

12133
08:33:04,760 --> 08:33:08,360
you're going to add X and Y which are

12134
08:33:06,958 --> 08:33:10,920
tensors and then you're going to return

12135
08:33:08,360 --> 08:33:14,520
a tensor right so it's just X Plus y

12136
08:33:10,920 --> 08:33:17,000
return output that's it very simple um

12137
08:33:14,520 --> 08:33:19,120
this is essentially uh an easier way of

12138
08:33:17,000 --> 08:33:23,760
doing Cuda Malik so instead of Cuda

12139
08:33:19,120 --> 08:33:25,478
Malik and C or or CA c c C++ you're

12140
08:33:23,760 --> 08:33:27,440
going to go tor. empty like so it's

12141
08:33:25,478 --> 08:33:29,718
going to have the same shape as X but

12142
08:33:27,440 --> 08:33:31,558
it's going to be populated with zeros uh

12143
08:33:29,718 --> 08:33:33,798
we do an assert so we make sure all of

12144
08:33:31,558 --> 08:33:36,760
our all of our tensors are on the

12145
08:33:33,798 --> 08:33:39,360
device we set num elements to literally

12146
08:33:36,760 --> 08:33:41,398
just num elements of the output so how

12147
08:33:39,360 --> 08:33:45,360
many what is like the length of this

12148
08:33:41,398 --> 08:33:48,798
array um and then we have this weird uh

12149
08:33:45,360 --> 08:33:50,318
configuration of uh of like how we how

12150
08:33:48,798 --> 08:33:52,318
we actually launch a kernel and how we

12151
08:33:50,318 --> 08:33:55,398
Define like the the dim three and and

12152
08:33:52,318 --> 08:33:57,440
the grid size and all that stuff um so

12153
08:33:55,398 --> 08:33:58,798
we have a Lambda function here don't

12154
08:33:57,440 --> 08:34:01,000
worry about this too much by the way

12155
08:33:58,798 --> 08:34:02,920
this this you care about performance not

12156
08:34:01,000 --> 08:34:06,878
not like understanding how the syntax

12157
08:34:02,920 --> 08:34:08,878
Works under the hood um but this is a

12158
08:34:06,878 --> 08:34:10,840
just a Lambda function and inside of

12159
08:34:08,878 --> 08:34:12,840
here all like all you have to worry

12160
08:34:10,840 --> 08:34:16,360
about is that we're doing a ceiling

12161
08:34:12,840 --> 08:34:18,920
division of n elements so let's say n

12162
08:34:16,360 --> 08:34:21,280
elements is like24 you have an array

12163
08:34:18,920 --> 08:34:22,718
1024 elements and you want to add them

12164
08:34:21,280 --> 08:34:25,360
together in blocks and your block size

12165
08:34:22,718 --> 08:34:28,280
is 256 so what you would do is you would

12166
08:34:25,360 --> 08:34:33,000
par that 1024 over four different blocks

12167
08:34:28,280 --> 08:34:35,638
of size 256 right but if you have say

12168
08:34:33,000 --> 08:34:37,280
1,25 elements then you want to make sure

12169
08:34:35,638 --> 08:34:38,760
that you give it an extra block so that

12170
08:34:37,280 --> 08:34:39,920
it doesn't miss that element right or

12171
08:34:38,760 --> 08:34:41,718
else you're not going to get the right

12172
08:34:39,920 --> 08:34:44,718
answer so you have to actually do a

12173
08:34:41,718 --> 08:34:46,558
ceiling div um so that it rounds up to

12174
08:34:44,718 --> 08:34:48,000
five blocks instead of just rounding

12175
08:34:46,558 --> 08:34:50,160
back down to four because then you'll

12176
08:34:48,000 --> 08:34:52,760
end up missing that extra one right uh

12177
08:34:50,160 --> 08:34:55,200
so that's all we're doing here um and

12178
08:34:52,760 --> 08:34:58,000
then just to launch the kernel you I

12179
08:34:55,200 --> 08:35:00,318
know it's you do like this this this uh

12180
08:34:58,000 --> 08:35:02,478
this function and then you index with

12181
08:35:00,318 --> 08:35:04,160
this grid term and then you pass in your

12182
08:35:02,478 --> 08:35:07,238
variables after it's like hm that's

12183
08:35:04,160 --> 08:35:10,318
weird um but don't don't don't worry

12184
08:35:07,238 --> 08:35:13,238
about this too much so we just pass in

12185
08:35:10,318 --> 08:35:15,920
the grid as like that uh those like

12186
08:35:13,238 --> 08:35:18,280
those like alligator symbols um that's

12187
08:35:15,920 --> 08:35:20,238
that's all this is that's your kernel

12188
08:35:18,280 --> 08:35:22,120
launch configuration and then you have

12189
08:35:20,238 --> 08:35:25,558
the actual parameters themselves so

12190
08:35:22,120 --> 08:35:25,558
instead of going

12191
08:35:26,398 --> 08:35:28,840
we just

12192
08:35:45,840 --> 08:35:50,638
go we just do that very so it's it's

12193
08:35:48,878 --> 08:35:52,638
like a syntactical thing

12194
08:35:50,638 --> 08:35:55,600
right

12195
08:35:52,638 --> 08:35:57,760
um anyways that's that that that

12196
08:35:55,600 --> 08:35:59,200
shouldn't be too much of a question I'm

12197
08:35:57,760 --> 08:36:01,360
I'm not going over this because we care

12198
08:35:59,200 --> 08:36:03,398
about performance as opposed to like

12199
08:36:01,360 --> 08:36:06,200
what the heck what the heck all these

12200
08:36:03,398 --> 08:36:09,280
what the heck the syntax is um but yeah

12201
08:36:06,200 --> 08:36:13,040
so so going up to the actual mechanics

12202
08:36:09,280 --> 08:36:16,840
of a kernel inen and comparing that sort

12203
08:36:13,040 --> 08:36:18,200
of side by side with with Cuda um we

12204
08:36:16,840 --> 08:36:20,760
have we have

12205
08:36:18,200 --> 08:36:22,920
X we have we have to actually I forgot

12206
08:36:20,760 --> 08:36:24,520
to say we add this Tron jet decorator at

12207
08:36:22,920 --> 08:36:26,160
the top to say that we want it to we

12208
08:36:24,520 --> 08:36:28,000
want it to be jet compiled by tridon

12209
08:36:26,160 --> 08:36:28,920
just like be aware of that CU if you

12210
08:36:28,000 --> 08:36:31,520
don't add this you're going to get

12211
08:36:28,920 --> 08:36:33,238
errors um but for all the different

12212
08:36:31,520 --> 08:36:35,080
variables you know you have your X and Y

12213
08:36:33,238 --> 08:36:37,878
your output your Nom elements and then

12214
08:36:35,080 --> 08:36:40,680
the block size um which you you know

12215
08:36:37,878 --> 08:36:42,440
pass into here XY output output elements

12216
08:36:40,680 --> 08:36:45,958
and elements block

12217
08:36:42,440 --> 08:36:48,318
size now we have these as pointers

12218
08:36:45,958 --> 08:36:51,440
because in memory it's going to be it's

12219
08:36:48,318 --> 08:36:54,600
going to be laid out as the the first

12220
08:36:51,440 --> 08:36:56,680
element in that array is going to or

12221
08:36:54,600 --> 08:36:58,558
essentially this this pointer is going

12222
08:36:56,680 --> 08:37:00,040
to be the first element in that array so

12223
08:36:58,558 --> 08:37:02,318
you want to start from the you want to

12224
08:37:00,040 --> 08:37:04,318
base it off of the start of that entire

12225
08:37:02,318 --> 08:37:05,840
tensor or array and you want to continue

12226
08:37:04,318 --> 08:37:07,680
from there right so that's how Triton is

12227
08:37:05,840 --> 08:37:11,798
going to interpret that uh and then you

12228
08:37:07,680 --> 08:37:14,160
do the same for uh y output pointer um

12229
08:37:11,798 --> 08:37:17,440
and then we just continue on

12230
08:37:14,160 --> 08:37:21,718
so so jumping down a little bit we see

12231
08:37:17,440 --> 08:37:24,200
uh p and so P essentially says which

12232
08:37:21,718 --> 08:37:29,440
which block are we add in the grid right

12233
08:37:24,200 --> 08:37:32,638
um now we have to be careful about this

12234
08:37:29,440 --> 08:37:34,558
so a a good way to think about which

12235
08:37:32,638 --> 08:37:36,520
block we are in the grid is to actually

12236
08:37:34,558 --> 08:37:39,280
go down to here and see how does this

12237
08:37:36,520 --> 08:37:42,360
apply right so when we're actually in

12238
08:37:39,280 --> 08:37:46,160
this array um let's say block size is 64

12239
08:37:42,360 --> 08:37:49,120
and we have 256 elements in this array

12240
08:37:46,160 --> 08:37:50,760
um you don't want to just say zero or

12241
08:37:49,120 --> 08:37:52,478
one or two cuz those are going to be

12242
08:37:50,760 --> 08:37:54,440
individual elements right when we're

12243
08:37:52,478 --> 08:37:56,478
looking at the actual data cuz remember

12244
08:37:54,440 --> 08:37:59,878
this is how we're actually indexing data

12245
08:37:56,478 --> 08:38:02,040
we want to make sure that uh we're

12246
08:37:59,878 --> 08:38:03,878
keeping this block length in mind CU

12247
08:38:02,040 --> 08:38:06,120
block size is Big right so we're going

12248
08:38:03,878 --> 08:38:09,798
to advance

12249
08:38:06,120 --> 08:38:11,558
um block size times the number of blocks

12250
08:38:09,798 --> 08:38:13,760
and then the offsets are actually what's

12251
08:38:11,558 --> 08:38:16,040
important so it's like whatever that

12252
08:38:13,760 --> 08:38:20,398
number is and then we're going to

12253
08:38:16,040 --> 08:38:24,878
arrange an additional thing of 02 block

12254
08:38:20,398 --> 08:38:26,600
size so for like say 64 to 128 you're

12255
08:38:24,878 --> 08:38:28,878
going to start start the block start is

12256
08:38:26,600 --> 08:38:32,238
going to be 64 and then we're going to

12257
08:38:28,878 --> 08:38:35,440
arrange a bunch of different uh indices

12258
08:38:32,238 --> 08:38:38,360
between 64 uh which essentially just

12259
08:38:35,440 --> 08:38:41,840
going to be an array of 64 up to one up

12260
08:38:38,360 --> 08:38:43,318
to 128 right and uh that's going to

12261
08:38:41,840 --> 08:38:45,318
that's how we're going to index our data

12262
08:38:43,318 --> 08:38:47,200
so we do Triton language. a range zero

12263
08:38:45,318 --> 08:38:48,520
to block size and that's what that is

12264
08:38:47,200 --> 08:38:50,718
and then we just add whatever block

12265
08:38:48,520 --> 08:38:52,478
start is to that so hopefully that kind

12266
08:38:50,718 --> 08:38:54,878
of like makes sense in your head now

12267
08:38:52,478 --> 08:38:57,238
jumping back to P here you might want to

12268
08:38:54,878 --> 08:38:59,558
pay attention attention to this term so

12269
08:38:57,238 --> 08:39:02,200
in here we have axis and Builder don't

12270
08:38:59,558 --> 08:39:08,718
worry about this just worry about axis

12271
08:39:02,200 --> 08:39:13,280
so axis is like uh block idx dox right

12272
08:39:08,718 --> 08:39:17,558
um block block like X is the same as

12273
08:39:13,280 --> 08:39:20,840
axis zero a uh block idx Y is the same

12274
08:39:17,558 --> 08:39:23,520
as axis one and Z is equal to three

12275
08:39:20,840 --> 08:39:26,000
right so we have to keep this in mind

12276
08:39:23,520 --> 08:39:28,840
when we're writing more like you know 2D

12277
08:39:26,000 --> 08:39:31,718
you know 3D spatial uh kernels then we

12278
08:39:28,840 --> 08:39:33,680
have to keep in mind this access term so

12279
08:39:31,718 --> 08:39:34,718
luckily right now this is very simple

12280
08:39:33,680 --> 08:39:36,558
but this is something we're going to

12281
08:39:34,718 --> 08:39:39,280
want to keep in mind if if would you end

12282
08:39:36,558 --> 08:39:41,080
up writing uh this is something you want

12283
08:39:39,280 --> 08:39:45,360
to keep in mind if you end up writing uh

12284
08:39:41,080 --> 08:39:47,718
more you know 2D 3D stuff um now going

12285
08:39:45,360 --> 08:39:51,360
down a little bit um we have this mask

12286
08:39:47,718 --> 08:39:54,680
term so mask equals

12287
08:39:51,360 --> 08:39:57,958
offsets mask equals whatever the Boolean

12288
08:39:54,680 --> 08:40:01,040
uh I guess the the the Boolean output of

12289
08:39:57,958 --> 08:40:04,638
whatever uh of if offsets is less than

12290
08:40:01,040 --> 08:40:08,478
num elements so what this essentially

12291
08:40:04,638 --> 08:40:11,840
does is we have these offsets which are

12292
08:40:08,478 --> 08:40:14,280
the indices in the actual arrays itself

12293
08:40:11,840 --> 08:40:16,558
so the input and the output Point input

12294
08:40:14,280 --> 08:40:18,318
sorry the X and Y pointers whichever

12295
08:40:16,558 --> 08:40:20,878
indices we're at in those we want to

12296
08:40:18,318 --> 08:40:22,120
make sure that those do not surpass uh

12297
08:40:20,878 --> 08:40:25,558
we don't we want to make sure those

12298
08:40:22,120 --> 08:40:27,000
don't do not equal to or uh pass num

12299
08:40:25,558 --> 08:40:29,558
elements right it's going to be zero up

12300
08:40:27,000 --> 08:40:31,878
to num elements minus one so we want to

12301
08:40:29,558 --> 08:40:34,120
make sure that we mask everything off

12302
08:40:31,878 --> 08:40:36,398
that is num elements or past that point

12303
08:40:34,120 --> 08:40:38,718
right and that's what this is so mask is

12304
08:40:36,398 --> 08:40:41,160
just going to be essentially an array

12305
08:40:38,718 --> 08:40:43,398
it's going to do uh like it's

12306
08:40:41,160 --> 08:40:44,840
essentially going to uh say you know we

12307
08:40:43,398 --> 08:40:47,318
have all these offsets we have this

12308
08:40:44,840 --> 08:40:51,318
giant like array in memory and we're

12309
08:40:47,318 --> 08:40:56,398
going to see um if like we have a you

12310
08:40:51,318 --> 08:40:58,440
know 64 65 67 122 whatever we want to

12311
08:40:56,398 --> 08:41:01,160
make sure that those numbers are less

12312
08:40:58,440 --> 08:41:02,680
than a numb elements right um so we want

12313
08:41:01,160 --> 08:41:04,200
to essentially just making sure that

12314
08:41:02,680 --> 08:41:06,360
we're not accessing something that's

12315
08:41:04,200 --> 08:41:08,160
outside of uh our data structure in

12316
08:41:06,360 --> 08:41:10,958
memory we have this whole Space we want

12317
08:41:08,160 --> 08:41:13,360
to keep it masked to this one section

12318
08:41:10,958 --> 08:41:15,920
right that's what this does and so we

12319
08:41:13,360 --> 08:41:17,840
get a new array which is mask and that's

12320
08:41:15,920 --> 08:41:20,360
just a bunch of essentially just a bunch

12321
08:41:17,840 --> 08:41:21,760
of ones and zeros um and this this

12322
08:41:20,360 --> 08:41:25,440
behaves the same way that this if

12323
08:41:21,760 --> 08:41:28,040
statement does inside of an addition

12324
08:41:25,440 --> 08:41:32,080
Vector Edition kernel in Cuda right same

12325
08:41:28,040 --> 08:41:35,600
idea um now going down further um we

12326
08:41:32,080 --> 08:41:38,958
actually load these efficiently into a

12327
08:41:35,600 --> 08:41:41,718
shared memory so don't

12328
08:41:38,958 --> 08:41:43,920
um don't worry about how it actually how

12329
08:41:41,718 --> 08:41:45,398
Triton handles data how how it you know

12330
08:41:43,920 --> 08:41:46,958
transfers data that's a that's an

12331
08:41:45,398 --> 08:41:49,238
optimization that's taken care of for

12332
08:41:46,958 --> 08:41:52,398
you so just assume that these are going

12333
08:41:49,238 --> 08:41:56,398
to load in um like starting from the

12334
08:41:52,398 --> 08:42:00,160
like the beginning of X up to

12335
08:41:56,398 --> 08:42:02,638
um up to the end so it's going to

12336
08:42:00,160 --> 08:42:04,238
essentially just each each point each

12337
08:42:02,638 --> 08:42:05,760
point in memory it's going to each each

12338
08:42:04,238 --> 08:42:08,200
data point in memory it's going to load

12339
08:42:05,760 --> 08:42:10,080
those in and it's going to do a mask and

12340
08:42:08,200 --> 08:42:12,000
it's going to say do we want to actually

12341
08:42:10,080 --> 08:42:14,160
compute these values or not that's what

12342
08:42:12,000 --> 08:42:16,238
this is doing so whichever point it

12343
08:42:14,160 --> 08:42:17,760
starts at all the way up to all the

12344
08:42:16,238 --> 08:42:19,398
different offsets so it's like this

12345
08:42:17,760 --> 08:42:21,878
point and then it's copied and then all

12346
08:42:19,398 --> 08:42:23,840
the different offsets um and then you

12347
08:42:21,878 --> 08:42:25,558
have a mask applied to those as well to

12348
08:42:23,840 --> 08:42:27,718
say which ones do we want want to

12349
08:42:25,558 --> 08:42:30,200
compute so if these ones are like extra

12350
08:42:27,718 --> 08:42:31,760
just like compute this section um that

12351
08:42:30,200 --> 08:42:33,040
that's what that's what this uh load is

12352
08:42:31,760 --> 08:42:37,120
going to do and it's going to

12353
08:42:33,040 --> 08:42:39,680
efficiently load this into um shared

12354
08:42:37,120 --> 08:42:42,160
memory so the the the fast memory on the

12355
08:42:39,680 --> 08:42:44,160
GPU the one on the actual uh streaming

12356
08:42:42,160 --> 08:42:46,360
multiprocessor that that's what that's

12357
08:42:44,160 --> 08:42:49,760
what this is taking care of for you and

12358
08:42:46,360 --> 08:42:53,718
then we do the exact same thing for y

12359
08:42:49,760 --> 08:42:55,558
now try and Implement um these blockwise

12360
08:42:53,718 --> 08:42:57,760
operations very efficiently so you don't

12361
08:42:55,558 --> 08:43:00,080
actually need to do any advanced stuff

12362
08:42:57,760 --> 08:43:03,440
you literally just um have these loaded

12363
08:43:00,080 --> 08:43:05,680
onto SRAM and it'll do a uh element wise

12364
08:43:03,440 --> 08:43:08,680
addition so it'll it'll go through each

12365
08:43:05,680 --> 08:43:10,440
one and just add together and notice how

12366
08:43:08,680 --> 08:43:12,760
this is blue instead of red that means

12367
08:43:10,440 --> 08:43:15,718
like it's taking care of by uh Tron for

12368
08:43:12,760 --> 08:43:19,080
us so um that's that's literally how you

12369
08:43:15,718 --> 08:43:21,798
compute the output um and then we just

12370
08:43:19,080 --> 08:43:23,798
store this back again with another um

12371
08:43:21,798 --> 08:43:25,760
you know with another with another data

12372
08:43:23,798 --> 08:43:27,718
transfer operation

12373
08:43:25,760 --> 08:43:30,440
find and so that's just going to be the

12374
08:43:27,718 --> 08:43:32,878
same idea as here so uh the starting

12375
08:43:30,440 --> 08:43:35,680
place in memory plus you know the offset

12376
08:43:32,878 --> 08:43:37,718
of all the offset indices for for that

12377
08:43:35,680 --> 08:43:40,238
block

12378
08:43:37,718 --> 08:43:43,318
um the output itself so we're going to

12379
08:43:40,238 --> 08:43:46,600
just store the output

12380
08:43:43,318 --> 08:43:49,200
um and then have this mask as we did

12381
08:43:46,600 --> 08:43:51,920
here and that's how a tri kernel works

12382
08:43:49,200 --> 08:43:53,600
so hopefully this makes sense feel free

12383
08:43:51,920 --> 08:43:56,238
to rewatch some parts if they didn't

12384
08:43:53,600 --> 08:43:57,398
entirely you know click in your head uh

12385
08:43:56,238 --> 08:44:01,520
but now we're actually going to jump

12386
08:43:57,398 --> 08:44:04,238
into the uh softmax Trident function

12387
08:44:01,520 --> 08:44:05,840
okay so now we jump into softmax and

12388
08:44:04,238 --> 08:44:07,398
instead of just jumping right into code

12389
08:44:05,840 --> 08:44:09,920
I'm going to do a manual like hand

12390
08:44:07,398 --> 08:44:11,600
example with this

12391
08:44:09,920 --> 08:44:13,318
um

12392
08:44:11,600 --> 08:44:17,238
so let

12393
08:44:13,318 --> 08:44:18,958
me delete that real quick um essentially

12394
08:44:17,238 --> 08:44:20,638
what we're doing here I have a C file

12395
08:44:18,958 --> 08:44:23,958
we're just doing this in C to understand

12396
08:44:20,638 --> 08:44:26,798
what it's doing intuitively um we have

12397
08:44:23,958 --> 08:44:31,798
an array of floats 1 to three so it it

12398
08:44:26,798 --> 08:44:31,798
literally just looks like this um

12399
08:44:31,878 --> 08:44:37,558
x x is that and so we're going to

12400
08:44:34,280 --> 08:44:37,558
calculate the soft Max of

12401
08:44:42,760 --> 08:44:45,760
X

12402
08:44:46,238 --> 08:44:53,638
um so how this typically goes is if we

12403
08:44:49,798 --> 08:44:57,200
search this up on uh on

12404
08:44:53,638 --> 08:44:59,478
Google soft Max activation function it

12405
08:44:57,200 --> 08:45:01,798
looks like this right so you have this

12406
08:44:59,478 --> 08:45:04,920
exponentiate this input so you have an

12407
08:45:01,798 --> 08:45:07,478
input Vector um and this this this

12408
08:45:04,920 --> 08:45:09,718
symbol is a softmax function and you go

12409
08:45:07,478 --> 08:45:11,120
over each one uh and then you

12410
08:45:09,718 --> 08:45:13,798
essentially see how much each

12411
08:45:11,120 --> 08:45:16,280
exponentiated value contributes to the

12412
08:45:13,798 --> 08:45:19,200
total uh the total sum of all the

12413
08:45:16,280 --> 08:45:24,120
exponential all the exponentiated values

12414
08:45:19,200 --> 08:45:27,638
right so if we do if we open here uh if

12415
08:45:24,120 --> 08:45:27,638
I just open I

12416
08:45:28,360 --> 08:45:35,558
python if we import math and go say we

12417
08:45:31,920 --> 08:45:38,040
go math.exp of

12418
08:45:35,558 --> 08:45:39,558
1.0 we're going to get 2.71 because

12419
08:45:38,040 --> 08:45:43,920
that's what e is right it's going to be

12420
08:45:39,558 --> 08:45:46,718
e to the 1 um so our first

12421
08:45:43,920 --> 08:45:48,638
value is going to be

12422
08:45:46,718 --> 08:45:52,238
uh

12423
08:45:48,638 --> 08:45:57,238
2.71 and then the second

12424
08:45:52,238 --> 08:45:57,238
one is going to be two so 7 say

12425
08:45:58,200 --> 08:46:05,760
7.39 and then the third one number uh

12426
08:46:01,520 --> 08:46:05,760
three is going to be 20.1

12427
08:46:08,600 --> 08:46:14,760
roughly now we sum these Al together

12428
08:46:15,840 --> 08:46:22,558
so it's trying to autocomplete for me

12429
08:46:19,200 --> 08:46:26,478
2.71 +

12430
08:46:22,558 --> 08:46:30,520
7.39 + 20.1 0 and we get

12431
08:46:26,478 --> 08:46:32,638
30.2 right now in order to get the

12432
08:46:30,520 --> 08:46:34,200
actual softmax output we see how much

12433
08:46:32,638 --> 08:46:36,520
each of these each of these

12434
08:46:34,200 --> 08:46:39,360
exponentiated values contributes to the

12435
08:46:36,520 --> 08:46:40,958
exponentiated sum of all of them um well

12436
08:46:39,360 --> 08:46:42,520
not exponentiated sum but when you

12437
08:46:40,958 --> 08:46:46,120
exponentiate all of them you take the

12438
08:46:42,520 --> 08:46:47,440
sum of those which is 32.2 or 30.2 and

12439
08:46:46,120 --> 08:46:50,600
then you you see how much each

12440
08:46:47,440 --> 08:46:57,280
contributes so

12441
08:46:50,600 --> 08:47:01,120
2.71 / 30.2 so that about 9% roughly so

12442
08:46:57,280 --> 08:47:03,120
we're going to go um

12443
08:47:01,120 --> 08:47:06,798
0.09

12444
08:47:03,120 --> 08:47:10,478
um and then we go

12445
08:47:06,798 --> 08:47:10,478
7.39 is

12446
08:47:10,920 --> 08:47:17,600
0.24 and then the last one's going to be

12447
08:47:13,520 --> 08:47:21,238
67 if we go and do it

12448
08:47:17,600 --> 08:47:22,958
uh if you round up it's 67 so you have

12449
08:47:21,238 --> 08:47:29,398
that right and then if we add these all

12450
08:47:22,958 --> 08:47:32,958
these numbers together 0.0 09 + 0.24 +

12451
08:47:29,398 --> 08:47:35,440
0.67 you get 1.0 right and these are all

12452
08:47:32,958 --> 08:47:38,160
rounded of course um but but but the

12453
08:47:35,440 --> 08:47:39,760
point here is this is the softmax notice

12454
08:47:38,160 --> 08:47:41,798
how we had this this initial

12455
08:47:39,760 --> 08:47:44,000
distribution here and then we went to a

12456
08:47:41,798 --> 08:47:46,958
new one which kind of just made

12457
08:47:44,000 --> 08:47:49,680
everything add up to one and uh gave it

12458
08:47:46,958 --> 08:47:52,478
a bit of gave it a bit of extra weight

12459
08:47:49,680 --> 08:47:55,440
to the three right so notice how like a

12460
08:47:52,478 --> 08:47:59,040
like three is three times that

12461
08:47:55,440 --> 08:48:04,760
1.0 um the

12462
08:47:59,040 --> 08:48:06,478
6.67 is way more than 3 times uh 0.09 so

12463
08:48:04,760 --> 08:48:07,958
you kind of have that extra waiting for

12464
08:48:06,478 --> 08:48:10,600
the bigger values right and that's what

12465
08:48:07,958 --> 08:48:12,238
the softmax does um so it's going to

12466
08:48:10,600 --> 08:48:14,840
like essentially find like the biggest

12467
08:48:12,238 --> 08:48:16,638
number and and um put a lot of weight on

12468
08:48:14,840 --> 08:48:20,478
it or highlight it the

12469
08:48:16,638 --> 08:48:23,280
most um but there is a flaw with this

12470
08:48:20,478 --> 08:48:26,040
approach and I'm going to explain this

12471
08:48:23,280 --> 08:48:30,878
right now so essentially the flaw here

12472
08:48:26,040 --> 08:48:33,318
is if you have say um if you have

12473
08:48:30,878 --> 08:48:36,200
X as say

12474
08:48:33,318 --> 08:48:37,798
1

12475
08:48:36,200 --> 08:48:39,718
1,000

12476
08:48:37,798 --> 08:48:42,718
zero

12477
08:48:39,718 --> 08:48:42,718
and

12478
08:48:44,200 --> 08:48:50,878
uh negative 1,000 right so some of these

12479
08:48:48,680 --> 08:48:52,638
the sum of these is zero right you have

12480
08:48:50,878 --> 08:48:55,360
you have a problem with that and and

12481
08:48:52,638 --> 08:48:58,840
that essentially means this is is going

12482
08:48:55,360 --> 08:49:01,958
to um this is going to contribute a big

12483
08:48:58,840 --> 08:49:04,798
chunk of this or at least it's supposed

12484
08:49:01,958 --> 08:49:06,398
to It's supposed to contribute all of it

12485
08:49:04,798 --> 08:49:09,040
um but it's not going to because you

12486
08:49:06,398 --> 08:49:11,760
cannot contribute like it just doesn't

12487
08:49:09,040 --> 08:49:13,280
make sense how can you contribute like

12488
08:49:11,760 --> 08:49:14,958
what what percentage does a th

12489
08:49:13,280 --> 08:49:17,718
contribute to zero it's like Infinity

12490
08:49:14,958 --> 08:49:22,440
right so if you end up doing softmax on

12491
08:49:17,718 --> 08:49:25,958
this um like even if you try to math.

12492
08:49:22,440 --> 08:49:27,920
exp of 1,000 you're going to get math

12493
08:49:25,958 --> 08:49:29,680
out of overflow error math range error

12494
08:49:27,920 --> 08:49:31,238
because it's just too big of a number

12495
08:49:29,680 --> 08:49:34,798
right

12496
08:49:31,238 --> 08:49:37,920
so what you can actually do is you can

12497
08:49:34,798 --> 08:49:40,440
subtract by the max of of all of these

12498
08:49:37,920 --> 08:49:44,360
right so we can see

12499
08:49:40,440 --> 08:49:46,160
um what is the maximum uh what is the

12500
08:49:44,360 --> 08:49:49,040
maximum number here so the biggest one

12501
08:49:46,160 --> 08:49:51,798
is a th and so what we can do is we can

12502
08:49:49,040 --> 08:49:55,160
subtract each number by that value so we

12503
08:49:51,798 --> 08:49:57,238
go we can say x we'll just say like X2

12504
08:49:55,160 --> 08:49:58,920
equals

12505
08:49:57,238 --> 08:50:04,520
um

12506
08:49:58,920 --> 08:50:07,920
0 and then 0 - 1,000

12507
08:50:04,520 --> 08:50:13,120
is th000 and then 1,00 minus th000 is

12508
08:50:07,920 --> 08:50:13,120
2,000 right and so if we go

12509
08:50:15,478 --> 08:50:22,238
um we go math.exp of

12510
08:50:22,638 --> 08:50:29,318
zero notice we get one

12511
08:50:25,520 --> 08:50:31,200
math. exp of 1000 is going to give us

12512
08:50:29,318 --> 08:50:32,600
well pretty close to zero it's not quite

12513
08:50:31,200 --> 08:50:34,200
going to be zero but it's going to be

12514
08:50:32,600 --> 08:50:37,200
very very it's going to be a very small

12515
08:50:34,200 --> 08:50:38,760
number right and then of course the same

12516
08:50:37,200 --> 08:50:40,760
thing with 2,000 is just going to be the

12517
08:50:38,760 --> 08:50:43,280
same right it's going to be very small

12518
08:50:40,760 --> 08:50:45,680
and so you notice how if we if we

12519
08:50:43,280 --> 08:50:48,120
originally just if we just did this one

12520
08:50:45,680 --> 08:50:51,398
normally using these big numbers would

12521
08:50:48,120 --> 08:50:55,080
have actually given us errors so we can

12522
08:50:51,398 --> 08:50:56,840
subtract by the max we can we can um

12523
08:50:55,080 --> 08:50:58,280
you know pointwise subtract by the max

12524
08:50:56,840 --> 08:51:00,080
number and then we get something that

12525
08:50:58,280 --> 08:51:01,760
makes more sense so you know zero

12526
08:51:00,080 --> 08:51:05,160
contributes to all of it because that's

12527
08:51:01,760 --> 08:51:08,160
what the that's what the the sum

12528
08:51:05,160 --> 08:51:10,558
is and so this is how we actually follow

12529
08:51:08,160 --> 08:51:12,638
through in the softmax function in C so

12530
08:51:10,558 --> 08:51:15,160
we have three separate for Loops here

12531
08:51:12,638 --> 08:51:17,558
the first one finds the max value in the

12532
08:51:15,160 --> 08:51:19,840
array so we just set the initial max

12533
08:51:17,558 --> 08:51:22,120
value to be the first number and then we

12534
08:51:19,840 --> 08:51:23,398
iterate through this uh starting at the

12535
08:51:22,120 --> 08:51:24,680
next number we don't want to start at

12536
08:51:23,398 --> 08:51:26,080
the first we don't want to start at the

12537
08:51:24,680 --> 08:51:27,398
first one we started the one after it

12538
08:51:26,080 --> 08:51:29,680
cuz we already we already know what this

12539
08:51:27,398 --> 08:51:31,958
one is um and we essentially just

12540
08:51:29,680 --> 08:51:34,878
compare if if the new one is bigger than

12541
08:51:31,958 --> 08:51:36,798
the max we set the max to that one um

12542
08:51:34,878 --> 08:51:39,558
and we end up with this this Max term

12543
08:51:36,798 --> 08:51:42,200
which you know same as we did up here

12544
08:51:39,558 --> 08:51:45,200
and then we have this we have the

12545
08:51:42,200 --> 08:51:49,318
accumulation sum so we iterate through

12546
08:51:45,200 --> 08:51:52,760
the length of the array and we do that

12547
08:51:49,318 --> 08:51:55,120
index minus the maximum and then we we

12548
08:51:52,760 --> 08:51:57,318
add that total to the sum right we we

12549
08:51:55,120 --> 08:51:58,920
add whatever this was to the sum so that

12550
08:51:57,318 --> 08:52:02,080
we can you know accumulate all of it and

12551
08:51:58,920 --> 08:52:05,120
then we just write out

12552
08:52:02,080 --> 08:52:06,440
um we just write out the same value we

12553
08:52:05,120 --> 08:52:09,000
we essentially just take the input and

12554
08:52:06,440 --> 08:52:12,040
then replace it one by one uh and we set

12555
08:52:09,000 --> 08:52:14,798
it to uh whatever that exponentiated

12556
08:52:12,040 --> 08:52:19,000
value is uh

12557
08:52:14,798 --> 08:52:21,360
minus uh or sorry not minus but we we

12558
08:52:19,000 --> 08:52:23,558
have that exponen exponentiated value we

12559
08:52:21,360 --> 08:52:24,958
divide that by the sum uh very simple I

12560
08:52:23,558 --> 08:52:27,160
probably could have explained that in in

12561
08:52:24,958 --> 08:52:29,160
a bit shorter time but I just want you

12562
08:52:27,160 --> 08:52:30,318
to like gain the int intuition for that

12563
08:52:29,160 --> 08:52:32,878
you've probably already written the soft

12564
08:52:30,318 --> 08:52:34,920
Max and Pi torch and nump I get that uh

12565
08:52:32,878 --> 08:52:36,878
but just to kind of provide that review

12566
08:52:34,920 --> 08:52:39,200
as to how it works under the hood and

12567
08:52:36,878 --> 08:52:42,000
this uh you know numerical stability

12568
08:52:39,200 --> 08:52:46,398
add-on we have um so

12569
08:52:42,000 --> 08:52:49,638
now I have an example in uh Cuda as well

12570
08:52:46,398 --> 08:52:53,200
so I'm not going to go over this um but

12571
08:52:49,638 --> 08:52:55,238
the idea here is uh in Cuda mean yes you

12572
08:52:53,200 --> 08:52:57,040
could just you could do

12573
08:52:55,238 --> 08:53:00,160
um you could go over you know the length

12574
08:52:57,040 --> 08:53:01,360
of one single array but in deep learning

12575
08:53:00,160 --> 08:53:02,680
you're not actually trying to do that

12576
08:53:01,360 --> 08:53:04,238
you're typically going to have a batch

12577
08:53:02,680 --> 08:53:05,798
size and this batch size is going to

12578
08:53:04,238 --> 08:53:07,160
have like a bunch of these it's going to

12579
08:53:05,798 --> 08:53:08,600
be a batch of arrays that you're going

12580
08:53:07,160 --> 08:53:10,718
to softmax right and you're going to do

12581
08:53:08,600 --> 08:53:13,840
the softmax on them rowwise you're going

12582
08:53:10,718 --> 08:53:15,238
to do here here here uh right instead of

12583
08:53:13,840 --> 08:53:17,520
just like to each element so it's going

12584
08:53:15,238 --> 08:53:19,120
to go through the rows and this is where

12585
08:53:17,520 --> 08:53:21,680
we can actually get a speed up from Cuda

12586
08:53:19,120 --> 08:53:23,680
because we can give each each of the

12587
08:53:21,680 --> 08:53:26,958
threads we know we have batch size many

12588
08:53:23,680 --> 08:53:29,520
threads and we give each of them uh we

12589
08:53:26,958 --> 08:53:31,600
give each of them a softmax job right so

12590
08:53:29,520 --> 08:53:33,760
we see in

12591
08:53:31,600 --> 08:53:36,600
here

12592
08:53:33,760 --> 08:53:41,160
how we see in here how we actually give

12593
08:53:36,600 --> 08:53:43,878
one to each um we we we give each we

12594
08:53:41,160 --> 08:53:46,120
give each thread a separate softmax drop

12595
08:53:43,878 --> 08:53:48,798
that's going to go it's going to span

12596
08:53:46,120 --> 08:53:51,200
1,24 iterations or goes up to the length

12597
08:53:48,798 --> 08:53:53,520
of n right as we can see in these so

12598
08:53:51,200 --> 08:53:55,680
like theoretically it's going to take 3

12599
08:53:53,520 --> 08:53:57,958
n iteration because has to go three

12600
08:53:55,680 --> 08:53:59,398
different for Loops um that's that's

12601
08:53:57,958 --> 08:54:00,718
kind of how that works there so we want

12602
08:53:59,398 --> 08:54:03,280
to we want to pay attention to batch

12603
08:54:00,718 --> 08:54:05,238
sizes right um but there's there's an

12604
08:54:03,280 --> 08:54:07,120
example in in Cuda here which you can

12605
08:54:05,238 --> 08:54:10,000
look over on your own and and get an in

12606
08:54:07,120 --> 08:54:12,440
intuition for the batch St Max but it's

12607
08:54:10,000 --> 08:54:16,520
very similar to the one we didn't

12608
08:54:12,440 --> 08:54:19,360
see now we jump into the one in Triton

12609
08:54:16,520 --> 08:54:22,000
okay so now we actually go into the uh

12610
08:54:19,360 --> 08:54:23,798
softmax Trion kernel itself uh a little

12611
08:54:22,000 --> 08:54:25,878
bit more advanced in Vector Edition but

12612
08:54:23,798 --> 08:54:28,718
we're going to go through step by step

12613
08:54:25,878 --> 08:54:30,798
so we obviously import at the top I'm

12614
08:54:28,718 --> 08:54:32,000
going to go sort of from the bottom here

12615
08:54:30,798 --> 08:54:34,398
explain what's happening and then what

12616
08:54:32,000 --> 08:54:37,318
we're going as we're calculating the the

12617
08:54:34,398 --> 08:54:41,160
output um so we set our manual seed we

12618
08:54:37,318 --> 08:54:45,280
make a random uh normally

12619
08:54:41,160 --> 08:54:49,200
uh normally distributed random tensor of

12620
08:54:45,280 --> 08:54:52,080
floats um so mean zero and variance one

12621
08:54:49,200 --> 08:54:55,398
so it's a normal distribution and all of

12622
08:54:52,080 --> 08:55:00,718
the uh essentially the shape is is batch

12623
08:54:55,398 --> 08:55:04,398
size by um by n so B by n batch size is

12624
08:55:00,718 --> 08:55:08,600
256 n is 1,24 elements long we're just

12625
08:55:04,398 --> 08:55:10,760
going to softmax um those 1024 element

12626
08:55:08,600 --> 08:55:13,558
rows uh and we're going to do this on

12627
08:55:10,760 --> 08:55:16,000
Cuda now we're going to calculate and

12628
08:55:13,558 --> 08:55:17,440
make sure that torch and TR now put the

12629
08:55:16,000 --> 08:55:20,120
same thing so typically you would do

12630
08:55:17,440 --> 08:55:23,160
torch. softmax or or f. soft Max or

12631
08:55:20,120 --> 08:55:26,478
whatever and then you would go um Dem

12632
08:55:23,160 --> 08:55:27,920
equals 1 right and then we want to do

12633
08:55:26,478 --> 08:55:30,638
that same thing for Trident and then

12634
08:55:27,920 --> 08:55:31,878
make sure that um you know the max value

12635
08:55:30,638 --> 08:55:33,120
isn't too ridiculous we're going to

12636
08:55:31,878 --> 08:55:35,318
print that out then we're going to make

12637
08:55:33,120 --> 08:55:40,318
sure that this is this is all close with

12638
08:55:35,318 --> 08:55:42,920
torch all close right um so we do the

12639
08:55:40,318 --> 08:55:45,920
Triton Triton softmax we go

12640
08:55:42,920 --> 08:55:48,920
here and in here we have uh rows and

12641
08:55:45,920 --> 08:55:53,160
columns so when we print out um I'm just

12642
08:55:48,920 --> 08:55:56,238
going to print out uh input shape so we

12643
08:55:53,160 --> 08:55:56,238
go here

12644
08:55:59,160 --> 08:56:05,200
input shape is as we want right

12645
08:56:05,520 --> 08:56:11,680
um output we're just going to do a malic

12646
08:56:08,478 --> 08:56:13,798
essentially right there and then uh

12647
08:56:11,680 --> 08:56:15,798
instead of doing some weird like meta

12648
08:56:13,798 --> 08:56:17,638
parameter uh like doing like the whole

12649
08:56:15,798 --> 08:56:19,360
like Lambda function calculating we're

12650
08:56:17,638 --> 08:56:22,120
just going to do Tron and then next

12651
08:56:19,360 --> 08:56:24,238
power of two so what this says is it

12652
08:56:22,120 --> 08:56:26,280
Returns the smallest Power of Two

12653
08:56:24,238 --> 08:56:27,440
greater than or equal to n so similar to

12654
08:56:26,280 --> 08:56:29,440
what we were doing before but just a

12655
08:56:27,440 --> 08:56:31,680
little simpler um and then we're going

12656
08:56:29,440 --> 08:56:37,558
to we're going to set block size equal

12657
08:56:31,680 --> 08:56:39,238
to the minimum of 1024 and this right um

12658
08:56:37,558 --> 08:56:42,238
so you can kind of see how we you can

12659
08:56:39,238 --> 08:56:46,798
kind of see why we why we would do that

12660
08:56:42,238 --> 08:56:49,398
now we set our grid to n rows because in

12661
08:56:46,798 --> 08:56:51,718
our grid we want a block for each row

12662
08:56:49,398 --> 08:56:54,200
right when we're processing in parallel

12663
08:56:51,718 --> 08:56:57,160
we want we we can't uh we can't split a

12664
08:56:54,200 --> 08:56:59,280
single row amongst multiple um or at

12665
08:56:57,160 --> 08:57:01,798
least naively we can't split it across

12666
08:56:59,280 --> 08:57:04,878
multiple threads so we' want to do it uh

12667
08:57:01,798 --> 08:57:06,718
for each block so each different um each

12668
08:57:04,878 --> 08:57:08,280
different row would get its own block

12669
08:57:06,718 --> 08:57:10,558
essentially and that's how we launch the

12670
08:57:08,280 --> 08:57:13,798
grid here um and then you could assume

12671
08:57:10,558 --> 08:57:16,878
that you know why we have this um why we

12672
08:57:13,798 --> 08:57:18,878
have this trailing is just that the the

12673
08:57:16,878 --> 08:57:21,398
trailing Dimensions you can just assume

12674
08:57:18,878 --> 08:57:23,680
are one right and so that's how like Pi

12675
08:57:21,398 --> 08:57:26,920
torch shap shapes work if you do like

12676
08:57:23,680 --> 08:57:28,958
batch comma it'll be like B by one and

12677
08:57:26,920 --> 08:57:30,760
so it'll just be like a column Vector

12678
08:57:28,958 --> 08:57:33,680
sort of you you could think of it that

12679
08:57:30,760 --> 08:57:35,280
way uh and then the softmax kernel we do

12680
08:57:33,680 --> 08:57:37,360
this we call this the same way as we did

12681
08:57:35,280 --> 08:57:41,120
Vector ad except we add two more things

12682
08:57:37,360 --> 08:57:42,680
in so we add in this x. stride this this

12683
08:57:41,120 --> 08:57:44,718
is just an input parameter to what

12684
08:57:42,680 --> 08:57:48,360
what's going to be upstairs here so we

12685
08:57:44,718 --> 08:57:52,040
got x. stride which is the essentially

12686
08:57:48,360 --> 08:57:54,840
the The Stride of of the row so we when

12687
08:57:52,040 --> 08:57:56,920
we um when we go across it's like it's

12688
08:57:54,840 --> 08:57:59,280
stacked in memory it's like Row one row

12689
08:57:56,920 --> 08:58:01,000
two Row three row four right um but the

12690
08:57:59,280 --> 08:58:03,280
stride is like how far do you have to go

12691
08:58:01,000 --> 08:58:05,840
to wrap around to a new one so when we

12692
08:58:03,280 --> 08:58:07,680
say we want to go to row four um you

12693
08:58:05,840 --> 08:58:09,680
actually have to go okay well what is

12694
08:58:07,680 --> 08:58:11,440
the what is the strride right how long

12695
08:58:09,680 --> 08:58:12,638
do you have to go across to get to the

12696
08:58:11,440 --> 08:58:17,120
next one it's like essentially the

12697
08:58:12,638 --> 08:58:19,440
length um and that's you know we we we

12698
08:58:17,120 --> 08:58:21,920
we could say n columns but we're just

12699
08:58:19,440 --> 08:58:25,238
going to use the um you know objective

12700
08:58:21,920 --> 08:58:27,840
attribute x. stride

12701
08:58:25,238 --> 08:58:31,878
um passing and columns and then block

12702
08:58:27,840 --> 08:58:34,520
size right so we go up and we see that

12703
08:58:31,878 --> 08:58:39,520
we get the uh where did it

12704
08:58:34,520 --> 08:58:45,718
go output X stride stride and columns

12705
08:58:39,520 --> 08:58:45,718
and then block size uh output

12706
08:58:47,440 --> 08:58:53,398
input stride stride and columns block

12707
08:58:50,238 --> 08:58:56,718
size now we do the same we we we do the

12708
08:58:53,398 --> 08:58:59,638
same like pram ID and and the the start

12709
08:58:56,718 --> 08:59:02,478
pointers the same way

12710
08:58:59,638 --> 08:59:04,478
um we calculate this normally so which

12711
08:59:02,478 --> 08:59:07,478
block are we at right and then we have

12712
08:59:04,478 --> 08:59:10,680
to advance a number of indices forward

12713
08:59:07,478 --> 08:59:13,520
in memory so it's like we we essentially

12714
08:59:10,680 --> 08:59:14,878
we take the initial uh input pointer so

12715
08:59:13,520 --> 08:59:19,120
where does it start at where does where

12716
08:59:14,878 --> 08:59:22,600
does a starting place in memory and then

12717
08:59:19,120 --> 08:59:26,958
uh we add that

12718
08:59:22,600 --> 08:59:31,520
to we we we take that and then we plus

12719
08:59:26,958 --> 08:59:33,200
uh row idx so which row is it times the

12720
08:59:31,520 --> 08:59:36,680
length of the row right so then we can

12721
08:59:33,200 --> 08:59:39,398
get which row where is the starting row

12722
08:59:36,680 --> 08:59:40,840
within that batch right so which row do

12723
08:59:39,398 --> 08:59:42,558
we want to go to and then this whole

12724
08:59:40,840 --> 08:59:44,120
thing is actually in the bigger picture

12725
08:59:42,558 --> 08:59:45,478
which is the whole memory right that's

12726
08:59:44,120 --> 08:59:47,718
that's what we're doing there and then

12727
08:59:45,478 --> 08:59:49,238
same idea for the output uh start

12728
08:59:47,718 --> 08:59:50,680
pointer so we're essentially just

12729
08:59:49,238 --> 08:59:54,638
finding the place in memory where we

12730
08:59:50,680 --> 08:59:57,398
start based on based on these three um

12731
08:59:54,638 --> 09:00:01,360
and then we load in so uh you know we do

12732
08:59:57,398 --> 09:00:04,000
the typical load so uh pointer and then

12733
09:00:01,360 --> 09:00:05,798
we want to uh load in all of the indices

12734
09:00:04,000 --> 09:00:09,360
so similar to how we were doing it

12735
09:00:05,798 --> 09:00:10,638
before and then our mask is uh we're

12736
09:00:09,360 --> 09:00:13,440
just going to make sure that this is

12737
09:00:10,638 --> 09:00:15,638
smaller than n columns um right making

12738
09:00:13,440 --> 09:00:18,040
sure that we don't go out of bounds and

12739
09:00:15,638 --> 09:00:19,958
then this this important term other is

12740
09:00:18,040 --> 09:00:23,958
is a bit critical as

12741
09:00:19,958 --> 09:00:23,958
well so

12742
09:00:25,318 --> 09:00:29,878
this other term means that if you have

12743
09:00:28,638 --> 09:00:33,558
um for

12744
09:00:29,878 --> 09:00:36,360
example if you have 1,000 elements in a

12745
09:00:33,558 --> 09:00:39,040
row and the block size is24 so it's

12746
09:00:36,360 --> 09:00:40,478
going to be a bit longer um then that

12747
09:00:39,040 --> 09:00:41,798
means you're going to process some

12748
09:00:40,478 --> 09:00:44,080
additional elements it's going to think

12749
09:00:41,798 --> 09:00:46,360
that there's 24 extra ones at the end so

12750
09:00:44,080 --> 09:00:47,520
what you can do I mean this is not this

12751
09:00:46,360 --> 09:00:49,000
is not going to happen in our case

12752
09:00:47,520 --> 09:00:51,318
because we've very objective about how

12753
09:00:49,000 --> 09:00:53,280
we Define things but in some cases this

12754
09:00:51,318 --> 09:00:56,440
will be important to pay attention to

12755
09:00:53,280 --> 09:00:58,718
where um you actually want to explicitly

12756
09:00:56,440 --> 09:01:00,680
set the edge so that it doesn't mess up

12757
09:00:58,718 --> 09:01:03,200
everything else right if that was like

12758
09:01:00,680 --> 09:01:04,600
one for example then in our whole

12759
09:01:03,200 --> 09:01:06,718
softmax calculation that would

12760
09:01:04,600 --> 09:01:08,920
contribute to it quite significantly

12761
09:01:06,718 --> 09:01:11,398
because it's you know it's it's not just

12762
09:01:08,920 --> 09:01:13,440
like massive sparse integers it's like a

12763
09:01:11,398 --> 09:01:16,238
it's like decimal places that are around

12764
09:01:13,440 --> 09:01:18,638
zero right um so it's important to do

12765
09:01:16,238 --> 09:01:20,558
this when when we exponentiate when you

12766
09:01:18,638 --> 09:01:24,520
do

12767
09:01:20,558 --> 09:01:30,238
um if we go into uh python here

12768
09:01:24,520 --> 09:01:31,920
import uh import math we go Um can just

12769
09:01:30,238 --> 09:01:36,080
say

12770
09:01:31,920 --> 09:01:37,920
x xal negative float and then

12771
09:01:36,080 --> 09:01:42,680
INF

12772
09:01:37,920 --> 09:01:45,718
right and then we go um math.exp

12773
09:01:42,680 --> 09:01:48,958
X we notice that we get zero right where

12774
09:01:45,718 --> 09:01:51,200
we we did math. exp of one we would get

12775
09:01:48,958 --> 09:01:52,318
we would get this number so we just want

12776
09:01:51,200 --> 09:01:54,160
to make sure that that is not

12777
09:01:52,318 --> 09:01:56,920
contributing anything at all that that's

12778
09:01:54,160 --> 09:02:00,318
all we're doing there

12779
09:01:56,920 --> 09:02:01,638
um so all the other additional values if

12780
09:02:00,318 --> 09:02:03,238
they happen we don't want those to

12781
09:02:01,638 --> 09:02:06,160
contribute at all just kind of a safety

12782
09:02:03,238 --> 09:02:08,238
thing it's good to look out for this um

12783
09:02:06,160 --> 09:02:09,958
and then we just do the the the normal s

12784
09:02:08,238 --> 09:02:12,318
softmax calculation which is actually

12785
09:02:09,958 --> 09:02:15,920
only takes four lines um so we calculate

12786
09:02:12,318 --> 09:02:18,958
the max across across you know that row

12787
09:02:15,920 --> 09:02:21,080
and then we get the numerator which is

12788
09:02:18,958 --> 09:02:23,840
remember we we subtract the Max from

12789
09:02:21,080 --> 09:02:25,718
that uh we exponentiate this so we

12790
09:02:23,840 --> 09:02:26,760
exponentiate whatever that whatever that

12791
09:02:25,718 --> 09:02:29,160
result

12792
09:02:26,760 --> 09:02:31,160
is and then we get the denominator which

12793
09:02:29,160 --> 09:02:33,920
is a sum

12794
09:02:31,160 --> 09:02:36,440
across each individual each individual

12795
09:02:33,920 --> 09:02:38,920
one so there there's going to be some

12796
09:02:36,440 --> 09:02:40,398
sum number and then we're going to do

12797
09:02:38,920 --> 09:02:42,958
essentially this array it's going to be

12798
09:02:40,398 --> 09:02:45,600
an array of elements and we're going to

12799
09:02:42,958 --> 09:02:47,160
divide this by a scalar value so it's

12800
09:02:45,600 --> 09:02:49,760
going to just like take this and it's

12801
09:02:47,160 --> 09:02:51,718
going to divide divide divide divide

12802
09:02:49,760 --> 09:02:53,520
divide right and it's and that's how we

12803
09:02:51,718 --> 09:02:56,000
get our softmax output and then we'll

12804
09:02:53,520 --> 09:02:58,478
see that we store this

12805
09:02:56,000 --> 09:03:01,440
um similar to how we actually loaded

12806
09:02:58,478 --> 09:03:03,760
things in in initially so instead of the

12807
09:03:01,440 --> 09:03:07,318
row starting pointer it's the out row

12808
09:03:03,760 --> 09:03:10,280
starting pointer same idea um we want to

12809
09:03:07,318 --> 09:03:12,000
store a value which is the softmax

12810
09:03:10,280 --> 09:03:14,080
output which is what we calculated here

12811
09:03:12,000 --> 09:03:18,280
so that's the actual value uh inside of

12812
09:03:14,080 --> 09:03:20,080
it um and then our mask is same as as we

12813
09:03:18,280 --> 09:03:22,520
did up here so we we want to make sure

12814
09:03:20,080 --> 09:03:24,558
that we're not uh doing reads and wrs

12815
09:03:22,520 --> 09:03:26,200
out of bounds right and that's that's

12816
09:03:24,558 --> 09:03:30,000
pretty much the entire softx calculation

12817
09:03:26,200 --> 09:03:31,238
in Triton so you know I encourage you to

12818
09:03:30,000 --> 09:03:34,760
uh you know play around with this you

12819
09:03:31,238 --> 09:03:36,920
can do you can actually do

12820
09:03:34,760 --> 09:03:43,558
um you can actually

12821
09:03:36,920 --> 09:03:47,238
go uh t. device print and then we can go

12822
09:03:43,558 --> 09:03:48,920
um like P and then just put like row idx

12823
09:03:47,238 --> 09:03:53,920
or we could

12824
09:03:48,920 --> 09:03:53,920
do row idx like this

12825
09:03:56,238 --> 09:04:01,360
and we can actually print that

12826
09:03:58,680 --> 09:04:04,040
out so you know maximum difference

12827
09:04:01,360 --> 09:04:06,680
between TR TR pytorch and Tron results

12828
09:04:04,040 --> 09:04:09,000
is is very small results are close is

12829
09:04:06,680 --> 09:04:12,040
true um and then we can see each each

12830
09:04:09,000 --> 09:04:14,958
individual one of these so like P ID is

12831
09:04:12,040 --> 09:04:18,200
you know 255 this is in the X Dimension

12832
09:04:14,958 --> 09:04:20,280
right so if you had like a you had like

12833
09:04:18,200 --> 09:04:24,558
a

12834
09:04:20,280 --> 09:04:27,958
um this is Axis so we do axis Z same

12835
09:04:24,558 --> 09:04:30,238
thing we access of one then you know

12836
09:04:27,958 --> 09:04:33,238
this would be on this on this spot

12837
09:04:30,238 --> 09:04:36,440
instead of instead of X

12838
09:04:33,238 --> 09:04:39,200
right so yeah we we print out whatever

12839
09:04:36,440 --> 09:04:40,958
that value is and we get uh we we can

12840
09:04:39,200 --> 09:04:44,558
print stuff out in the terminal if I do

12841
09:04:40,958 --> 09:04:44,558
like normal just like

12842
09:04:45,040 --> 09:04:50,478
save now it might be a good idea to

12843
09:04:47,520 --> 09:04:52,120
actually dig into pytorch or not even

12844
09:04:50,478 --> 09:04:54,040
dig into it but rather understand how

12845
09:04:52,120 --> 09:04:56,120
you can add on top of it make things a

12846
09:04:54,040 --> 09:04:59,040
little bit faster uh for your own custom

12847
09:04:56,120 --> 09:05:01,120
use cases and whatnot so we just

12848
09:04:59,040 --> 09:05:02,798
finished off the Trident chapter uh I

12849
09:05:01,120 --> 09:05:04,558
hope you enjoyed that but now we're

12850
09:05:02,798 --> 09:05:07,398
going to go a little bit more into

12851
09:05:04,558 --> 09:05:10,160
Python and more on the torch side so

12852
09:05:07,398 --> 09:05:14,238
I've written a few files here I have

12853
09:05:10,160 --> 09:05:16,478
some descriptions about what's going on

12854
09:05:14,238 --> 09:05:19,160
uh what's going on with all these like

12855
09:05:16,478 --> 09:05:21,120
different types and names and stuff in

12856
09:05:19,160 --> 09:05:22,680
uh in the readme as well as some

12857
09:05:21,120 --> 09:05:24,200
intuitive examples of what we're going

12858
09:05:22,680 --> 09:05:27,600
to be looking at

12859
09:05:24,200 --> 09:05:30,120
I wrote a setup script for just

12860
09:05:27,600 --> 09:05:31,318
compiling a separate Pythor extension uh

12861
09:05:30,120 --> 09:05:34,440
a separate function that we're going to

12862
09:05:31,318 --> 09:05:38,600
use to do a polinomial it's very simple

12863
09:05:34,440 --> 09:05:41,878
operation um and then uh a Cuda script

12864
09:05:38,600 --> 09:05:43,760
that is going to uh compile and bind

12865
09:05:41,878 --> 09:05:46,280
into pytorch so that we can actually use

12866
09:05:43,760 --> 09:05:49,080
it um in Python and then the python

12867
09:05:46,280 --> 09:05:52,280
script itself which we Benchmark against

12868
09:05:49,080 --> 09:05:55,200
uh naive pytorch so just all we're

12869
09:05:52,280 --> 09:05:58,840
really doing here is x^2 + x + 1 that's

12870
09:05:55,200 --> 09:06:00,160
all we're doing so going here we have

12871
09:05:58,840 --> 09:06:02,200
this include and this is giving us

12872
09:06:00,160 --> 09:06:04,840
errors but don't worry about this uh

12873
09:06:02,200 --> 09:06:06,718
when we set up it's going to actually uh

12874
09:06:04,840 --> 09:06:09,040
it's going to handle this properly so we

12875
09:06:06,718 --> 09:06:11,800
don't actually need um a dedicated

12876
09:06:09,040 --> 09:06:14,000
include file or or any arguments for

12877
09:06:11,800 --> 09:06:17,598
this we don't need to specify where the

12878
09:06:14,000 --> 09:06:20,558
extension. H file is um but going into

12879
09:06:17,598 --> 09:06:22,278
the actual kernel itself um so just like

12880
09:06:20,558 --> 09:06:24,078
top to bottom we have this new thing

12881
09:06:22,278 --> 09:06:28,278
called a template I didn't go over this

12882
09:06:24,078 --> 09:06:32,320
yet but uh well I I did I I guess I did

12883
09:06:28,278 --> 09:06:33,960
in the uh in the manal section but uh

12884
09:06:32,320 --> 09:06:37,040
template

12885
09:06:33,960 --> 09:06:38,800
is it essentially if we go down it'll

12886
09:06:37,040 --> 09:06:41,558
help if if we kind of go down and and

12887
09:06:38,800 --> 09:06:44,078
look at where this supplies um so when

12888
09:06:41,558 --> 09:06:46,040
we call this kernel we do this and then

12889
09:06:44,078 --> 09:06:48,598
we provide our arguments right we have

12890
09:06:46,040 --> 09:06:50,160
the kernel configuration here and then

12891
09:06:48,598 --> 09:06:52,238
we have this this this is where the

12892
09:06:50,160 --> 09:06:56,398
template comes in so we specify this

12893
09:06:52,238 --> 09:06:59,718
scaler t type and that essentially means

12894
09:06:56,398 --> 09:07:03,122
um we're just going to make sure

12895
09:06:59,718 --> 09:07:07,480
that this uh the

12896
09:07:03,122 --> 09:07:10,438
X the y or the the input the output uh

12897
09:07:07,480 --> 09:07:11,878
are of this type and so this type is

12898
09:07:10,438 --> 09:07:13,918
essentially going to be handled by P

12899
09:07:11,878 --> 09:07:17,598
torch and instead of specifying like a

12900
09:07:13,918 --> 09:07:19,160
float or or a double or like I don't

12901
09:07:17,598 --> 09:07:22,960
know some other some other like maybe

12902
09:07:19,160 --> 09:07:24,918
quantize like fp16 type um it's going to

12903
09:07:22,960 --> 09:07:26,520
handle that for us and it's going to

12904
09:07:24,918 --> 09:07:28,078
just automatically recognize which one

12905
09:07:26,520 --> 09:07:29,918
it is and it's going to compile that

12906
09:07:28,078 --> 09:07:31,278
down and deal with it appropriately so

12907
09:07:29,918 --> 09:07:33,718
this is something built into py torch

12908
09:07:31,278 --> 09:07:35,278
and it's custom type that we have uh so

12909
09:07:33,718 --> 09:07:38,040
this is kind of just like the default to

12910
09:07:35,278 --> 09:07:41,758
use here um easiest and then we have

12911
09:07:38,040 --> 09:07:43,878
this restrict so restrict in short

12912
09:07:41,758 --> 09:07:46,320
essentially means we're not going to be

12913
09:07:43,878 --> 09:07:49,122
overlapping memory uh we're not going to

12914
09:07:46,320 --> 09:07:52,558
be overlapping memory accesses so we

12915
09:07:49,122 --> 09:07:54,480
have uh we have X here we have an output

12916
09:07:52,558 --> 09:07:57,598
here all we're doing is we're doing an

12917
09:07:54,480 --> 09:08:00,160
operation on this thing and then we're

12918
09:07:57,598 --> 09:08:02,200
storing that in here um we're not we're

12919
09:08:00,160 --> 09:08:04,238
not mixing things over we're not doing

12920
09:08:02,200 --> 09:08:06,160
two things on like the same location and

12921
09:08:04,238 --> 09:08:08,238
then having some output that's stored

12922
09:08:06,160 --> 09:08:10,718
we're not doing anything messy like that

12923
09:08:08,238 --> 09:08:12,840
so we can just say restrict and that'll

12924
09:08:10,718 --> 09:08:15,918
allow the compiler to aggressively

12925
09:08:12,840 --> 09:08:19,480
optimize U essentially what the binary

12926
09:08:15,918 --> 09:08:22,360
code code is um and so in here we we

12927
09:08:19,480 --> 09:08:26,238
simply just do a uh we do this over the

12928
09:08:22,360 --> 09:08:29,040
X Dimension the typical Kuda kernel

12929
09:08:26,238 --> 09:08:33,238
indexing um and then we do you know the

12930
09:08:29,040 --> 09:08:35,480
square the plus and then the plus one so

12931
09:08:33,238 --> 09:08:36,840
uh that's the I mean the the kernel is

12932
09:08:35,480 --> 09:08:38,520
surprisingly simple there's just some

12933
09:08:36,840 --> 09:08:43,200
new colors and new keywords to pay

12934
09:08:38,520 --> 09:08:44,640
attention uh to pay attention for but uh

12935
09:08:43,200 --> 09:08:47,480
then we scroll down a bit and then we

12936
09:08:44,640 --> 09:08:50,278
got some C++ syntax going on what the

12937
09:08:47,480 --> 09:08:52,320
heck does this do well we have this Auto

12938
09:08:50,278 --> 09:08:54,640
which essentially figures out like which

12939
09:08:52,320 --> 09:08:56,122
which type to this to so it's going to

12940
09:08:54,640 --> 09:08:57,840
recognize that this is going to be some

12941
09:08:56,122 --> 09:08:59,640
torch type and it's going to

12942
09:08:57,840 --> 09:09:01,278
automatically select that and this is

12943
09:08:59,640 --> 09:09:04,360
just torch. empty like so it's going to

12944
09:09:01,278 --> 09:09:07,320
have the same shape as X the input um

12945
09:09:04,360 --> 09:09:08,918
which is a torch tensor type so Auto is

12946
09:09:07,320 --> 09:09:12,320
going to recognize it's going

12947
09:09:08,918 --> 09:09:14,558
essentially going to uh it's going to

12948
09:09:12,320 --> 09:09:16,078
it's going to do this we'll just leave

12949
09:09:14,558 --> 09:09:18,878
it as Auto for now because it looks

12950
09:09:16,078 --> 09:09:21,438
nicer um and then we have our threads

12951
09:09:18,878 --> 09:09:24,000
which the typical 10,24 threads per

12952
09:09:21,438 --> 09:09:26,360
block as per the maximum and then we

12953
09:09:24,000 --> 09:09:28,122
have our you know numb elements plus

12954
09:09:26,360 --> 09:09:29,718
threads minus one divided by number of

12955
09:09:28,122 --> 09:09:31,000
threads so this is the typical we

12956
09:09:29,718 --> 09:09:33,520
already went over this this should be

12957
09:09:31,000 --> 09:09:37,558
very trivial stuff

12958
09:09:33,520 --> 09:09:40,878
um and then we have um essentially just

12959
09:09:37,558 --> 09:09:43,598
some some extra stuff about

12960
09:09:40,878 --> 09:09:45,320
uh how the kernel is going to be called

12961
09:09:43,598 --> 09:09:49,558
so which which what things are we

12962
09:09:45,320 --> 09:09:52,840
feeding into it um we return an output

12963
09:09:49,558 --> 09:09:57,000
based on that um and then down here we

12964
09:09:52,840 --> 09:09:58,758
just have our are python bindings so uh

12965
09:09:57,000 --> 09:10:01,878
don't worry about this too much just

12966
09:09:58,758 --> 09:10:04,758
kind of trust the process um I can kind

12967
09:10:01,878 --> 09:10:08,122
of read off of like what I was able to

12968
09:10:04,758 --> 09:10:13,320
uh find about this exactly

12969
09:10:08,122 --> 09:10:16,122
so um Pi bind 11 module

12970
09:10:13,320 --> 09:10:19,078
this this is a macro that defines the

12971
09:10:16,122 --> 09:10:21,238
entry point for the python module um

12972
09:10:19,078 --> 09:10:24,320
torch extension

12973
09:10:21,238 --> 09:10:26,800
name uh is it's a macro defined by

12974
09:10:24,320 --> 09:10:29,360
pytorch that expands to the name of the

12975
09:10:26,800 --> 09:10:31,680
extension so usually defined from the

12976
09:10:29,360 --> 09:10:35,238
setup.py file over

12977
09:10:31,680 --> 09:10:38,598
here then we have an m and an

12978
09:10:35,238 --> 09:10:41,758
m.de so that adds a new function to the

12979
09:10:38,598 --> 09:10:43,800
module so the first argument polinomial

12980
09:10:41,758 --> 09:10:46,000
activation uh is the name of the

12981
09:10:43,800 --> 09:10:48,320
function in Python so that's the

12982
09:10:46,000 --> 09:10:51,160
function we end up calling be P

12983
09:10:48,320 --> 09:10:54,598
polinomial activation open close

12984
09:10:51,160 --> 09:10:58,200
parentheses um

12985
09:10:54,598 --> 09:11:02,000
the polinomial activation Cuda pointer

12986
09:10:58,200 --> 09:11:05,160
uh or it is it is a pointer to the C++

12987
09:11:02,000 --> 09:11:07,040
function to be called and then the last

12988
09:11:05,160 --> 09:11:08,960
argument is a dock string for the

12989
09:11:07,040 --> 09:11:11,000
function so this it's just like a a

12990
09:11:08,960 --> 09:11:14,360
simple doc string it's not like entirely

12991
09:11:11,000 --> 09:11:16,398
required but we we put it there

12992
09:11:14,360 --> 09:11:19,122
anyways

12993
09:11:16,398 --> 09:11:20,598
so anyways this is the this is the full

12994
09:11:19,122 --> 09:11:22,640
Cuda script this is essentially as

12995
09:11:20,598 --> 09:11:25,480
simple as it gets you can't really do

12996
09:11:22,640 --> 09:11:27,122
anything more simple than this uh so

12997
09:11:25,480 --> 09:11:31,200
this is kind of like a template you can

12998
09:11:27,122 --> 09:11:33,878
work off of and then in here we uh we

12999
09:11:31,200 --> 09:11:35,438
import our compiled polinomial Cuda

13000
09:11:33,878 --> 09:11:38,800
function which we'll compile in a second

13001
09:11:35,438 --> 09:11:42,200
here we have a class so we use torch

13002
09:11:38,800 --> 09:11:44,078
autograd and by default when we when we

13003
09:11:42,200 --> 09:11:45,758
when we do an autograd when we do an

13004
09:11:44,078 --> 09:11:47,758
autograd function we have to include a

13005
09:11:45,758 --> 09:11:49,758
forward and backward method um these are

13006
09:11:47,758 --> 09:11:51,480
both going to be static so we just add

13007
09:11:49,758 --> 09:11:54,040
this decorator that says this is

13008
09:11:51,480 --> 09:11:57,640
compiled elsewhere um

13009
09:11:54,040 --> 09:12:00,438
and then forward we're going to do um

13010
09:11:57,640 --> 09:12:03,122
just this polinomial Activation so comes

13011
09:12:00,438 --> 09:12:07,122
from the bin error that we compiled and

13012
09:12:03,122 --> 09:12:11,000
then uh polinomial activation of X right

13013
09:12:07,122 --> 09:12:12,598
as we said before and then backward uh

13014
09:12:11,000 --> 09:12:14,200
backward we just don't support it yet so

13015
09:12:12,598 --> 09:12:16,878
we could just say not not implemented

13016
09:12:14,200 --> 09:12:18,200
error backward pass not implemented um

13017
09:12:16,878 --> 09:12:20,800
and then in here where we actually do

13018
09:12:18,200 --> 09:12:22,398
our n and do module the py should be

13019
09:12:20,800 --> 09:12:24,200
familiar to of course this this should

13020
09:12:22,398 --> 09:12:27,398
look this should be super easy to

13021
09:12:24,200 --> 09:12:30,918
understand um from a from a separate

13022
09:12:27,398 --> 09:12:34,680
standpoint um but we just do the init we

13023
09:12:30,918 --> 09:12:37,122
do the forward in here we we say you

13024
09:12:34,680 --> 09:12:38,918
know implementation is going to be the P

13025
09:12:37,122 --> 09:12:41,878
we just set it to a string P Tor so it's

13026
09:12:38,918 --> 09:12:44,438
kind of easy to read um and then if the

13027
09:12:41,878 --> 09:12:48,278
implementation is p torch we do this raw

13028
09:12:44,438 --> 09:12:50,680
and if it's not then we do this uh if if

13029
09:12:48,278 --> 09:12:52,480
it's Cuda then we do this this other one

13030
09:12:50,680 --> 09:12:53,960
which we specified up here so it's going

13031
09:12:52,480 --> 09:12:57,398
to do this

13032
09:12:53,960 --> 09:12:59,878
um and it's going to apply that to X and

13033
09:12:57,398 --> 09:13:02,480
then else we just we just say oh if you

13034
09:12:59,878 --> 09:13:05,520
did like uh if you like missed the T it'

13035
09:13:02,480 --> 09:13:07,558
be like oh you Pi orch is not

13036
09:13:05,520 --> 09:13:10,640
implemented right unknown

13037
09:13:07,558 --> 09:13:12,438
implementation and then so we go down

13038
09:13:10,640 --> 09:13:14,480
and let's just go to the main function

13039
09:13:12,438 --> 09:13:15,680
first then to the Benchmark so in here

13040
09:13:14,480 --> 09:13:18,680
random

13041
09:13:15,680 --> 09:13:19,800
seed normally distributed random tensor

13042
09:13:18,680 --> 09:13:22,398
on

13043
09:13:19,800 --> 09:13:25,040
device implementation on pytorch

13044
09:13:22,398 --> 09:13:28,000
implementation on Cuda we move these to

13045
09:13:25,040 --> 09:13:30,558
the device so to Cuda this is just like

13046
09:13:28,000 --> 09:13:32,078
essentially uh saying that this function

13047
09:13:30,558 --> 09:13:33,758
is going to be executed on that you

13048
09:13:32,078 --> 09:13:37,918
should have probably seen this before

13049
09:13:33,758 --> 09:13:40,598
it's not too bad um and then we do um we

13050
09:13:37,918 --> 09:13:42,078
can actually just print out uh whatever

13051
09:13:40,598 --> 09:13:47,438
that input

13052
09:13:42,078 --> 09:13:49,278
was um and then here we do um like here

13053
09:13:47,438 --> 09:13:51,558
we we essentially just Benchmark so we

13054
09:13:49,278 --> 09:13:53,840
go um the actual activation function

13055
09:13:51,558 --> 09:13:55,680
itself the input we're doing it on

13056
09:13:53,840 --> 09:13:58,320
um and then just a string that we're

13057
09:13:55,680 --> 09:14:01,878
going to print in the uh in the name

13058
09:13:58,320 --> 09:14:03,438
here so here we just set a time we go

13059
09:14:01,878 --> 09:14:06,360
for a number of

13060
09:14:03,438 --> 09:14:08,238
runs uh and then we we we Cuda

13061
09:14:06,360 --> 09:14:10,520
synchronize so we ensure everything is

13062
09:14:08,238 --> 09:14:12,598
all caught up and then we end the time

13063
09:14:10,520 --> 09:14:15,718
and then we essentially return the name

13064
09:14:12,598 --> 09:14:20,480
so whatever whichever these it

13065
09:14:15,718 --> 09:14:22,238
was um the the Delta time so like the

13066
09:14:20,480 --> 09:14:24,398
essentially the not the Delta time but

13067
09:14:22,238 --> 09:14:25,598
the difference um and then we divide

13068
09:14:24,398 --> 09:14:28,558
that by the number of runs because we're

13069
09:14:25,598 --> 09:14:31,238
doing an average times a th so that we

13070
09:14:28,558 --> 09:14:32,718
can get it in milliseconds format um and

13071
09:14:31,238 --> 09:14:36,878
that just that just kind of works as we

13072
09:14:32,718 --> 09:14:41,480
expected to now to actually compile this

13073
09:14:36,878 --> 09:14:43,480
um we go to uh we go to the read me and

13074
09:14:41,480 --> 09:14:47,558
literally all you have to do is just

13075
09:14:43,480 --> 09:14:47,558
python uh setup.py

13076
09:14:50,360 --> 09:14:57,438
install give that a second it's going to

13077
09:14:53,398 --> 09:14:57,438
use ninja to build

13078
09:14:58,480 --> 09:15:04,278
um so we'll we'll give that a moment um

13079
09:15:02,238 --> 09:15:08,878
but this is going to create see a build

13080
09:15:04,278 --> 09:15:08,878
a build build folder in here

13081
09:15:24,718 --> 09:15:28,520
and we'll print out um forward and

13082
09:15:27,122 --> 09:15:31,320
backward in a second we're going we're

13083
09:15:28,520 --> 09:15:33,758
going to print both of these

13084
09:15:31,320 --> 09:15:36,558
so okay awesome it just it just wrote

13085
09:15:33,758 --> 09:15:38,718
that out so now we can go ahead and

13086
09:15:36,558 --> 09:15:42,000
python polinomial

13087
09:15:38,718 --> 09:15:45,360
activation and it'll it'll do these this

13088
09:15:42,000 --> 09:15:47,360
Cuda activation. forward of x uh and

13089
09:15:45,360 --> 09:15:50,160
then it'll print whatever that output

13090
09:15:47,360 --> 09:15:51,878
was and so we get like a tensor and it

13091
09:15:50,160 --> 09:15:53,640
it actually formats this properly which

13092
09:15:51,878 --> 09:15:56,480
you're like oh my gosh we just we wrote

13093
09:15:53,640 --> 09:15:58,360
this in in in Cuda and c and now it

13094
09:15:56,480 --> 09:16:00,598
prints so nicely like this yeah that's

13095
09:15:58,360 --> 09:16:03,122
what pytorch does for you um and then we

13096
09:16:00,598 --> 09:16:07,000
see that the pytorch built-in gets about

13097
09:16:03,122 --> 09:16:09,278
10.47 milliseconds over uh over a th000

13098
09:16:07,000 --> 09:16:11,078
runs on average and then the Cuda

13099
09:16:09,278 --> 09:16:13,960
extension gets about

13100
09:16:11,078 --> 09:16:16,640
0.0 243

13101
09:16:13,960 --> 09:16:20,000
milliseconds so if we actually compare

13102
09:16:16,640 --> 09:16:24,598
these so if I go

13103
09:16:20,000 --> 09:16:26,960
um this divided by uh this we will see

13104
09:16:24,598 --> 09:16:28,918
the speed up that we get from kud so we

13105
09:16:26,960 --> 09:16:31,640
get about a

13106
09:16:28,918 --> 09:16:34,278
431x speed up which is pretty good if

13107
09:16:31,640 --> 09:16:37,360
you ask me especially on bigger tensors

13108
09:16:34,278 --> 09:16:40,480
right this would be awesome uh so so

13109
09:16:37,360 --> 09:16:43,278
that's that and if we actually do a DOT

13110
09:16:40,480 --> 09:16:45,000
backward here you'll see that it's both

13111
09:16:43,278 --> 09:16:46,918
you know it changed to White so that

13112
09:16:45,000 --> 09:16:49,320
that's a good good sign that it doesn't

13113
09:16:46,918 --> 09:16:53,238
work and if we actually run this it'll

13114
09:16:49,320 --> 09:16:56,320
say uh has no attribute backward right

13115
09:16:53,238 --> 09:17:00,000
um so essentially raise the attribute

13116
09:16:56,320 --> 09:17:02,718
error um doesn't work and then we

13117
09:17:00,000 --> 09:17:02,718
default back to

13118
09:17:04,320 --> 09:17:10,558
forward and it'll pop back to this to

13119
09:17:07,360 --> 09:17:12,878
this nn. module uh awesome okay so

13120
09:17:10,558 --> 09:17:14,918
that's uh that's pytorch extensions for

13121
09:17:12,878 --> 09:17:17,918
you you can feel free to add whatever

13122
09:17:14,918 --> 09:17:19,718
you want to this um so you know if you

13123
09:17:17,918 --> 09:17:21,558
have your own custom like research that

13124
09:17:19,718 --> 09:17:23,320
you want to add and make it super easy

13125
09:17:21,558 --> 09:17:25,598
for others and and yourself and your

13126
09:17:23,320 --> 09:17:27,360
organization to use uh totally feel free

13127
09:17:25,598 --> 09:17:28,718
to go down this route copy this copy

13128
09:17:27,360 --> 09:17:30,960
this template code do whatever you want

13129
09:17:28,718 --> 09:17:33,360
with it um this is just the easy the

13130
09:17:30,960 --> 09:17:38,480
easiest example I found um to both you

13131
09:17:33,360 --> 09:17:42,238
know write and explain so uh yeah that's

13132
09:17:38,480 --> 09:17:44,800
that uh that is pretty much the

13133
09:17:42,238 --> 09:17:46,800
first the first uh I don't even know I'm

13134
09:17:44,800 --> 09:17:49,278
not going to unify a percent but that

13135
09:17:46,800 --> 09:17:51,758
was the majority of the course now we

13136
09:17:49,278 --> 09:17:54,598
actually have a final project to go into

13137
09:17:51,758 --> 09:17:57,438
so this final project is super exciting

13138
09:17:54,598 --> 09:17:59,878
and it's going to help you understand uh

13139
09:17:57,438 --> 09:18:03,398
neural networks from scratch how to

13140
09:17:59,878 --> 09:18:05,122
optimize them for performance um as well

13141
09:18:03,398 --> 09:18:06,680
as like data loaders we're going to add

13142
09:18:05,122 --> 09:18:09,200
a bunch of things a bunch of

13143
09:18:06,680 --> 09:18:11,918
optimizations into making a real world

13144
09:18:09,200 --> 09:18:11,918
training run

13145
09:18:12,718 --> 09:18:19,238
work I am so so excited for this part in

13146
09:18:17,360 --> 09:18:21,598
this last section of the course we're

13147
09:18:19,238 --> 09:18:23,718
going to be doing the final project this

13148
09:18:21,598 --> 09:18:27,480
final project is awesome we're going to

13149
09:18:23,718 --> 09:18:29,078
do an mnist MLP training run from

13150
09:18:27,480 --> 09:18:31,000
scratch it's going to go in the

13151
09:18:29,078 --> 09:18:33,918
following order we're going to start in

13152
09:18:31,000 --> 09:18:35,918
Python pytorch super easy right then

13153
09:18:33,918 --> 09:18:37,360
we're going to go to numpy make it a

13154
09:18:35,918 --> 09:18:40,078
little harder but understand what's

13155
09:18:37,360 --> 09:18:42,480
going on under the hood and then we're

13156
09:18:40,078 --> 09:18:44,040
going to go take our numpy code and Port

13157
09:18:42,480 --> 09:18:45,758
that over to C right it's going to run

13158
09:18:44,040 --> 09:18:47,878
on CPU it's going to be super easy to

13159
09:18:45,758 --> 09:18:49,200
read and comprehend and understand right

13160
09:18:47,878 --> 09:18:51,160
then we're going to push that over to

13161
09:18:49,200 --> 09:18:55,918
Cuda and then we're going to rank it

13162
09:18:51,160 --> 09:18:57,520
fast in Cuda awesome so navigate over to

13163
09:18:55,918 --> 09:18:59,200
this folder we're going to do a

13164
09:18:57,520 --> 09:19:00,878
different one so it's not going to be uh

13165
09:18:59,200 --> 09:19:02,918
SL Cuda course it's going to be slash

13166
09:19:00,878 --> 09:19:04,520
mnus Cuda is like a just a separate

13167
09:19:02,918 --> 09:19:05,758
thing to help organize things better

13168
09:19:04,520 --> 09:19:08,360
maybe you want to show this to a friend

13169
09:19:05,758 --> 09:19:09,398
or whatever or or present as someone and

13170
09:19:08,360 --> 09:19:10,918
this way you can kind of just have

13171
09:19:09,398 --> 09:19:12,840
things separated out and neatly

13172
09:19:10,918 --> 09:19:14,520
organized right so you can consider this

13173
09:19:12,840 --> 09:19:16,120
as like kind of a separate part we're

13174
09:19:14,520 --> 09:19:18,720
just kind of building on what we did

13175
09:19:16,120 --> 09:19:20,276
previously uh so if we copy this we're

13176
09:19:18,720 --> 09:19:22,960
going to go ahead and actually clone

13177
09:19:20,276 --> 09:19:24,800
this into uh a new directory and I'll

13178
09:19:22,960 --> 09:19:26,680
kind of walk you through how things are

13179
09:19:24,800 --> 09:19:29,240
going to go so I'll go ahead and full

13180
09:19:26,680 --> 09:19:29,240
screen this

13181
09:19:31,596 --> 09:19:39,720
up sure already saved now I'm going to

13182
09:19:35,240 --> 09:19:42,400
CD into that going to go uh UV VNV just

13183
09:19:39,720 --> 09:19:43,640
like that I'm going to activate this I

13184
09:19:42,400 --> 09:19:44,916
just create like a python virtual

13185
09:19:43,640 --> 09:19:48,880
environment that's all I really did

13186
09:19:44,916 --> 09:19:54,560
there uh and then we're going to go pip

13187
09:19:48,880 --> 09:19:56,320
uh UV pip install d r

13188
09:19:54,560 --> 09:20:00,120
requirements and it's going to go ahead

13189
09:19:56,320 --> 09:20:04,120
and install everything that we need um

13190
09:20:00,120 --> 09:20:06,360
okay sweet now we're going to pop into

13191
09:20:04,120 --> 09:20:10,160
the uh we're actually just going to open

13192
09:20:06,360 --> 09:20:14,680
this up in vs code go ahead and do that

13193
09:20:10,160 --> 09:20:18,200
um now if we pop into the uh let me full

13194
09:20:14,680 --> 09:20:20,800
screen this perfect okay so if we go

13195
09:20:18,200 --> 09:20:22,360
into this python folder just to like

13196
09:20:20,800 --> 09:20:23,480
help you navigate the structure of this

13197
09:20:22,360 --> 09:20:26,240
so we're we're going to what we're going

13198
09:20:23,480 --> 09:20:27,756
to do is we're going to progress up from

13199
09:20:26,240 --> 09:20:31,320
uh python so we're going to go through

13200
09:20:27,756 --> 09:20:34,160
pytorch uh as seen in

13201
09:20:31,320 --> 09:20:36,360
here uh so that pretty much just like an

13202
09:20:34,160 --> 09:20:37,560
entire P torch training run for a

13203
09:20:36,360 --> 09:20:40,400
multi-layer

13204
09:20:37,560 --> 09:20:42,560
perceptron uh and then a little Jupiter

13205
09:20:40,400 --> 09:20:45,040
notebook that's like the same as that uh

13206
09:20:42,560 --> 09:20:47,960
but just in a notebook format instead

13207
09:20:45,040 --> 09:20:50,720
and then we have a numpy script so just

13208
09:20:47,960 --> 09:20:53,120
uh going through and writing everything

13209
09:20:50,720 --> 09:20:55,560
from scratch and numpy

13210
09:20:53,120 --> 09:20:57,200
um this may not look the same by the

13211
09:20:55,560 --> 09:21:00,080
time you're watching this but it'll be

13212
09:20:57,200 --> 09:21:02,200
very similar and very easy to understand

13213
09:21:00,080 --> 09:21:04,080
um at least for the python stuff so I

13214
09:21:02,200 --> 09:21:05,840
mean you you've probably already written

13215
09:21:04,080 --> 09:21:07,840
some pytorch already or at least you

13216
09:21:05,840 --> 09:21:09,480
understand like the basic you know nn.

13217
09:21:07,840 --> 09:21:11,680
linear layer right you probably

13218
09:21:09,480 --> 09:21:14,560
understand that already so all we're

13219
09:21:11,680 --> 09:21:16,000
actually doing in this one and this is

13220
09:21:14,560 --> 09:21:18,960
what I'm going to demo initially is

13221
09:21:16,000 --> 09:21:22,680
we're just uh we're literally just

13222
09:21:18,960 --> 09:21:24,800
training an mnist MLP from scratch using

13223
09:21:22,680 --> 09:21:28,840
basic Pi torch so I import everything

13224
09:21:24,800 --> 09:21:31,720
initially so like time nump torch right

13225
09:21:28,840 --> 09:21:34,400
uh the data loader and the torch vision

13226
09:21:31,720 --> 09:21:38,276
for the data set itself which is mest um

13227
09:21:34,400 --> 09:21:41,276
we specify hybrid parameters here so um

13228
09:21:38,276 --> 09:21:43,080
you know learning rate uh batch size

13229
09:21:41,276 --> 09:21:44,400
number of epoch I think there's like an

13230
09:21:43,080 --> 09:21:47,720
extra one here I don't know why that's

13231
09:21:44,400 --> 09:21:50,040
there um you know train size for like

13232
09:21:47,720 --> 09:21:51,436
number of elements in the train set and

13233
09:21:50,040 --> 09:21:54,520
then we're going to go and adjust this

13234
09:21:51,436 --> 09:21:54,520
data directory

13235
09:21:58,520 --> 09:22:02,320
uh just print this out

13236
09:22:05,320 --> 09:22:12,436
actually take

13237
09:22:08,560 --> 09:22:12,436
that inject it into

13238
09:22:13,160 --> 09:22:19,916
here and we'll go slash SL python all

13239
09:22:18,120 --> 09:22:24,680
right

13240
09:22:19,916 --> 09:22:27,000
now uh sure SL data why

13241
09:22:24,680 --> 09:22:29,040
not now this is going to make sure that

13242
09:22:27,000 --> 09:22:31,520
we're using tf32 so that's going to use

13243
09:22:29,040 --> 09:22:33,200
tensor cores and make things really fast

13244
09:22:31,520 --> 09:22:35,040
um this is going to initialize the data

13245
09:22:33,200 --> 09:22:37,160
set properly with like you know mean and

13246
09:22:35,040 --> 09:22:39,400
standard deviation um this is just kind

13247
09:22:37,160 --> 09:22:41,200
of the best the best practice for mnus

13248
09:22:39,400 --> 09:22:43,840
so a lot of this here I'm kind of just

13249
09:22:41,200 --> 09:22:46,680
following uh some boiler plate uh code

13250
09:22:43,840 --> 09:22:49,400
template examples that kind of thing uh

13251
09:22:46,680 --> 09:22:51,640
and now we go down further and

13252
09:22:49,400 --> 09:22:54,880
it's you know we initialize our data

13253
09:22:51,640 --> 09:22:57,436
sets we initialize our loaders we load

13254
09:22:54,880 --> 09:22:59,360
in all of the data uh and we have this

13255
09:22:57,436 --> 09:23:02,560
exist on the CPU so notice how we're not

13256
09:22:59,360 --> 09:23:04,160
doing um device equals Cuda or do Cuda

13257
09:23:02,560 --> 09:23:09,560
yet we're not doing that this is this

13258
09:23:04,160 --> 09:23:14,720
just exists on CPU right uh system Ram

13259
09:23:09,560 --> 09:23:18,276
so we load all of our train get in

13260
09:23:14,720 --> 09:23:19,800
right Lo of our test data in we we're

13261
09:23:18,276 --> 09:23:22,800
just printing some stuff at each step

13262
09:23:19,800 --> 09:23:25,560
here it per Epoch so per epoch we're

13263
09:23:22,800 --> 09:23:27,800
going to do train size which is 10,000

13264
09:23:25,560 --> 09:23:30,640
divided by batch size which in this case

13265
09:23:27,800 --> 09:23:32,480
is four so it's going to be about 2,500

13266
09:23:30,640 --> 09:23:36,000
iterations per Epoch and we're going to

13267
09:23:32,480 --> 09:23:39,080
do uh three different epochs so

13268
09:23:36,000 --> 09:23:42,276
7,500 uh steps

13269
09:23:39,080 --> 09:23:44,240
total now we go down further and we have

13270
09:23:42,276 --> 09:23:47,880
the actual architecture for this itself

13271
09:23:44,240 --> 09:23:49,200
right um so we take in we take in an X

13272
09:23:47,880 --> 09:23:53,276
which is our which is our input we

13273
09:23:49,200 --> 09:23:56,160
reshape this to batch size by 784

13274
09:23:53,276 --> 09:23:59,800
um we do our first layer second layer

13275
09:23:56,160 --> 09:24:02,520
third right so it's just like mat one

13276
09:23:59,800 --> 09:24:05,276
activation m 2 and this is organized as

13277
09:24:02,520 --> 09:24:06,800
such um in features hidden features

13278
09:24:05,276 --> 09:24:08,640
hidden hidden features and then the

13279
09:24:06,800 --> 09:24:12,560
output Dimension or num

13280
09:24:08,640 --> 09:24:15,916
class uh we have our in feature set to

13281
09:24:12,560 --> 09:24:20,916
784 so uh this way it's going to be like

13282
09:24:15,916 --> 09:24:24,160
a like a batch size by um batch size is

13283
09:24:20,916 --> 09:24:28,040
the X and then weight is going to be 784

13284
09:24:24,160 --> 09:24:29,276
by 256 right so that's what that's

13285
09:24:28,040 --> 09:24:31,520
that's the shape of this it's going to

13286
09:24:29,276 --> 09:24:33,000
multiply by this linear layer and then

13287
09:24:31,520 --> 09:24:34,160
we have the rally which is going to take

13288
09:24:33,000 --> 09:24:35,720
that and it's going to do the Rue

13289
09:24:34,160 --> 09:24:36,840
activation on it you can search that up

13290
09:24:35,720 --> 09:24:40,560
if you don't know what that is already

13291
09:24:36,840 --> 09:24:42,840
it's very very basic to understand um

13292
09:24:40,560 --> 09:24:45,756
it's literally just a

13293
09:24:42,840 --> 09:24:48,480
graph and in. linear same idea we're

13294
09:24:45,756 --> 09:24:50,520
going to take that previous um batch

13295
09:24:48,480 --> 09:24:52,320
size by hidden features we're going to

13296
09:24:50,520 --> 09:24:53,680
multiply that by a hidden featur

13297
09:24:52,320 --> 09:24:55,916
feachers by num classes and we're going

13298
09:24:53,680 --> 09:24:57,480
to end up with B by num classes right or

13299
09:24:55,916 --> 09:25:01,080
B by output Dimension whatever you want

13300
09:24:57,480 --> 09:25:02,800
to say right and that's the idea is we

13301
09:25:01,080 --> 09:25:04,160
just want to keep forward and back

13302
09:25:02,800 --> 09:25:06,360
propagating through that and making the

13303
09:25:04,160 --> 09:25:07,880
network smarter we're going to go Ahad

13304
09:25:06,360 --> 09:25:10,400
and transfer the model to

13305
09:25:07,880 --> 09:25:12,840
Cuda we're going to we can do we can do

13306
09:25:10,400 --> 09:25:13,960
torw compile for a faster version but

13307
09:25:12,840 --> 09:25:15,680
we're not going to do that just because

13308
09:25:13,960 --> 09:25:18,916
it like takes a little bit of extra time

13309
09:25:15,680 --> 09:25:21,596
to do that um we're going to use cross

13310
09:25:18,916 --> 09:25:24,040
entropy lws for this for the entire

13311
09:25:21,596 --> 09:25:28,756
project here cross entropy loss is is

13312
09:25:24,040 --> 09:25:31,276
critical to uh understand

13313
09:25:28,756 --> 09:25:33,000
um I'm going to go over some of this as

13314
09:25:31,276 --> 09:25:34,360
well like don't worry if this doesn't

13315
09:25:33,000 --> 09:25:36,480
make sense yet we're going to like

13316
09:25:34,360 --> 09:25:37,916
literally write this in C so don't worry

13317
09:25:36,480 --> 09:25:39,880
if this doesn't entirely make sense I'm

13318
09:25:37,916 --> 09:25:42,040
just kind of giving you the rundown as

13319
09:25:39,880 --> 09:25:43,480
to like what everything is based off of

13320
09:25:42,040 --> 09:25:45,720
we're going to use stochastic gradient

13321
09:25:43,480 --> 09:25:47,960
descent so that's just your literally

13322
09:25:45,720 --> 09:25:50,480
your Optimizer so it's going to just

13323
09:25:47,960 --> 09:25:52,596
nudge the uh you know when you calculate

13324
09:25:50,480 --> 09:25:55,596
like a gradient or the S so the error

13325
09:25:52,596 --> 09:25:58,680
for a given weight uh it's going to just

13326
09:25:55,596 --> 09:26:01,200
literally do learning rate times that

13327
09:25:58,680 --> 09:26:03,040
and then subtract that result from the

13328
09:26:01,200 --> 09:26:07,400
from the actual weight

13329
09:26:03,040 --> 09:26:09,560
itself so uh stochastic rate in a sense

13330
09:26:07,400 --> 09:26:12,840
sounds comple uh complicated but it's

13331
09:26:09,560 --> 09:26:16,080
not it it's not that bad uh and then we

13332
09:26:12,840 --> 09:26:18,000
just have our training Loop here um and

13333
09:26:16,080 --> 09:26:20,880
I'll I'll I'll go through sort of more

13334
09:26:18,000 --> 09:26:23,756
intimately what this means in the in the

13335
09:26:20,880 --> 09:26:25,640
numpy script in a second here here but

13336
09:26:23,756 --> 09:26:28,596
all that matters is that we kind of

13337
09:26:25,640 --> 09:26:31,200
understand uh this boiler plate this is

13338
09:26:28,596 --> 09:26:33,120
designed to be like quite minimalistic

13339
09:26:31,200 --> 09:26:37,120
um just kind of match what we're doing

13340
09:26:33,120 --> 09:26:38,520
everywhere else and uh yeah I'm not

13341
09:26:37,120 --> 09:26:40,720
going to highlight this too much but

13342
09:26:38,520 --> 09:26:45,360
we'll go ahead and run this uh just just

13343
09:26:40,720 --> 09:26:48,240
for just for fun you know CD into there

13344
09:26:45,360 --> 09:26:50,000
we'll do python forch reference and

13345
09:26:48,240 --> 09:26:51,840
literally what this is going to do it's

13346
09:26:50,000 --> 09:26:54,240
just going to install our nness data so

13347
09:26:51,840 --> 09:26:57,240
notice how we got it in the python data

13348
09:26:54,240 --> 09:26:57,240
directory

13349
09:26:59,276 --> 09:27:05,040
um awesome so now it's initializing

13350
09:27:02,040 --> 09:27:07,960
everything it's loading all the data um

13351
09:27:05,040 --> 09:27:11,480
so we can see this this is learning over

13352
09:27:07,960 --> 09:27:13,880
three Epoch total of 25 200 each and we

13353
09:27:11,480 --> 09:27:16,960
end up with an average batch accuracy of

13354
09:27:13,880 --> 09:27:19,120
90% which is really good so when it's

13355
09:27:16,960 --> 09:27:20,916
classifying those digits it's getting

13356
09:27:19,120 --> 09:27:23,360
about n out of 10 of those guesses

13357
09:27:20,916 --> 09:27:25,400
correct which is really good uh and we

13358
09:27:23,360 --> 09:27:30,200
can see that by the loss here right the

13359
09:27:25,400 --> 09:27:33,000
loss is if I move this up the loss we

13360
09:27:30,200 --> 09:27:36,480
start from you know loading everything

13361
09:27:33,000 --> 09:27:39,680
in start from a loss of about 2.38 which

13362
09:27:36,480 --> 09:27:43,560
if we pop into um if we pop into Chrome

13363
09:27:39,680 --> 09:27:46,960
here go to go to wol from alpha just to

13364
09:27:43,560 --> 09:27:49,080
like provide a reference

13365
09:27:46,960 --> 09:27:53,240
um

13366
09:27:49,080 --> 09:27:56,360
exp of -2. was it

13367
09:27:53,240 --> 09:28:02,040
2.38 that's about 0.92 if it's like if

13368
09:27:56,360 --> 09:28:04,320
we round down and say -2.3 it's about uh

13369
09:28:02,040 --> 09:28:06,916
0.1 which if you convert that to a

13370
09:28:04,320 --> 09:28:08,840
percentage is about 10% accuracy which

13371
09:28:06,916 --> 09:28:11,360
is exactly what we want they their

13372
09:28:08,840 --> 09:28:13,800
images arranged or or the labels are

13373
09:28:11,360 --> 09:28:16,436
between zero and N9 so zero like

13374
09:28:13,800 --> 09:28:19,160
literally there's 10 values there so

13375
09:28:16,436 --> 09:28:20,640
there should be at initialization a 10%

13376
09:28:19,160 --> 09:28:23,240
chance of getting one correct because

13377
09:28:20,640 --> 09:28:24,640
it's randomly initialized right right um

13378
09:28:23,240 --> 09:28:27,840
and so that's exactly what we want

13379
09:28:24,640 --> 09:28:32,720
everything is initialized properly there

13380
09:28:27,840 --> 09:28:34,916
um now before I actually go into uh the

13381
09:28:32,720 --> 09:28:36,560
numpy implementation like numpy is like

13382
09:28:34,916 --> 09:28:39,720
taking P torch and seeing like what the

13383
09:28:36,560 --> 09:28:41,916
heck that's doing under the hood um just

13384
09:28:39,720 --> 09:28:43,916
like kind of a from scratch approach I

13385
09:28:41,916 --> 09:28:48,200
would recommend that you pause the video

13386
09:28:43,916 --> 09:28:51,040
right now and watch um Andre carpo's

13387
09:28:48,200 --> 09:28:52,640
micro guide video so he made this micro

13388
09:28:51,040 --> 09:28:55,160
guide video about two years ago it's

13389
09:28:52,640 --> 09:28:57,240
done really really well and it literally

13390
09:28:55,160 --> 09:28:58,840
just explains the concept of back

13391
09:28:57,240 --> 09:29:01,560
propagation on the level of scalar

13392
09:28:58,840 --> 09:29:03,240
values so in this one we're using the uh

13393
09:29:01,560 --> 09:29:06,480
we're using back propagation on the

13394
09:29:03,240 --> 09:29:09,276
level of tensors but understanding this

13395
09:29:06,480 --> 09:29:12,360
uh is crucial for moving up the tensors

13396
09:29:09,276 --> 09:29:13,916
so after after you're done this and it's

13397
09:29:12,360 --> 09:29:15,720
like still a little bit confusing about

13398
09:29:13,916 --> 09:29:18,040
how we're going to bring this up to like

13399
09:29:15,720 --> 09:29:20,596
bigger matrices I would recommend that

13400
09:29:18,040 --> 09:29:22,240
you uh also take a peek at my back

13401
09:29:20,596 --> 09:29:24,560
propagation video this is just on on my

13402
09:29:22,240 --> 09:29:27,756
YouTube channel here Elliot codes um

13403
09:29:24,560 --> 09:29:31,320
about right now 5.7 th000 subscribers

13404
09:29:27,756 --> 09:29:33,720
you should be able to find me um and I

13405
09:29:31,320 --> 09:29:35,596
just kind of made like a funny title um

13406
09:29:33,720 --> 09:29:37,680
because back propagation is annoying to

13407
09:29:35,596 --> 09:29:40,756
understand sometimes and so I I did a I

13408
09:29:37,680 --> 09:29:42,640
tried to do my best job to break down uh

13409
09:29:40,756 --> 09:29:44,916
what the heck that means on the on the

13410
09:29:42,640 --> 09:29:48,520
level of tensors on the Whiteboard um

13411
09:29:44,916 --> 09:29:50,756
this does have this does have uh 1440p

13412
09:29:48,520 --> 09:29:54,040
quality so you can actually you can you

13413
09:29:50,756 --> 09:29:55,560
can see stuff um but yeah and then there

13414
09:29:54,040 --> 09:29:56,720
there's just like a ton of like content

13415
09:29:55,560 --> 09:29:58,240
that you can watch right I don't

13416
09:29:56,720 --> 09:30:00,360
encourage you to just like consume all

13417
09:29:58,240 --> 09:30:02,480
the content out there but these are the

13418
09:30:00,360 --> 09:30:04,720
three that I thought were the easiest uh

13419
09:30:02,480 --> 09:30:06,880
to like build an intuition on what back

13420
09:30:04,720 --> 09:30:09,360
propagation is um we're going to be

13421
09:30:06,880 --> 09:30:10,276
going over back propagation as well um

13422
09:30:09,360 --> 09:30:13,160
but this

13423
09:30:10,276 --> 09:30:15,560
is uh this is like where you want to

13424
09:30:13,160 --> 09:30:18,756
start from if this is like a completely

13425
09:30:15,560 --> 09:30:22,756
foreign concept to you um so Entre aroy

13426
09:30:18,756 --> 09:30:26,436
does really good I I go above Andre arpo

13427
09:30:22,756 --> 09:30:27,960
um lecture and go up to tensors and then

13428
09:30:26,436 --> 09:30:29,040
three blue and brown which you you've

13429
09:30:27,960 --> 09:30:32,520
probably heard of

13430
09:30:29,040 --> 09:30:35,276
already uh does does a a quick little

13431
09:30:32,520 --> 09:30:37,480
lecture on uh on back propagation

13432
09:30:35,276 --> 09:30:38,480
intuitively so let's go Ahad and jump

13433
09:30:37,480 --> 09:30:41,640
into

13434
09:30:38,480 --> 09:30:43,756
numpy okay awesome so now uh let's go

13435
09:30:41,640 --> 09:30:45,800
Ahad and take a look at what exactly

13436
09:30:43,756 --> 09:30:48,000
this diagram is supposed to be um I

13437
09:30:45,800 --> 09:30:51,276
could I could walk through and sort of

13438
09:30:48,000 --> 09:30:52,680
just like explain what's going down uh

13439
09:30:51,276 --> 09:30:54,916
like what essentially what is happening

13440
09:30:52,680 --> 09:30:56,520
step by step in here but first I want to

13441
09:30:54,916 --> 09:30:57,880
make this obvious how our neuronet

13442
09:30:56,520 --> 09:30:59,960
architecture is structured and how the

13443
09:30:57,880 --> 09:31:02,320
neurons actually work right what is a

13444
09:30:59,960 --> 09:31:06,360
neuron that type of thing

13445
09:31:02,320 --> 09:31:07,520
um so I drew this little diagram to help

13446
09:31:06,360 --> 09:31:09,640
us understand what's happening and I'm

13447
09:31:07,520 --> 09:31:11,880
going to lay this out here so we

13448
09:31:09,640 --> 09:31:14,916
essentially take an image this is the uh

13449
09:31:11,880 --> 09:31:16,080
hand handwritten digit it's 28 by 28 and

13450
09:31:14,916 --> 09:31:17,960
we're going to have this in a batch

13451
09:31:16,080 --> 09:31:20,160
right so it's going to be 28 by 28 and

13452
09:31:17,960 --> 09:31:22,916
then depth be our batch batch size you

13453
09:31:20,160 --> 09:31:26,000
could say um just a bunch of panes

13454
09:31:22,916 --> 09:31:28,800
essentially layered on top of each other

13455
09:31:26,000 --> 09:31:30,596
we flatten these so 28 * 28 that's going

13456
09:31:28,800 --> 09:31:32,120
to is going to have this this whole

13457
09:31:30,596 --> 09:31:33,756
image and we're going to take a row and

13458
09:31:32,120 --> 09:31:35,120
we're just going to add it to the end

13459
09:31:33,756 --> 09:31:37,400
until it stretches out to the full

13460
09:31:35,120 --> 09:31:40,240
length it's going to be 28 of these rows

13461
09:31:37,400 --> 09:31:42,276
that are added to the edge right and

13462
09:31:40,240 --> 09:31:46,640
then we we plug this into the first

13463
09:31:42,276 --> 09:31:52,916
weight so X1 * W1 that's going to be b b

13464
09:31:46,640 --> 09:31:53,880
by 784 so B by 784 and then times 7 84

13465
09:31:52,916 --> 09:31:58,240
by

13466
09:31:53,880 --> 09:32:01,080
256 so a column uh the the the the size

13467
09:31:58,240 --> 09:32:04,360
of a column in the weight Matrix is 784

13468
09:32:01,080 --> 09:32:06,720
and then the length of a row in X is

13469
09:32:04,360 --> 09:32:08,360
also the 784 so it's going to layer on

13470
09:32:06,720 --> 09:32:10,720
top of those

13471
09:32:08,360 --> 09:32:13,960
right um and we're going to end up with

13472
09:32:10,720 --> 09:32:15,640
P by 256 then we go to the next one and

13473
09:32:13,960 --> 09:32:16,916
we get this we essentially just feed

13474
09:32:15,640 --> 09:32:18,680
this through the Rue so it's going to

13475
09:32:16,916 --> 09:32:21,480
just it's going to do the the rally

13476
09:32:18,680 --> 09:32:24,360
function on each value pretty simple

13477
09:32:21,480 --> 09:32:26,800
then we do X2 * W2 and that's going to

13478
09:32:24,360 --> 09:32:28,916
do B by 256 which was output from The

13479
09:32:26,800 --> 09:32:32,200
Rue it's going to Matrix multiply that

13480
09:32:28,916 --> 09:32:34,720
with 256 by 10 10 is our output size

13481
09:32:32,200 --> 09:32:37,596
right so we have this B by headden size

13482
09:32:34,720 --> 09:32:39,840
which is 256 that's each uh that's each

13483
09:32:37,596 --> 09:32:42,120
neuron output that's that's uh that's

13484
09:32:39,840 --> 09:32:46,040
essentially how many um neurons we have

13485
09:32:42,120 --> 09:32:48,880
is 256 in our in our in our hidden layer

13486
09:32:46,040 --> 09:32:51,596
there and then we just have to ensure

13487
09:32:48,880 --> 09:32:53,320
that broadcasting is done correctly so

13488
09:32:51,596 --> 09:32:56,160
uh I mean you could always you could pop

13489
09:32:53,320 --> 09:32:58,436
over to like pytorch uh

13490
09:32:56,160 --> 09:33:01,560
broadcasting broadcast

13491
09:32:58,436 --> 09:33:03,080
rules broadcasting semantics here so

13492
09:33:01,560 --> 09:33:05,116
this is pretty much just how you're

13493
09:33:03,080 --> 09:33:06,960
allowed to like multiply things and and

13494
09:33:05,116 --> 09:33:10,400
do operations with tensors when they're

13495
09:33:06,960 --> 09:33:12,080
certain sizes right so um you know if

13496
09:33:10,400 --> 09:33:14,400
you haven't gone through this already

13497
09:33:12,080 --> 09:33:17,200
it's it probably is a good idea um it's

13498
09:33:14,400 --> 09:33:19,360
not too hard to learn pytor handles this

13499
09:33:17,200 --> 09:33:21,360
stuff pretty well so it's just a little

13500
09:33:19,360 --> 09:33:22,560
short short read to do just kind of

13501
09:33:21,360 --> 09:33:24,596
understand like what's happening there

13502
09:33:22,560 --> 09:33:25,960
but um essentially you just want to make

13503
09:33:24,596 --> 09:33:27,160
sure those inner values are the same and

13504
09:33:25,960 --> 09:33:29,960
then the outer ones are going to be your

13505
09:33:27,160 --> 09:33:33,596
output shape right so we have this B

13506
09:33:29,960 --> 09:33:35,596
this B by 256 and then the the 256 here

13507
09:33:33,596 --> 09:33:38,240
is going to just it's going to do

13508
09:33:35,596 --> 09:33:39,916
product it's going to take each column

13509
09:33:38,240 --> 09:33:43,720
and do product there and you're going to

13510
09:33:39,916 --> 09:33:47,240
end up with a total of uh 10 values for

13511
09:33:43,720 --> 09:33:50,360
each batch element right and this is

13512
09:33:47,240 --> 09:33:53,960
essentially uh this this this 10 here is

13513
09:33:50,360 --> 09:33:56,596
a distribution of probabilities so prob

13514
09:33:53,960 --> 09:34:00,040
probability distribution in batches and

13515
09:33:56,596 --> 09:34:02,880
in in a batch um and those are going to

13516
09:34:00,040 --> 09:34:05,560
be our predictions the probability of

13517
09:34:02,880 --> 09:34:08,000
which digit it is so if the screen says

13518
09:34:05,560 --> 09:34:10,400
like a zero

13519
09:34:08,000 --> 09:34:13,480
um then what's going to happen after our

13520
09:34:10,400 --> 09:34:16,560
network is really smart is the if we go

13521
09:34:13,480 --> 09:34:18,916
to say any batch element so any any

13522
09:34:16,560 --> 09:34:21,720
essentially row and we go to the first

13523
09:34:18,916 --> 09:34:22,756
the zero with index in that um that's

13524
09:34:21,720 --> 09:34:24,436
most likely going to be the highest

13525
09:34:22,756 --> 09:34:27,240
number in the entire distribution right

13526
09:34:24,436 --> 09:34:29,040
that's kind of what's going there um and

13527
09:34:27,240 --> 09:34:30,720
then we just do the a loss function on

13528
09:34:29,040 --> 09:34:32,840
this called cross entropy loss which

13529
09:34:30,720 --> 09:34:35,720
I'll explain in a second CR cross

13530
09:34:32,840 --> 09:34:37,240
entropy loss isn't too bad um it's

13531
09:34:35,720 --> 09:34:38,756
different than MSE loss but it's better

13532
09:34:37,240 --> 09:34:41,400
for batches and kind of what we're doing

13533
09:34:38,756 --> 09:34:43,720
here um

13534
09:34:41,400 --> 09:34:44,960
so we get our loss which initially is

13535
09:34:43,720 --> 09:34:48,116
going to be around

13536
09:34:44,960 --> 09:34:50,320
2.3 um based on the wol from alpha based

13537
09:34:48,116 --> 09:34:53,480
on the W wol from alpha calculation you

13538
09:34:50,320 --> 09:34:57,596
can do right where you go exp uh like

13539
09:34:53,480 --> 09:35:00,320
exponentiate negative uh Negative X so

13540
09:34:57,596 --> 09:35:03,800
um exponentiate negative 2.3 you get

13541
09:35:00,320 --> 09:35:06,080
about 0.1 for that um which is 10%

13542
09:35:03,800 --> 09:35:08,320
accuracy and then you do the derivative

13543
09:35:06,080 --> 09:35:09,800
of the loss which is just going to

13544
09:35:08,320 --> 09:35:12,360
essentially element wise subtract the

13545
09:35:09,800 --> 09:35:13,916
softmax probabilities by the true one

13546
09:35:12,360 --> 09:35:16,560
hot probabilities and I'll explain those

13547
09:35:13,916 --> 09:35:17,916
as well what one hot means and then we

13548
09:35:16,560 --> 09:35:19,320
just back propagate right so this is

13549
09:35:17,916 --> 09:35:22,116
kind of the forward pass here where we

13550
09:35:19,320 --> 09:35:24,960
go um matl one

13551
09:35:22,116 --> 09:35:28,240
activation m 2 calculate the loss and

13552
09:35:24,960 --> 09:35:30,116
then we backwards this way um so we do

13553
09:35:28,240 --> 09:35:32,040
derivative derivative of the loss and

13554
09:35:30,116 --> 09:35:35,596
then we do our chain roll back

13555
09:35:32,040 --> 09:35:37,480
propagation so we calculate dw2 and

13556
09:35:35,596 --> 09:35:39,960
that's just going to be uh keep in mind

13557
09:35:37,480 --> 09:35:42,360
the gradient of the weight is supposed

13558
09:35:39,960 --> 09:35:46,596
to match the shape of the weight itself

13559
09:35:42,360 --> 09:35:49,960
so notice how W2 here was 26 256 by 10

13560
09:35:46,596 --> 09:35:52,160
here we do 256x B matal that with B by

13561
09:35:49,960 --> 09:35:56,200
10 and so we end up with the Dimensions

13562
09:35:52,160 --> 09:35:58,916
256x 10 and that matches up um then we

13563
09:35:56,200 --> 09:36:01,040
do dx2 which is needed for you know

13564
09:35:58,916 --> 09:36:04,276
continually back prop back propagating

13565
09:36:01,040 --> 09:36:07,080
through the Rue out layer um and so here

13566
09:36:04,276 --> 09:36:08,916
it's literally just taking the dx2 and

13567
09:36:07,080 --> 09:36:10,756
then uh doing an element wise

13568
09:36:08,916 --> 09:36:13,400
multiplication with the with the

13569
09:36:10,756 --> 09:36:16,320
derivative of the r function of X so

13570
09:36:13,400 --> 09:36:17,756
whatever the whatever the um the input

13571
09:36:16,320 --> 09:36:19,756
was into

13572
09:36:17,756 --> 09:36:22,360
that and then we end up with the same

13573
09:36:19,756 --> 09:36:25,160
shape that we got here we simply plug

13574
09:36:22,360 --> 09:36:26,436
this we plug this Rue out into both of

13575
09:36:25,160 --> 09:36:28,320
these we don't actually need to do it

13576
09:36:26,436 --> 09:36:29,880
for dx1 but it's just kind of here just

13577
09:36:28,320 --> 09:36:33,160
in case in case you have like a deeper

13578
09:36:29,880 --> 09:36:34,160
Network and you want to modify things um

13579
09:36:33,160 --> 09:36:38,640
but

13580
09:36:34,160 --> 09:36:42,160
dw1 it's going to be so x x1. t right so

13581
09:36:38,640 --> 09:36:43,840
we're transposing X1 X1 is over here so

13582
09:36:42,160 --> 09:36:47,480
just flipping those flipping those

13583
09:36:43,840 --> 09:36:52,080
Dimensions instead of being B by 784

13584
09:36:47,480 --> 09:36:54,276
it's now going to be um 784 by B so you

13585
09:36:52,080 --> 09:36:57,560
have each column is an image right

13586
09:36:54,276 --> 09:37:01,240
instead of each row being an image

13587
09:36:57,560 --> 09:37:03,276
um we we we map that with B by 256 which

13588
09:37:01,240 --> 09:37:07,240
was the Rue output so that's like the

13589
09:37:03,276 --> 09:37:11,360
the last output uh gradient um and then

13590
09:37:07,240 --> 09:37:15,000
we get our our our weight value right

13591
09:37:11,360 --> 09:37:18,320
so uh and then from here we pretty much

13592
09:37:15,000 --> 09:37:20,080
just uh modify the the weights value we

13593
09:37:18,320 --> 09:37:22,160
get the same shape there so that kind of

13594
09:37:20,080 --> 09:37:23,916
checks out

13595
09:37:22,160 --> 09:37:26,560
and then dx1 I mean we don't actually

13596
09:37:23,916 --> 09:37:30,276
need dx1 here but I include it uh just

13597
09:37:26,560 --> 09:37:34,960
in case um kind of the point there is uh

13598
09:37:30,276 --> 09:37:36,756
you know dx1 relies on uh on

13599
09:37:34,960 --> 09:37:39,756
W1

13600
09:37:36,756 --> 09:37:41,400
so uh we we we don't actually need to

13601
09:37:39,756 --> 09:37:43,756
like update anything for this it's just

13602
09:37:41,400 --> 09:37:46,840
if there was like a a dw0 if there was

13603
09:37:43,756 --> 09:37:51,880
like a layer before that um cuz notice

13604
09:37:46,840 --> 09:37:53,880
how uh sorry notice how this Ru layer

13605
09:37:51,880 --> 09:37:57,880
relies on this right so if you had like

13606
09:37:53,880 --> 09:38:00,360
a like if we had a Ru um if we had a ra

13607
09:37:57,880 --> 09:38:02,200
layer like before this if we had a rail

13608
09:38:00,360 --> 09:38:05,116
layer somewhere in here then you would

13609
09:38:02,200 --> 09:38:08,720
actually need to like dx0 for example

13610
09:38:05,116 --> 09:38:11,000
and dw0 you would actually need to uh

13611
09:38:08,720 --> 09:38:12,640
use this but since we're not we can kind

13612
09:38:11,000 --> 09:38:14,000
of just understand that it's something

13613
09:38:12,640 --> 09:38:15,560
you would have if it was a deeper

13614
09:38:14,000 --> 09:38:17,520
network but you don't actually need it

13615
09:38:15,560 --> 09:38:19,200
to make the network improve in

13616
09:38:17,520 --> 09:38:21,240
performance right it's just like an

13617
09:38:19,200 --> 09:38:22,596
extra calculation that's redundant so

13618
09:38:21,240 --> 09:38:25,560
you you can just kind of exclude it

13619
09:38:22,596 --> 09:38:26,960
unless you want to modify it um but I'm

13620
09:38:25,560 --> 09:38:28,880
not going to ramble on about that for

13621
09:38:26,960 --> 09:38:31,916
too long let's actually get to what the

13622
09:38:28,880 --> 09:38:35,800
heck these neurons are doing all

13623
09:38:31,916 --> 09:38:38,200
right so I got this little like editor

13624
09:38:35,800 --> 09:38:41,320
open here it's like my first time using

13625
09:38:38,200 --> 09:38:44,040
it but essentially what's what's H how a

13626
09:38:41,320 --> 09:38:46,720
neuron works is we're going to just like

13627
09:38:44,040 --> 09:38:48,240
zoom in how a neuron works is we're

13628
09:38:46,720 --> 09:38:49,520
going to take a bunch of X values all

13629
09:38:48,240 --> 09:38:53,320
right so

13630
09:38:49,520 --> 09:38:53,320
X1 X2

13631
09:38:54,200 --> 09:38:59,720
X3 and uh these are going to go see

13632
09:38:58,480 --> 09:39:02,960
these are going

13633
09:38:59,720 --> 09:39:06,000
forward uh into a neuron

13634
09:39:02,960 --> 09:39:09,276
right what's going to happen here is

13635
09:39:06,000 --> 09:39:12,160
these are going to go um maybe I should

13636
09:39:09,276 --> 09:39:12,160
draw these a little

13637
09:39:15,000 --> 09:39:18,480
better so this is our neuron here we're

13638
09:39:17,360 --> 09:39:20,560
just going to put a plus there and

13639
09:39:18,480 --> 09:39:23,560
you'll understand why in a second this

13640
09:39:20,560 --> 09:39:26,080
neuron is g to have a bunch of different

13641
09:39:23,560 --> 09:39:28,240
weights okay this neuron is g to have

13642
09:39:26,080 --> 09:39:32,320
this neuron is gonna have one one weight

13643
09:39:28,240 --> 09:39:34,240
for each of these X values so w

13644
09:39:32,320 --> 09:39:37,880
W1

13645
09:39:34,240 --> 09:39:39,480
W2 and W3 and all this is going to do is

13646
09:39:37,880 --> 09:39:41,916
just going to do product these so it's

13647
09:39:39,480 --> 09:39:43,720
going to go

13648
09:39:41,916 --> 09:39:46,276
W1

13649
09:39:43,720 --> 09:39:49,436
X1 plus

13650
09:39:46,276 --> 09:39:49,436
W2 uh

13651
09:39:49,480 --> 09:39:56,560
X2 plus W

13652
09:39:52,276 --> 09:40:00,160
3 X3 and that's going to give

13653
09:39:56,560 --> 09:40:04,680
us output right

13654
09:40:00,160 --> 09:40:06,436
so this this times this this times this

13655
09:40:04,680 --> 09:40:08,400
and this times this right that's a

13656
09:40:06,436 --> 09:40:11,756
single neuron and what we're going to

13657
09:40:08,400 --> 09:40:12,800
end up with is uh when we sum these

13658
09:40:11,756 --> 09:40:18,436
together we're going to end up with a

13659
09:40:12,800 --> 09:40:18,436
single value end up with a single value

13660
09:40:22,916 --> 09:40:25,916
one

13661
09:40:26,116 --> 09:40:32,360
Val one value

13662
09:40:28,756 --> 09:40:37,680
here now when we jump up to a bunch of

13663
09:40:32,360 --> 09:40:40,160
neurons so if we had say um like

13664
09:40:37,680 --> 09:40:42,320
X

13665
09:40:40,160 --> 09:40:48,400
X1

13666
09:40:42,320 --> 09:40:50,720
X2 and we have say 1 two three

13667
09:40:48,400 --> 09:40:52,160
neurons this one is going to have each

13668
09:40:50,720 --> 09:40:56,560
of these are going to have two neurons

13669
09:40:52,160 --> 09:40:58,960
right so this going to have um one

13670
09:40:56,560 --> 09:41:03,276
two one

13671
09:40:58,960 --> 09:41:05,116
2 and one two this looks very familiar

13672
09:41:03,276 --> 09:41:06,800
to something you've seen before which is

13673
09:41:05,116 --> 09:41:08,040
the MLP that I previously showed you

13674
09:41:06,800 --> 09:41:10,240
right there the images youve probably

13675
09:41:08,040 --> 09:41:11,916
seen like you know clickbait thumbnails

13676
09:41:10,240 --> 09:41:15,116
with these before and this is exactly

13677
09:41:11,916 --> 09:41:17,916
what it is right so you're going to have

13678
09:41:15,116 --> 09:41:20,240
uh two weights in here two weights in

13679
09:41:17,916 --> 09:41:22,720
here and two weights in

13680
09:41:20,240 --> 09:41:24,800
here and so it's going to do prodct with

13681
09:41:22,720 --> 09:41:28,480
each of these you're going to have G to

13682
09:41:24,800 --> 09:41:31,756
have this which is going to be

13683
09:41:28,480 --> 09:41:34,240
um you know like a

13684
09:41:31,756 --> 09:41:36,276
W1

13685
09:41:34,240 --> 09:41:38,116
X1 plus a

13686
09:41:36,276 --> 09:41:40,560
W2

13687
09:41:38,116 --> 09:41:42,800
X2 and then the same for these as well

13688
09:41:40,560 --> 09:41:44,520
right it's going to do all of these and

13689
09:41:42,800 --> 09:41:45,756
you're going to end up with three total

13690
09:41:44,520 --> 09:41:49,880
values so you're going to end up with

13691
09:41:45,756 --> 09:41:51,680
one two three values in the

13692
09:41:49,880 --> 09:41:53,596
output now

13693
09:41:51,680 --> 09:41:55,640
if we actually go to linear algebra and

13694
09:41:53,596 --> 09:41:56,880
try to understand this concept things

13695
09:41:55,640 --> 09:42:00,436
are going to actually make a lot more

13696
09:41:56,880 --> 09:42:04,276
sense okay so we go to linear algebra

13697
09:42:00,436 --> 09:42:06,640
say we have um something of size one

13698
09:42:04,276 --> 09:42:10,720
by uh

13699
09:42:06,640 --> 09:42:13,756
say 784 all

13700
09:42:10,720 --> 09:42:17,360
right and then we matal this with

13701
09:42:13,756 --> 09:42:17,360
something of size

13702
09:42:18,436 --> 09:42:24,756
784 by 256 okay ignore my handwriting

13703
09:42:21,880 --> 09:42:27,520
it's terrible um but this is this is

13704
09:42:24,756 --> 09:42:30,200
going to be each x value

13705
09:42:27,520 --> 09:42:31,880
right

13706
09:42:30,200 --> 09:42:36,320
X1 and

13707
09:42:31,880 --> 09:42:36,320
then X um

13708
09:42:39,276 --> 09:42:47,596
784 and we're going to have 256 neurons

13709
09:42:42,640 --> 09:42:52,916
okay so this is number one and this is

13710
09:42:47,596 --> 09:42:52,916
uh 256 okay

13711
09:42:53,160 --> 09:43:00,400
now all we're going to do is literally

13712
09:42:56,916 --> 09:43:03,436
take the at this is going to be like

13713
09:43:00,400 --> 09:43:05,800
essentially a it's going to have this is

13714
09:43:03,436 --> 09:43:07,400
the number of uh rows it has right this

13715
09:43:05,800 --> 09:43:08,520
is the this is the height of the Matrix

13716
09:43:07,400 --> 09:43:10,916
so it's going to essentially just look

13717
09:43:08,520 --> 09:43:12,000
like a vector it's going to be it's

13718
09:43:10,916 --> 09:43:12,880
going to be this and it's just going to

13719
09:43:12,000 --> 09:43:15,756
be a

13720
09:43:12,880 --> 09:43:17,436
line and this one is going to be quite

13721
09:43:15,756 --> 09:43:21,200
tall so it's actually going to look like

13722
09:43:17,436 --> 09:43:21,200
this it's going to be

13723
09:43:25,480 --> 09:43:33,916
784 by 256 like that now

13724
09:43:30,080 --> 09:43:33,916
this this column right

13725
09:43:34,400 --> 09:43:39,320
here is going to be a single neuron

13726
09:43:37,240 --> 09:43:42,116
right this is going to be

13727
09:43:39,320 --> 09:43:44,116
um say

13728
09:43:42,116 --> 09:43:46,000
one

13729
09:43:44,116 --> 09:43:48,720
neuron this is going to be a set of

13730
09:43:46,000 --> 09:43:50,560
Weights that's going to map it's each

13731
09:43:48,720 --> 09:43:53,436
each of those weight values is going to

13732
09:43:50,560 --> 09:43:58,080
multiply with a single x value right so

13733
09:43:53,436 --> 09:44:00,276
W1 is going to multiply with this one um

13734
09:43:58,080 --> 09:44:01,560
W2 is going to multiply with X2 down

13735
09:44:00,276 --> 09:44:04,680
here and then all the way down to

13736
09:44:01,560 --> 09:44:06,916
whatever n is or or sorry down to down

13737
09:44:04,680 --> 09:44:09,680
to the down to 784 which is the length

13738
09:44:06,916 --> 09:44:11,080
of it or or the the length of the column

13739
09:44:09,680 --> 09:44:13,200
I guess you the height of the column

13740
09:44:11,080 --> 09:44:16,596
whatever you want to interpret that as

13741
09:44:13,200 --> 09:44:19,520
um and this a is a single neuron right

13742
09:44:16,596 --> 09:44:21,320
as long as that's clear we're good now

13743
09:44:19,520 --> 09:44:22,756
we have a bunch of these neurons as a

13744
09:44:21,320 --> 09:44:25,800
matter of fact we have

13745
09:44:22,756 --> 09:44:28,800
256 different

13746
09:44:25,800 --> 09:44:28,800
neurons

13747
09:44:31,080 --> 09:44:34,960
right I don't know how to write with my

13748
09:44:33,040 --> 09:44:38,000
hand yet I'm still getting used to

13749
09:44:34,960 --> 09:44:41,960
it but we have 256 different neurons

13750
09:44:38,000 --> 09:44:43,360
right and they're each dot producting

13751
09:44:41,960 --> 09:44:47,200
with the input

13752
09:44:43,360 --> 09:44:49,756
itself and we end up with a final output

13753
09:44:47,200 --> 09:44:52,916
so notice these you know cancel out and

13754
09:44:49,756 --> 09:44:52,916
we end up with

13755
09:44:55,916 --> 09:45:04,000
1 by 256 so for one example we're going

13756
09:45:00,200 --> 09:45:06,480
to get 256 neuron outputs now if you

13757
09:45:04,000 --> 09:45:11,200
envision this as

13758
09:45:06,480 --> 09:45:13,840
say um batch by 2 784 so instead of one

13759
09:45:11,200 --> 09:45:16,276
image flattened it's a bunch of

13760
09:45:13,840 --> 09:45:17,276
flattened images right each flatten

13761
09:45:16,276 --> 09:45:20,520
image is a

13762
09:45:17,276 --> 09:45:24,000
row then essentially all you end up with

13763
09:45:20,520 --> 09:45:26,916
is this B just replaces the one so you

13764
09:45:24,000 --> 09:45:26,916
you you get rid of

13765
09:45:28,080 --> 09:45:34,400
that and you end up with this B here and

13766
09:45:31,116 --> 09:45:35,916
this is just uh a batch of neuron

13767
09:45:34,400 --> 09:45:38,800
outputs right so it's like think it's

13768
09:45:35,916 --> 09:45:40,960
like imagine thinking about this entire

13769
09:45:38,800 --> 09:45:42,560
this architecture here where you have

13770
09:45:40,960 --> 09:45:45,200
each x value plugging into like a

13771
09:45:42,560 --> 09:45:47,200
different neuron just imagine this but

13772
09:45:45,200 --> 09:45:48,880
you but you like layer them on top of

13773
09:45:47,200 --> 09:45:51,240
each other so you have like you have

13774
09:45:48,880 --> 09:45:53,276
like say you have this like as it's as

13775
09:45:51,240 --> 09:45:56,320
it's looking at you at the screen and

13776
09:45:53,276 --> 09:45:58,520
then you you plop it down like this then

13777
09:45:56,320 --> 09:46:00,116
you put another one on top batch element

13778
09:45:58,520 --> 09:46:02,200
one batch element two batch element

13779
09:46:00,116 --> 09:46:05,880
three batch element four and each of

13780
09:46:02,200 --> 09:46:07,916
these is going to give you something so

13781
09:46:05,880 --> 09:46:11,640
this all these outputs is going to give

13782
09:46:07,916 --> 09:46:12,770
you like say uh this is going to give

13783
09:46:11,640 --> 09:46:14,000
you

13784
09:46:12,770 --> 09:46:18,000
[Music]

13785
09:46:14,000 --> 09:46:19,756
um what's it called 256 different

13786
09:46:18,000 --> 09:46:22,880
outputs and you're going to have these

13787
09:46:19,756 --> 09:46:24,800
in a batch batch size right so there 256

13788
09:46:22,880 --> 09:46:28,000
outputs are all layered out here and

13789
09:46:24,800 --> 09:46:30,840
then each one stacked on top is just

13790
09:46:28,000 --> 09:46:33,520
another set of 256 right that's how I

13791
09:46:30,840 --> 09:46:35,320
like sort of like to to reason through

13792
09:46:33,520 --> 09:46:36,680
this in my head you might want to reason

13793
09:46:35,320 --> 09:46:38,720
through it differently but I find that

13794
09:46:36,680 --> 09:46:40,240
to be a pretty cool trick to

13795
09:46:38,720 --> 09:46:43,200
understanding how these are

13796
09:46:40,240 --> 09:46:46,436
working now there's another

13797
09:46:43,200 --> 09:46:49,400
term that I did sort of leave out and

13798
09:46:46,436 --> 09:46:52,680
this is the bias right so the

13799
09:46:49,400 --> 09:46:54,596
bias each one of these each little one

13800
09:46:52,680 --> 09:46:56,720
of these neurons is going to have its

13801
09:46:54,596 --> 09:47:00,160
own bias right

13802
09:46:56,720 --> 09:47:03,320
so you know

13803
09:47:00,160 --> 09:47:07,560
X X1 X

13804
09:47:03,320 --> 09:47:07,560
xn uh and now each of these little

13805
09:47:07,916 --> 09:47:12,320
neurons is going to have its own bias so

13806
09:47:10,560 --> 09:47:14,040
we're going to do you know a do product

13807
09:47:12,320 --> 09:47:20,560
operation as we normally would we're

13808
09:47:14,040 --> 09:47:23,240
going to do like X2 or X1 * um X1 * W1

13809
09:47:20,560 --> 09:47:24,400
Plus X2 * W2 all the all the way through

13810
09:47:23,240 --> 09:47:26,560
all the different weights inside of a

13811
09:47:24,400 --> 09:47:29,360
neuron and then once we get that we're

13812
09:47:26,560 --> 09:47:30,640
going to add a bias to that and the bias

13813
09:47:29,360 --> 09:47:32,200
is just going to be a single number

13814
09:47:30,640 --> 09:47:34,640
because of course it's a scalar value

13815
09:47:32,200 --> 09:47:35,960
right so it's going to be essentially

13816
09:47:34,640 --> 09:47:40,000
it's going to look like

13817
09:47:35,960 --> 09:47:41,840
this W * X we're going to do we're going

13818
09:47:40,000 --> 09:47:44,880
to do all of these that we need to dot

13819
09:47:41,840 --> 09:47:48,240
product essentially a like a like a

13820
09:47:44,880 --> 09:47:49,596
vector a vector vector multiplication

13821
09:47:48,240 --> 09:47:52,240
and we're going to add a single bias

13822
09:47:49,596 --> 09:47:55,360
value to that right

13823
09:47:52,240 --> 09:47:57,000
so the vector um Vector X is going to

13824
09:47:55,360 --> 09:48:00,116
look like

13825
09:47:57,000 --> 09:48:02,040
this and the vector W is going to look

13826
09:48:00,116 --> 09:48:05,720
like this it's going to be a

13827
09:48:02,040 --> 09:48:07,916
column so we take this column and we we

13828
09:48:05,720 --> 09:48:09,360
do a essentially a matrix multiplication

13829
09:48:07,916 --> 09:48:11,240
but they're vectors so it's like

13830
09:48:09,360 --> 09:48:14,520
literally what it looks like

13831
09:48:11,240 --> 09:48:17,320
is and you just take this element and

13832
09:48:14,520 --> 09:48:19,000
you you multiply with this one right so

13833
09:48:17,320 --> 09:48:21,640
you kind of you kind of like put like a

13834
09:48:19,000 --> 09:48:23,160
stick through it you go

13835
09:48:21,640 --> 09:48:25,596
and it's like cut up into a bunch of

13836
09:48:23,160 --> 09:48:28,916
little multiplications and you sum them

13837
09:48:25,596 --> 09:48:31,276
together and you do your

13838
09:48:28,916 --> 09:48:33,116
bias and we can back pop get through

13839
09:48:31,276 --> 09:48:34,960
that as well so that that hopefully that

13840
09:48:33,116 --> 09:48:37,436
just clears up some of the intuition as

13841
09:48:34,960 --> 09:48:40,200
to how we're actually structuring um

13842
09:48:37,436 --> 09:48:43,200
this part here so that rule that I just

13843
09:48:40,200 --> 09:48:45,640
showed you also applies for the X2 and

13844
09:48:43,200 --> 09:48:47,400
W2 right the point is we're just trying

13845
09:48:45,640 --> 09:48:48,960
to understand what the whole design of

13846
09:48:47,400 --> 09:48:51,040
the neuron is and how we can abstract

13847
09:48:48,960 --> 09:48:53,240
that up to linear algebra and then write

13848
09:48:51,040 --> 09:48:55,800
really really fast C and C++ and Cuda

13849
09:48:53,240 --> 09:48:57,400
code to to make this like run fast

13850
09:48:55,800 --> 09:48:59,840
instead of just considering each little

13851
09:48:57,400 --> 09:49:01,680
neuron operation independently we can

13852
09:48:59,840 --> 09:49:04,400
build we can put a layer of abstraction

13853
09:49:01,680 --> 09:49:06,680
up and say this is we we've proven that

13854
09:49:04,400 --> 09:49:09,200
this is uh this is how this works in

13855
09:49:06,680 --> 09:49:10,916
linear linear algebra and now we can

13856
09:49:09,200 --> 09:49:12,596
step forward and try to really optimize

13857
09:49:10,916 --> 09:49:13,960
that based on our knowledge about matrix

13858
09:49:12,596 --> 09:49:16,520
multiplication right this is one of the

13859
09:49:13,960 --> 09:49:17,880
reasons why emphasize the Mel so much is

13860
09:49:16,520 --> 09:49:19,360
because it's an extremely important

13861
09:49:17,880 --> 09:49:21,240
algorithm in all this deep learning

13862
09:49:19,360 --> 09:49:24,400
stuff right um

13863
09:49:21,240 --> 09:49:26,880
but yeah now we can uh now let's look at

13864
09:49:24,400 --> 09:49:29,080
how we can like you know cross entropy

13865
09:49:26,880 --> 09:49:32,840
loss and back propagating through all of

13866
09:49:29,080 --> 09:49:34,756
this all right awesome so now we next

13867
09:49:32,840 --> 09:49:36,916
have to do the loss function and the

13868
09:49:34,756 --> 09:49:40,080
loss function isn't actually too bad so

13869
09:49:36,916 --> 09:49:43,040
if we pop over to here notice how we

13870
09:49:40,080 --> 09:49:44,680
have uh this cross entropy loss right

13871
09:49:43,040 --> 09:49:48,276
this is our this is our loss function

13872
09:49:44,680 --> 09:49:50,916
once we get the we do model. forbo from

13873
09:49:48,276 --> 09:49:53,040
our X batch and we get the Y predict did

13874
09:49:50,916 --> 09:49:54,880
and then a cache right uh don't worry

13875
09:49:53,040 --> 09:49:58,800
about the C just worry about these this

13876
09:49:54,880 --> 09:50:01,400
y PR and the the batch y right so we go

13877
09:49:58,800 --> 09:50:02,800
over to our cross entropy loss and

13878
09:50:01,400 --> 09:50:05,116
inside here we're going to dissect this

13879
09:50:02,800 --> 09:50:07,596
thing by the way don't worry too much

13880
09:50:05,116 --> 09:50:09,160
inside of this thing we have batch size

13881
09:50:07,596 --> 09:50:10,756
um so just we essentially just take an

13882
09:50:09,160 --> 09:50:12,320
element from the shape do our

13883
09:50:10,756 --> 09:50:15,000
probabilities from the softmax function

13884
09:50:12,320 --> 09:50:19,756
which we did go over before we have this

13885
09:50:15,000 --> 09:50:21,240
piece of work here uh and then uh and

13886
09:50:19,756 --> 09:50:24,800
then we just finish it up by calculating

13887
09:50:21,240 --> 09:50:25,916
the loss right so assuming that we

13888
09:50:24,800 --> 09:50:27,596
understand how our softmax function

13889
09:50:25,916 --> 09:50:29,320
works we can actually go ahead and dig

13890
09:50:27,596 --> 09:50:32,960
into this

13891
09:50:29,320 --> 09:50:32,960
so I'm going to open

13892
09:50:33,000 --> 09:50:40,560
up open up a Jupiter notebook here so

13893
09:50:38,116 --> 09:50:43,160
going to go ahead and import

13894
09:50:40,560 --> 09:50:47,916
numpy then we're going

13895
09:50:43,160 --> 09:50:50,040
to define the cross entropy loss uh

13896
09:50:47,916 --> 09:50:51,160
function itself and softmax so let's go

13897
09:50:50,040 --> 09:50:54,916
ahead and take

13898
09:50:51,160 --> 09:50:57,080
these and uh pop these into

13899
09:50:54,916 --> 09:51:00,680
here then we're going to say set our

13900
09:50:57,080 --> 09:51:03,680
batch size as um as two and then we're

13901
09:51:00,680 --> 09:51:08,276
going to set uh

13902
09:51:03,680 --> 09:51:08,276
num classes to five all

13903
09:51:08,880 --> 09:51:13,720
right then we're going to go and make a

13904
09:51:12,040 --> 09:51:16,560
shape we're going to go ahe and make our

13905
09:51:13,720 --> 09:51:19,480
both of our um inputs to cross inoc here

13906
09:51:16,560 --> 09:51:22,916
the Y PR and the Y true all right so

13907
09:51:19,480 --> 09:51:26,360
yred we're going to set this equal to uh

13908
09:51:22,916 --> 09:51:27,840
this nump uh Rand n batch sized by num

13909
09:51:26,360 --> 09:51:30,400
classes right so that's going to be our

13910
09:51:27,840 --> 09:51:32,080
predictions we have a bunch of uh we

13911
09:51:30,400 --> 09:51:35,240
have a bunch of batch elements each with

13912
09:51:32,080 --> 09:51:37,436
a probability distribution about which

13913
09:51:35,240 --> 09:51:40,116
which uh which output it think should be

13914
09:51:37,436 --> 09:51:43,160
those are the predictions right and then

13915
09:51:40,116 --> 09:51:47,080
we have a y true which is going to be

13916
09:51:43,160 --> 09:51:47,080
random integers

13917
09:51:48,880 --> 09:51:52,840
um and I'm going to print these out so

13918
09:51:50,960 --> 09:51:56,596
you can

13919
09:51:52,840 --> 09:52:00,116
see so y looks like this we have batch

13920
09:51:56,596 --> 09:52:01,480
size of two so 1 two and we have a bunch

13921
09:52:00,116 --> 09:52:03,520
of these elements right this is our

13922
09:52:01,480 --> 09:52:05,596
probability uh actually these are not

13923
09:52:03,520 --> 09:52:07,520
our our probab probability distribution

13924
09:52:05,596 --> 09:52:09,800
yet these are actually called lits so I

13925
09:52:07,520 --> 09:52:12,756
skipped a step there but lits are the

13926
09:52:09,800 --> 09:52:15,160
step before the softmax right softmax is

13927
09:52:12,756 --> 09:52:16,640
going to make sure that everything uh is

13928
09:52:15,160 --> 09:52:19,240
above zero right it's going to make sure

13929
09:52:16,640 --> 09:52:21,040
everything is like zero between zero and

13930
09:52:19,240 --> 09:52:22,756
one right that that's idea there and

13931
09:52:21,040 --> 09:52:24,436
it's going to express with confidence

13932
09:52:22,756 --> 09:52:25,756
which numbers should should be high

13933
09:52:24,436 --> 09:52:29,080
right because it's

13934
09:52:25,756 --> 09:52:32,800
exponentiating um these are lits because

13935
09:52:29,080 --> 09:52:35,116
it's like um like the lodge it I guess

13936
09:52:32,800 --> 09:52:37,080
you could say it's like the log like the

13937
09:52:35,116 --> 09:52:38,800
log probabilities so because you have to

13938
09:52:37,080 --> 09:52:40,276
exponentiate up to get to softmax it's

13939
09:52:38,800 --> 09:52:42,840
like log because you go

13940
09:52:40,276 --> 09:52:44,756
down it's going to do Ln not actually

13941
09:52:42,840 --> 09:52:47,756
log with base two it's or log base 10

13942
09:52:44,756 --> 09:52:48,756
it's going to be Ln so base two base e

13943
09:52:47,756 --> 09:52:51,040
or

13944
09:52:48,756 --> 09:52:55,640
2.71 and we can go ahead and print out

13945
09:52:51,040 --> 09:52:57,720
our y true so this is

13946
09:52:55,640 --> 09:53:02,000
um this is an

13947
09:52:57,720 --> 09:53:02,916
array it's very yeah you're you're

13948
09:53:02,000 --> 09:53:04,240
you're going to see this you're going to

13949
09:53:02,916 --> 09:53:07,520
see why this is important in a second

13950
09:53:04,240 --> 09:53:11,800
here um but if we actually pop to our

13951
09:53:07,520 --> 09:53:14,116
next step we can go probabilities equals

13952
09:53:11,800 --> 09:53:16,040
softmax of Y PR so we're going to see

13953
09:53:14,116 --> 09:53:19,200
these values get exponentiated we can

13954
09:53:16,040 --> 09:53:21,800
print out our probabilities so we can

13955
09:53:19,200 --> 09:53:23,800
see that as a part of this which values

13956
09:53:21,800 --> 09:53:25,880
were the biggest right so this one was

13957
09:53:23,800 --> 09:53:28,160
the biggest like you know closer to

13958
09:53:25,880 --> 09:53:30,040
positive Infinity um and the this is

13959
09:53:28,160 --> 09:53:31,360
like negative negative negative negative

13960
09:53:30,040 --> 09:53:33,200
right so this is like the biggest this

13961
09:53:31,360 --> 09:53:35,000
is the second biggest right so we can

13962
09:53:33,200 --> 09:53:36,276
see this is the biggest number this is

13963
09:53:35,000 --> 09:53:37,960
the second biggest and this is like

13964
09:53:36,276 --> 09:53:41,400
these are these numbers are like smaller

13965
09:53:37,960 --> 09:53:44,116
right um and same for this one we notice

13966
09:53:41,400 --> 09:53:46,320
how oh this 1.95 is really big and then

13967
09:53:44,116 --> 09:53:48,640
we have a 1.2 so it's like this is the

13968
09:53:46,320 --> 09:53:51,680
biggest second biggest right that's kind

13969
09:53:48,640 --> 09:53:54,800
of how that plays out

13970
09:53:51,680 --> 09:54:00,240
then we're going to go and do correct uh

13971
09:53:54,800 --> 09:54:01,800
correct log probs is going to be um or

13972
09:54:00,240 --> 09:54:05,116
correct correct

13973
09:54:01,800 --> 09:54:06,480
probs we're going to do um this part

13974
09:54:05,116 --> 09:54:07,756
we're going to do this part here so

13975
09:54:06,480 --> 09:54:10,240
probabilities and then we're going to

13976
09:54:07,756 --> 09:54:12,916
stick these two inside of it all right

13977
09:54:10,240 --> 09:54:12,916
so when we do

13978
09:54:15,700 --> 09:54:22,116
[Music]

13979
09:54:17,640 --> 09:54:27,320
this we notice that what this does is it

13980
09:54:22,116 --> 09:54:30,040
gives us um it gives us the indices of

13981
09:54:27,320 --> 09:54:32,960
the correct probs inside of the

13982
09:54:30,040 --> 09:54:34,756
prediction in inside of the prediction

13983
09:54:32,960 --> 09:54:39,040
uh Matrix

13984
09:54:34,756 --> 09:54:44,520
right so inside of here if we actually

13985
09:54:39,040 --> 09:54:47,200
print out um if we print out you know

13986
09:54:44,520 --> 09:54:47,200
np.

13987
09:54:47,240 --> 09:54:51,360
arrange right and then we have our y

13988
09:54:49,596 --> 09:54:53,360
true

13989
09:54:51,360 --> 09:54:56,320
we can essentially think of this four

13990
09:54:53,360 --> 09:54:59,040
and one as our correct labels so in in

13991
09:54:56,320 --> 09:55:01,560
batch number in batch uh in in this

13992
09:54:59,040 --> 09:55:04,960
first batch element the correct index

13993
09:55:01,560 --> 09:55:08,560
the the the correct answer is uh is

13994
09:55:04,960 --> 09:55:10,080
number four right so it Go 0 1 2 3 4

13995
09:55:08,560 --> 09:55:11,720
this is supposed to be close to one if

13996
09:55:10,080 --> 09:55:13,000
this is close to one we're doing a good

13997
09:55:11,720 --> 09:55:14,400
job that means the loss is going to be

13998
09:55:13,000 --> 09:55:16,560
low right this is the this is the

13999
09:55:14,400 --> 09:55:18,560
correct answer so if everything is like

14000
09:55:16,560 --> 09:55:20,960
favoring this index here if this was the

14001
09:55:18,560 --> 09:55:22,640
correct say class to pick

14002
09:55:20,960 --> 09:55:25,640
um that that's what we want to optimize

14003
09:55:22,640 --> 09:55:27,000
for then this one is picking out the

14004
09:55:25,640 --> 09:55:28,680
same thing but for the second element

14005
09:55:27,000 --> 09:55:32,116
right so that's why we do when we go up

14006
09:55:28,680 --> 09:55:34,560
to Y true here we're doing um that's why

14007
09:55:32,116 --> 09:55:36,960
we do this batch size here so it's going

14008
09:55:34,560 --> 09:55:39,680
to span the elements the total number of

14009
09:55:36,960 --> 09:55:42,040
elements in batch size um and it's going

14010
09:55:39,680 --> 09:55:44,000
to just select the the indices right so

14011
09:55:42,040 --> 09:55:46,000
we kind of just do a random number here

14012
09:55:44,000 --> 09:55:48,916
because it's between zero and numb

14013
09:55:46,000 --> 09:55:52,720
classes so you know it goes from zero to

14014
09:55:48,916 --> 09:55:55,276
essentially four so one 2 3 4 5 or 0 1 2

14015
09:55:52,720 --> 09:55:56,520
3 4 um and so that that's what's

14016
09:55:55,276 --> 09:56:00,756
happening there we just initialize

14017
09:55:56,520 --> 09:56:02,680
things randomly um but we see that we

14018
09:56:00,756 --> 09:56:06,160
did number four so it's going to pluck

14019
09:56:02,680 --> 09:56:08,160
out number four of the first one so 1

14020
09:56:06,160 --> 09:56:09,756
0.165 then same thing for the second

14021
09:56:08,160 --> 09:56:12,160
right we have we have index one so it's

14022
09:56:09,756 --> 09:56:15,160
going to go to

14023
09:56:12,160 --> 09:56:15,160
0.227

14024
09:56:16,840 --> 09:56:24,080
now we have this we we have this correct

14025
09:56:19,720 --> 09:56:27,276
probs which I just explained here

14026
09:56:24,080 --> 09:56:29,680
and if we go down or sorry if we if we

14027
09:56:27,276 --> 09:56:31,680
pop up a little bit further we see that

14028
09:56:29,680 --> 09:56:34,520
we just did this part right so now if I

14029
09:56:31,680 --> 09:56:40,520
wrap np. log around that

14030
09:56:34,520 --> 09:56:44,436
term np. log we'll just say

14031
09:56:40,520 --> 09:56:47,640
um we'll just say correct uh log probs

14032
09:56:44,436 --> 09:56:51,200
we'll just do np. log of that

14033
09:56:47,640 --> 09:56:54,116
um and then if we do correct log

14034
09:56:51,200 --> 09:56:56,276
probs uh we notice that we're just doing

14035
09:56:54,116 --> 09:56:59,560
the log of each of these of each of

14036
09:56:56,276 --> 09:57:01,800
these number right so so if we go import

14037
09:56:59,560 --> 09:57:07,720
math and then we go

14038
09:57:01,800 --> 09:57:07,720
um math. log of

14039
09:57:08,080 --> 09:57:14,596
0.165 we get this number right so uh -

14040
09:57:12,640 --> 09:57:16,960
1.8 we're just doing this for each of

14041
09:57:14,596 --> 09:57:20,720
these elements right and then we

14042
09:57:16,960 --> 09:57:24,000
continue further and we go

14043
09:57:20,720 --> 09:57:27,756
loss is the sum

14044
09:57:24,000 --> 09:57:29,320
of loss is the uh sum of all of those so

14045
09:57:27,756 --> 09:57:30,800
the correct log problem is going to sum

14046
09:57:29,320 --> 09:57:33,160
these all up together so it's going to

14047
09:57:30,800 --> 09:57:35,880
be like uh negative 1.8 and then

14048
09:57:33,160 --> 09:57:38,400
negative we're on 1.5 so they're going

14049
09:57:35,880 --> 09:57:43,116
to add and they're going to get about -

14050
09:57:38,400 --> 09:57:44,880
3.3 umga 3.47 or

14051
09:57:43,116 --> 09:57:47,160
3.27

14052
09:57:44,880 --> 09:57:48,840
um and then we divide all of this by

14053
09:57:47,160 --> 09:57:50,320
batch size to kind of you know normalize

14054
09:57:48,840 --> 09:57:51,640
AC cross batch we don't want it to to

14055
09:57:50,320 --> 09:57:53,480
get too massive like if you increase

14056
09:57:51,640 --> 09:57:54,880
your batch size to like a thousand then

14057
09:57:53,480 --> 09:57:55,840
your loss is going to be insanely high

14058
09:57:54,880 --> 09:57:57,116
and you're going to get numerical

14059
09:57:55,840 --> 09:57:59,276
instability from that you just don't

14060
09:57:57,116 --> 09:58:01,360
want it so you want to normalize over

14061
09:57:59,276 --> 09:58:02,436
the over the amount of samples that you

14062
09:58:01,360 --> 09:58:04,276
actually have in the batch and that's

14063
09:58:02,436 --> 09:58:06,400
going to help stabilize things for us

14064
09:58:04,276 --> 09:58:08,200
later during the training process we

14065
09:58:06,400 --> 09:58:10,640
don't want things to like step up and

14066
09:58:08,200 --> 09:58:12,800
get worse every single training step um

14067
09:58:10,640 --> 09:58:15,880
so if we go ahead and print out the loss

14068
09:58:12,800 --> 09:58:21,520
we notice um we notice how we get this

14069
09:58:15,880 --> 09:58:26,680
1.64 right so go back going back up here

14070
09:58:21,520 --> 09:58:28,960
um this was you know this was quite off

14071
09:58:26,680 --> 09:58:30,560
this was a little bit less off but it's

14072
09:58:28,960 --> 09:58:33,520
still fairly high like these are still

14073
09:58:30,560 --> 09:58:37,560
like this is like 16% 177% chance and

14074
09:58:33,520 --> 09:58:39,400
this is 23% chance and so that's not

14075
09:58:37,560 --> 09:58:43,436
very good so our loss is going to be

14076
09:58:39,400 --> 09:58:45,800
higher but if our um if our correct

14077
09:58:43,436 --> 09:58:45,800
probs

14078
09:58:45,960 --> 09:58:54,320
array sorry our correct probs array was

14079
09:58:50,840 --> 09:58:59,800
um see numpy do

14080
09:58:54,320 --> 09:58:59,800
array and we go say

14081
09:58:59,840 --> 09:59:02,840
um

14082
09:59:02,960 --> 09:59:06,116
0.9 and

14083
09:59:06,480 --> 09:59:12,400
0.8 and then we uh you know continue to

14084
09:59:09,200 --> 09:59:16,840
go through this so correct uh correct

14085
09:59:12,400 --> 09:59:20,160
log probs uh equals and then and then we

14086
09:59:16,840 --> 09:59:22,000
do our loss right and we print out out

14087
09:59:20,160 --> 09:59:24,800
our loss again loss is going to be

14088
09:59:22,000 --> 09:59:28,916
significantly lower because these values

14089
09:59:24,800 --> 09:59:31,840
the the the difference between uh the

14090
09:59:28,916 --> 09:59:34,560
difference between our prediction

14091
09:59:31,840 --> 09:59:36,400
accuracy and the actual label was very

14092
09:59:34,560 --> 09:59:38,360
close right so we we thought there was a

14093
09:59:36,400 --> 09:59:39,916
90% chance that this index was going to

14094
09:59:38,360 --> 09:59:41,596
be it right and we were we we were we

14095
09:59:39,916 --> 09:59:42,800
were correct right so that's a very high

14096
09:59:41,596 --> 09:59:44,800
confidence that we were correct and

14097
09:59:42,800 --> 09:59:46,160
that's a good thing this is also quite

14098
09:59:44,800 --> 09:59:48,040
high confidence that we were correct so

14099
09:59:46,160 --> 09:59:50,480
we want to reward ourselves for that or

14100
09:59:48,040 --> 09:59:52,640
minimize the loss right so if we have

14101
09:59:50,480 --> 09:59:55,160
values like 10% chance or 5% chance our

14102
09:59:52,640 --> 09:59:57,116
loss is going to be stupid high but uh

14103
09:59:55,160 --> 09:59:58,960
if we get closer and closer to what what

14104
09:59:57,116 --> 10:00:01,276
we sort of minimizing the difference

14105
09:59:58,960 --> 10:00:03,560
between what the model thinks it is and

14106
10:00:01,276 --> 10:00:04,640
what the actual thing is that is a good

14107
10:00:03,560 --> 10:00:06,160
thing that's that's what we're trying to

14108
10:00:04,640 --> 10:00:09,480
optimize for

14109
10:00:06,160 --> 10:00:12,276
here now there's another little step

14110
10:00:09,480 --> 10:00:15,240
that comes along with this

14111
10:00:12,276 --> 10:00:18,480
and that's the derivative of the loss

14112
10:00:15,240 --> 10:00:20,960
right so if you go over to Gro here and

14113
10:00:18,480 --> 10:00:23,080
say um

14114
10:00:20,960 --> 10:00:27,080
what what

14115
10:00:23,080 --> 10:00:29,680
is what is the derivative of cross

14116
10:00:27,080 --> 10:00:29,680
entropy

14117
10:00:32,360 --> 10:00:38,480
loss so it's defined as this which we

14118
10:00:35,040 --> 10:00:41,276
just went over so Yi is the true label

14119
10:00:38,480 --> 10:00:41,276
either zero or

14120
10:00:43,560 --> 10:00:49,240
one and this is the predict so so Yi

14121
10:00:47,080 --> 10:00:51,400
without a hat is the true label and Y

14122
10:00:49,240 --> 10:00:53,400
hat I is the protective probability and

14123
10:00:51,400 --> 10:00:54,800
we run through this we do some we do

14124
10:00:53,400 --> 10:00:57,960
some

14125
10:00:54,800 --> 10:00:59,360
differentiation um we go a few steps

14126
10:00:57,960 --> 10:01:01,000
later I'm not going to really cover the

14127
10:00:59,360 --> 10:01:02,960
math behind this differentiation part

14128
10:01:01,000 --> 10:01:04,560
this part is you know you can you can do

14129
10:01:02,960 --> 10:01:06,480
that on your own if you really feel it's

14130
10:01:04,560 --> 10:01:10,480
necessary um but really what we care

14131
10:01:06,480 --> 10:01:12,320
about is just the answer so the actual

14132
10:01:10,480 --> 10:01:15,436
derivative so putting it all together

14133
10:01:12,320 --> 10:01:18,436
look at the gradient um what we what we

14134
10:01:15,436 --> 10:01:24,960
actually care about is

14135
10:01:18,436 --> 10:01:27,320
um why hat minus y and notice y hat was

14136
10:01:24,960 --> 10:01:30,916
uh the predicted probability so this so

14137
10:01:27,320 --> 10:01:33,360
that soft maxed uh the soft maxed logits

14138
10:01:30,916 --> 10:01:35,240
right uh the probability distribution

14139
10:01:33,360 --> 10:01:39,720
and then just normal Y is the true label

14140
10:01:35,240 --> 10:01:41,800
so if we go back to our code here um and

14141
10:01:39,720 --> 10:01:45,116
we look at how that's

14142
10:01:41,800 --> 10:01:47,200
calculated we pop down and we see it is

14143
10:01:45,116 --> 10:01:49,520
the softmax probability distribution

14144
10:01:47,200 --> 10:01:52,480
minus the true labels right so that's

14145
10:01:49,520 --> 10:01:54,480
exact what we want it to be um and a lot

14146
10:01:52,480 --> 10:01:56,080
of this what we're doing here this is

14147
10:01:54,480 --> 10:02:01,200
mainly just converting things to the

14148
10:01:56,080 --> 10:02:03,720
actual one hot Vector um so uh yeah

14149
10:02:01,200 --> 10:02:05,160
that's that that's pretty much that's

14150
10:02:03,720 --> 10:02:07,680
pretty much how you do soft Max and then

14151
10:02:05,160 --> 10:02:10,756
derivative or sorry cross entropy loss

14152
10:02:07,680 --> 10:02:14,960
and derivative of it right now going

14153
10:02:10,756 --> 10:02:16,880
back um what's next so now we have now

14154
10:02:14,960 --> 10:02:22,320
we actually have to go through and

14155
10:02:16,880 --> 10:02:26,596
calculate the the gradient of of our W2

14156
10:02:22,320 --> 10:02:29,400
our X2 even maybe possibly Ford ones and

14157
10:02:26,596 --> 10:02:32,916
the bias right so let's go and look at

14158
10:02:29,400 --> 10:02:35,880
that I'm going to do an example where we

14159
10:02:32,916 --> 10:02:37,916
pretty much do uh we calculate dw1 and

14160
10:02:35,880 --> 10:02:39,596
then we kind of just bring uh the

14161
10:02:37,916 --> 10:02:41,960
intuition from that into everything else

14162
10:02:39,596 --> 10:02:44,720
and then go ahead and apply it right so

14163
10:02:41,960 --> 10:02:47,520
in in dw1 I'm going to I'm actually

14164
10:02:44,720 --> 10:02:51,200
going to go to this in a second here

14165
10:02:47,520 --> 10:02:55,040
um but I'll bring my

14166
10:02:51,200 --> 10:02:58,400
whiteb a little closer so I don't know

14167
10:02:55,040 --> 10:03:00,880
if you can see this but I pretty much

14168
10:02:58,400 --> 10:03:04,116
did um based on you know the micr

14169
10:03:00,880 --> 10:03:08,116
tutorial I did uh like an output this is

14170
10:03:04,116 --> 10:03:10,596
a neuron output right so X1 * W1 + X2 *

14171
10:03:08,116 --> 10:03:12,560
W2 where it's like each of the X values

14172
10:03:10,596 --> 10:03:18,040
go in and

14173
10:03:12,560 --> 10:03:19,840
then um the the neuron will a s like two

14174
10:03:18,040 --> 10:03:22,596
x values go in and there's a single

14175
10:03:19,840 --> 10:03:25,436
neuron that has uh two weights in it um

14176
10:03:22,596 --> 10:03:30,880
W1 and W2 and then it outputs those by

14177
10:03:25,436 --> 10:03:35,360
doing um by doing a doc product between

14178
10:03:30,880 --> 10:03:38,596
so it does X1 * W1 and then plus that to

14179
10:03:35,360 --> 10:03:40,916
X2 * W2 and I just wrote this in the

14180
10:03:38,596 --> 10:03:44,436
context of microG grad so there's a data

14181
10:03:40,916 --> 10:03:46,436
and a grad attribute for this there is a

14182
10:03:44,436 --> 10:03:48,800
a data and a grad attribute for this so

14183
10:03:46,436 --> 10:03:51,276
notice how I did the the this is a plus

14184
10:03:48,800 --> 10:03:54,116
sign I know you can't see it because

14185
10:03:51,276 --> 10:03:56,480
it's yeah it's very small but ient did a

14186
10:03:54,116 --> 10:03:58,436
plus sign so those two added together

14187
10:03:56,480 --> 10:04:00,560
and the gradients are going to the this

14188
10:03:58,436 --> 10:04:03,840
grad right here of two that's going to

14189
10:04:00,560 --> 10:04:07,040
flow to both of these nodes this X1 * W1

14190
10:04:03,840 --> 10:04:09,960
and X2 * W2 that's going to flow to

14191
10:04:07,040 --> 10:04:12,640
these um the same so when you when you

14192
10:04:09,960 --> 10:04:15,880
like uh for example when you're try to

14193
10:04:12,640 --> 10:04:17,756
differentiate a constant um it it ends

14194
10:04:15,880 --> 10:04:19,116
up just it ends up just becoming one

14195
10:04:17,756 --> 10:04:22,560
right so there's actually no change when

14196
10:04:19,116 --> 10:04:24,720
you have um like a graph which is which

14197
10:04:22,560 --> 10:04:27,160
you add like a bias to it and just shift

14198
10:04:24,720 --> 10:04:29,436
that curve upwards it still has the same

14199
10:04:27,160 --> 10:04:31,320
slope right so that's essentially what's

14200
10:04:29,436 --> 10:04:33,560
happening here and then we continue

14201
10:04:31,320 --> 10:04:37,800
forward and this now becomes just a

14202
10:04:33,560 --> 10:04:39,596
multiplication so W2 uh or or sorry X1 *

14203
10:04:37,800 --> 10:04:41,520
W1 and then we have a data and a grad

14204
10:04:39,596 --> 10:04:43,240
attribute for those so that's just like

14205
10:04:41,520 --> 10:04:45,520
kind of how a neuron is going to be

14206
10:04:43,240 --> 10:04:50,520
structured in the context of

14207
10:04:45,520 --> 10:04:53,596
micrograph now uh if we pop over to

14208
10:04:50,520 --> 10:04:55,800
um if we pop over to this

14209
10:04:53,596 --> 10:05:00,800
example which

14210
10:04:55,800 --> 10:05:04,116
is over here so this dw1

14211
10:05:00,800 --> 10:05:08,116
output what I pretty much did is we go

14212
10:05:04,116 --> 10:05:11,240
uh 784 Time by B so let's just draw that

14213
10:05:08,116 --> 10:05:14,240
out really quick

14214
10:05:11,240 --> 10:05:14,240
um

14215
10:05:17,360 --> 10:05:23,116
so 784 by

14216
10:05:20,436 --> 10:05:29,116
by

14217
10:05:23,116 --> 10:05:29,116
B and then we have um B by 256

14218
10:05:29,560 --> 10:05:33,756
right this is an at symbol it's really

14219
10:05:31,960 --> 10:05:36,160
bad

14220
10:05:33,756 --> 10:05:38,400
drawing

14221
10:05:36,160 --> 10:05:41,480
B by

14222
10:05:38,400 --> 10:05:43,756
256 all right so how do we actually

14223
10:05:41,480 --> 10:05:45,360
Matrix multiply here well we take a

14224
10:05:43,756 --> 10:05:47,720
column of

14225
10:05:45,360 --> 10:05:50,520
this and we draw product with a row of

14226
10:05:47,720 --> 10:05:51,960
this right and notice how this is

14227
10:05:50,520 --> 10:05:53,400
batches right so this is going to be a

14228
10:05:51,960 --> 10:05:54,960
batch element this is a batch element

14229
10:05:53,400 --> 10:05:58,080
this is a batch element so it's going

14230
10:05:54,960 --> 10:06:01,800
this is we're essentially doing

14231
10:05:58,080 --> 10:06:03,436
um we're we're taking the first pixel

14232
10:06:01,800 --> 10:06:05,320
across the first batch element so if we

14233
10:06:03,436 --> 10:06:06,520
were to just go here that would be an

14234
10:06:05,320 --> 10:06:07,640
entire image right that would be the

14235
10:06:06,520 --> 10:06:09,800
first batch element it would be an

14236
10:06:07,640 --> 10:06:12,116
entire image but we're doing the first

14237
10:06:09,800 --> 10:06:13,880
pixel across the entire batch right so

14238
10:06:12,116 --> 10:06:15,436
across all the different batch elements

14239
10:06:13,880 --> 10:06:20,360
we're taking we're do producting the

14240
10:06:15,436 --> 10:06:23,640
pixel values um so like for example X

14241
10:06:20,360 --> 10:06:23,640
um x.

14242
10:06:24,276 --> 10:06:30,000
data at

14243
10:06:26,400 --> 10:06:32,960
index0 right and then in this context

14244
10:06:30,000 --> 10:06:35,160
this is B by 256 which is the same as

14245
10:06:32,960 --> 10:06:39,160
the uh output layer so if we did like

14246
10:06:35,160 --> 10:06:41,040
for example the the classical input of

14247
10:06:39,160 --> 10:06:44,520
um you go back to this it's like B by

14248
10:06:41,040 --> 10:06:47,800
784 * 784 by 256 you end up with B by

14249
10:06:44,520 --> 10:06:49,720
256 and that's our output gradient right

14250
10:06:47,800 --> 10:06:51,680
so we're we're essentially just just

14251
10:06:49,720 --> 10:06:53,960
taking this and we're shifting it around

14252
10:06:51,680 --> 10:06:56,000
so we're doing we're taking this B by

14253
10:06:53,960 --> 10:06:58,880
256 that stays the same but we're

14254
10:06:56,000 --> 10:07:02,320
transposing the input so it's like x.t

14255
10:06:58,880 --> 10:07:05,040
or sorry x. data

14256
10:07:02,320 --> 10:07:06,800
transpose um Matrix multiply that with

14257
10:07:05,040 --> 10:07:08,640
the output output gradient and we're

14258
10:07:06,800 --> 10:07:12,960
getting the gradient for the weights

14259
10:07:08,640 --> 10:07:14,720
right the dw1 value um so if we go back

14260
10:07:12,960 --> 10:07:19,040
to here

14261
10:07:14,720 --> 10:07:21,080
256 remember um 784 is how many weights

14262
10:07:19,040 --> 10:07:22,480
are in a single neuron but 256 is the

14263
10:07:21,080 --> 10:07:26,116
amount of neurons we actually have in

14264
10:07:22,480 --> 10:07:27,400
that hidden layer so 256 is going to be

14265
10:07:26,116 --> 10:07:29,200
like this is this these are all the

14266
10:07:27,400 --> 10:07:31,640
neurons laid out right but what we're

14267
10:07:29,200 --> 10:07:34,240
going to do is instead of taking like a

14268
10:07:31,640 --> 10:07:36,240
a single a single set of all the neurons

14269
10:07:34,240 --> 10:07:38,640
from a single batch element we're going

14270
10:07:36,240 --> 10:07:41,720
to take all of the all of the the index

14271
10:07:38,640 --> 10:07:45,000
zero gradients uh across

14272
10:07:41,720 --> 10:07:47,360
the uh across the

14273
10:07:45,000 --> 10:07:50,520
across

14274
10:07:47,360 --> 10:07:53,360
essentially uh batch element

14275
10:07:50,520 --> 10:07:55,756
no sorry not batch element zero across

14276
10:07:53,360 --> 10:07:57,560
we're going to take the first neurons

14277
10:07:55,756 --> 10:08:01,400
across the entire batch that's what

14278
10:07:57,560 --> 10:08:06,000
we're doing so this ends up being um

14279
10:08:01,400 --> 10:08:06,000
we'll just say y.

14280
10:08:06,916 --> 10:08:09,916
grad

14281
10:08:14,720 --> 10:08:20,276
um y. grad at index zero we're doing it

14282
10:08:18,596 --> 10:08:22,276
across the entire B batch so it's just a

14283
10:08:20,276 --> 10:08:24,000
single neuron right it's just a single

14284
10:08:22,276 --> 10:08:26,520
neuron but we're generalizing that

14285
10:08:24,000 --> 10:08:28,200
across the entire batch so we have this

14286
10:08:26,520 --> 10:08:29,560
x component generalizing across the

14287
10:08:28,200 --> 10:08:31,480
whole batch and this y component

14288
10:08:29,560 --> 10:08:33,800
generalizing across the whole batch we

14289
10:08:31,480 --> 10:08:37,000
use the same intuition that we got from

14290
10:08:33,800 --> 10:08:39,200
the microgr lecture and we just apply it

14291
10:08:37,000 --> 10:08:41,520
here except we get that generalization

14292
10:08:39,200 --> 10:08:42,756
ability from using batch processing

14293
10:08:41,520 --> 10:08:44,116
that's where the big thing comes in here

14294
10:08:42,756 --> 10:08:47,200
because we're doing we're essentially

14295
10:08:44,116 --> 10:08:48,756
doing uh columns here do product with

14296
10:08:47,200 --> 10:08:51,320
rows and notice the shapes are oriented

14297
10:08:48,756 --> 10:08:51,320
in a certain way

14298
10:08:51,680 --> 10:08:55,800
yeah

14299
10:08:52,640 --> 10:09:00,320
it's it it is a lot to take in but in

14300
10:08:55,800 --> 10:09:03,040
the end we end up with um so this is

14301
10:09:00,320 --> 10:09:04,640
this is like 784 by B and then B by 256

14302
10:09:03,040 --> 10:09:07,320
so the B's cancel out and we're left

14303
10:09:04,640 --> 10:09:07,320
with

14304
10:09:08,040 --> 10:09:12,160
um I know these shapes are out of

14305
10:09:10,040 --> 10:09:17,276
proportion

14306
10:09:12,160 --> 10:09:19,720
784 by 256 right so this 784 that's the

14307
10:09:17,276 --> 10:09:22,116
first pixel value so each of these or

14308
10:09:19,720 --> 10:09:23,520
sorry each each of these elements that's

14309
10:09:22,116 --> 10:09:25,436
a pixel

14310
10:09:23,520 --> 10:09:28,320
right that's a pixel in the whole

14311
10:09:25,436 --> 10:09:31,800
flatten image and these are all the

14312
10:09:28,320 --> 10:09:34,040
neurons so we end up calculating these

14313
10:09:31,800 --> 10:09:36,276
um in a way that we can that we can

14314
10:09:34,040 --> 10:09:38,240
actually update uh all of the weights

14315
10:09:36,276 --> 10:09:40,880
properly right so when all these are

14316
10:09:38,240 --> 10:09:43,720
laid out you have um the gradient for

14317
10:09:40,880 --> 10:09:47,040
each neuron across the first uh across

14318
10:09:43,720 --> 10:09:49,040
the first pixel value and that is the

14319
10:09:47,040 --> 10:09:51,276
that is the same way that our initial

14320
10:09:49,040 --> 10:09:53,560
weight Matrix is organized right so in

14321
10:09:51,276 --> 10:09:56,200
our weight matrix it's taking these

14322
10:09:53,560 --> 10:09:58,960
Columns of of like a single neuron of

14323
10:09:56,200 --> 10:10:02,360
weights and do producting that with with

14324
10:09:58,960 --> 10:10:04,560
a single um with a with a single image

14325
10:10:02,360 --> 10:10:06,680
right and it and it's doing that and

14326
10:10:04,560 --> 10:10:10,680
that's very intuitive and it makes sense

14327
10:10:06,680 --> 10:10:10,680
uh but in this example

14328
10:10:10,960 --> 10:10:15,596
um you know we we end up getting those

14329
10:10:14,160 --> 10:10:17,520
we end up getting the same ones that we

14330
10:10:15,596 --> 10:10:19,756
would we would want to update those with

14331
10:10:17,520 --> 10:10:21,596
so you can kind of Translate and see

14332
10:10:19,756 --> 10:10:23,160
like what is this column here what does

14333
10:10:21,596 --> 10:10:26,640
that consist of and what does this row

14334
10:10:23,160 --> 10:10:31,160
here consist of and we can plug that in

14335
10:10:26,640 --> 10:10:33,680
and we can see that our weight updates

14336
10:10:31,160 --> 10:10:35,880
are actually going to make sense here so

14337
10:10:33,680 --> 10:10:39,400
in this one we we essentially end up

14338
10:10:35,880 --> 10:10:43,360
with this thing of 256 values and that

14339
10:10:39,400 --> 10:10:46,400
is just going to be the essentially the

14340
10:10:43,360 --> 10:10:48,960
gradient of all of the neurons with

14341
10:10:46,400 --> 10:10:51,080
respect to a single Pixel right cuz this

14342
10:10:48,960 --> 10:10:52,436
is a single Pixel across an entire batch

14343
10:10:51,080 --> 10:10:55,160
we're again we're just using the batch

14344
10:10:52,436 --> 10:10:56,480
generalization idea here um we're just

14345
10:10:55,160 --> 10:10:58,276
like generalizing across whole batch

14346
10:10:56,480 --> 10:10:59,560
instead of using one specific sample

14347
10:10:58,276 --> 10:11:02,800
that way it's like better for

14348
10:10:59,560 --> 10:11:04,360
stabilizing training um and and we're

14349
10:11:02,800 --> 10:11:08,040
just we're taking that and we're just

14350
10:11:04,360 --> 10:11:12,116
laying everything out for

14351
10:11:08,040 --> 10:11:15,680
um for all of the neurons across a

14352
10:11:12,116 --> 10:11:17,040
single um across a single Pixel right

14353
10:11:15,680 --> 10:11:19,040
that's how we're calculating these so

14354
10:11:17,040 --> 10:11:21,756
this this uh it's going to be like this

14355
10:11:19,040 --> 10:11:23,960
row this is our pixel AC like all the

14356
10:11:21,756 --> 10:11:27,400
pixels for like for all all the first

14357
10:11:23,960 --> 10:11:30,520
pixel for all the batches and then we

14358
10:11:27,400 --> 10:11:33,040
have our our single

14359
10:11:30,520 --> 10:11:35,320
um we have our our first neuron and it's

14360
10:11:33,040 --> 10:11:36,880
going to go first neuron boom it's going

14361
10:11:35,320 --> 10:11:39,200
to put the first value there and then

14362
10:11:36,880 --> 10:11:41,916
second neuron boom all the way to 256

14363
10:11:39,200 --> 10:11:44,680
neurons and it's going to spit this out

14364
10:11:41,916 --> 10:11:48,116
in a column

14365
10:11:44,680 --> 10:11:50,400
right that's all of the neurons um for

14366
10:11:48,116 --> 10:11:52,840
the for the entire first

14367
10:11:50,400 --> 10:11:53,960
pixel and then if we look at how it goes

14368
10:11:52,840 --> 10:11:57,320
down

14369
10:11:53,960 --> 10:11:59,080
columnwise then we get um then we get

14370
10:11:57,320 --> 10:12:01,560
all the neurons for the second pixel and

14371
10:11:59,080 --> 10:12:03,000
the third pixel and the fourth one right

14372
10:12:01,560 --> 10:12:06,116
and so we can sort of

14373
10:12:03,000 --> 10:12:07,840
see that this ties back again into uh

14374
10:12:06,116 --> 10:12:10,916
the original forward pass example that

14375
10:12:07,840 --> 10:12:12,800
we were doing um if this if this doesn't

14376
10:12:10,916 --> 10:12:16,160
make sense I know my explanations might

14377
10:12:12,800 --> 10:12:18,360
be bad um you know you might want to go

14378
10:12:16,160 --> 10:12:20,840
back and and I mean you you could draw

14379
10:12:18,360 --> 10:12:22,720
this out your yourself um that does work

14380
10:12:20,840 --> 10:12:25,436
and just sort of visualizing it in your

14381
10:12:22,720 --> 10:12:27,000
head that's a good idea to do uh that is

14382
10:12:25,436 --> 10:12:29,680
a good idea to try to understand how

14383
10:12:27,000 --> 10:12:31,756
this is working um alternatively you can

14384
10:12:29,680 --> 10:12:33,640
you can watch my tensor uh my tensor

14385
10:12:31,756 --> 10:12:35,800
level back propop video I think I did a

14386
10:12:33,640 --> 10:12:38,560
good job on that um it's about 30

14387
10:12:35,800 --> 10:12:41,436
minutes long so uh

14388
10:12:38,560 --> 10:12:44,116
anyways well we if if this doesn't

14389
10:12:41,436 --> 10:12:49,400
entirely make sense we're going to use

14390
10:12:44,116 --> 10:12:49,400
this intuition of we have this 784

14391
10:12:49,840 --> 10:12:54,240
by

14392
10:12:50,960 --> 10:12:59,000
256 and this is the same as just our our

14393
10:12:54,240 --> 10:12:59,000
W um our w. dat

14394
10:13:00,116 --> 10:13:04,116
right so our w.g

14395
10:13:08,200 --> 10:13:14,436
grad and our

14396
10:13:11,040 --> 10:13:17,916
w. dat have the same

14397
10:13:14,436 --> 10:13:19,960
shape so what we can essentially do is

14398
10:13:17,916 --> 10:13:22,880
we can take

14399
10:13:19,960 --> 10:13:25,116
um say in in the grad in the gradient

14400
10:13:22,880 --> 10:13:27,880
for example we can take this value

14401
10:13:25,116 --> 10:13:30,240
multiply it by a learning rate of say LR

14402
10:13:27,880 --> 10:13:33,880
equal

14403
10:13:30,240 --> 10:13:36,520
0.1 and we can then subtract this from

14404
10:13:33,880 --> 10:13:38,480
the original one right so if the

14405
10:13:36,520 --> 10:13:40,200
gradient is really high that means

14406
10:13:38,480 --> 10:13:43,240
there's a lot of error and if the

14407
10:13:40,200 --> 10:13:46,800
gradient is really low um that means

14408
10:13:43,240 --> 10:13:48,000
that there's there's going to be um that

14409
10:13:46,800 --> 10:13:49,240
means there's going to be less error

14410
10:13:48,000 --> 10:13:51,200
right this is why it's called a

14411
10:13:49,240 --> 10:13:53,596
stochastic gradient descent cuz you're

14412
10:13:51,200 --> 10:14:01,360
descending the gradient um so it's going

14413
10:13:53,596 --> 10:14:05,880
to be essentially um w. data equals LR

14414
10:14:01,360 --> 10:14:08,756
times um just do X for times learning

14415
10:14:05,880 --> 10:14:11,840
ratees

14416
10:14:08,756 --> 10:14:16,080
times the grad element

14417
10:14:11,840 --> 10:14:19,560
right and then we do um minus

14418
10:14:16,080 --> 10:14:21,756
equals so that if the gradient values

14419
10:14:19,560 --> 10:14:23,116
really high and learning rate is 0.1

14420
10:14:21,756 --> 10:14:25,200
then it's going to be positive time

14421
10:14:23,116 --> 10:14:26,640
negative and then this is going to get

14422
10:14:25,200 --> 10:14:30,200
adjusted in the negative direction if

14423
10:14:26,640 --> 10:14:32,520
the gradient is really high um and if

14424
10:14:30,200 --> 10:14:34,720
the gradient is really if it's really

14425
10:14:32,520 --> 10:14:37,596
negative gradient is really

14426
10:14:34,720 --> 10:14:41,560
negative

14427
10:14:37,596 --> 10:14:43,756
um then this is going to this is going

14428
10:14:41,560 --> 10:14:45,276
to multiply with this it's going to give

14429
10:14:43,756 --> 10:14:47,116
a negative value and then since we're

14430
10:14:45,276 --> 10:14:49,560
subtracting a negative it's going to go

14431
10:14:47,116 --> 10:14:50,560
up and the way it's going to go up so

14432
10:14:49,560 --> 10:14:52,960
that's that's literally all we're doing

14433
10:14:50,560 --> 10:14:55,560
in gradient

14434
10:14:52,960 --> 10:14:57,960
descent and we're doing that for each

14435
10:14:55,560 --> 10:15:01,240
element now we can sort of take that

14436
10:14:57,960 --> 10:15:05,560
intuition and Branch it off to you know

14437
10:15:01,240 --> 10:15:08,520
dw2 right and we can apply that to

14438
10:15:05,560 --> 10:15:10,720
dx2 um you know this isn't really a

14439
10:15:08,520 --> 10:15:12,436
course on back propop so don't worry too

14440
10:15:10,720 --> 10:15:14,436
much if that doesn't completely make

14441
10:15:12,436 --> 10:15:16,116
sense it is important but if it doesn't

14442
10:15:14,436 --> 10:15:19,080
completely make sense you're still going

14443
10:15:16,116 --> 10:15:19,916
to be okay um cuz we're going to

14444
10:15:19,080 --> 10:15:21,240
implement this and you're going to

14445
10:15:19,916 --> 10:15:23,160
actually be able to see the network

14446
10:15:21,240 --> 10:15:24,880
learning and you can actually print out

14447
10:15:23,160 --> 10:15:26,800
things to understand what's happening

14448
10:15:24,880 --> 10:15:29,596
under the

14449
10:15:26,800 --> 10:15:32,436
hood and of course we do need these uh

14450
10:15:29,596 --> 10:15:34,480
these these X values for calculating the

14451
10:15:32,436 --> 10:15:37,560
intermediate these intermediate layers

14452
10:15:34,480 --> 10:15:39,960
right with activation functions um and

14453
10:15:37,560 --> 10:15:42,160
the activation functions like literally

14454
10:15:39,960 --> 10:15:45,400
if you want me to like draw out how that

14455
10:15:42,160 --> 10:15:45,400
would work

14456
10:15:47,400 --> 10:15:50,916
um Rue

14457
10:15:51,116 --> 10:15:55,680
it goes like this

14458
10:15:52,596 --> 10:15:59,720
right it's like it's like a line and

14459
10:15:55,680 --> 10:16:01,436
then it goes up just like that so what

14460
10:15:59,720 --> 10:16:03,916
you can do for this is you can say if my

14461
10:16:01,436 --> 10:16:05,360
value is zero or less I'm going to set

14462
10:16:03,916 --> 10:16:07,040
the gradient to zero because there's no

14463
10:16:05,360 --> 10:16:09,756
slope the slope is

14464
10:16:07,040 --> 10:16:12,756
zero and if it's if it's if it's greater

14465
10:16:09,756 --> 10:16:15,116
than that if it's greater than zero then

14466
10:16:12,756 --> 10:16:16,400
uh we're going to set that to one right

14467
10:16:15,116 --> 10:16:17,880
because if it's it's really is going to

14468
10:16:16,400 --> 10:16:20,320
make it remain the same if it's above

14469
10:16:17,880 --> 10:16:23,520
zero

14470
10:16:20,320 --> 10:16:23,520
um okay

14471
10:16:24,596 --> 10:16:29,680
awesome now let's pop into um sort of

14472
10:16:28,040 --> 10:16:32,596
this this python script that we have

14473
10:16:29,680 --> 10:16:34,116
running here and uh just dissect

14474
10:16:32,596 --> 10:16:37,116
everything that's happening so we can

14475
10:16:34,116 --> 10:16:39,116
just tie it back to uh this image that

14476
10:16:37,116 --> 10:16:40,560
we that that I wrote in excal draw just

14477
10:16:39,116 --> 10:16:42,720
so that everything kind of makes sense

14478
10:16:40,560 --> 10:16:43,560
on the code on on a conceptual and a

14479
10:16:42,720 --> 10:16:45,680
code

14480
10:16:43,560 --> 10:16:48,160
level all right so just kind of going

14481
10:16:45,680 --> 10:16:51,000
through uh this numpy script now uh we

14482
10:16:48,160 --> 10:16:52,040
import numpy normally um I mean I

14483
10:16:51,000 --> 10:16:54,000
actually already went through this part

14484
10:16:52,040 --> 10:16:55,800
I'm not going to review this again um

14485
10:16:54,000 --> 10:16:56,960
but if we go down you can see a bunch of

14486
10:16:55,800 --> 10:16:58,360
functions here right and these are all

14487
10:16:56,960 --> 10:17:00,720
these are all super useful functions

14488
10:16:58,360 --> 10:17:02,800
that we're going to use to essentially

14489
10:17:00,720 --> 10:17:04,400
uh train a single layer single hidden

14490
10:17:02,800 --> 10:17:06,560
layer multi-layer perceptron from

14491
10:17:04,400 --> 10:17:08,200
scratch um using the intuition we

14492
10:17:06,560 --> 10:17:12,116
previously built on that from you know

14493
10:17:08,200 --> 10:17:14,116
microgr Etc right so we we scroll down

14494
10:17:12,116 --> 10:17:16,276
and in this main function we declare

14495
10:17:14,116 --> 10:17:19,040
some some variables here so like hidden

14496
10:17:16,276 --> 10:17:21,840
size which I'll just set to like 256 in

14497
10:17:19,040 --> 10:17:23,916
this case um output size is 10 right so

14498
10:17:21,840 --> 10:17:26,360
10 different digits 0 through 9 and our

14499
10:17:23,916 --> 10:17:28,596
input size is 28x 28 pixels flattened

14500
10:17:26,360 --> 10:17:30,960
out um and then we're going to put our

14501
10:17:28,596 --> 10:17:34,080
bat size I'll just put eight here so we

14502
10:17:30,960 --> 10:17:40,040
can get speed um learning rate of

14503
10:17:34,080 --> 10:17:42,560
0.1 or or um alternatively 1 * 10 -3 and

14504
10:17:40,040 --> 10:17:44,040
then epox we're going to do five so epox

14505
10:17:42,560 --> 10:17:45,520
is how many times you go through the

14506
10:17:44,040 --> 10:17:47,240
entire training set right so you go

14507
10:17:45,520 --> 10:17:49,200
through it once that's like certain

14508
10:17:47,240 --> 10:17:50,680
number of iterations certain number of

14509
10:17:49,200 --> 10:17:52,240
like forward and backward passes it's

14510
10:17:50,680 --> 10:17:54,800
like one iteration and then you do

14511
10:17:52,240 --> 10:17:56,520
multiple epochs which is you know times

14512
10:17:54,800 --> 10:17:59,436
that you go over the train

14513
10:17:56,520 --> 10:18:01,596
data so inside of here we declare this

14514
10:17:59,436 --> 10:18:03,436
model neural network right input hidden

14515
10:18:01,596 --> 10:18:04,720
and output size and then we just have

14516
10:18:03,436 --> 10:18:06,756
this train function which is going to

14517
10:18:04,720 --> 10:18:09,436
actually do that for us so inside of

14518
10:18:06,756 --> 10:18:10,916
here we have number of epochs that we're

14519
10:18:09,436 --> 10:18:12,436
going to do and inside of each eoch

14520
10:18:10,916 --> 10:18:13,960
we're going to do we're going to train

14521
10:18:12,436 --> 10:18:15,480
in batches right so we have this batch

14522
10:18:13,960 --> 10:18:18,756
size a batch of images that are all

14523
10:18:15,480 --> 10:18:20,596
flattened so it's like B by 7 784 and

14524
10:18:18,756 --> 10:18:22,880
we're just going to step through um kind

14525
10:18:20,596 --> 10:18:24,960
of going through one by one here so in

14526
10:18:22,880 --> 10:18:27,000
the model. forward I mean we're just

14527
10:18:24,960 --> 10:18:30,040
we're in this this one we're just taking

14528
10:18:27,000 --> 10:18:31,800
um the X train is just input images and

14529
10:18:30,040 --> 10:18:34,840
then this is the output labels right

14530
10:18:31,800 --> 10:18:37,680
it's the it's the batch y so in the

14531
10:18:34,840 --> 10:18:39,840
model forward um we input a batch and we

14532
10:18:37,680 --> 10:18:41,800
get an output cache and we get a we get

14533
10:18:39,840 --> 10:18:44,436
y right the Y predictions the

14534
10:18:41,800 --> 10:18:46,720
probability distribution in a batch in

14535
10:18:44,436 --> 10:18:49,480
batches

14536
10:18:46,720 --> 10:18:51,520
right uh after we do model forward we're

14537
10:18:49,480 --> 10:18:54,960
going to do take the loss function of Y

14538
10:18:51,520 --> 10:18:58,276
PR and batch Y which we got from

14539
10:18:54,960 --> 10:19:01,160
here and then we're going so cross cross

14540
10:18:58,276 --> 10:19:04,000
entropy loss calculates the loss and

14541
10:19:01,160 --> 10:19:06,040
then separately we're going to take that

14542
10:19:04,000 --> 10:19:09,240
output again which keep in mind this is

14543
10:19:06,040 --> 10:19:11,800
logits so not actual um probability

14544
10:19:09,240 --> 10:19:14,680
distribution it's uh cross entropy loss

14545
10:19:11,800 --> 10:19:17,040
is going to softmax those

14546
10:19:14,680 --> 10:19:18,840
um and then it's going to return a loss

14547
10:19:17,040 --> 10:19:20,640
right so it does soft Max inside of here

14548
10:19:18,840 --> 10:19:22,436
so when we're actually getting the uh

14549
10:19:20,640 --> 10:19:23,756
derivative of the Cross entropy loss we

14550
10:19:22,436 --> 10:19:25,080
have to go and do that separately right

14551
10:19:23,756 --> 10:19:27,480
we have to get our probability

14552
10:19:25,080 --> 10:19:30,000
distribution and then we have to

14553
10:19:27,480 --> 10:19:32,680
essentially just um like I walked

14554
10:19:30,000 --> 10:19:33,960
through before how we actually do the

14555
10:19:32,680 --> 10:19:35,436
how we do the cross entropy loss

14556
10:19:33,960 --> 10:19:37,800
derivative I walked through that with

14557
10:19:35,436 --> 10:19:40,080
grock previously and uh this is

14558
10:19:37,800 --> 10:19:41,240
literally all we do so uh you know feel

14559
10:19:40,080 --> 10:19:42,560
free to like print this out but this

14560
10:19:41,240 --> 10:19:45,596
should be fairly intuitive if you work

14561
10:19:42,560 --> 10:19:47,480
with python and pytorch before um and

14562
10:19:45,596 --> 10:19:49,560
then we just you know do the do the

14563
10:19:47,480 --> 10:19:51,240
minus for our gr out output and then we

14564
10:19:49,560 --> 10:19:52,880
back propagate from there right so we

14565
10:19:51,240 --> 10:19:55,240
take our grad output and we use the

14566
10:19:52,880 --> 10:19:58,680
cache the

14567
10:19:55,240 --> 10:20:01,360
cache um so keep in mind when we do

14568
10:19:58,680 --> 10:20:03,720
model. forward we have ypr which is the

14569
10:20:01,360 --> 10:20:06,276
logits and then we have the cache which

14570
10:20:03,720 --> 10:20:07,880
is literally just um the inputs right so

14571
10:20:06,276 --> 10:20:09,320
all the different pieces of the layer

14572
10:20:07,880 --> 10:20:11,916
that we're going to need like for

14573
10:20:09,320 --> 10:20:16,000
example our dx2 the derivative of the

14574
10:20:11,916 --> 10:20:18,080
second um the second x value or um for

14575
10:20:16,000 --> 10:20:19,596
example The Rue output right so so just

14576
10:20:18,080 --> 10:20:21,840
stuff like this that we're going to need

14577
10:20:19,596 --> 10:20:23,560
to back propagate through all the layers

14578
10:20:21,840 --> 10:20:25,080
and not just like a single weight or

14579
10:20:23,560 --> 10:20:26,400
single weight here we're going to need

14580
10:20:25,080 --> 10:20:28,756
that whole cache of like through the

14581
10:20:26,400 --> 10:20:31,000
forward pass right um so that's that's

14582
10:20:28,756 --> 10:20:33,400
all that is it's just just a coule of

14583
10:20:31,000 --> 10:20:35,276
those and then of course that output the

14584
10:20:33,400 --> 10:20:36,640
loits right so just to clear up like

14585
10:20:35,276 --> 10:20:40,000
what the heck cache means there I know

14586
10:20:36,640 --> 10:20:42,680
that can be like sometimes misleading um

14587
10:20:40,000 --> 10:20:44,880
so we do model. backward and then we

14588
10:20:42,680 --> 10:20:48,640
just do model upd weights and we pass in

14589
10:20:44,880 --> 10:20:51,000
you know weights bias um weights and

14590
10:20:48,640 --> 10:20:53,240
bias again

14591
10:20:51,000 --> 10:20:56,200
so let's walk through what's happening

14592
10:20:53,240 --> 10:20:57,960
in forward this part I assume kind of

14593
10:20:56,200 --> 10:21:00,000
just makes sense we'll walk through

14594
10:20:57,960 --> 10:21:01,680
model. backward and then update weights

14595
10:21:00,000 --> 10:21:05,640
so in

14596
10:21:01,680 --> 10:21:08,840
forward uh model. forward over here so

14597
10:21:05,640 --> 10:21:11,160
inside of here we have the batch size as

14598
10:21:08,840 --> 10:21:12,560
x. shape at position zero so it's going

14599
10:21:11,160 --> 10:21:16,276
to it's going to list the shape it's

14600
10:21:12,560 --> 10:21:18,916
going to be B by 784 or sorry B by is

14601
10:21:16,276 --> 10:21:20,880
going to be um the batch that we get so

14602
10:21:18,916 --> 10:21:23,276
when we when we take this part we're

14603
10:21:20,880 --> 10:21:25,040
getting an actual batch so I and then to

14604
10:21:23,276 --> 10:21:28,080
I plus batch size so it gets a little

14605
10:21:25,040 --> 10:21:29,480
segment of like eight eight images um

14606
10:21:28,080 --> 10:21:32,080
and inside of here we're going to take

14607
10:21:29,480 --> 10:21:34,200
the first the the the leading dimension

14608
10:21:32,080 --> 10:21:36,916
of that which is batch size and we set

14609
10:21:34,200 --> 10:21:42,840
batch size here and then we do reshape

14610
10:21:36,916 --> 10:21:45,160
we go batch size by um and then the

14611
10:21:42,840 --> 10:21:47,960
essentially just the the last the last

14612
10:21:45,160 --> 10:21:49,240
one so it's going to be reshaped to

14613
10:21:47,960 --> 10:21:52,756
batch size

14614
10:21:49,240 --> 10:21:56,000
by um this is just a short way of doing

14615
10:21:52,756 --> 10:21:58,040
28 * 28 so 784 that's what this is going

14616
10:21:56,000 --> 10:22:00,480
to reshape to and then we go ahead and

14617
10:21:58,040 --> 10:22:02,560
do our our linear forwards and these are

14618
10:22:00,480 --> 10:22:06,000
these are should be very uh sensical

14619
10:22:02,560 --> 10:22:09,520
right so in our linear forwards

14620
10:22:06,000 --> 10:22:11,960
um we take in a weight sorry we take in

14621
10:22:09,520 --> 10:22:17,360
an x a weight and a bias right so we do

14622
10:22:11,960 --> 10:22:21,320
X at w + B right instead of w WX plus b

14623
10:22:17,360 --> 10:22:25,116
it's x w Plus plus b um so that's it's I

14624
10:22:21,320 --> 10:22:30,560
mean it's it's B by 784 we go back to

14625
10:22:25,116 --> 10:22:34,680
this so we go in here B by 784 * 784 by

14626
10:22:30,560 --> 10:22:36,436
256 and we end up with B by 256 right um

14627
10:22:34,680 --> 10:22:38,596
should be fairly

14628
10:22:36,436 --> 10:22:40,080
intuitive and then we add the we add the

14629
10:22:38,596 --> 10:22:41,596
bias as well which is another term I

14630
10:22:40,080 --> 10:22:43,320
actually did not include in this but you

14631
10:22:41,596 --> 10:22:45,400
can kind of just we can kind of just

14632
10:22:43,320 --> 10:22:50,116
think of the bias as like an extra extra

14633
10:22:45,400 --> 10:22:50,116
thing that it adds on um

14634
10:22:50,480 --> 10:22:56,840
now scroll down to relu right so relu I

14635
10:22:55,160 --> 10:22:59,080
mean this is self-explanatory it's just

14636
10:22:56,840 --> 10:23:01,116
going to do a point R it's going to be

14637
10:22:59,080 --> 10:23:03,880
num. maximum it's going to apply that to

14638
10:23:01,116 --> 10:23:07,116
every single value in there doing a you

14639
10:23:03,880 --> 10:23:08,756
know if it's uh if if the value is like

14640
10:23:07,116 --> 10:23:10,880
negative 1 then zero is going to be the

14641
10:23:08,756 --> 10:23:12,800
maximum it's like which one is higher Z

14642
10:23:10,880 --> 10:23:14,800
or negative 1 then it's going to pick

14643
10:23:12,800 --> 10:23:16,000
zero and if it's like one then it's

14644
10:23:14,800 --> 10:23:17,436
going to be like oh one is higher than

14645
10:23:16,000 --> 10:23:20,200
zero right so it's just kind of the

14646
10:23:17,436 --> 10:23:21,800
relue and then really derivative is you

14647
10:23:20,200 --> 10:23:24,436
know as we explained before how you have

14648
10:23:21,800 --> 10:23:26,800
like the chart and then goes like this D

14649
10:23:24,436 --> 10:23:28,880
gradient of derivative of zero then it's

14650
10:23:26,800 --> 10:23:31,240
going to go gradient one after after the

14651
10:23:28,880 --> 10:23:33,640
zero right um so that's just that's what

14652
10:23:31,240 --> 10:23:37,320
this is doing here um because we want to

14653
10:23:33,640 --> 10:23:40,240
ra you derivative right

14654
10:23:37,320 --> 10:23:44,720
um

14655
10:23:40,240 --> 10:23:47,040
now going back another linear forward we

14656
10:23:44,720 --> 10:23:48,520
take the Rue output so that's the new

14657
10:23:47,040 --> 10:23:50,880
input to the to the next line your

14658
10:23:48,520 --> 10:23:53,000
forward layer the the weights the

14659
10:23:50,880 --> 10:23:54,680
weights two and then the bias two right

14660
10:23:53,000 --> 10:23:56,276
so that that should also make sense and

14661
10:23:54,680 --> 10:23:58,160
then we just return that so forward pass

14662
10:23:56,276 --> 10:23:59,480
isn't actually too complicated we can

14663
10:23:58,160 --> 10:24:01,320
sort of just walk through and understand

14664
10:23:59,480 --> 10:24:02,840
how the shapes are changing this is more

14665
10:24:01,320 --> 10:24:05,320
a template example of how to understand

14666
10:24:02,840 --> 10:24:07,880
this from scratch Now we move down to

14667
10:24:05,320 --> 10:24:13,720
backward which is a little harder um go

14668
10:24:07,880 --> 10:24:15,916
to backward here we have the W1 B1 W2 B2

14669
10:24:13,720 --> 10:24:17,800
right so we put in the grad output so

14670
10:24:15,916 --> 10:24:19,800
the starting the wherever we start from

14671
10:24:17,800 --> 10:24:21,840
in back proper ation and go for and go

14672
10:24:19,800 --> 10:24:23,840
backward through the layers and then

14673
10:24:21,840 --> 10:24:25,756
cache as well which is the which is the

14674
10:24:23,840 --> 10:24:28,560
forward pass like intermediate cach

14675
10:24:25,756 --> 10:24:31,756
stored values right um so we go into

14676
10:24:28,560 --> 10:24:34,480
backward here and we see

14677
10:24:31,756 --> 10:24:37,276
um we get in this grad output and the

14678
10:24:34,480 --> 10:24:40,800
cache as we'd expect um and then we just

14679
10:24:37,276 --> 10:24:43,800
lay out that Tuple so fc1 input so we

14680
10:24:40,800 --> 10:24:43,800
just

14681
10:24:44,080 --> 10:24:52,916
um fc1 input fc1 output uh value output

14682
10:24:49,116 --> 10:24:56,320
right um so just kind of just unpacking

14683
10:24:52,916 --> 10:24:58,720
this again um now we go to here and it's

14684
10:24:56,320 --> 10:25:02,756
linear backward so if we step back to

14685
10:24:58,720 --> 10:25:04,276
this linear backward one is going to um

14686
10:25:02,756 --> 10:25:06,436
it's going to calculate both of these

14687
10:25:04,276 --> 10:25:10,720
right so linear backward is a bit bigger

14688
10:25:06,436 --> 10:25:13,200
actually um we go here taking a grad

14689
10:25:10,720 --> 10:25:14,520
output select the output thing the input

14690
10:25:13,200 --> 10:25:17,840
and then the weights right so we can

14691
10:25:14,520 --> 10:25:20,840
calculate both the uh the the grad

14692
10:25:17,840 --> 10:25:25,360
attribute for both the X and the W value

14693
10:25:20,840 --> 10:25:28,000
right so in here we do um grad weights

14694
10:25:25,360 --> 10:25:30,840
um is X is X transpose times the grad

14695
10:25:28,000 --> 10:25:33,360
output and so if we go to here we can

14696
10:25:30,840 --> 10:25:34,916
see uh X transpose and then times the

14697
10:25:33,360 --> 10:25:38,000
grad output which in this case and the

14698
10:25:34,916 --> 10:25:40,960
first layer is is derivative of the loss

14699
10:25:38,000 --> 10:25:43,400
um and then in this dx2 for example we

14700
10:25:40,960 --> 10:25:44,800
see the gr output times the transpose

14701
10:25:43,400 --> 10:25:47,480
weight 2

14702
10:25:44,800 --> 10:25:50,756
right so gr output times transpose

14703
10:25:47,480 --> 10:25:53,320
weight two and then the bias

14704
10:25:50,756 --> 10:25:56,640
um I'll break that down more so in in

14705
10:25:53,320 --> 10:25:59,800
the C- section but um this is this is

14706
10:25:56,640 --> 10:26:02,596
the grad bias right so um I can actually

14707
10:25:59,800 --> 10:26:05,320
just like print that out let's let's pop

14708
10:26:02,596 --> 10:26:08,840
into here really quick and just exit

14709
10:26:05,320 --> 10:26:15,360
that um so how did this go again we do

14710
10:26:08,840 --> 10:26:20,116
np. suum so I'll just do um NP just do I

14711
10:26:15,360 --> 10:26:23,640
python import numpy

14712
10:26:20,116 --> 10:26:27,720
as n p then we go x

14713
10:26:23,640 --> 10:26:31,160
equals just do torch. and um we'll do 3

14714
10:26:27,720 --> 10:26:36,436
by we'll say 2 by 4 right we print

14715
10:26:31,160 --> 10:26:38,880
out Imports import torch then go back up

14716
10:26:36,436 --> 10:26:45,596
print out X and we get this right so if

14717
10:26:38,880 --> 10:26:50,276
we do um we print out torch. suum of X

14718
10:26:45,596 --> 10:26:53,640
we do axis equal 0 and we go at keep

14719
10:26:50,276 --> 10:26:55,480
dims I think it's keep is it keep dims

14720
10:26:53,640 --> 10:26:59,200
How does it

14721
10:26:55,480 --> 10:27:01,560
go keep dims equals

14722
10:26:59,200 --> 10:27:04,160
true we can see that literally all this

14723
10:27:01,560 --> 10:27:07,320
does is it is it mushes these together

14724
10:27:04,160 --> 10:27:11,240
so it's going to go two + 1 and then

14725
10:27:07,320 --> 10:27:13,080
this is like 21 plus that so it's 3.23

14726
10:27:11,240 --> 10:27:15,160
and then so essentially like mushing

14727
10:27:13,080 --> 10:27:17,240
knees moing knees mushing knes right

14728
10:27:15,160 --> 10:27:20,160
right um and it's going to do this

14729
10:27:17,240 --> 10:27:22,160
across the the across like this the

14730
10:27:20,160 --> 10:27:25,560
horizontal right so it's going to Mush

14731
10:27:22,160 --> 10:27:27,916
cross hor vertical sorry because that is

14732
10:27:25,560 --> 10:27:29,720
the that is the zero axis right the the

14733
10:27:27,916 --> 10:27:31,916
leading Dimension here so that's that's

14734
10:27:29,720 --> 10:27:33,436
this vertical part so it's going to Mush

14735
10:27:31,916 --> 10:27:37,116
vertically

14736
10:27:33,436 --> 10:27:39,800
um and so that that's really all that is

14737
10:27:37,116 --> 10:27:41,756
um so we're just combining things across

14738
10:27:39,800 --> 10:27:44,800
the entire batch right but you'll see

14739
10:27:41,756 --> 10:27:47,720
this more intuitively in the in the C

14740
10:27:44,800 --> 10:27:49,480
version now we do the you know this as I

14741
10:27:47,720 --> 10:27:50,960
as I mentioned before and we just

14742
10:27:49,480 --> 10:27:53,596
essentially return those right for the

14743
10:27:50,960 --> 10:27:56,116
linear backward

14744
10:27:53,596 --> 10:27:58,800
layer we return all of our all of our

14745
10:27:56,116 --> 10:28:01,000
gradients and then we perform a

14746
10:27:58,800 --> 10:28:03,240
optimization step so we do the model.

14747
10:28:01,000 --> 10:28:05,960
backward and we do model. update weights

14748
10:28:03,240 --> 10:28:08,520
and we pass one 2 3 4 as well as the

14749
10:28:05,960 --> 10:28:10,596
learning rate in and inside of update

14750
10:28:08,520 --> 10:28:12,840
weights we'll see right here we

14751
10:28:10,596 --> 10:28:15,720
literally just do self. weights self

14752
10:28:12,840 --> 10:28:19,680
dobias weights and bias and we do minus

14753
10:28:15,720 --> 10:28:21,596
equals the learning rate times the

14754
10:28:19,680 --> 10:28:23,840
gradient right so as I was talking about

14755
10:28:21,596 --> 10:28:27,240
before you're trying to reduce you're

14756
10:28:23,840 --> 10:28:29,360
trying to essentially gr do gradient um

14757
10:28:27,240 --> 10:28:30,916
gradient descent that's what this is

14758
10:28:29,360 --> 10:28:32,680
just that the stochastic gradient

14759
10:28:30,916 --> 10:28:35,840
descent because it's it's doing it

14760
10:28:32,680 --> 10:28:37,520
constantly every single time and uh yeah

14761
10:28:35,840 --> 10:28:40,880
that's that's how we update the network

14762
10:28:37,520 --> 10:28:42,960
so you know if if if the gradient is if

14763
10:28:40,880 --> 10:28:45,040
the gradient is really high that means

14764
10:28:42,960 --> 10:28:47,040
there's a lot of error right so if we do

14765
10:28:45,040 --> 10:28:49,400
um learning rate times something really

14766
10:28:47,040 --> 10:28:52,400
high so a positive times a positive and

14767
10:28:49,400 --> 10:28:54,276
then subtract that from here um that's

14768
10:28:52,400 --> 10:28:56,080
going to mean it's contributing a lot

14769
10:28:54,276 --> 10:28:58,320
and then it's going to mean it's it's

14770
10:28:56,080 --> 10:29:00,800
initially contributing you know a lot of

14771
10:28:58,320 --> 10:29:02,080
error and we want to reduce that we want

14772
10:29:00,800 --> 10:29:04,520
to change it

14773
10:29:02,080 --> 10:29:06,040
significantly um and then if it's like

14774
10:29:04,520 --> 10:29:07,560
say lower we don't we don't we don't

14775
10:29:06,040 --> 10:29:09,040
want to adjust that as much right so

14776
10:29:07,560 --> 10:29:10,520
it's kind of just going to balance

14777
10:29:09,040 --> 10:29:13,840
between in the middle whichever one

14778
10:29:10,520 --> 10:29:16,680
gives us the most error right

14779
10:29:13,840 --> 10:29:18,400
um and we do this we do the same idea

14780
10:29:16,680 --> 10:29:21,000
for all of these and we just just do

14781
10:29:18,400 --> 10:29:23,000
this essentially this scalar value

14782
10:29:21,000 --> 10:29:25,756
multiplied by each thing in the entire

14783
10:29:23,000 --> 10:29:28,000
weights and the bias Matrix we do that

14784
10:29:25,756 --> 10:29:30,080
everywhere and that's pretty much it U

14785
10:29:28,000 --> 10:29:33,560
we do that for every single every single

14786
10:29:30,080 --> 10:29:35,320
iteration we do a we do a we get we get

14787
10:29:33,560 --> 10:29:37,160
whatever inputs and output uh

14788
10:29:35,320 --> 10:29:39,000
predictions we need we do a forward pass

14789
10:29:37,160 --> 10:29:42,160
we do loss function derivative of the

14790
10:29:39,000 --> 10:29:44,200
loss model. backward so backward pass

14791
10:29:42,160 --> 10:29:45,596
update weights and then if we need to we

14792
10:29:44,200 --> 10:29:47,800
just like print out the progress over

14793
10:29:45,596 --> 10:29:49,960
time right so if we go ahead and run

14794
10:29:47,800 --> 10:29:49,960
this

14795
10:29:50,080 --> 10:29:56,756
um python C friendly we can go and see

14796
10:29:54,200 --> 10:29:59,000
this is actually training quite well so

14797
10:29:56,756 --> 10:30:01,360
we this is training quite fast as well

14798
10:29:59,000 --> 10:30:03,520
you know numpy is bed to C which C is

14799
10:30:01,360 --> 10:30:05,680
really fast we can see that you know

14800
10:30:03,520 --> 10:30:09,560
over the first one over the first 7500

14801
10:30:05,680 --> 10:30:11,800
iterations we get 93% accuracy so just

14802
10:30:09,560 --> 10:30:15,160
to iterate a little more about this

14803
10:30:11,800 --> 10:30:18,116
whole linear backward um np. suum cross

14804
10:30:15,160 --> 10:30:20,116
axis zero where you take each column and

14805
10:30:18,116 --> 10:30:24,360
you squash it together the reason why we

14806
10:30:20,116 --> 10:30:26,320
do that is because each of those is like

14807
10:30:24,360 --> 10:30:30,520
across the entire batch right so this

14808
10:30:26,320 --> 10:30:33,560
would be like a single uh uh a single

14809
10:30:30,520 --> 10:30:35,040
layer right single bunch of a bunch of

14810
10:30:33,560 --> 10:30:37,840
biases for like all the neurons or

14811
10:30:35,040 --> 10:30:39,756
whatever you want to say and what we're

14812
10:30:37,840 --> 10:30:42,800
doing here is we're taking a single

14813
10:30:39,756 --> 10:30:44,916
neuron and we're squashing everything

14814
10:30:42,800 --> 10:30:48,000
together cuz like imagine if you have a

14815
10:30:44,916 --> 10:30:49,840
really big um like a really sparse

14816
10:30:48,000 --> 10:30:51,800
really big reward signal for a single

14817
10:30:49,840 --> 10:30:53,276
example and then like you do 20 other

14818
10:30:51,800 --> 10:30:54,880
ones and they have the complete opposite

14819
10:30:53,276 --> 10:30:56,800
right the idea is to like kind of

14820
10:30:54,880 --> 10:30:58,240
average all them together you're not

14821
10:30:56,800 --> 10:31:00,116
you're not like divide you're not adding

14822
10:30:58,240 --> 10:31:02,276
them all together and then dividing but

14823
10:31:00,116 --> 10:31:03,680
you're you're just like accumulating all

14824
10:31:02,276 --> 10:31:05,520
of them together so you end up with

14825
10:31:03,680 --> 10:31:07,160
something that's like close and pushes

14826
10:31:05,520 --> 10:31:09,276
in the direction of like where

14827
10:31:07,160 --> 10:31:11,080
generalization should be so I know that

14828
10:31:09,276 --> 10:31:14,160
sounds like really conceptually Advanced

14829
10:31:11,080 --> 10:31:16,360
but it's not it's you're just trying to

14830
10:31:14,160 --> 10:31:18,400
push in whatever way the average favors

14831
10:31:16,360 --> 10:31:20,680
so that's why you do it AC cross the

14832
10:31:18,400 --> 10:31:22,520
actual batch itself CU if you had just

14833
10:31:20,680 --> 10:31:24,360
this Vector laid out I mean you could do

14834
10:31:22,520 --> 10:31:26,360
that but training might not go as

14835
10:31:24,360 --> 10:31:28,040
smoothly whereas if you were to just

14836
10:31:26,360 --> 10:31:30,240
accumulate everything so you get like on

14837
10:31:28,040 --> 10:31:31,596
average what is the best way to move

14838
10:31:30,240 --> 10:31:33,276
what is the best way to move that bias

14839
10:31:31,596 --> 10:31:35,720
value then it then it helps a little bit

14840
10:31:33,276 --> 10:31:38,080
more so that's why we do that um but now

14841
10:31:35,720 --> 10:31:41,520
let's get into C this is pretty much a

14842
10:31:38,080 --> 10:31:44,116
port of just the last script that we ran

14843
10:31:41,520 --> 10:31:45,880
so this v1c you'll find this in the

14844
10:31:44,116 --> 10:31:47,240
naive CPU because this is a naive

14845
10:31:45,880 --> 10:31:49,080
algorithms these aren't like really

14846
10:31:47,240 --> 10:31:51,720
really fast are just like the easiest

14847
10:31:49,080 --> 10:31:54,080
way to write them um very like intuitive

14848
10:31:51,720 --> 10:31:56,040
to understand um but gives us a basis

14849
10:31:54,080 --> 10:32:00,560
for how we can modify this and turn turn

14850
10:31:56,040 --> 10:32:01,640
it to Cuda right um so inside of here we

14851
10:32:00,560 --> 10:32:05,720
do the same

14852
10:32:01,640 --> 10:32:05,720
idea we have a little neural network

14853
10:32:06,160 --> 10:32:10,640
thing at the top I mean it should

14854
10:32:08,000 --> 10:32:13,240
probably go from like top to bottom but

14855
10:32:10,640 --> 10:32:15,040
uh yeah so inside of here learning rate

14856
10:32:13,240 --> 10:32:17,080
same learning rate we have 10 hex which

14857
10:32:15,040 --> 10:32:18,916
is a bit different I Chang batch size to

14858
10:32:17,080 --> 10:32:20,960
four because have having it as 8 or 16

14859
10:32:18,916 --> 10:32:22,840
or 32 just took a ridiculous amount of

14860
10:32:20,960 --> 10:32:25,000
time to compute for through each layer

14861
10:32:22,840 --> 10:32:26,960
so I set this a little lower to four

14862
10:32:25,000 --> 10:32:29,756
input size Remains the Same it has to hi

14863
10:32:26,960 --> 10:32:31,596
and size 256 output size is 10 train

14864
10:32:29,756 --> 10:32:35,160
size 10,000 test size we're not going to

14865
10:32:31,596 --> 10:32:36,400
really need this but 1,000 for that um

14866
10:32:35,160 --> 10:32:38,116
and then we have this neural network

14867
10:32:36,400 --> 10:32:39,960
struct right so we can't actually do a

14868
10:32:38,116 --> 10:32:41,560
CL we can't do a class in C but we can

14869
10:32:39,960 --> 10:32:44,480
do struct we don't we don't have like

14870
10:32:41,560 --> 10:32:46,880
the the class and and objectoriented as

14871
10:32:44,480 --> 10:32:49,200
aspect that we do in C++ right this is a

14872
10:32:46,880 --> 10:32:51,080
functional functional language so we're

14873
10:32:49,200 --> 10:32:52,916
only allowed to use strs and inside of

14874
10:32:51,080 --> 10:32:54,400
here we just store a bunch of arrays so

14875
10:32:52,916 --> 10:32:56,680
all the weights and biases and then the

14876
10:32:54,400 --> 10:32:59,080
gradients for those right uh just to

14877
10:32:56,680 --> 10:33:02,200
kind of Mark everything down easily and

14878
10:32:59,080 --> 10:33:06,320
and use this very simple uh struct

14879
10:33:02,200 --> 10:33:08,116
right now we have some functions for um

14880
10:33:06,320 --> 10:33:09,436
for actually loading the data now I

14881
10:33:08,116 --> 10:33:12,276
don't want you to worry too much about

14882
10:33:09,436 --> 10:33:13,960
the loading data aspect um this part it

14883
10:33:12,276 --> 10:33:16,520
kind of just depends on like which use

14884
10:33:13,960 --> 10:33:19,160
case you have but in this case um I I

14885
10:33:16,520 --> 10:33:21,436
run the uh down downloader script so

14886
10:33:19,160 --> 10:33:26,160
this downloader script uh just saves

14887
10:33:21,436 --> 10:33:29,160
everything to a binary file um and then

14888
10:33:26,160 --> 10:33:31,560
inside of C we just write those back

14889
10:33:29,160 --> 10:33:33,840
again so or sorry we we we read them we

14890
10:33:31,560 --> 10:33:36,240
read from the binary so notice how we do

14891
10:33:33,840 --> 10:33:38,520
like file open and then the file name

14892
10:33:36,240 --> 10:33:41,200
and then a read binary um and then it

14893
10:33:38,520 --> 10:33:45,276
just turns that into into a usful format

14894
10:33:41,200 --> 10:33:48,840
right so uh yeah it's not not entirely

14895
10:33:45,276 --> 10:33:51,756
uh too too crazy um we essentially just

14896
10:33:48,840 --> 10:33:54,276
like directly read this into um into

14897
10:33:51,756 --> 10:33:56,080
bytes and then we modify that as needed

14898
10:33:54,276 --> 10:33:58,756
later on we do the same thing for labels

14899
10:33:56,080 --> 10:34:00,916
so very simple uh data loading functions

14900
10:33:58,756 --> 10:34:02,800
um not too crazy compared to the mest

14901
10:34:00,916 --> 10:34:05,240
one but this is in C right so it's

14902
10:34:02,800 --> 10:34:08,040
obviously going to be a bit different

14903
10:34:05,240 --> 10:34:09,436
now we have this interesting thing here

14904
10:34:08,040 --> 10:34:12,880
called initialize wage which I probably

14905
10:34:09,436 --> 10:34:15,320
should have shown you back here in our

14906
10:34:12,880 --> 10:34:18,480
um where did it go in our C friendly

14907
10:34:15,320 --> 10:34:20,436
script so notice in here how we have

14908
10:34:18,480 --> 10:34:21,756
multiple functions right we have r r

14909
10:34:20,436 --> 10:34:23,160
derivative initialize weights

14910
10:34:21,756 --> 10:34:25,960
initialized bias and then these other

14911
10:34:23,160 --> 10:34:28,560
ones which I already went over um

14912
10:34:25,960 --> 10:34:31,436
initialize initializing weights and

14913
10:34:28,560 --> 10:34:33,880
biases are kind of simple all right they

14914
10:34:31,436 --> 10:34:36,000
just follow specific guide so if we do

14915
10:34:33,880 --> 10:34:38,640
start off with initialized bias it's

14916
10:34:36,000 --> 10:34:40,040
literally just going to be um just a

14917
10:34:38,640 --> 10:34:41,480
bunch of zeros right it's all it's going

14918
10:34:40,040 --> 10:34:42,960
to be just need the bias is a bunch of

14919
10:34:41,480 --> 10:34:45,840
zeros that's okay we can start and we

14920
10:34:42,960 --> 10:34:48,200
can move up and down from there

14921
10:34:45,840 --> 10:34:50,480
um but the weights cuz

14922
10:34:48,200 --> 10:34:51,916
so so bias is bias is floating flowing

14923
10:34:50,480 --> 10:34:53,116
from the previous layer so having it as

14924
10:34:51,916 --> 10:34:55,400
a zero doesn't actually matter it

14925
10:34:53,116 --> 10:34:57,800
doesn't affect anything the bias the the

14926
10:34:55,400 --> 10:34:59,520
bias gradient are just flowing directly

14927
10:34:57,800 --> 10:35:02,680
from the previous layer those unmodified

14928
10:34:59,520 --> 10:35:04,960
the same gradients um but the weights

14929
10:35:02,680 --> 10:35:07,360
themselves are a little different so how

14930
10:35:04,960 --> 10:35:09,720
we initialize weights is I'm actually

14931
10:35:07,360 --> 10:35:14,640
going to go over to here we going to

14932
10:35:09,720 --> 10:35:16,436
search up P torch timing

14933
10:35:14,640 --> 10:35:19,360
initialization and this is how you

14934
10:35:16,436 --> 10:35:23,640
actually initialize um I'll just do I'll

14935
10:35:19,360 --> 10:35:26,596
just do uh torch do uh nn.

14936
10:35:23,640 --> 10:35:28,276
linear um we'll go to

14937
10:35:26,596 --> 10:35:34,400
linear

14938
10:35:28,276 --> 10:35:34,400
so in pytorch we do this

14939
10:35:34,596 --> 10:35:39,040
um where is

14940
10:35:36,640 --> 10:35:43,756
it yes

14941
10:35:39,040 --> 10:35:45,840
so the biases are initialized um in this

14942
10:35:43,756 --> 10:35:47,436
distribution which we we can we we can

14943
10:35:45,840 --> 10:35:51,720
do we don't entirely have to worry about

14944
10:35:47,436 --> 10:35:54,000
that it's not a big deal um but then the

14945
10:35:51,720 --> 10:35:56,756
then the weights themselves these are

14946
10:35:54,000 --> 10:35:59,520
nor these are initialized uh on this

14947
10:35:56,756 --> 10:36:02,276
basis right so we have um of shape

14948
10:35:59,520 --> 10:36:06,800
output features by input

14949
10:36:02,276 --> 10:36:09,800
features and we make this from uh from

14950
10:36:06,800 --> 10:36:15,200
negative square Ro TK of K to positive

14951
10:36:09,800 --> 10:36:19,436
square root of K where K is

14952
10:36:15,200 --> 10:36:22,640
um K is one over the input features so

14953
10:36:19,436 --> 10:36:25,960
if we pop back to this we're doing a

14954
10:36:22,640 --> 10:36:27,840
random normal distribution right and

14955
10:36:25,960 --> 10:36:29,436
with each of these numbers we have to we

14956
10:36:27,840 --> 10:36:31,480
have to Clump them to this range right

14957
10:36:29,436 --> 10:36:33,960
so these values are these values are in

14958
10:36:31,480 --> 10:36:38,680
some normally distributed range and all

14959
10:36:33,960 --> 10:36:38,680
we have to do there is

14960
10:36:39,000 --> 10:36:47,480
um we do we we initialize to this right

14961
10:36:42,960 --> 10:36:47,480
so K in this case

14962
10:36:49,040 --> 10:36:54,960
okay so if we actually go to the H

14963
10:36:51,640 --> 10:36:56,596
ination paper he init

14964
10:36:54,960 --> 10:36:59,400
paper

14965
10:36:56,596 --> 10:37:02,520
um this is I think this is

14966
10:36:59,400 --> 10:37:04,240
it climing hey so climing and hay

14967
10:37:02,520 --> 10:37:07,520
climing and it hey and it are the are

14968
10:37:04,240 --> 10:37:10,840
the same thing um but if we go into here

14969
10:37:07,520 --> 10:37:14,840
we go two ided by no maybe it's not

14970
10:37:10,840 --> 10:37:19,436
there formula is somewhere in here

14971
10:37:14,840 --> 10:37:19,436
um where did it go

14972
10:37:27,720 --> 10:37:33,276
I search up

14973
10:37:30,276 --> 10:37:33,276
um

14974
10:37:44,596 --> 10:37:51,080
Rue initialization

14975
10:37:47,720 --> 10:37:51,080
maybe that's a better term to look

14976
10:38:02,720 --> 10:38:07,040
for

14977
10:38:05,040 --> 10:38:11,040
yes this right

14978
10:38:07,040 --> 10:38:13,720
here so this leads to a zero mean G CH

14979
10:38:11,040 --> 10:38:16,880
distribution whose standard deviation is

14980
10:38:13,720 --> 10:38:21,400
square < TK of two over um over this

14981
10:38:16,880 --> 10:38:22,640
term and this term is the um I can't

14982
10:38:21,400 --> 10:38:24,720
remember exactly what this is but I

14983
10:38:22,640 --> 10:38:27,560
think this is length

14984
10:38:24,720 --> 10:38:29,040
so we have an input size here which you

14985
10:38:27,560 --> 10:38:31,560
could say as the length I don't know if

14986
10:38:29,040 --> 10:38:33,320
that's specifically what L ties to but

14987
10:38:31,560 --> 10:38:35,160
we'll just hold that assumption for now

14988
10:38:33,320 --> 10:38:37,960
that that's that's the idea there is you

14989
10:38:35,160 --> 10:38:41,276
would have

14990
10:38:37,960 --> 10:38:45,160
um standard deviation is

14991
10:38:41,276 --> 10:38:46,960
this um and if we continue to go forward

14992
10:38:45,160 --> 10:38:48,436
maybe we might find something else here

14993
10:38:46,960 --> 10:38:51,040
too

14994
10:38:48,436 --> 10:38:51,040
um if we

14995
10:38:51,916 --> 10:38:57,160
continue yeah so some

14996
10:38:54,640 --> 10:39:00,116
layers other solution is to small Factor

14997
10:38:57,160 --> 10:39:00,116
on the weights

14998
10:39:00,436 --> 10:39:03,436
right

14999
10:39:03,720 --> 10:39:06,720
anyways

15000
10:39:09,160 --> 10:39:14,520
uh yeah this is pretty much the the

15001
10:39:12,160 --> 10:39:17,916
inspiration from it so just looking at

15002
10:39:14,520 --> 10:39:20,880
like kind of the purpose of this um

15003
10:39:17,916 --> 10:39:24,360
this this specific initialization as

15004
10:39:20,880 --> 10:39:25,436
compared to the P torch one um which I

15005
10:39:24,360 --> 10:39:27,680
probably should have looked into

15006
10:39:25,436 --> 10:39:31,360
beforehand the pytorch one is a little

15007
10:39:27,680 --> 10:39:33,520
different um but the hay initialization

15008
10:39:31,360 --> 10:39:38,960
is designed to work well with railu so

15009
10:39:33,520 --> 10:39:41,840
it uses Square < TK of two / um input

15010
10:39:38,960 --> 10:39:43,560
size as a standard deviation for the

15011
10:39:41,840 --> 10:39:45,320
distribution that it's generated on

15012
10:39:43,560 --> 10:39:47,680
right

15013
10:39:45,320 --> 10:39:50,200
um it essentially counts out for the

15014
10:39:47,680 --> 10:39:52,480
rause activation to zero out negative

15015
10:39:50,200 --> 10:39:54,720
values so you might have these um

15016
10:39:52,480 --> 10:39:56,320
so-called dead neurons that come up when

15017
10:39:54,720 --> 10:39:58,240
you have like the Rue that just zeros

15018
10:39:56,320 --> 10:40:00,080
something out and then when you try to

15019
10:39:58,240 --> 10:40:01,680
uh like multiply that by something it

15020
10:40:00,080 --> 10:40:03,480
ends up just like zeroing it out and you

15021
10:40:01,680 --> 10:40:05,480
might like end up through the training

15022
10:40:03,480 --> 10:40:06,880
process with like a row of zeros that

15023
10:40:05,480 --> 10:40:08,560
just don't do anything and they're like

15024
10:40:06,880 --> 10:40:10,596
useless and so you're not actually

15025
10:40:08,560 --> 10:40:12,680
compressing information down into those

15026
10:40:10,596 --> 10:40:13,880
because they're just zero so this helps

15027
10:40:12,680 --> 10:40:18,400
deal with

15028
10:40:13,880 --> 10:40:19,960
that now jumping back to C

15029
10:40:18,400 --> 10:40:22,640
script

15030
10:40:19,960 --> 10:40:24,560
um this is what we're doing here so we

15031
10:40:22,640 --> 10:40:26,200
essentially have this we're we're just

15032
10:40:24,560 --> 10:40:28,960
we just have to use like what we're what

15033
10:40:26,200 --> 10:40:31,320
we're defaulted to with C we get this

15034
10:40:28,960 --> 10:40:34,080
weights with the size that it's in we

15035
10:40:31,320 --> 10:40:36,116
make this scale so square root function

15036
10:40:34,080 --> 10:40:38,320
you know like we were doing before um

15037
10:40:36,116 --> 10:40:43,320
square otk of two divided by size in

15038
10:40:38,320 --> 10:40:45,436
this case um which size would be um you

15039
10:40:43,320 --> 10:40:46,720
know size may not be appropriate I just

15040
10:40:45,436 --> 10:40:49,080
kind of found this to work and and

15041
10:40:46,720 --> 10:40:51,560
training exceptionally well with this so

15042
10:40:49,080 --> 10:40:53,960
we're going to stick with that um but

15043
10:40:51,560 --> 10:40:55,916
the size we're going to iterate through

15044
10:40:53,960 --> 10:40:57,840
this and essentially for each weight

15045
10:40:55,916 --> 10:41:00,200
value we're going to generate a value

15046
10:40:57,840 --> 10:41:03,480
between Rand so Rand is going to be

15047
10:41:00,200 --> 10:41:06,520
anywhere between zero and Rand Max so

15048
10:41:03,480 --> 10:41:09,160
two one whatever this is so essentially

15049
10:41:06,520 --> 10:41:10,720
this this in here is going to be zero

15050
10:41:09,160 --> 10:41:12,200
between that Max number it's going to

15051
10:41:10,720 --> 10:41:14,520
this is going to simplify to between a

15052
10:41:12,200 --> 10:41:17,320
value between 0 and one decimal floating

15053
10:41:14,520 --> 10:41:19,040
Point 32 number between 0 and one we're

15054
10:41:17,320 --> 10:41:22,116
going to multiply this by the

15055
10:41:19,040 --> 10:41:25,400
scale um which is which is going to be

15056
10:41:22,116 --> 10:41:29,680
that and then we're going to subtract it

15057
10:41:25,400 --> 10:41:31,276
by the um by the scale divided by uh

15058
10:41:29,680 --> 10:41:33,880
divided by

15059
10:41:31,276 --> 10:41:36,756
two and this is just going to give us a

15060
10:41:33,880 --> 10:41:38,360
nice normal distribution for uh for our

15061
10:41:36,756 --> 10:41:39,520
weights right this is going to do

15062
10:41:38,360 --> 10:41:41,160
essentially the same job as we were

15063
10:41:39,520 --> 10:41:43,960
doing

15064
10:41:41,160 --> 10:41:46,880
before bias initialize all these to zero

15065
10:41:43,960 --> 10:41:47,880
as we were doing before Ru is also very

15066
10:41:46,880 --> 10:41:49,560
simple

15067
10:41:47,880 --> 10:41:52,916
um the softmax I mean I think I showed

15068
10:41:49,560 --> 10:41:55,240
you the softmax in the Triton section so

15069
10:41:52,916 --> 10:41:57,000
um this is yeah this should be fairly

15070
10:41:55,240 --> 10:41:59,720
intuitive we we get this you know we get

15071
10:41:57,000 --> 10:42:01,560
this max value right and then when we're

15072
10:41:59,720 --> 10:42:03,800
actually doing the exponentiation we we

15073
10:42:01,560 --> 10:42:06,320
subtract the max value so we get still

15074
10:42:03,800 --> 10:42:07,720
remain with numerical stability um not

15075
10:42:06,320 --> 10:42:10,000
having that would just like give these

15076
10:42:07,720 --> 10:42:12,320
crazy you know e to the whatever super

15077
10:42:10,000 --> 10:42:15,000
crazy numbers when we have um ridiculous

15078
10:42:12,320 --> 10:42:16,680
arrays um with like you know th negative

15079
10:42:15,000 --> 10:42:18,756
a th000 negative 10 it's just it just

15080
10:42:16,680 --> 10:42:20,840
gets out of hand right so we want we

15081
10:42:18,756 --> 10:42:22,560
want Max to normalize that and just get

15082
10:42:20,840 --> 10:42:24,160
rid of any of those

15083
10:42:22,560 --> 10:42:26,800
instability

15084
10:42:24,160 --> 10:42:28,756
um and then we yeah we just compute the

15085
10:42:26,800 --> 10:42:31,560
soft Max right this is this isn't too

15086
10:42:28,756 --> 10:42:33,436
bad same function we looked at before we

15087
10:42:31,560 --> 10:42:35,200
have a matte mole so I specifically

15088
10:42:33,436 --> 10:42:37,116
worded these so that it would make make

15089
10:42:35,200 --> 10:42:39,160
the most sense so it's a map mole but

15090
10:42:37,116 --> 10:42:41,640
this treats it as like you're taking in

15091
10:42:39,160 --> 10:42:45,960
an array a and you Matrix multiply that

15092
10:42:41,640 --> 10:42:48,400
with B right so say for example 2x4 is a

15093
10:42:45,960 --> 10:42:50,276
and 4x three and so you'd end up with a

15094
10:42:48,400 --> 10:42:52,276
2x3 and it would organize that in row

15095
10:42:50,276 --> 10:42:54,520
major order right made these as simple

15096
10:42:52,276 --> 10:42:56,200
as possible um you can dissect these but

15097
10:42:54,520 --> 10:42:57,960
we already did a ton of stuff on matal

15098
10:42:56,200 --> 10:43:02,000
so don't I'm not going to go over this

15099
10:42:57,960 --> 10:43:05,436
for the like 20th time um then we have a

15100
10:43:02,000 --> 10:43:08,560
a * B transposed so it's going to take

15101
10:43:05,436 --> 10:43:13,276
in B is like say it's like a is 2x4 and

15102
10:43:08,560 --> 10:43:14,840
then B is a um 3x4 and so it's going to

15103
10:43:13,276 --> 10:43:17,640
uh do this operation as if it's

15104
10:43:14,840 --> 10:43:21,400
transposing B to a 4x3 so end up with a

15105
10:43:17,640 --> 10:43:24,160
2x3 right um and then same idea for this

15106
10:43:21,400 --> 10:43:29,960
one if you end up with a 4X two and then

15107
10:43:24,160 --> 10:43:33,040
a four four 4X two as as a and then 4 by

15108
10:43:29,960 --> 10:43:34,560
uh 3 as B it's going to transpose a and

15109
10:43:33,040 --> 10:43:35,560
it's going to make it a 2x4 and they're

15110
10:43:34,560 --> 10:43:37,520
going to match up you're going to get

15111
10:43:35,560 --> 10:43:41,880
two and three right so that that's just

15112
10:43:37,520 --> 10:43:45,680
kind of the idea there and then we do um

15113
10:43:41,880 --> 10:43:48,116
the the ru forward I probably wrote an

15114
10:43:45,680 --> 10:43:50,400
additional um might have

15115
10:43:48,116 --> 10:43:53,520
wrote yeah so this is like designed to

15116
10:43:50,400 --> 10:43:55,200
work in batches um I might have probably

15117
10:43:53,520 --> 10:43:59,800
written an additional one just like

15118
10:43:55,200 --> 10:43:59,800
accidentally which I'll probably remove

15119
10:44:04,320 --> 10:44:12,240
um oh R you there yeah so I should

15120
10:44:09,160 --> 10:44:13,680
probably remove this actually but um

15121
10:44:12,240 --> 10:44:14,880
well we'll I'll worry about that later

15122
10:44:13,680 --> 10:44:17,080
and this will be updated by the time

15123
10:44:14,880 --> 10:44:18,596
you're working on it um and then we just

15124
10:44:17,080 --> 10:44:22,080
just have the bias forwarded which is

15125
10:44:18,596 --> 10:44:23,720
going to you know add the bias um so

15126
10:44:22,080 --> 10:44:26,320
literally what this is doing is it's

15127
10:44:23,720 --> 10:44:29,200
iterating through uh it's it's going

15128
10:44:26,320 --> 10:44:31,640
through batch size right and then we

15129
10:44:29,200 --> 10:44:33,840
iterate through the actual size itself

15130
10:44:31,640 --> 10:44:37,680
which is

15131
10:44:33,840 --> 10:44:39,756
um which is the actual uh like the the

15132
10:44:37,680 --> 10:44:41,680
the row length so it's going to go skip

15133
10:44:39,756 --> 10:44:43,000
over as many number batch elements as it

15134
10:44:41,680 --> 10:44:44,276
needs to so it's going to skip it's

15135
10:44:43,000 --> 10:44:46,520
going to stride

15136
10:44:44,276 --> 10:44:48,756
over then it's going to add this ey

15137
10:44:46,520 --> 10:44:52,596
offset to it and it's just going to plus

15138
10:44:48,756 --> 10:44:54,160
equals bias at that that value right so

15139
10:44:52,596 --> 10:44:59,040
we we essentially just have this this

15140
10:44:54,160 --> 10:45:02,560
row of biases and it's going to um just

15141
10:44:59,040 --> 10:45:04,160
essentially add each of those it's for

15142
10:45:02,560 --> 10:45:07,400
for like a given batch element for like

15143
10:45:04,160 --> 10:45:09,200
batch element one um it's going to just

15144
10:45:07,400 --> 10:45:10,400
add you know say it's like 10 values

15145
10:45:09,200 --> 10:45:12,080
here it's going to add all 10 to the

15146
10:45:10,400 --> 10:45:13,240
bias values and it's going to go down

15147
10:45:12,080 --> 10:45:14,800
it's going to add those same values

15148
10:45:13,240 --> 10:45:16,480
again it's just applying the same bias

15149
10:45:14,800 --> 10:45:18,916
to each to each row right that's what

15150
10:45:16,480 --> 10:45:18,916
this is doing

15151
10:45:19,240 --> 10:45:22,040
scrolling down further I'm not going to

15152
10:45:20,560 --> 10:45:24,756
go into these quite yet because there's

15153
10:45:22,040 --> 10:45:27,840
like more happening but scroll down

15154
10:45:24,756 --> 10:45:31,596
to um the

15155
10:45:27,840 --> 10:45:33,080
actual uh train function and the where

15156
10:45:31,596 --> 10:45:36,680
is

15157
10:45:33,080 --> 10:45:40,320
this the in main function so in here we

15158
10:45:36,680 --> 10:45:42,640
have a a pseudo random number generator

15159
10:45:40,320 --> 10:45:44,160
um these are pseudo random they're not

15160
10:45:42,640 --> 10:45:46,240
you can actually have completely random

15161
10:45:44,160 --> 10:45:47,640
numbers that's like a very hard uh you

15162
10:45:46,240 --> 10:45:49,520
know cryptographic graic problem and

15163
10:45:47,640 --> 10:45:51,596
everything that's like something I'm not

15164
10:45:49,520 --> 10:45:54,880
going to go into um you know in this

15165
10:45:51,596 --> 10:45:56,880
case we're using srand to to to Generate

15166
10:45:54,880 --> 10:45:58,840
random numbers but in Cuda you can use Q

15167
10:45:56,880 --> 10:46:00,436
Rand so it's going to Generate random

15168
10:45:58,840 --> 10:46:01,880
numbers in parallel really fast so you

15169
10:46:00,436 --> 10:46:03,560
don't have to like wait for the CPU to

15170
10:46:01,880 --> 10:46:05,640
do this one and this one then this one

15171
10:46:03,560 --> 10:46:07,040
right it's kind of faster um we

15172
10:46:05,640 --> 10:46:10,520
initialize this neural network class

15173
10:46:07,040 --> 10:46:12,960
with NN or sorry struct struct got to

15174
10:46:10,520 --> 10:46:15,720
use the politically cor correct terms we

15175
10:46:12,960 --> 10:46:18,240
initialize the neuronet so we go here

15176
10:46:15,720 --> 10:46:21,480
and we just have this NN and then the

15177
10:46:18,240 --> 10:46:24,080
weight attribute right uh equals then we

15178
10:46:21,480 --> 10:46:26,040
do Malik just a regular C Malik so

15179
10:46:24,080 --> 10:46:28,960
weights one is going to be um hidden

15180
10:46:26,040 --> 10:46:30,800
size by input size right so that's the

15181
10:46:28,960 --> 10:46:33,840
256 by

15182
10:46:30,800 --> 10:46:36,880
784 and then the weights two which is

15183
10:46:33,840 --> 10:46:39,200
output size by hidden size so um this is

15184
10:46:36,880 --> 10:46:42,880
going to be

15185
10:46:39,200 --> 10:46:47,360
um what's it called 256x 10 so it's

15186
10:46:42,880 --> 10:46:48,480
going to take the BX 256 and then 2 6 x

15187
10:46:47,360 --> 10:46:50,116
10 and it's going to multiply those and

15188
10:46:48,480 --> 10:46:53,160
it's going to get a B by10 output right

15189
10:46:50,116 --> 10:46:56,160
for the after the weights um you know

15190
10:46:53,160 --> 10:46:58,080
bias bias one is hidden size just adding

15191
10:46:56,160 --> 10:47:01,400
again to that to that output of all of

15192
10:46:58,080 --> 10:47:02,880
those neurons each value for each neuron

15193
10:47:01,400 --> 10:47:06,320
IUS 2 same

15194
10:47:02,880 --> 10:47:09,000
idea the grad weight so just this but

15195
10:47:06,320 --> 10:47:10,436
the it's just a different uh it's just a

15196
10:47:09,000 --> 10:47:13,160
different variable right so we're

15197
10:47:10,436 --> 10:47:14,560
storing the gradients of those the error

15198
10:47:13,160 --> 10:47:17,320
um and they're just going to be the same

15199
10:47:14,560 --> 10:47:19,116
shape right

15200
10:47:17,320 --> 10:47:21,080
um and then we initialize so we have

15201
10:47:19,116 --> 10:47:23,640
initialized weight and initialize bias

15202
10:47:21,080 --> 10:47:25,320
now this is going back to um the the

15203
10:47:23,640 --> 10:47:27,640
climing init that we did the the hay

15204
10:47:25,320 --> 10:47:30,116
initialization and the initialized bias

15205
10:47:27,640 --> 10:47:33,320
that we did before

15206
10:47:30,116 --> 10:47:36,800
um now now that we've initialized those

15207
10:47:33,320 --> 10:47:40,116
with random values we go into here so

15208
10:47:36,800 --> 10:47:43,596
our X train is going to be the train

15209
10:47:40,116 --> 10:47:46,916
size so train size in this case is uh

15210
10:47:43,596 --> 10:47:46,916
you know 10,000

15211
10:47:48,000 --> 10:47:53,080
let me go down a little bit more train

15212
10:47:49,916 --> 10:47:55,960
size times input size so an image is

15213
10:47:53,080 --> 10:47:57,360
784 um you know flattened and then we

15214
10:47:55,960 --> 10:47:58,116
have the train size which in this case

15215
10:47:57,360 --> 10:48:01,040
is

15216
10:47:58,116 --> 10:48:04,160
10,000 um the Y train is just going to

15217
10:48:01,040 --> 10:48:05,960
be a bunch of integers that are that

15218
10:48:04,160 --> 10:48:08,560
span this so we don't we don't actually

15219
10:48:05,960 --> 10:48:11,916
need 784 there's only one integer value

15220
10:48:08,560 --> 10:48:13,756
per sample um which is the label and

15221
10:48:11,916 --> 10:48:15,720
then we have the same for for the for

15222
10:48:13,756 --> 10:48:18,276
the test set

15223
10:48:15,720 --> 10:48:20,756
right we load these in using the

15224
10:48:18,276 --> 10:48:24,000
previous um loading loading scripts that

15225
10:48:20,756 --> 10:48:25,840
I showed you before we can print the

15226
10:48:24,000 --> 10:48:27,116
first image in the terminal so this

15227
10:48:25,840 --> 10:48:30,916
going just going to print things out

15228
10:48:27,116 --> 10:48:32,160
using the X thing right um so if I I'm

15229
10:48:30,916 --> 10:48:33,400
going to compile this later and you'll

15230
10:48:32,160 --> 10:48:36,880
kind of see what I mean but it just kind

15231
10:48:33,400 --> 10:48:38,960
of shows us um like how good our our

15232
10:48:36,880 --> 10:48:40,520
actual predictions are going to be um so

15233
10:48:38,960 --> 10:48:42,520
we can actually like look at look at an

15234
10:48:40,520 --> 10:48:44,160
image in the terminal and see okay what

15235
10:48:42,520 --> 10:48:45,436
did it think this was what was the

15236
10:48:44,160 --> 10:48:46,960
actual label right so we can kind of

15237
10:48:45,436 --> 10:48:48,880
like look and sort of match things up in

15238
10:48:46,960 --> 10:48:50,960
our own head I don't want to use like

15239
10:48:48,880 --> 10:48:52,596
open CV or a custom extension to put a

15240
10:48:50,960 --> 10:48:54,360
window cuz that's just a bunch of extra

15241
10:48:52,596 --> 10:48:56,480
work it's easier to do it in the

15242
10:48:54,360 --> 10:48:58,640
terminal um and then just the training

15243
10:48:56,480 --> 10:49:00,640
labels for those as well right and then

15244
10:48:58,640 --> 10:49:02,480
we go through and we do the train

15245
10:49:00,640 --> 10:49:04,596
function which is comprehensive and then

15246
10:49:02,480 --> 10:49:07,200
we free everything up we don't do Cuda

15247
10:49:04,596 --> 10:49:10,520
free this just C we just do free and it

15248
10:49:07,200 --> 10:49:12,840
get gets rid of the weights uh biases

15249
10:49:10,520 --> 10:49:14,960
all the gradients for those the train

15250
10:49:12,840 --> 10:49:18,200
set and the test set

15251
10:49:14,960 --> 10:49:19,436
right now we go into TR inside of here

15252
10:49:18,200 --> 10:49:22,000
there's a bunch of things happening all

15253
10:49:19,436 --> 10:49:23,040
right so if I click this can see where

15254
10:49:22,000 --> 10:49:26,596
the end

15255
10:49:23,040 --> 10:49:28,680
is we have this we have this um this

15256
10:49:26,596 --> 10:49:32,800
hidden right here so that's going to be

15257
10:49:28,680 --> 10:49:36,480
you know the B by 256 as we looked at in

15258
10:49:32,800 --> 10:49:38,756
um the B

15259
10:49:36,480 --> 10:49:42,596
by where was

15260
10:49:38,756 --> 10:49:44,200
it batch size by hidden size so that's

15261
10:49:42,596 --> 10:49:47,080
like going to be the actual hidden layer

15262
10:49:44,200 --> 10:49:49,560
output right so so here we get a B by

15263
10:49:47,080 --> 10:49:51,436
256 that's the hidden layer it's the

15264
10:49:49,560 --> 10:49:53,400
that's the first you know mmal output

15265
10:49:51,436 --> 10:49:55,840
essentially and then we get the output

15266
10:49:53,400 --> 10:49:58,680
which is batch size by output size which

15267
10:49:55,840 --> 10:50:01,400
in this case is p

15268
10:49:58,680 --> 10:50:02,480
by10 and then we do num batches so the

15269
10:50:01,400 --> 10:50:04,880
number of batches we're actually going

15270
10:50:02,480 --> 10:50:07,680
to do is train size divided by batch

15271
10:50:04,880 --> 10:50:09,360
size and the reason we do this is

15272
10:50:07,680 --> 10:50:10,800
because we don't want to just like

15273
10:50:09,360 --> 10:50:12,916
offset each time we don't want to give

15274
10:50:10,800 --> 10:50:15,200
it the same data each sample so if we

15275
10:50:12,916 --> 10:50:17,840
take that total 60,000 and divide that

15276
10:50:15,200 --> 10:50:19,756
by batch size meaning like four

15277
10:50:17,840 --> 10:50:21,680
uh or sorry train size is 10,000 and

15278
10:50:19,756 --> 10:50:26,560
then batch size is four we're going to

15279
10:50:21,680 --> 10:50:28,756
get 2,500 total um total batches each

15280
10:50:26,560 --> 10:50:30,960
with four images in them so we're going

15281
10:50:28,756 --> 10:50:32,160
to do four and then four and then four

15282
10:50:30,960 --> 10:50:35,116
and then four this way we don't like

15283
10:50:32,160 --> 10:50:37,200
overlap right containing the same images

15284
10:50:35,116 --> 10:50:40,276
in adjacent batches we just kind of give

15285
10:50:37,200 --> 10:50:42,200
it new data every time and we do Epoch

15286
10:50:40,276 --> 10:50:44,400
over that right so we do all of the

15287
10:50:42,200 --> 10:50:45,680
batches and then we do another Epoch

15288
10:50:44,400 --> 10:50:47,200
over that so what's going to happen it's

15289
10:50:45,680 --> 10:50:49,840
it's going to it's going to do like up

15290
10:50:47,200 --> 10:50:52,040
to 50% in the first Epoch or some some

15291
10:50:49,840 --> 10:50:54,560
some number like that 50% and then it's

15292
10:50:52,040 --> 10:50:55,880
going to start the next Epoch and it's

15293
10:50:54,560 --> 10:50:56,960
going to learn from all the examples

15294
10:50:55,880 --> 10:50:58,960
that it had previously and it's going to

15295
10:50:56,960 --> 10:51:00,116
be like oh we saw those again we know

15296
10:50:58,960 --> 10:51:02,160
exactly what to do there so it's going

15297
10:51:00,116 --> 10:51:04,116
to it's going to get you know accuracy

15298
10:51:02,160 --> 10:51:05,840
is going to like it's the loss is going

15299
10:51:04,116 --> 10:51:07,840
to look like this it's going to figure

15300
10:51:05,840 --> 10:51:09,080
start figuring everything out tuning

15301
10:51:07,840 --> 10:51:10,400
whatever it can get its hands on and

15302
10:51:09,080 --> 10:51:12,436
it's going to drop because it figures

15303
10:51:10,400 --> 10:51:14,840
out what to optimize then it's going to

15304
10:51:12,436 --> 10:51:16,880
Plateau through that Epoch it's going to

15305
10:51:14,840 --> 10:51:18,840
sort of plateau out and then the next

15306
10:51:16,880 --> 10:51:20,880
next Epoch starts and it's going to it's

15307
10:51:18,840 --> 10:51:22,320
going to drop again because it's because

15308
10:51:20,880 --> 10:51:25,320
it's seen those again and it can

15309
10:51:22,320 --> 10:51:27,080
optimize for more it can compress more

15310
10:51:25,320 --> 10:51:28,916
features into that because it's seen

15311
10:51:27,080 --> 10:51:30,800
that already um so it's going to you

15312
10:51:28,916 --> 10:51:32,276
know continue dropping again and then

15313
10:51:30,800 --> 10:51:35,080
it's going to sort of plateau and then

15314
10:51:32,276 --> 10:51:36,480
it's going to drop again and and then

15315
10:51:35,080 --> 10:51:38,560
we're just going to end up at some place

15316
10:51:36,480 --> 10:51:41,000
when whenever all the epoch are done

15317
10:51:38,560 --> 10:51:42,840
right so we iterate through Epoch now

15318
10:51:41,000 --> 10:51:46,200
this is

15319
10:51:42,840 --> 10:51:49,240
um if I look at this actually um this

15320
10:51:46,200 --> 10:51:50,960
finish is here so inside of a single

15321
10:51:49,240 --> 10:51:52,720
Epoch this is where most of the work is

15322
10:51:50,960 --> 10:51:54,400
done right we just do like free hidden

15323
10:51:52,720 --> 10:51:56,720
and output so most of the work is

15324
10:51:54,400 --> 10:51:59,040
actually done in the epox loop inside of

15325
10:51:56,720 --> 10:52:01,520
here we do a total loss which we

15326
10:51:59,040 --> 10:52:03,596
initialize to zero the number of correct

15327
10:52:01,520 --> 10:52:05,436
answers so we also set that to zero this

15328
10:52:03,596 --> 10:52:07,756
is just for tracking the accuracy so we

15329
10:52:05,436 --> 10:52:10,276
can see the loss dropping versus what

15330
10:52:07,756 --> 10:52:12,520
the percent accuracy is over the over

15331
10:52:10,276 --> 10:52:14,756
the U over the the training samples

15332
10:52:12,520 --> 10:52:18,840
right so each training step we can see

15333
10:52:14,756 --> 10:52:21,840
which or every sorry every um every

15334
10:52:18,840 --> 10:52:24,560
thousand every 1,000 or or every 100

15335
10:52:21,840 --> 10:52:27,276
training steps we can see what the

15336
10:52:24,560 --> 10:52:29,640
accuracy is over the

15337
10:52:27,276 --> 10:52:33,480
batches then inside of here we iterate

15338
10:52:29,640 --> 10:52:38,960
over num batches increasing by um by

15339
10:52:33,480 --> 10:52:38,960
this each time right um batch batch

15340
10:52:40,880 --> 10:52:46,200
Plus+ and

15341
10:52:43,000 --> 10:52:49,916
then we use start idx this is going to

15342
10:52:46,200 --> 10:52:52,040
be batch times batch times batch size we

15343
10:52:49,916 --> 10:52:55,080
do our forward so we get essentially

15344
10:52:52,040 --> 10:52:56,480
inside of here we pass in our neural net

15345
10:52:55,080 --> 10:53:00,160
we passing our like our neural net

15346
10:52:56,480 --> 10:53:02,240
struct pointer um an input so that's

15347
10:53:00,160 --> 10:53:05,040
going to be train and it's going to be

15348
10:53:02,240 --> 10:53:08,720
the um the start

15349
10:53:05,040 --> 10:53:12,080
idx so whichever whichever this is um

15350
10:53:08,720 --> 10:53:15,000
whichever actual batch it is um times

15351
10:53:12,080 --> 10:53:17,400
the times the batch size so it's going

15352
10:53:15,000 --> 10:53:19,840
to skip in increments a batch size right

15353
10:53:17,400 --> 10:53:19,840
so it's going

15354
10:53:22,360 --> 10:53:27,916
to it's going to instead of uh skipping

15355
10:53:26,116 --> 10:53:31,040
like this isn't going to actually like

15356
10:53:27,916 --> 10:53:33,040
plus equals um like batch itself this is

15357
10:53:31,040 --> 10:53:34,520
going to add one each time so this is

15358
10:53:33,040 --> 10:53:37,160
just going to act as like an increment

15359
10:53:34,520 --> 10:53:38,160
so we're jumping right that's what

15360
10:53:37,160 --> 10:53:39,840
that's what that's doing is it's going

15361
10:53:38,160 --> 10:53:41,680
to jump four at a time instead of just

15362
10:53:39,840 --> 10:53:44,360
one we just plus

15363
10:53:41,680 --> 10:53:47,320
plus uh and

15364
10:53:44,360 --> 10:53:50,200
then we pass in the hidden layer

15365
10:53:47,320 --> 10:53:52,756
we pass in the output and then the batch

15366
10:53:50,200 --> 10:53:57,360
size as well right so all of the inputs

15367
10:53:52,756 --> 10:53:58,800
um hidden output size right um and this

15368
10:53:57,360 --> 10:54:01,080
is just going to do our forward pass all

15369
10:53:58,800 --> 10:54:03,276
the way from uh you know taking this

15370
10:54:01,080 --> 10:54:04,640
flattened image which we've done already

15371
10:54:03,276 --> 10:54:07,000
I'm just because it's just laid out in

15372
10:54:04,640 --> 10:54:09,000
binary it just like exists as that you

15373
10:54:07,000 --> 10:54:10,756
can reformat it and interpret it as

15374
10:54:09,000 --> 10:54:13,400
whatever you want but in see in memory

15375
10:54:10,756 --> 10:54:15,840
it's actually laid out as literally one

15376
10:54:13,400 --> 10:54:17,880
through zero you know Z through 784 or

15377
10:54:15,840 --> 10:54:21,040
whatever um so it's like not that hard

15378
10:54:17,880 --> 10:54:21,040
to actually like mess around

15379
10:54:21,276 --> 10:54:29,040
with we do the forward pass we calculate

15380
10:54:24,916 --> 10:54:31,000
our cross entropy loss right using the

15381
10:54:29,040 --> 10:54:32,640
uh same cross entropy loss idea that we

15382
10:54:31,000 --> 10:54:36,276
did in the C friendly script so we're

15383
10:54:32,640 --> 10:54:38,000
just porting that over to C um which if

15384
10:54:36,276 --> 10:54:39,276
there's like if if if this doesn't like

15385
10:54:38,000 --> 10:54:40,596
make sense then you can actually go in

15386
10:54:39,276 --> 10:54:42,360
and you can see okay well what are we

15387
10:54:40,596 --> 10:54:44,320
doing here versus here right you have

15388
10:54:42,360 --> 10:54:45,880
tools like language models and and the

15389
10:54:44,320 --> 10:54:47,240
internet which you can investigate these

15390
10:54:45,880 --> 10:54:50,880
things through through and you can kind

15391
10:54:47,240 --> 10:54:56,276
of see what's happening um yeah so we

15392
10:54:50,880 --> 10:54:58,240
calculate our loss we add um we add the

15393
10:54:56,276 --> 10:55:01,240
uh we add that loss to the total loss so

15394
10:54:58,240 --> 10:55:03,040
inside of that actual Epoch we see okay

15395
10:55:01,240 --> 10:55:05,560
well what was the uh what was the

15396
10:55:03,040 --> 10:55:08,240
average loss right um you know in here

15397
10:55:05,560 --> 10:55:10,320
we do total loss divided by you know

15398
10:55:08,240 --> 10:55:12,916
numb batches so we kind of we kind of

15399
10:55:10,320 --> 10:55:12,916
average that

15400
10:55:13,960 --> 10:55:19,640
out um

15401
10:55:17,520 --> 10:55:20,880
we do that every single Epoch and then

15402
10:55:19,640 --> 10:55:24,116
inside of

15403
10:55:20,880 --> 10:55:27,080
here we simply just this just acts as

15404
10:55:24,116 --> 10:55:30,436
little increment for the correct counter

15405
10:55:27,080 --> 10:55:34,800
so this is just going to see okay well

15406
10:55:30,436 --> 10:55:34,800
um were we close or not um

15407
10:55:35,680 --> 10:55:41,200
so this should be self-explanatory um

15408
10:55:39,116 --> 10:55:42,520
this is also just like not necessarily

15409
10:55:41,200 --> 10:55:44,160
part of the training run but just like

15410
10:55:42,520 --> 10:55:46,320
an extra feature that you can use to

15411
10:55:44,160 --> 10:55:48,520
print out uh what the accuracy was over

15412
10:55:46,320 --> 10:55:49,880
time um not the loss but the actual

15413
10:55:48,520 --> 10:55:52,800
percent

15414
10:55:49,880 --> 10:55:56,320
accuracy the backward function so this

15415
10:55:52,800 --> 10:56:00,080
takes in you know NN and it takes a uh

15416
10:55:56,320 --> 10:56:02,560
pointer to uh to the input right so this

15417
10:56:00,080 --> 10:56:06,160
is this is a memory address to this at

15418
10:56:02,560 --> 10:56:09,200
this index which is going to be um

15419
10:56:06,160 --> 10:56:11,160
starting index times input size right

15420
10:56:09,200 --> 10:56:14,680
and then inside of here we pass in a few

15421
10:56:11,160 --> 10:56:16,000
things meaning hidden um you know this

15422
10:56:14,680 --> 10:56:17,560
neural net is going to contain all of

15423
10:56:16,000 --> 10:56:19,320
our all of our weights and stuff so

15424
10:56:17,560 --> 10:56:23,276
don't this is like all contained within

15425
10:56:19,320 --> 10:56:27,276
that uh the input itself

15426
10:56:23,276 --> 10:56:28,916
um hidden output labels and batch size

15427
10:56:27,276 --> 10:56:31,916
so very similar structure to the forward

15428
10:56:28,916 --> 10:56:34,880
pass except we also include uh we also

15429
10:56:31,916 --> 10:56:38,080
include labels

15430
10:56:34,880 --> 10:56:40,520
right update weights everything is now

15431
10:56:38,080 --> 10:56:43,480
updated after that and then we can print

15432
10:56:40,520 --> 10:56:44,560
out some useful stuff okay and that's

15433
10:56:43,480 --> 10:56:45,840
pretty much all that happens in this

15434
10:56:44,560 --> 10:56:49,360
training Loop a lot of it is just like

15435
10:56:45,840 --> 10:56:52,436
print and keeping track of stuff um but

15436
10:56:49,360 --> 10:56:56,160
yeah so going

15437
10:56:52,436 --> 10:56:57,840
up if we actually look at our uh if we

15438
10:56:56,160 --> 10:57:00,436
actually look at

15439
10:56:57,840 --> 10:57:02,880
our let me jump up to the forward path

15440
10:57:00,436 --> 10:57:06,436
where did this

15441
10:57:02,880 --> 10:57:09,800
go okay awesome so inside the forward um

15442
10:57:06,436 --> 10:57:14,596
we it's very simple right 1 2 3 four

15443
10:57:09,800 --> 10:57:16,916
five six um not too bad now inside of

15444
10:57:14,596 --> 10:57:18,160
here I added the extra sof Max just

15445
10:57:16,916 --> 10:57:19,756
because I didn't want to be redundant

15446
10:57:18,160 --> 10:57:21,400
and included in the whole like training

15447
10:57:19,756 --> 10:57:23,240
Loop thing there was a lot happening in

15448
10:57:21,400 --> 10:57:24,916
there it was it was quite you

15449
10:57:23,240 --> 10:57:27,116
complicated to sort through

15450
10:57:24,916 --> 10:57:28,320
everything but yeah it's it's really

15451
10:57:27,116 --> 10:57:30,680
helpful when you break things up into

15452
10:57:28,320 --> 10:57:31,916
smaller chunks this is super manageable

15453
10:57:30,680 --> 10:57:34,360
I wanted to make the forward and

15454
10:57:31,916 --> 10:57:35,596
backward pass as as modular as possible

15455
10:57:34,360 --> 10:57:37,240
so that you guys could like really

15456
10:57:35,596 --> 10:57:40,436
performance optimize it if you wanted to

15457
10:57:37,240 --> 10:57:42,916
like on the side um but this is like

15458
10:57:40,436 --> 10:57:44,720
literally identical to what we did in or

15459
10:57:42,916 --> 10:57:46,320
almost identical to what we did in the C

15460
10:57:44,720 --> 10:57:48,200
friendly function or the C friendly

15461
10:57:46,320 --> 10:57:50,080
script right so in there we had this

15462
10:57:48,200 --> 10:57:54,360
like linear forward method which would

15463
10:57:50,080 --> 10:57:56,200
do the M mole and the bias um and then

15464
10:57:54,360 --> 10:57:59,436
the linear backward which would do you

15465
10:57:56,200 --> 10:58:02,160
know two mmoles and and uh and and a

15466
10:57:59,436 --> 10:58:03,520
bias backward um but in this one we kind

15467
10:58:02,160 --> 10:58:05,840
of just split it into easier more

15468
10:58:03,520 --> 10:58:08,276
manageable chunks so a map mole

15469
10:58:05,840 --> 10:58:10,360
specifically uh is like an operation

15470
10:58:08,276 --> 10:58:12,200
that you can optimize on its own and so

15471
10:58:10,360 --> 10:58:14,240
like optimizing a linear forward or or

15472
10:58:12,200 --> 10:58:15,436
even more generally linear backward it's

15473
10:58:14,240 --> 10:58:16,560
like kind of hard to do that right you

15474
10:58:15,436 --> 10:58:18,596
have multiple things in there you have

15475
10:58:16,560 --> 10:58:21,116
to like fuse kernels together it's more

15476
10:58:18,596 --> 10:58:23,560
complicated so I decided to keep this

15477
10:58:21,116 --> 10:58:25,960
like as as manageable as possible super

15478
10:58:23,560 --> 10:58:31,560
hackable um you know

15479
10:58:25,960 --> 10:58:33,436
modular but in the map mole the

15480
10:58:31,560 --> 10:58:35,916
ab

15481
10:58:33,436 --> 10:58:38,560
um yeah this

15482
10:58:35,916 --> 10:58:41,916
is this is literally the same as C

15483
10:58:38,560 --> 10:58:43,680
friendly so if I pop this over here um

15484
10:58:41,916 --> 10:58:46,680
actually maybe I'll bring it actually

15485
10:58:43,680 --> 10:58:50,360
downward um

15486
10:58:46,680 --> 10:58:55,436
but in here linear forward right we just

15487
10:58:50,360 --> 10:58:57,116
do x * W this is x * W we're not uh x *

15488
10:58:55,436 --> 10:59:00,680
W A and B same thing we're not

15489
10:58:57,116 --> 10:59:02,320
transposing anything and inside of there

15490
10:59:00,680 --> 10:59:04,240
and I already went over this function

15491
10:59:02,320 --> 10:59:08,400
but you you kind of get the idea we just

15492
10:59:04,240 --> 10:59:11,640
do a maple between A and B that goes um

15493
10:59:08,400 --> 10:59:14,640
we do a bi bias forward so that's also

15494
10:59:11,640 --> 10:59:16,436
going to we pop over to there yeah I

15495
10:59:14,640 --> 10:59:19,276
already reviewed that too don't need to

15496
10:59:16,436 --> 10:59:21,756
go over that again Rue forward it's just

15497
10:59:19,276 --> 10:59:24,960
going to apply that to each element very

15498
10:59:21,756 --> 10:59:26,560
simple M Mo A and B so same thing again

15499
10:59:24,960 --> 10:59:28,360
except it's the hidden it's it's

15500
10:59:26,560 --> 10:59:30,400
actually the the hidden to Output

15501
10:59:28,360 --> 10:59:32,840
instead of the output to Hidden so next

15502
10:59:30,400 --> 10:59:35,040
one and then we do the bias forward

15503
10:59:32,840 --> 10:59:36,916
again and then softmax right so

15504
10:59:35,040 --> 10:59:39,000
obviously like this is very very simple

15505
10:59:36,916 --> 10:59:40,480
to follow um it's more like more of the

15506
10:59:39,000 --> 10:59:43,400
complexity happens within the actual

15507
10:59:40,480 --> 10:59:48,200
function so like this is a lot

15508
10:59:43,400 --> 10:59:50,520
more to handle than the than the uh than

15509
10:59:48,200 --> 10:59:55,756
just the the for pass function itself

15510
10:59:50,520 --> 11:00:00,800
right so I exit on that and then

15511
10:59:55,756 --> 11:00:03,040
um and then go down to say the backward

15512
11:00:00,800 --> 11:00:06,320
function which is a little

15513
11:00:03,040 --> 11:00:10,596
worse to be honest go down to here

15514
11:00:06,320 --> 11:00:13,756
backward function so inside of here

15515
11:00:10,596 --> 11:00:16,960
um we're going to zerog grad everything

15516
11:00:13,756 --> 11:00:20,276
so what this means is previously

15517
11:00:16,960 --> 11:00:23,756
our gradients we're accumulating items

15518
11:00:20,276 --> 11:00:25,080
right so our grad um our grad bias and

15519
11:00:23,756 --> 11:00:29,000
our grad weights are actually

15520
11:00:25,080 --> 11:00:30,680
accumulating stuff so we want to just

15521
11:00:29,000 --> 11:00:32,756
zero those out right this is the

15522
11:00:30,680 --> 11:00:34,840
equivalent of zero grad and P torch when

15523
11:00:32,756 --> 11:00:38,640
we go here

15524
11:00:34,840 --> 11:00:40,320
um we do Optimizer do step so that's

15525
11:00:38,640 --> 11:00:41,960
like the actual update weight so you you

15526
11:00:40,320 --> 11:00:43,800
know do the forward and then you

15527
11:00:41,960 --> 11:00:45,080
calculate the loss then you do backward

15528
11:00:43,800 --> 11:00:46,480
and then you update the weights with

15529
11:00:45,080 --> 11:00:48,320
gradient descent and and then you zero

15530
11:00:46,480 --> 11:00:52,680
grad so you're just zeroing out every

15531
11:00:48,320 --> 11:00:54,040
single gradient um in the entire network

15532
11:00:52,680 --> 11:00:56,640
and then you're going to recalculate

15533
11:00:54,040 --> 11:01:00,160
those when you do the next Optimizer do

15534
11:00:56,640 --> 11:01:02,640
step right or sorry no loss. backward

15535
11:01:00,160 --> 11:01:03,916
rather when you calculate the gradients

15536
11:01:02,640 --> 11:01:05,436
again they're not going to accumulate

15537
11:01:03,916 --> 11:01:07,116
further they're just going to set them

15538
11:01:05,436 --> 11:01:08,800
initially and you're going to update

15539
11:01:07,116 --> 11:01:11,840
based on those gradients and then and

15540
11:01:08,800 --> 11:01:14,596
then put them down to zero again

15541
11:01:11,840 --> 11:01:16,276
um so that that's all we do here we zero

15542
11:01:14,596 --> 11:01:19,200
grad and there's actually a function for

15543
11:01:16,276 --> 11:01:26,000
this so mem set literally just a c

15544
11:01:19,200 --> 11:01:27,320
function um Set Set n bytes of s to C so

15545
11:01:26,000 --> 11:01:32,916
Set

15546
11:01:27,320 --> 11:01:34,640
n set set um so byes of s which is the

15547
11:01:32,916 --> 11:01:37,960
first one so

15548
11:01:34,640 --> 11:01:41,360
grad and then C is the value we want to

15549
11:01:37,960 --> 11:01:44,960
set it to so zero and then the N the

15550
11:01:41,360 --> 11:01:46,400
length of it is just the size of grad

15551
11:01:44,960 --> 11:01:49,400
right so size

15552
11:01:46,400 --> 11:01:51,400
is the number of numbers and then a

15553
11:01:49,400 --> 11:01:53,200
float is like the number of bytes

15554
11:01:51,400 --> 11:01:55,960
essentially so the number of bytes that

15555
11:01:53,200 --> 11:01:59,436
grad occupies in memory we're going to

15556
11:01:55,960 --> 11:02:01,200
start from the beginning of that uh and

15557
11:01:59,436 --> 11:02:03,480
we're going to initialize all those

15558
11:02:01,200 --> 11:02:05,000
values at the at those sequential memory

15559
11:02:03,480 --> 11:02:07,200
addresses we're going to set those to

15560
11:02:05,000 --> 11:02:08,756
zero that's what zero grab does on a

15561
11:02:07,200 --> 11:02:11,960
very low

15562
11:02:08,756 --> 11:02:12,916
level um so we do that with our weights

15563
11:02:11,960 --> 11:02:15,320
and our

15564
11:02:12,916 --> 11:02:17,916
biases now we do a grad output so this

15565
11:02:15,320 --> 11:02:22,320
is just going to a malic batch size time

15566
11:02:17,916 --> 11:02:24,480
output size so B * 10 right B by 10 they

15567
11:02:22,320 --> 11:02:26,200
compute output gradients so we go to

15568
11:02:24,480 --> 11:02:27,840
this Lally all this is doing is it's

15569
11:02:26,200 --> 11:02:29,916
taking this this grat output which we

15570
11:02:27,840 --> 11:02:32,320
initialize everything is like zero and

15571
11:02:29,916 --> 11:02:34,480
we have this output which is the um

15572
11:02:32,320 --> 11:02:36,240
which remember from the forward pass is

15573
11:02:34,480 --> 11:02:37,596
the output of the softmax which is

15574
11:02:36,240 --> 11:02:40,320
actually a probability distribution it

15575
11:02:37,596 --> 11:02:41,680
is not low jits it's not before the

15576
11:02:40,320 --> 11:02:43,720
exponentiation it is actually a

15577
11:02:41,680 --> 11:02:46,116
probability distribution with each value

15578
11:02:43,720 --> 11:02:48,720
between 0 and one then we have the

15579
11:02:46,116 --> 11:02:50,720
labels and literally all we do here is

15580
11:02:48,720 --> 11:02:53,640
we just element wise we set the grad

15581
11:02:50,720 --> 11:02:55,360
output to um the actual output value so

15582
11:02:53,640 --> 11:02:58,200
which which uh which floating Point

15583
11:02:55,360 --> 11:03:01,756
number is it right and then we subtract

15584
11:02:58,200 --> 11:03:03,840
one based on the actual label of it so

15585
11:03:01,756 --> 11:03:06,640
notice how in before in C friendly we

15586
11:03:03,840 --> 11:03:09,000
did um we did out grad output equals

15587
11:03:06,640 --> 11:03:10,520
softmax probs minus y true we're doing

15588
11:03:09,000 --> 11:03:12,240
the same thing here except we can't just

15589
11:03:10,520 --> 11:03:14,560
do a simple numpy element wise

15590
11:03:12,240 --> 11:03:16,240
operations we can't just be be dumb and

15591
11:03:14,560 --> 11:03:18,400
say this right we have to actually

15592
11:03:16,240 --> 11:03:20,040
we have to be explicit and we can be a

15593
11:03:18,400 --> 11:03:25,116
little clever by just doing the minus

15594
11:03:20,040 --> 11:03:26,276
equals 1 uh there so uh that's it's

15595
11:03:25,116 --> 11:03:28,276
literally doing the same thing we're

15596
11:03:26,276 --> 11:03:30,756
just using a different

15597
11:03:28,276 --> 11:03:32,680
trick if we just continue going through

15598
11:03:30,756 --> 11:03:34,116
we just did compute output gradients now

15599
11:03:32,680 --> 11:03:36,320
we now we can actually use those

15600
11:03:34,116 --> 11:03:38,916
gradients and start using like Matrix

15601
11:03:36,320 --> 11:03:41,480
multiple highs and bias backwards and Ru

15602
11:03:38,916 --> 11:03:44,916
backwards and all this stuff right so

15603
11:03:41,480 --> 11:03:48,436
the first one here is Matt Mo um we go

15604
11:03:44,916 --> 11:03:52,720
through uh we we calculate w2. grad now

15605
11:03:48,436 --> 11:03:54,436
w2. grad is right here so x2. T times

15606
11:03:52,720 --> 11:03:57,560
the derivative of the loss which is grat

15607
11:03:54,436 --> 11:04:03,320
output so here we do a hidden which is

15608
11:03:57,560 --> 11:04:03,320
X2 here in this case um or

15609
11:04:03,360 --> 11:04:14,320
sorry yeah X2 so the um essentially the

15610
11:04:10,320 --> 11:04:16,800
the input going into that um and then

15611
11:04:14,320 --> 11:04:19,160
times so this is times B right so B is

15612
11:04:16,800 --> 11:04:21,680
in this case is the grad output so we're

15613
11:04:19,160 --> 11:04:23,200
transposing this with this a here and

15614
11:04:21,680 --> 11:04:25,520
then times this and then that's going to

15615
11:04:23,200 --> 11:04:29,840
equal C or the grad weights 2 right so

15616
11:04:25,520 --> 11:04:32,360
grad weights number W2 right

15617
11:04:29,840 --> 11:04:33,520
um yeah so that that that that should be

15618
11:04:32,360 --> 11:04:36,000
fairly

15619
11:04:33,520 --> 11:04:38,360
intuitive now we go further we go to

15620
11:04:36,000 --> 11:04:40,080
bias backward bias backward isn't

15621
11:04:38,360 --> 11:04:43,960
actually too bad so bias

15622
11:04:40,080 --> 11:04:46,276
backward is uh literally just going to

15623
11:04:43,960 --> 11:04:47,880
we're going to iterate through um the

15624
11:04:46,276 --> 11:04:49,000
size I mean keep in mind we have batch

15625
11:04:47,880 --> 11:04:52,000
size right so we're going to iterate

15626
11:04:49,000 --> 11:04:54,640
through the entire size of it uh you

15627
11:04:52,000 --> 11:04:58,276
know iterating and incrementing I each

15628
11:04:54,640 --> 11:05:03,320
time gra we're going to uh store uh a

15629
11:04:58,276 --> 11:05:05,680
bias value a gradient bias gradient this

15630
11:05:03,320 --> 11:05:08,720
index as

15631
11:05:05,680 --> 11:05:11,360
zero and we're going to iterate through

15632
11:05:08,720 --> 11:05:13,520
the entire batch size and we're

15633
11:05:11,360 --> 11:05:16,116
literally just going to set that

15634
11:05:13,520 --> 11:05:17,320
specific value we're going to iterate

15635
11:05:16,116 --> 11:05:18,840
remember we're iterating through the

15636
11:05:17,320 --> 11:05:20,480
entire batch size so like we were doing

15637
11:05:18,840 --> 11:05:23,596
before how we were like smooshing the

15638
11:05:20,480 --> 11:05:27,960
numbers we're going to go through that

15639
11:05:23,596 --> 11:05:29,680
we're going to just um essentially do B

15640
11:05:27,960 --> 11:05:33,400
times

15641
11:05:29,680 --> 11:05:37,520
size so the entire uh the entire length

15642
11:05:33,400 --> 11:05:39,040
of that the entire length of of um of

15643
11:05:37,520 --> 11:05:42,276
like a of like a

15644
11:05:39,040 --> 11:05:46,160
row and then plus

15645
11:05:42,276 --> 11:05:47,680
I and we're just going to set um um

15646
11:05:46,160 --> 11:05:49,436
because this is going to increase each

15647
11:05:47,680 --> 11:05:52,320
time so we're essentially just going

15648
11:05:49,436 --> 11:05:53,880
down one one row at a time right and

15649
11:05:52,320 --> 11:05:55,680
we're we're smooshing it together

15650
11:05:53,880 --> 11:05:58,840
because we're plus equal accumulating

15651
11:05:55,680 --> 11:06:01,080
that value here so our grad bias is the

15652
11:05:58,840 --> 11:06:03,640
this is the equivalent of np.sum across

15653
11:06:01,080 --> 11:06:07,560
axis zero keeping dims true

15654
11:06:03,640 --> 11:06:10,276
right uh and then we go back

15655
11:06:07,560 --> 11:06:13,116
to where is

15656
11:06:10,276 --> 11:06:15,320
it we pop back to the backward function

15657
11:06:13,116 --> 11:06:18,800
awesome so now we get bias backward we

15658
11:06:15,320 --> 11:06:21,080
pass in the uh grad bias right so we're

15659
11:06:18,800 --> 11:06:23,756
calculating the the gradient bias and we

15660
11:06:21,080 --> 11:06:25,800
pass in the grad so that would in this

15661
11:06:23,756 --> 11:06:29,040
case would be the grad output and then

15662
11:06:25,800 --> 11:06:30,436
batch size and size right so that should

15663
11:06:29,040 --> 11:06:31,960
be fairly intuitive it's just the

15664
11:06:30,436 --> 11:06:34,720
gradients are directly flowing and we

15665
11:06:31,960 --> 11:06:36,360
just do an accumulate operation across

15666
11:06:34,720 --> 11:06:38,000
the batch because we want to generalize

15667
11:06:36,360 --> 11:06:40,720
over a batch right it's more useful to

15668
11:06:38,000 --> 11:06:43,756
do that um and then we just do since

15669
11:06:40,720 --> 11:06:47,640
we're done the W2 now we actually move

15670
11:06:43,756 --> 11:06:49,720
on to uh the DX X2 right so

15671
11:06:47,640 --> 11:06:52,800
dx2 uh is right

15672
11:06:49,720 --> 11:06:54,040
here we Malik this cuz remember this is

15673
11:06:52,800 --> 11:06:55,640
just temporary we don't we don't

15674
11:06:54,040 --> 11:06:57,240
actually need to store this this doesn't

15675
11:06:55,640 --> 11:06:58,640
need to be updated anywhere we're just

15676
11:06:57,240 --> 11:07:01,840
calculating this because it's a

15677
11:06:58,640 --> 11:07:03,480
prerequisite for calculating W1 so we we

15678
11:07:01,840 --> 11:07:06,640
can free it after we don't need to store

15679
11:07:03,480 --> 11:07:09,320
this in memory just be like efficient um

15680
11:07:06,640 --> 11:07:12,840
and inside of here we literally just go

15681
11:07:09,320 --> 11:07:17,560
uh grad output time W2 transpose right

15682
11:07:12,840 --> 11:07:20,880
so we go grad output a W2 transpose T

15683
11:07:17,560 --> 11:07:24,916
and we store that in dx2 right going

15684
11:07:20,880 --> 11:07:29,520
down further D out which is this part

15685
11:07:24,916 --> 11:07:33,160
here d r out is dx2 * d r of

15686
11:07:29,520 --> 11:07:35,480
X we allocate memory for this we go

15687
11:07:33,160 --> 11:07:40,436
through um we go through the entire

15688
11:07:35,480 --> 11:07:42,160
batch we literally just do dx2 so dx2

15689
11:07:40,436 --> 11:07:43,916
time

15690
11:07:42,160 --> 11:07:47,080
D

15691
11:07:43,916 --> 11:07:49,916
um d of of whatever of whatever that

15692
11:07:47,080 --> 11:07:53,436
value is just just going through it

15693
11:07:49,916 --> 11:07:58,240
right and this is going to be um if this

15694
11:07:53,436 --> 11:08:02,160
is this is going to evaluate to a a true

15695
11:07:58,240 --> 11:08:05,800
so or essentially a one if this if this

15696
11:08:02,160 --> 11:08:08,520
if this value is um essentially just

15697
11:08:05,800 --> 11:08:11,240
relue derivative right Rue derivative

15698
11:08:08,520 --> 11:08:12,800
that's all this is um we might not

15699
11:08:11,240 --> 11:08:14,160
actually even need the Rue derivative

15700
11:08:12,800 --> 11:08:16,116
backwards in in this whole thing but

15701
11:08:14,160 --> 11:08:19,436
we're going to keep it anyways um this

15702
11:08:16,116 --> 11:08:23,276
this does work though so this stores the

15703
11:08:19,436 --> 11:08:27,840
D out that's what we want right d out is

15704
11:08:23,276 --> 11:08:31,200
good then we update using the using the

15705
11:08:27,840 --> 11:08:34,276
um what's it called we calculate D out

15706
11:08:31,200 --> 11:08:36,520
from dx2 which we which we used up here

15707
11:08:34,276 --> 11:08:38,240
right we stored that stored the gradient

15708
11:08:36,520 --> 11:08:39,916
for that the temporary gradient there we

15709
11:08:38,240 --> 11:08:42,400
don't need it for later we just need it

15710
11:08:39,916 --> 11:08:45,640
for now calculate dout based on that and

15711
11:08:42,400 --> 11:08:49,840
then we use D out d

15712
11:08:45,640 --> 11:08:55,720
Rue out uh later and we essentially go

15713
11:08:49,840 --> 11:08:58,480
to calculate W1 we go x1. t time d r out

15714
11:08:55,720 --> 11:09:00,360
right so transpose so a tore B that's

15715
11:08:58,480 --> 11:09:06,080
what this is going to

15716
11:09:00,360 --> 11:09:10,720
be a tore B we go input

15717
11:09:06,080 --> 11:09:13,000
so um input X1 transpose that times D

15718
11:09:10,720 --> 11:09:15,560
value out and store that in we it's

15719
11:09:13,000 --> 11:09:18,720
one awesome

15720
11:09:15,560 --> 11:09:23,960
bias backward as as per usual so we just

15721
11:09:18,720 --> 11:09:27,040
have the uh the bias itself um and then

15722
11:09:23,960 --> 11:09:30,720
the the one that it's flowing uh flowing

15723
11:09:27,040 --> 11:09:33,320
from right so whatever it's whatever the

15724
11:09:30,720 --> 11:09:35,000
um whatever the previous layer gradient

15725
11:09:33,320 --> 11:09:38,200
was that's just going to flow directly

15726
11:09:35,000 --> 11:09:40,720
into bias right because um you know

15727
11:09:38,200 --> 11:09:42,436
adding adding does not uh change the

15728
11:09:40,720 --> 11:09:44,520
gradient of something it just changes

15729
11:09:42,436 --> 11:09:47,360
the like position the offset of it but

15730
11:09:44,520 --> 11:09:48,596
the slope remains same um but yeah

15731
11:09:47,360 --> 11:09:51,080
that's that's literally the backward

15732
11:09:48,596 --> 11:09:53,080
pass not too bad that might have been

15733
11:09:51,080 --> 11:09:54,916
like a little hard to keep up with my my

15734
11:09:53,080 --> 11:09:58,560
D you outs constantly it might have

15735
11:09:54,916 --> 11:10:01,960
confused you but uh yeah that's that's

15736
11:09:58,560 --> 11:10:05,040
pretty much it and then inside of um

15737
11:10:01,960 --> 11:10:08,560
inside of yeah so that that's backward

15738
11:10:05,040 --> 11:10:10,160
and then after backward after this we do

15739
11:10:08,560 --> 11:10:11,960
update weights then we print some stuff

15740
11:10:10,160 --> 11:10:13,200
out so let's just pay attention to

15741
11:10:11,960 --> 11:10:14,640
update weights here don't don't worry

15742
11:10:13,200 --> 11:10:16,240
about the rest of this you can parse

15743
11:10:14,640 --> 11:10:17,840
this on your own what we really care

15744
11:10:16,240 --> 11:10:19,720
about is the actual learning mechanics

15745
11:10:17,840 --> 11:10:22,320
of it right you can print anything out

15746
11:10:19,720 --> 11:10:24,040
any day you want it's very easy um but

15747
11:10:22,320 --> 11:10:25,640
we care about what the actual mechanism

15748
11:10:24,040 --> 11:10:28,756
is

15749
11:10:25,640 --> 11:10:31,000
here so if I go to update weights we

15750
11:10:28,756 --> 11:10:34,276
pass in the neuronet struct right the

15751
11:10:31,000 --> 11:10:37,436
pointer to that we access we go through

15752
11:10:34,276 --> 11:10:39,480
each one in here and we literally just

15753
11:10:37,436 --> 11:10:41,000
um we just do the same thing so hidden

15754
11:10:39,480 --> 11:10:45,436
size times input size so that's going to

15755
11:10:41,000 --> 11:10:48,880
be um you know 784 by 256 that's the

15756
11:10:45,436 --> 11:10:50,160
weight one we we iterate through each

15757
11:10:48,880 --> 11:10:51,640
index remember this is laid out

15758
11:10:50,160 --> 11:10:54,560
sequentially in memory so this is going

15759
11:10:51,640 --> 11:10:56,360
to evaluate to whatever 784 by 256 is

15760
11:10:54,560 --> 11:10:57,400
that's a large value just going to

15761
11:10:56,360 --> 11:10:59,916
iterate through that in memory it's

15762
11:10:57,400 --> 11:11:03,596
going to go through the lines or this

15763
11:10:59,916 --> 11:11:05,320
straight thing and it's just going to do

15764
11:11:03,596 --> 11:11:06,916
learning rate times whatever the grad

15765
11:11:05,320 --> 11:11:09,916
was and then it's going to minus equals

15766
11:11:06,916 --> 11:11:11,436
accumulate that into um weights one it's

15767
11:11:09,916 --> 11:11:14,116
going to do the same thing for weights

15768
11:11:11,436 --> 11:11:16,400
two remember weights two is um output

15769
11:11:14,116 --> 11:11:21,200
size times hidden size so hidden size

15770
11:11:16,400 --> 11:11:24,560
256 output size is 10 um bias hidden

15771
11:11:21,200 --> 11:11:27,520
size two out

15772
11:11:24,560 --> 11:11:29,520
size very

15773
11:11:27,520 --> 11:11:31,200
straightforward and that's pretty much

15774
11:11:29,520 --> 11:11:34,360
it

15775
11:11:31,200 --> 11:11:36,800
um let's go ahead and train this

15776
11:11:34,360 --> 11:11:39,160
thing so going back up let me just make

15777
11:11:36,800 --> 11:11:42,680
sure these are all set

15778
11:11:39,160 --> 11:11:44,720
correctly we'll do 256 sure we'll do

15779
11:11:42,680 --> 11:11:46,040
batch size of four that's fine learning

15780
11:11:44,720 --> 11:11:49,436
rate

15781
11:11:46,040 --> 11:11:50,480
0.01 that's okay as well we'll set theox

15782
11:11:49,436 --> 11:11:54,160
to

15783
11:11:50,480 --> 11:11:54,160
five just to be less

15784
11:11:54,240 --> 11:11:59,436
redundant or we we set it to actually

15785
11:11:56,800 --> 11:12:04,400
set it to three why not so now we go

15786
11:11:59,436 --> 11:12:05,640
into here GCC Das o V1 and then V1 doc

15787
11:12:04,400 --> 11:12:08,320
which is the file we're going to compile

15788
11:12:05,640 --> 11:12:10,436
and we do LM for link math right so

15789
11:12:08,320 --> 11:12:12,916
inside of here we do the math.h file we

15790
11:12:10,436 --> 11:12:17,000
need to link math for this to work

15791
11:12:12,916 --> 11:12:18,520
because if we don't right and um

15792
11:12:17,000 --> 11:12:19,960
unidentified reference to this these are

15793
11:12:18,520 --> 11:12:21,400
all the math functions right but if we

15794
11:12:19,960 --> 11:12:24,720
do

15795
11:12:21,400 --> 11:12:26,640
LM it'll work we can go and run this so

15796
11:12:24,720 --> 11:12:28,840
we get the first this is a five printed

15797
11:12:26,640 --> 11:12:33,240
out first Trend training labels is five

15798
11:12:28,840 --> 11:12:34,916
as we see here and then 04 1 9 2 1 31 4

15799
11:12:33,240 --> 11:12:37,916
and we can see the accuracy starts off

15800
11:12:34,916 --> 11:12:41,276
at about well the loss is about 2.3

15801
11:12:37,916 --> 11:12:44,640
which is uh random as we'd expect it to

15802
11:12:41,276 --> 11:12:46,840
so 2.3 evaluates to about 10% accuracy

15803
11:12:44,640 --> 11:12:49,960
and we can see that through the first

15804
11:12:46,840 --> 11:12:51,520
Epoch it goes through 2 200 HS and we

15805
11:12:49,960 --> 11:12:54,200
can see that the

15806
11:12:51,520 --> 11:12:56,240
accuracy goes up to about 60% which is

15807
11:12:54,200 --> 11:12:58,720
solid right loss is going

15808
11:12:56,240 --> 11:13:00,360
down and then in the next one it sees oh

15809
11:12:58,720 --> 11:13:01,916
my gosh wait we've seen these samples

15810
11:13:00,360 --> 11:13:04,000
before and it's going to drop even

15811
11:13:01,916 --> 11:13:06,720
further it's going to go down to 088 and

15812
11:13:04,000 --> 11:13:08,680
accuracy is going to fly up you know 15%

15813
11:13:06,720 --> 11:13:10,840
because it's already seen these samples

15814
11:13:08,680 --> 11:13:12,640
before um and then it's going to do that

15815
11:13:10,840 --> 11:13:16,480
again it's going to go up to

15816
11:13:12,640 --> 11:13:20,916
86% and we end up with about 88 8.6%

15817
11:13:16,480 --> 11:13:25,200
here um and uh yeah I mean you could

15818
11:13:20,916 --> 11:13:28,916
always print out the the ending samples

15819
11:13:25,200 --> 11:13:30,480
if you really wanted to um you could

15820
11:13:28,916 --> 11:13:33,116
always print out like some extra samples

15821
11:13:30,480 --> 11:13:35,840
and just like how it um how it matches

15822
11:13:33,116 --> 11:13:39,960
those up um but we're going to notice

15823
11:13:35,840 --> 11:13:42,116
that in our uh in our Cuda file so uh

15824
11:13:39,960 --> 11:13:43,840
anyways that that was a lot to unpack

15825
11:13:42,116 --> 11:13:46,276
there but that is the C file that's how

15826
11:13:43,840 --> 11:13:47,640
we transfer from numpy to see it's not

15827
11:13:46,276 --> 11:13:49,436
actually that crazy of a jump it's

15828
11:13:47,640 --> 11:13:52,160
mainly just writing the algorithm you

15829
11:13:49,436 --> 11:13:53,640
know the hard way um and just kind of

15830
11:13:52,160 --> 11:13:55,756
being more aware about things right it's

15831
11:13:53,640 --> 11:13:57,436
very easy to run into issues but as long

15832
11:13:55,756 --> 11:13:59,436
as we're careful about things it

15833
11:13:57,436 --> 11:14:02,000
shouldn't be too bad let's go and jump

15834
11:13:59,436 --> 11:14:03,720
into Cuda now okay okay everyone so this

15835
11:14:02,000 --> 11:14:05,720
is one of the last parts of the course

15836
11:14:03,720 --> 11:14:07,960
actually and this is part this part is

15837
11:14:05,720 --> 11:14:09,960
intended to be is intended to be a

15838
11:14:07,960 --> 11:14:11,800
little shorter so this is designed to

15839
11:14:09,960 --> 11:14:14,200
give you s of sort of a template for

15840
11:14:11,800 --> 11:14:16,240
continuing on this is the final project

15841
11:14:14,200 --> 11:14:18,200
right so uh I'm not going to give you

15842
11:14:16,240 --> 11:14:19,840
all the answers right away a part of

15843
11:14:18,200 --> 11:14:21,680
your job is to figure this out on your

15844
11:14:19,840 --> 11:14:23,720
own and use what we've developed

15845
11:14:21,680 --> 11:14:29,400
beforehand to continue and optimize

15846
11:14:23,720 --> 11:14:34,480
performance further right so I have this

15847
11:14:29,400 --> 11:14:37,000
um I have this naive GPU file right here

15848
11:14:34,480 --> 11:14:39,160
uh or sorry folder inside of Cuda so we

15849
11:14:37,000 --> 11:14:39,160
go

15850
11:14:39,276 --> 11:14:45,596
to we go to um go to the Cuda directory

15851
11:14:43,640 --> 11:14:49,160
and then we CD into n

15852
11:14:45,596 --> 11:14:50,880
View and inside of here um you're going

15853
11:14:49,160 --> 11:14:52,596
to find this file this vew one right I

15854
11:14:50,880 --> 11:14:54,436
just have this as like versions so you

15855
11:14:52,596 --> 11:14:57,160
can I'll like update more versions later

15856
11:14:54,436 --> 11:14:59,916
on if something breaks uh in future Cuda

15857
11:14:57,160 --> 11:15:01,680
releases or whatever but um yeah so this

15858
11:14:59,916 --> 11:15:05,520
is essentially just a direct Port from

15859
11:15:01,680 --> 11:15:08,680
our our C file so literally all we do

15860
11:15:05,520 --> 11:15:10,080
here is we we load in the same things um

15861
11:15:08,680 --> 11:15:12,360
we initialize way it's the same all

15862
11:15:10,080 --> 11:15:14,276
these are done on CPU as you can see the

15863
11:15:12,360 --> 11:15:17,720
only things that we actually change are

15864
11:15:14,276 --> 11:15:19,200
the m Kels right so we can see the well

15865
11:15:17,720 --> 11:15:22,756
there's more than just mapal kernels but

15866
11:15:19,200 --> 11:15:25,400
you can see so we have this mmal A and B

15867
11:15:22,756 --> 11:15:27,480
um so this is this is not

15868
11:15:25,400 --> 11:15:30,200
transposed uh and then this is B

15869
11:15:27,480 --> 11:15:31,596
transposed and then this is a transposed

15870
11:15:30,200 --> 11:15:33,680
right so when we're doing our backward

15871
11:15:31,596 --> 11:15:35,800
pass and we need to transpose certain

15872
11:15:33,680 --> 11:15:37,160
values uh that's what we'll use that for

15873
11:15:35,800 --> 11:15:39,116
right so we have certain kernels that

15874
11:15:37,160 --> 11:15:41,840
dedicated for that and we can actually

15875
11:15:39,116 --> 11:15:44,520
see based on the indexing scheme in here

15876
11:15:41,840 --> 11:15:47,080
um like notice how we we iterate Over N

15877
11:15:44,520 --> 11:15:48,116
every sing single time except this one

15878
11:15:47,080 --> 11:15:49,640
this one is a little different this one

15879
11:15:48,116 --> 11:15:53,680
is M

15880
11:15:49,640 --> 11:15:55,640
but uh if we go in here we can see a

15881
11:15:53,680 --> 11:15:57,080
this is just the normal one right so row

15882
11:15:55,640 --> 11:16:01,960
*

15883
11:15:57,080 --> 11:16:05,596
n u plus I and then if we go to this one

15884
11:16:01,960 --> 11:16:07,880
B it's I * k + column right and this is

15885
11:16:05,596 --> 11:16:10,436
different so notice how this a stays the

15886
11:16:07,880 --> 11:16:12,160
same but because we're changing B and

15887
11:16:10,436 --> 11:16:14,560
making it transpose this part this

15888
11:16:12,160 --> 11:16:20,960
indexing changes right and then same

15889
11:16:14,560 --> 11:16:24,400
idea here is uh we just transpose a so

15890
11:16:20,960 --> 11:16:27,880
this part I K and then column i k and

15891
11:16:24,400 --> 11:16:30,360
then column and um this part is going to

15892
11:16:27,880 --> 11:16:33,436
be different right so instead of row N I

15893
11:16:30,360 --> 11:16:33,436
it's I N

15894
11:16:33,560 --> 11:16:40,000
row and

15895
11:16:36,520 --> 11:16:41,720
uh yeah that's those are pretty much the

15896
11:16:40,000 --> 11:16:43,116
major changes the reason why we do this

15897
11:16:41,720 --> 11:16:46,400
for

15898
11:16:43,116 --> 11:16:49,596
um we do this for a is because a is

15899
11:16:46,400 --> 11:16:51,080
transposed um and and the m is m is a

15900
11:16:49,596 --> 11:16:52,596
little different but these are these are

15901
11:16:51,080 --> 11:16:55,720
naive kernels I expect you able to

15902
11:16:52,596 --> 11:16:58,960
dissect these but if we continue further

15903
11:16:55,720 --> 11:17:00,596
we have the ru we have our bias uh we

15904
11:16:58,960 --> 11:17:04,200
have a softmax kernel so this is going

15905
11:17:00,596 --> 11:17:05,720
to do a single softmax uh output so a

15906
11:17:04,200 --> 11:17:07,276
distribution for every single batch

15907
11:17:05,720 --> 11:17:09,680
element so it's going to go vertically

15908
11:17:07,276 --> 11:17:12,640
downwards each each thread is going to

15909
11:17:09,680 --> 11:17:12,640
do a a

15910
11:17:13,080 --> 11:17:16,756
row we zero our great gradiance with

15911
11:17:15,276 --> 11:17:18,560
this simple kernel so it's just going to

15912
11:17:16,756 --> 11:17:20,080
go you know it's going to go through

15913
11:17:18,560 --> 11:17:23,756
every single value and just set that to

15914
11:17:20,080 --> 11:17:26,916
zero um probably faster than mem set I'm

15915
11:17:23,756 --> 11:17:29,400
not sure but and then we have our D

15916
11:17:26,916 --> 11:17:35,000
kernel so this is just the derivative of

15917
11:17:29,400 --> 11:17:37,956
Ru multiply gradients um so element wise

15918
11:17:35,000 --> 11:17:39,276
multiplication of gradients which we can

15919
11:17:37,956 --> 11:17:42,916
see is

15920
11:17:39,276 --> 11:17:46,720
used down here in multiply

15921
11:17:42,916 --> 11:17:46,720
gradients and this function

15922
11:17:47,240 --> 11:17:52,560
is used uh is used right here so when

15923
11:17:49,560 --> 11:17:55,436
we're doing our when we're doing our Rue

15924
11:17:52,560 --> 11:17:59,480
um when we're we're doing our actual uh

15925
11:17:55,436 --> 11:18:01,436
we're going through D and we need to um

15926
11:17:59,480 --> 11:18:04,720
multiply those values it kind of makes

15927
11:18:01,436 --> 11:18:05,756
sense why we would put that there um

15928
11:18:04,720 --> 11:18:08,276
going back

15929
11:18:05,756 --> 11:18:12,560
up we just have our forward so the

15930
11:18:08,276 --> 11:18:15,560
typical a * B I don't know why this is T

15931
11:18:12,560 --> 11:18:15,560
um

15932
11:18:16,720 --> 11:18:22,956
okay

15933
11:18:18,436 --> 11:18:22,956
um Bim is not working all

15934
11:18:25,400 --> 11:18:32,800
right and we just do a ml add the bias

15935
11:18:28,916 --> 11:18:34,956
Ru ml add a bias then softmax right very

15936
11:18:32,800 --> 11:18:37,640
simple cross entropy loss is going to be

15937
11:18:34,956 --> 11:18:40,240
done on the host we're going to compute

15938
11:18:37,640 --> 11:18:41,840
our output gradients on the GPU because

15939
11:18:40,240 --> 11:18:43,640
there there is actually going to be a

15940
11:18:41,840 --> 11:18:45,720
lot of you have to consider when you're

15941
11:18:43,640 --> 11:18:47,560
actually writing Kel you're like what is

15942
11:18:45,720 --> 11:18:50,916
the how useful is it right to actually

15943
11:18:47,560 --> 11:18:54,160
go and write your own um actually go and

15944
11:18:50,916 --> 11:18:56,520
write your own so like for example this

15945
11:18:54,160 --> 11:18:59,040
one we could probably turn this into CPU

15946
11:18:56,520 --> 11:19:01,000
and it might be faster but who knows um

15947
11:18:59,040 --> 11:19:02,680
the kind of the idea here is like if you

15948
11:19:01,000 --> 11:19:04,800
have a big one if you have a big update

15949
11:19:02,680 --> 11:19:07,400
to do like update to gradients and you

15950
11:19:04,800 --> 11:19:09,596
have these big giant um you have these

15951
11:19:07,400 --> 11:19:11,956
giant W matrices that you're trying to

15952
11:19:09,596 --> 11:19:14,116
change and modify then having a thread

15953
11:19:11,956 --> 11:19:16,320
to do each little uh to do each little

15954
11:19:14,116 --> 11:19:18,400
like point update will help speed that

15955
11:19:16,320 --> 11:19:20,400
up but if you're doing just like a B

15956
11:19:18,400 --> 11:19:21,520
by10 for example which is what this is

15957
11:19:20,400 --> 11:19:23,240
you could probably get away with just

15958
11:19:21,520 --> 11:19:25,080
doing this really fast on CPU and you'd

15959
11:19:23,240 --> 11:19:26,680
be you'd be fine uh cuz there is like

15960
11:19:25,080 --> 11:19:28,560
kernel launch overhead when you have to

15961
11:19:26,680 --> 11:19:30,320
like literally launch this it has to

15962
11:19:28,560 --> 11:19:32,436
tell the GPU what to do and then it has

15963
11:19:30,320 --> 11:19:35,320
to trigger a bunch of threads to go and

15964
11:19:32,436 --> 11:19:37,520
execute that right um and then in our

15965
11:19:35,320 --> 11:19:38,360
backward pass we zero out the gradients

15966
11:19:37,520 --> 11:19:40,240
to make sure that they're not

15967
11:19:38,360 --> 11:19:42,916
accumulating and giving us you know mix

15968
11:19:40,240 --> 11:19:45,200
signals

15969
11:19:42,916 --> 11:19:47,276
um we have our

15970
11:19:45,200 --> 11:19:49,520
uh compute compute output gradient so

15971
11:19:47,276 --> 11:19:52,360
that's going to be the essentially the

15972
11:19:49,520 --> 11:19:54,080
these the output probabilities minus the

15973
11:19:52,360 --> 11:19:57,596
the true labels

15974
11:19:54,080 --> 11:20:02,800
right and then we have our our a

15975
11:19:57,596 --> 11:20:05,160
transpose times B so we go back to here

15976
11:20:02,800 --> 11:20:09,640
um this this

15977
11:20:05,160 --> 11:20:11,640
dw2 right we update um we update

15978
11:20:09,640 --> 11:20:13,756
gradients for bias 2 so there's specific

15979
11:20:11,640 --> 11:20:15,360
kernel for that now remember when we're

15980
11:20:13,756 --> 11:20:17,276
launching these there's it it seems like

15981
11:20:15,360 --> 11:20:20,080
there's a lot to sort through but really

15982
11:20:17,276 --> 11:20:21,840
all this is is looking at that previous

15983
11:20:20,080 --> 11:20:25,880
Technique we did where it's like you you

15984
11:20:21,840 --> 11:20:28,320
have to launch um say if you have 1,25

15985
11:20:25,880 --> 11:20:29,520
elements and you would normally only do

15986
11:20:28,320 --> 11:20:32,640
um like

15987
11:20:29,520 --> 11:20:35,160
1,024 uh threads right so what you do is

15988
11:20:32,640 --> 11:20:38,200
you add on add an additional block right

15989
11:20:35,160 --> 11:20:39,880
with a with a single um with a single

15990
11:20:38,200 --> 11:20:42,680
thread inside of it or or just or

15991
11:20:39,880 --> 11:20:45,400
another Block in general you could say

15992
11:20:42,680 --> 11:20:47,000
and since you have those bounds the like

15993
11:20:45,400 --> 11:20:49,276
that little if statement that checks if

15994
11:20:47,000 --> 11:20:51,080
everything is is worth so that it's not

15995
11:20:49,276 --> 11:20:53,200
like out of the out of the matrices that

15996
11:20:51,080 --> 11:20:55,720
you're working with like out like outer

15997
11:20:53,200 --> 11:20:56,880
outside of that memory um then you can

15998
11:20:55,720 --> 11:20:58,160
that that's essentially what this is

15999
11:20:56,880 --> 11:21:01,436
right so we're just being careful about

16000
11:20:58,160 --> 11:21:02,880
how we launch this stuff um and yeah

16001
11:21:01,436 --> 11:21:05,436
this this goes back to the you know

16002
11:21:02,880 --> 11:21:08,560
chapter number five on on kernels right

16003
11:21:05,436 --> 11:21:10,160
so uh these launch configurations can

16004
11:21:08,560 --> 11:21:12,436
like really mess you up and make things

16005
11:21:10,160 --> 11:21:13,596
look more complicated than they are but

16006
11:21:12,436 --> 11:21:17,200
if you can just like look through this

16007
11:21:13,596 --> 11:21:19,116
one bit at a time you have the uh you

16008
11:21:17,200 --> 11:21:21,596
have like the grid size and sorry the

16009
11:21:19,116 --> 11:21:23,520
grid dim and the block dim right and and

16010
11:21:21,596 --> 11:21:25,680
that's all you're really working with

16011
11:21:23,520 --> 11:21:28,560
and then of course the the arguments for

16012
11:21:25,680 --> 11:21:28,560
the kernel launch

16013
11:21:32,480 --> 11:21:41,200
itself more mmols right going back to

16014
11:21:36,200 --> 11:21:44,640
here this times so it's a * B

16015
11:21:41,200 --> 11:21:46,276
transpose a * B transpose

16016
11:21:44,640 --> 11:21:47,916
uh and then inside here we just do our

16017
11:21:46,276 --> 11:21:49,756
we just go backward through our ra

16018
11:21:47,916 --> 11:21:51,436
function all this is like very close to

16019
11:21:49,756 --> 11:21:55,916
our C file it's just like it's just run

16020
11:21:51,436 --> 11:21:57,720
in parallel right um same idea here I'm

16021
11:21:55,916 --> 11:22:00,320
just going to go further downwards

16022
11:21:57,720 --> 11:22:02,520
updating our weights um you know it's

16023
11:22:00,320 --> 11:22:02,520
kind

16024
11:22:12,360 --> 11:22:16,596
of I guess I have two of these

16025
11:22:15,320 --> 11:22:18,956
are we using

16026
11:22:16,596 --> 11:22:23,360
this let me

16027
11:22:18,956 --> 11:22:26,520
see yes we are we're using that one but

16028
11:22:23,360 --> 11:22:28,956
this one update

16029
11:22:26,520 --> 11:22:31,000
gradients yeah that's also okay so so

16030
11:22:28,956 --> 11:22:33,000
these are different things right yeah so

16031
11:22:31,000 --> 11:22:35,480
the the update gradients that's for the

16032
11:22:33,000 --> 11:22:37,200
bias and the update weights that is for

16033
11:22:35,480 --> 11:22:40,080
um that that is actually where gradient

16034
11:22:37,200 --> 11:22:44,880
descent itself takes place

16035
11:22:40,080 --> 11:22:44,880
um and then inside of this Loop

16036
11:22:46,400 --> 11:22:50,320
we go through uh initializing our our

16037
11:22:48,640 --> 11:22:53,040
device training sets so when you have

16038
11:22:50,320 --> 11:22:55,080
this D prefix remember that's for device

16039
11:22:53,040 --> 11:22:58,480
um you know Y is labels train is the

16040
11:22:55,080 --> 11:23:00,240
train set we C AAL all of those we mem

16041
11:22:58,480 --> 11:23:02,276
copy them over to device because they're

16042
11:23:00,240 --> 11:23:04,200
initialized as pointers on the host and

16043
11:23:02,276 --> 11:23:07,640
we have to copy those over with respect

16044
11:23:04,200 --> 11:23:09,520
to their memory addresses over to device

16045
11:23:07,640 --> 11:23:11,000
um and then aside of here we just

16046
11:23:09,520 --> 11:23:12,640
iterate over all the epochs we need to

16047
11:23:11,000 --> 11:23:16,160
do which in this case is Define at the

16048
11:23:12,640 --> 11:23:18,320
top and then number of batches right and

16049
11:23:16,160 --> 11:23:20,560
then we we set a starting index we make

16050
11:23:18,320 --> 11:23:22,116
sure to like stride a little bit so we

16051
11:23:20,560 --> 11:23:23,720
have whichever batch we're at here so

16052
11:23:22,116 --> 11:23:25,040
number of batches in the total thing

16053
11:23:23,720 --> 11:23:27,320
which we actually calculate by doing

16054
11:23:25,040 --> 11:23:30,000
train size number of divide by batch

16055
11:23:27,320 --> 11:23:32,560
size so if you have a th 10,000 training

16056
11:23:30,000 --> 11:23:35,240
uh examples and a batch size of four

16057
11:23:32,560 --> 11:23:38,240
you're going to have um 2500 batches

16058
11:23:35,240 --> 11:23:39,640
right and so when you do um whichever

16059
11:23:38,240 --> 11:23:41,200
batch you're at which is going to go

16060
11:23:39,640 --> 11:23:45,080
through you know it's essentially going

16061
11:23:41,200 --> 11:23:50,276
to go zero uh three

16062
11:23:45,080 --> 11:23:50,276
um 0 3

16063
11:23:51,000 --> 11:23:59,080
7 11 it's going to skip by four

16064
11:23:56,160 --> 11:24:01,756
right we do our forward pass so keeping

16065
11:23:59,080 --> 11:24:04,480
this uh this simple this nice little

16066
11:24:01,756 --> 11:24:09,320
concise NN struct right with all of our

16067
11:24:04,480 --> 11:24:13,436
our gradients and our our weights

16068
11:24:09,320 --> 11:24:17,000
right we calculate the uh we we C

16069
11:24:13,436 --> 11:24:19,640
essentially what this part is doing is

16070
11:24:17,000 --> 11:24:22,956
we're calculating the loss we're adding

16071
11:24:19,640 --> 11:24:26,200
it to the total running loss

16072
11:24:22,956 --> 11:24:26,200
of where is

16073
11:24:27,436 --> 11:24:32,400
it on this level we have we initialize

16074
11:24:30,436 --> 11:24:34,080
the total loss inside of the epoch Loop

16075
11:24:32,400 --> 11:24:35,116
right so this is for the entire Epoch

16076
11:24:34,080 --> 11:24:37,040
that's why we're that's what we're kind

16077
11:24:35,116 --> 11:24:39,040
of doing there is when we add the loss

16078
11:24:37,040 --> 11:24:40,640
we're just like appending it and then

16079
11:24:39,040 --> 11:24:42,116
we're you know dividing it accordingly

16080
11:24:40,640 --> 11:24:45,116
so we're just taking like the average

16081
11:24:42,116 --> 11:24:49,756
loss over the entire epoch

16082
11:24:45,116 --> 11:24:51,520
um in this case we are essentially just

16083
11:24:49,756 --> 11:24:55,116
like we were doing in the C file we're

16084
11:24:51,520 --> 11:24:56,640
just um adding to the correct counter so

16085
11:24:55,116 --> 11:24:58,200
however many samples we got correct

16086
11:24:56,640 --> 11:25:01,360
that's that percent

16087
11:24:58,200 --> 11:25:03,240
accuracy going into backward pass I

16088
11:25:01,360 --> 11:25:05,000
already walked through this there mean

16089
11:25:03,240 --> 11:25:06,320
you can kind of sort through all these

16090
11:25:05,000 --> 11:25:09,240
different

16091
11:25:06,320 --> 11:25:11,116
arguments uh we update we do an update

16092
11:25:09,240 --> 11:25:14,436
weights for each of our individual

16093
11:25:11,116 --> 11:25:15,800
weight matrices um so it's just going to

16094
11:25:14,436 --> 11:25:19,200
essentially element

16095
11:25:15,800 --> 11:25:21,276
wise uh it's going to element wise

16096
11:25:19,200 --> 11:25:22,756
multiply um the gradient times the

16097
11:25:21,276 --> 11:25:25,840
learning rate and it's going to

16098
11:25:22,756 --> 11:25:27,756
accumulate that into the weight right on

16099
11:25:25,840 --> 11:25:30,720
device and then in here we just print

16100
11:25:27,756 --> 11:25:33,040
some useful stuff right go down here

16101
11:25:30,720 --> 11:25:36,276
make sure that we free the training sets

16102
11:25:33,040 --> 11:25:38,400
uh the hidden and the D output that we

16103
11:25:36,276 --> 11:25:41,480
initialized

16104
11:25:38,400 --> 11:25:43,720
before we have our initialization of the

16105
11:25:41,480 --> 11:25:45,360
entire neural net so just essentially

16106
11:25:43,720 --> 11:25:47,800
doing our

16107
11:25:45,360 --> 11:25:49,200
malic initializing those so each of

16108
11:25:47,800 --> 11:25:50,840
these are going to be set to like random

16109
11:25:49,200 --> 11:25:53,116
values or in this case biases are going

16110
11:25:50,840 --> 11:25:56,160
to be set to zero and then we do our

16111
11:25:53,116 --> 11:25:59,400
Cuda Malik so we so we allocate on on

16112
11:25:56,160 --> 11:26:02,080
CPU we initialize everything on CPU we

16113
11:25:59,400 --> 11:26:04,360
allocate on device we move everything to

16114
11:26:02,080 --> 11:26:06,840
device um and then we're ready to run

16115
11:26:04,360 --> 11:26:11,276
right and then in this case all we would

16116
11:26:06,840 --> 11:26:15,360
do is um initialize this neural

16117
11:26:11,276 --> 11:26:18,240
net we we initialize it with with with

16118
11:26:15,360 --> 11:26:22,640
random data with random data

16119
11:26:18,240 --> 11:26:27,756
values uh then we load in our entire uh

16120
11:26:22,640 --> 11:26:30,640
training set into um into the host

16121
11:26:27,756 --> 11:26:32,680
memory we go and train everything and

16122
11:26:30,640 --> 11:26:34,800
when we're done we can free whatever we

16123
11:26:32,680 --> 11:26:37,240
need to on CPU and free whatever we need

16124
11:26:34,800 --> 11:26:39,800
to on on device so if I go ahead and

16125
11:26:37,240 --> 11:26:39,800
give this a run

16126
11:26:42,276 --> 11:26:48,080
here B1

16127
11:26:45,080 --> 11:26:48,080
not going to run with good

16128
11:26:48,200 --> 11:26:52,560
boss so variable I was used member

16129
11:26:50,880 --> 11:26:55,320
reference don't worry about that a

16130
11:26:52,560 --> 11:26:59,680
warning then we go and run this we can

16131
11:26:55,320 --> 11:27:02,040
see um this trains insanely fast uh we

16132
11:26:59,680 --> 11:27:04,436
go from uh Epoch one we have three

16133
11:27:02,040 --> 11:27:07,520
epochs in here total each one doing 2500

16134
11:27:04,436 --> 11:27:09,640
HS and we get a loss of about 2.3 which

16135
11:27:07,520 --> 11:27:12,560
is as we'd expect and then we see the

16136
11:27:09,640 --> 11:27:16,080
accuracy increase up to 60% and then it

16137
11:27:12,560 --> 11:27:18,560
gets even it jumps right goes up to goes

16138
11:27:16,080 --> 11:27:23,040
up to about 87% and then jumps even

16139
11:27:18,560 --> 11:27:25,480
higher and we end up at around 90% right

16140
11:27:23,040 --> 11:27:27,480
so that's pretty good and we can uh we

16141
11:27:25,480 --> 11:27:30,560
can actually run this with bigger Hye

16142
11:27:27,480 --> 11:27:33,720
parameters so I can go ahead and plug in

16143
11:27:30,560 --> 11:27:36,436
10,24 there and use a bigger batch size

16144
11:27:33,720 --> 11:27:39,840
maybe like maybe eight and then epox is

16145
11:27:36,436 --> 11:27:45,436
five we pop into here compile that we go

16146
11:27:39,840 --> 11:27:48,040
and run it we can see that um

16147
11:27:45,436 --> 11:27:51,276
um get a lot better accuracy right so

16148
11:27:48,040 --> 11:27:52,640
even up to 92% now so it's kind of what

16149
11:27:51,276 --> 11:27:54,480
what this part is is it's called

16150
11:27:52,640 --> 11:27:56,720
grocking so you get the first part where

16151
11:27:54,480 --> 11:27:58,400
it's where it's like just starting on on

16152
11:27:56,720 --> 11:28:00,160
its training steps and it's sort of

16153
11:27:58,400 --> 11:28:01,480
figuring out which weights to push in

16154
11:28:00,160 --> 11:28:02,840
the right direction and then you and

16155
11:28:01,480 --> 11:28:05,916
then it figures that out and then it

16156
11:28:02,840 --> 11:28:08,436
plummets the loss drops like really fast

16157
11:28:05,916 --> 11:28:10,080
which is what we're seeing um right here

16158
11:28:08,436 --> 11:28:13,560
with the loss is like 2.3 and then it's

16159
11:28:10,080 --> 11:28:17,436
1.72 and then boom 0.5 then all the way

16160
11:28:13,560 --> 11:28:20,320
down to like 2.9 0.3 is um and then

16161
11:28:17,436 --> 11:28:22,436
what's Happening Here is it now no it

16162
11:28:20,320 --> 11:28:24,160
now can no longer use the easy patterns

16163
11:28:22,436 --> 11:28:25,720
that it recognized and now it has to

16164
11:28:24,160 --> 11:28:27,240
search for more for more difficult

16165
11:28:25,720 --> 11:28:28,756
attributes there might be certain images

16166
11:28:27,240 --> 11:28:31,756
that it has a really hard time

16167
11:28:28,756 --> 11:28:33,436
recognizing and it has to and it has to

16168
11:28:31,756 --> 11:28:35,880
you know learn additional stuff which

16169
11:28:33,436 --> 11:28:37,596
takes more training steps to do um in

16170
11:28:35,880 --> 11:28:39,720
that process of deeply understanding the

16171
11:28:37,596 --> 11:28:42,756
data set or or generalizing over it

16172
11:28:39,720 --> 11:28:45,360
that's called grocking right

16173
11:28:42,756 --> 11:28:46,240
um hence the grock language model I was

16174
11:28:45,360 --> 11:28:50,276
using

16175
11:28:46,240 --> 11:28:54,320
before um but yeah so if we step out of

16176
11:28:50,276 --> 11:28:56,756
this and go over to um go over to this

16177
11:28:54,320 --> 11:28:58,276
this v file I named it V because it's

16178
11:28:56,756 --> 11:29:02,276
supposed to be fast and it's the one

16179
11:28:58,276 --> 11:29:05,116
that you're supposed to edit later go to

16180
11:29:02,276 --> 11:29:08,116
room and I was doing some comparisons

16181
11:29:05,116 --> 11:29:10,000
here but going remove those

16182
11:29:08,116 --> 11:29:12,400
compare and

16183
11:29:10,000 --> 11:29:13,840
compare and we have this other comparing

16184
11:29:12,400 --> 11:29:17,360
file which was experimental but I'll

16185
11:29:13,840 --> 11:29:21,160
probably remove that soon um and then we

16186
11:29:17,360 --> 11:29:23,680
go into this other one v1c inside of the

16187
11:29:21,160 --> 11:29:25,956
room file and this

16188
11:29:23,680 --> 11:29:27,640
one is pretty close to what we had

16189
11:29:25,956 --> 11:29:31,200
before now the only thing we actually

16190
11:29:27,640 --> 11:29:32,880
change here is instead of having uh in

16191
11:29:31,200 --> 11:29:35,080
we actually make this easier for you so

16192
11:29:32,880 --> 11:29:37,360
in the past one we kind of simplified it

16193
11:29:35,080 --> 11:29:39,080
and had all of the and had and did like

16194
11:29:37,360 --> 11:29:41,520
the map Ms a bit differently where we

16195
11:29:39,080 --> 11:29:43,436
transposed inside of the kernel but in

16196
11:29:41,520 --> 11:29:45,240
this example um we want to make it

16197
11:29:43,436 --> 11:29:48,480
easier for to just plug your own code

16198
11:29:45,240 --> 11:29:50,756
into here and have it work um so using

16199
11:29:48,480 --> 11:29:52,240
like the sjem the the sjem Cuda

16200
11:29:50,756 --> 11:29:54,040
optimizations we did before in the

16201
11:29:52,240 --> 11:29:56,200
faster mmal chapter like that's what you

16202
11:29:54,040 --> 11:29:57,480
would plug into here right um you'd have

16203
11:29:56,200 --> 11:30:01,080
your your own kernel and you would

16204
11:29:57,480 --> 11:30:03,240
launch it and from here um and then

16205
11:30:01,080 --> 11:30:06,240
inside

16206
11:30:03,240 --> 11:30:06,240
of

16207
11:30:07,680 --> 11:30:14,480
um and then inside of if I go

16208
11:30:11,840 --> 11:30:16,640
down we can see a transpose kernel up

16209
11:30:14,480 --> 11:30:18,000
there but if we go into backward pass

16210
11:30:16,640 --> 11:30:19,756
there's nothing modified in the forward

16211
11:30:18,000 --> 11:30:22,640
because there's no transposing there but

16212
11:30:19,756 --> 11:30:24,480
in the backward pass we can see that

16213
11:30:22,640 --> 11:30:26,080
there's just transpose Matrix function

16214
11:30:24,480 --> 11:30:27,520
right transpose transpose transpose

16215
11:30:26,080 --> 11:30:29,800
because we have to do this three times

16216
11:30:27,520 --> 11:30:32,160
we have to calculate uh this one this

16217
11:30:29,800 --> 11:30:33,436
one and this one there's no dx1 right we

16218
11:30:32,160 --> 11:30:35,276
we don't have to that's redundant

16219
11:30:33,436 --> 11:30:37,956
because we don't have a layer before it

16220
11:30:35,276 --> 11:30:40,000
um so we do three three of those three

16221
11:30:37,956 --> 11:30:42,800
transposed mammal

16222
11:30:40,000 --> 11:30:45,840
operations and so in here this literally

16223
11:30:42,800 --> 11:30:48,560
just switches it from column major to

16224
11:30:45,840 --> 11:30:50,520
row major that's all it does uh it's

16225
11:30:48,560 --> 11:30:52,000
just a cool little trick there is a

16226
11:30:50,520 --> 11:30:55,596
custom kernel for it that you can review

16227
11:30:52,000 --> 11:30:55,596
if you want to

16228
11:30:56,800 --> 11:31:03,520
um we where did it

16229
11:31:00,480 --> 11:31:05,480
go right here so there's this transpose

16230
11:31:03,520 --> 11:31:07,520
Matrix function that we call we pass in

16231
11:31:05,480 --> 11:31:08,720
these device inputs into it and then

16232
11:31:07,520 --> 11:31:10,880
inside of here we actually do the

16233
11:31:08,720 --> 11:31:13,400
transpose launch and we we make sure

16234
11:31:10,880 --> 11:31:16,560
that no errors happen and we synchronize

16235
11:31:13,400 --> 11:31:19,360
um all of the threads on on the GPU

16236
11:31:16,560 --> 11:31:21,520
right this is where the actual uh this

16237
11:31:19,360 --> 11:31:24,240
is where the actual trans uh trans

16238
11:31:21,520 --> 11:31:27,320
transposing happens um which isn't too

16239
11:31:24,240 --> 11:31:29,200
conceptually bad um but don't don't

16240
11:31:27,320 --> 11:31:30,956
worry about this too much it's more so

16241
11:31:29,200 --> 11:31:32,160
worrying about how do you speed this

16242
11:31:30,956 --> 11:31:34,560
thing

16243
11:31:32,160 --> 11:31:38,040
up so I can go Ahad and run this

16244
11:31:34,560 --> 11:31:40,680
actually and uh like I'll we change here

16245
11:31:38,040 --> 11:31:43,240
is literally just the just the transpose

16246
11:31:40,680 --> 11:31:46,720
oh I I had some KU stuff at the top let

16247
11:31:43,240 --> 11:31:46,720
me remove

16248
11:31:50,720 --> 11:31:53,720
that

16249
11:31:54,520 --> 11:31:58,320
Bloss I'll just add that in

16250
11:31:59,956 --> 11:32:04,520
temporarily we just

16251
11:32:02,240 --> 11:32:06,320
compile I was messing around with Koss

16252
11:32:04,520 --> 11:32:07,880
like this is a totally experimental file

16253
11:32:06,320 --> 11:32:10,800
so don't like I'm just kind of like

16254
11:32:07,880 --> 11:32:15,360
screwing around with this one um but we

16255
11:32:10,800 --> 11:32:17,080
could just do link Koss like

16256
11:32:15,360 --> 11:32:20,000
like

16257
11:32:17,080 --> 11:32:23,680
that and go and run this and we can see

16258
11:32:20,000 --> 11:32:26,000
it's it's also pretty quick too um yeah

16259
11:32:23,680 --> 11:32:28,436
so it trains it trains the same on 250

16260
11:32:26,000 --> 11:32:33,000
on hidden size 256 which is what we have

16261
11:32:28,436 --> 11:32:34,520
or this is 1024 actually um so 1024 and

16262
11:32:33,000 --> 11:32:37,480
we give it a batch size of eight only

16263
11:32:34,520 --> 11:32:39,040
three Epoch to learn though um it takes

16264
11:32:37,480 --> 11:32:42,400
you know it gets up to about 90%

16265
11:32:39,040 --> 11:32:44,400
accuracy which is still good um but yeah

16266
11:32:42,400 --> 11:32:46,080
on a reduced on a reduced um on a

16267
11:32:44,400 --> 11:32:48,400
reduced number of training samples so we

16268
11:32:46,080 --> 11:32:50,520
do batch size eight instead of four so

16269
11:32:48,400 --> 11:32:52,040
it gets it gets more like it gets twice

16270
11:32:50,520 --> 11:32:54,040
the amount of generalization because it

16271
11:32:52,040 --> 11:32:56,596
has double the batch size but the amount

16272
11:32:54,040 --> 11:32:58,756
of training samples is cut in half right

16273
11:32:56,596 --> 11:33:00,276
so it's actually like not that bad um

16274
11:32:58,756 --> 11:33:03,040
when you think about it so we could like

16275
11:33:00,276 --> 11:33:06,756
bump this epox number up to

16276
11:33:03,040 --> 11:33:09,320
six and you would see how how much of

16277
11:33:06,756 --> 11:33:12,320
a how much of a difference that actually

16278
11:33:09,320 --> 11:33:12,320
makes

16279
11:33:17,720 --> 11:33:22,320
so we can see that it's kind of going up

16280
11:33:19,436 --> 11:33:23,916
to 92 which is you know this last phase

16281
11:33:22,320 --> 11:33:26,880
here was it was grocking is what it was

16282
11:33:23,916 --> 11:33:30,560
doing there um but yeah so we got up to

16283
11:33:26,880 --> 11:33:33,160
about 92% um close to 90 93 in some

16284
11:33:30,560 --> 11:33:35,080
cases um but yeah that's decent um

16285
11:33:33,160 --> 11:33:37,040
that's you know I know most humans are

16286
11:33:35,080 --> 11:33:39,436
better than that but for 10 seconds of

16287
11:33:37,040 --> 11:33:40,880
training I think that's pretty good for

16288
11:33:39,436 --> 11:33:44,480
having no knowledge about the world at

16289
11:33:40,880 --> 11:33:46,276
all this neural network did fine

16290
11:33:44,480 --> 11:33:48,436
now over time I am going to add

16291
11:33:46,276 --> 11:33:50,276
optimizations to this but since you're

16292
11:33:48,436 --> 11:33:52,640
watching this right now this does not

16293
11:33:50,276 --> 11:33:54,240
exist in the current repo the version of

16294
11:33:52,640 --> 11:33:55,640
this course you're watching right now

16295
11:33:54,240 --> 11:33:57,956
whether that be 5 Years From when it was

16296
11:33:55,640 --> 11:34:00,880
posted whether that be two months one

16297
11:33:57,956 --> 11:34:03,436
day um this is the current version that

16298
11:34:00,880 --> 11:34:05,800
you're seeing right now and so this

16299
11:34:03,436 --> 11:34:08,040
might be different by the time I've

16300
11:34:05,800 --> 11:34:10,680
updated the GitHub repo in the future uh

16301
11:34:08,040 --> 11:34:12,596
I do plan to maintain this and add in

16302
11:34:10,680 --> 11:34:14,116
you know additional like maybe a V2 with

16303
11:34:12,596 --> 11:34:16,840
like you know I I'll make sure name

16304
11:34:14,116 --> 11:34:18,360
everything of course but uh just to just

16305
11:34:16,840 --> 11:34:20,800
to go in and like add some extra

16306
11:34:18,360 --> 11:34:23,320
features for example like I might I

16307
11:34:20,800 --> 11:34:27,160
might add in like a a really fast uh

16308
11:34:23,320 --> 11:34:29,480
like custom uh row major kernel where we

16309
11:34:27,160 --> 11:34:31,956
do like tensor core operations so the

16310
11:34:29,480 --> 11:34:35,916
warp uh the Warped Matrix multiply

16311
11:34:31,956 --> 11:34:39,160
accumulate the W MMA with tf32 Precision

16312
11:34:35,916 --> 11:34:41,000
that stuff is really fast uh and then as

16313
11:34:39,160 --> 11:34:44,000
well another optimization you could take

16314
11:34:41,000 --> 11:34:46,200
over is using Cuda streams so remember

16315
11:34:44,000 --> 11:34:49,116
in in uh in the concurrency chapter and

16316
11:34:46,200 --> 11:34:50,840
and no in the in chapter number five

16317
11:34:49,116 --> 11:34:52,240
where we went you know more deep deeper

16318
11:34:50,840 --> 11:34:55,436
into kernels and the whole Cuda

16319
11:34:52,240 --> 11:34:57,000
architecture you can use streams to uh

16320
11:34:55,436 --> 11:34:59,240
make things run concurrently right so

16321
11:34:57,000 --> 11:35:02,276
you could be loading in some data uh and

16322
11:34:59,240 --> 11:35:04,840
then you know doing say like a a forward

16323
11:35:02,276 --> 11:35:08,720
you could do like a forward

16324
11:35:04,840 --> 11:35:10,480
and um a forward and a backward pass and

16325
11:35:08,720 --> 11:35:12,680
then while that's happening you could be

16326
11:35:10,480 --> 11:35:14,480
loading in the next piece of data right

16327
11:35:12,680 --> 11:35:16,400
so I mean obviously this is just like a

16328
11:35:14,480 --> 11:35:17,640
digit digit classification and you're

16329
11:35:16,400 --> 11:35:19,720
not going to be super performance

16330
11:35:17,640 --> 11:35:21,680
limited here there's not a need for like

16331
11:35:19,720 --> 11:35:23,800
having super super high throughput CU

16332
11:35:21,680 --> 11:35:26,680
you can you can get this thing up to

16333
11:35:23,800 --> 11:35:28,240
like 99% accuracy if you if you make you

16334
11:35:26,680 --> 11:35:30,800
know deeper layers and you increase

16335
11:35:28,240 --> 11:35:32,240
hidden size and adjust all these things

16336
11:35:30,800 --> 11:35:34,116
it's pretty easy to get this thing to

16337
11:35:32,240 --> 11:35:35,880
perform well but this is the type of

16338
11:35:34,116 --> 11:35:39,596
thing you want to practice so that when

16339
11:35:35,880 --> 11:35:41,520
you write more more comp complex kernels

16340
11:35:39,596 --> 11:35:43,880
it's not as difficult to start with

16341
11:35:41,520 --> 11:35:46,320
right so you know there lots of

16342
11:35:43,880 --> 11:35:50,040
optimizations you can do you can add in

16343
11:35:46,320 --> 11:35:50,040
uh in this where is

16344
11:35:50,916 --> 11:35:59,116
it in this uh Matrix multiply kernel in

16345
11:35:56,240 --> 11:36:00,720
here you can switch this out with stuff

16346
11:35:59,116 --> 11:36:03,080
I'm not going to switch it out right now

16347
11:36:00,720 --> 11:36:04,436
because that's something that you kind

16348
11:36:03,080 --> 11:36:05,596
of want to do as a part of the final

16349
11:36:04,436 --> 11:36:07,756
project something you want to do

16350
11:36:05,596 --> 11:36:09,800
self-guided and and sort of go into it

16351
11:36:07,756 --> 11:36:11,840
on your own so I mean you can use this

16352
11:36:09,800 --> 11:36:14,400
as is but if you want to have some fun

16353
11:36:11,840 --> 11:36:16,000
this is just simple mamal kernel here

16354
11:36:14,400 --> 11:36:19,080
there's no transposing or anything this

16355
11:36:16,000 --> 11:36:21,400
is R major and you can have fun with it

16356
11:36:19,080 --> 11:36:23,880
uh so so that's that feel free to change

16357
11:36:21,400 --> 11:36:28,240
this kernel maybe experiment with tensor

16358
11:36:23,880 --> 11:36:29,916
core operations WMA stuff um and then

16359
11:36:28,240 --> 11:36:33,080
you know Cuda streams or something like

16360
11:36:29,916 --> 11:36:36,400
that feel free to use the uh ncu

16361
11:36:33,080 --> 11:36:39,276
profiler um and yeah hopefully this gave

16362
11:36:36,400 --> 11:36:42,360
you a better Insight on how to kind of

16363
11:36:39,276 --> 11:36:44,000
build up projects and how um and how

16364
11:36:42,360 --> 11:36:46,000
while they they might look Lex on the

16365
11:36:44,000 --> 11:36:47,436
outside you can sort of dig in and

16366
11:36:46,000 --> 11:36:49,840
figure out what's going

16367
11:36:47,436 --> 11:36:51,276
on now just really quick to run all

16368
11:36:49,840 --> 11:36:52,880
these again just so everything is

16369
11:36:51,276 --> 11:36:55,560
crystal clear that these are performing

16370
11:36:52,880 --> 11:37:00,560
the same um we'll go ahead and edit each

16371
11:36:55,560 --> 11:37:05,160
of them back to you know 56 so we'll do

16372
11:37:00,560 --> 11:37:09,640
256 there epox we'll do like three we'll

16373
11:37:05,160 --> 11:37:14,116
do bat size four learning rate is 1 *

16374
11:37:09,640 --> 11:37:17,240
103 and then in here we'll do

16375
11:37:14,116 --> 11:37:17,240
256 as

16376
11:37:17,880 --> 11:37:24,160
well uh batch size we'll low that down

16377
11:37:20,916 --> 11:37:25,956
lower that down before inside of our

16378
11:37:24,160 --> 11:37:29,956
CPU we

16379
11:37:25,956 --> 11:37:29,956
will uh turn this up to

16380
11:37:31,916 --> 11:37:39,200
2506 batch size is four Epoch three

16381
11:37:35,000 --> 11:37:43,160
that's good then we pop into our python

16382
11:37:39,200 --> 11:37:45,160
here set the uh torch reference script

16383
11:37:43,160 --> 11:37:50,640
to to

16384
11:37:45,160 --> 11:37:53,680
um this is 256 already uh 1 * 10-3 batch

16385
11:37:50,640 --> 11:37:56,560
size 4 we're looking good and then we go

16386
11:37:53,680 --> 11:38:00,680
to the C friendly

16387
11:37:56,560 --> 11:38:04,640
script I scrolled a little too far 26

16388
11:38:00,680 --> 11:38:07,640
good bat size four awesome so

16389
11:38:04,640 --> 11:38:12,080
now now we go into python I'll run

16390
11:38:07,640 --> 11:38:12,080
python uh torch reference

16391
11:38:17,040 --> 11:38:20,880
give this a second the data loading is

16392
11:38:19,160 --> 11:38:24,560
takes a little while in Python sometimes

16393
11:38:20,880 --> 11:38:27,480
it's not the most optimized thing ever

16394
11:38:24,560 --> 11:38:30,800
so

16395
11:38:27,480 --> 11:38:33,880
uh awesome so we end up with about 90%

16396
11:38:30,800 --> 11:38:37,436
90% accuracy in the end you know

16397
11:38:33,880 --> 11:38:40,040
89 here we have 87 in this case so we

16398
11:38:37,436 --> 11:38:42,756
end up with about 90 in the end let's

16399
11:38:40,040 --> 11:38:45,916
memorize that number 90 and then we go

16400
11:38:42,756 --> 11:38:49,240
to python C

16401
11:38:45,916 --> 11:38:51,520
friendly and we

16402
11:38:49,240 --> 11:38:54,080
get

16403
11:38:51,520 --> 11:38:57,840
87%

16404
11:38:54,080 --> 11:39:04,040
89% about 90% I had I had five ax in

16405
11:38:57,840 --> 11:39:08,640
there um yeah about 90 90% as well

16406
11:39:04,040 --> 11:39:11,160
91% and then we navigate over to the CD

16407
11:39:08,640 --> 11:39:15,640
into naive

16408
11:39:11,160 --> 11:39:16,596
CPU we go GCC compile with math we'll go

16409
11:39:15,640 --> 11:39:19,240
and run

16410
11:39:16,596 --> 11:39:21,436
this in

16411
11:39:19,240 --> 11:39:23,276
the this is going this is going to take

16412
11:39:21,436 --> 11:39:25,680
a

16413
11:39:23,276 --> 11:39:27,436
second um it's not used to going this

16414
11:39:25,680 --> 11:39:29,480
fast I know numpy probably has more

16415
11:39:27,436 --> 11:39:33,040
specialized Mill routine so a lot of

16416
11:39:29,480 --> 11:39:33,040
this is just

16417
11:39:33,880 --> 11:39:39,880
uh or sorry um yeah yeah numpy it

16418
11:39:38,276 --> 11:39:42,596
probably has more specialized routines

16419
11:39:39,880 --> 11:39:47,160
here so just doing it in raw C like

16420
11:39:42,596 --> 11:39:47,160
naively is going to take a while

16421
11:39:48,596 --> 11:39:52,480
um yeah so we can see

16422
11:39:52,720 --> 11:39:57,720
this we end up at

16423
11:39:58,756 --> 11:40:03,880
about give it a

16424
11:40:00,880 --> 11:40:07,596
second pretty close to 90% as well so

16425
11:40:03,880 --> 11:40:09,880
88.5 slightly worse um but that's that's

16426
11:40:07,596 --> 11:40:09,880
almost

16427
11:40:10,800 --> 11:40:14,800
negligible now

16428
11:40:18,040 --> 11:40:26,320
we have to what am I do CD into Cuda

16429
11:40:21,756 --> 11:40:27,840
slash GPU and then we'll do nvcc compile

16430
11:40:26,320 --> 11:40:31,160
without

16431
11:40:27,840 --> 11:40:33,596
Coss onun that look how much faster that

16432
11:40:31,160 --> 11:40:35,840
is

16433
11:40:33,596 --> 11:40:38,160
right new total of three Epoch and we

16434
11:40:35,840 --> 11:40:39,596
end up with boom 90% oh how how

16435
11:40:38,160 --> 11:40:42,116
convenient is that

16436
11:40:39,596 --> 11:40:44,160
hey and then we'll head out and we'll go

16437
11:40:42,116 --> 11:40:48,400
to um

16438
11:40:44,160 --> 11:40:50,436
the the room file

16439
11:40:48,400 --> 11:40:54,480
room

16440
11:40:50,436 --> 11:40:55,800
and pile with Koss you I just added the

16441
11:40:54,480 --> 11:40:58,240
kublos thing because you can add your

16442
11:40:55,800 --> 11:41:02,240
own kublos sjam or like the LT matol in

16443
11:40:58,240 --> 11:41:04,480
and just play with that um you run this

16444
11:41:02,240 --> 11:41:06,720
about the same speed and we end up with

16445
11:41:04,480 --> 11:41:09,360
about 90% as well so everything's

16446
11:41:06,720 --> 11:41:11,116
getting about 90% which is good shows

16447
11:41:09,360 --> 11:41:15,160
results are kind of consistent just like

16448
11:41:11,116 --> 11:41:16,560
make sure that's all cleared up

16449
11:41:15,160 --> 11:41:17,916
give yourself a pat on the back if you

16450
11:41:16,560 --> 11:41:21,040
made it this far it's pretty much the

16451
11:41:17,916 --> 11:41:22,916
end of the course you made it good job

16452
11:41:21,040 --> 11:41:25,560
um I'm just going to go over some quick

16453
11:41:22,916 --> 11:41:26,916
little little tips points in the right

16454
11:41:25,560 --> 11:41:29,360
direction if you want to continue with

16455
11:41:26,916 --> 11:41:31,596
this stuff um you know it it probably

16456
11:41:29,360 --> 11:41:34,680
was hard to grasp everything so I

16457
11:41:31,596 --> 11:41:37,436
understand if you don't but if you do uh

16458
11:41:34,680 --> 11:41:40,200
I have some extra resources for you so

16459
11:41:37,436 --> 11:41:42,240
inside of the read readme file here I

16460
11:41:40,200 --> 11:41:44,360
have a section on you know like what is

16461
11:41:42,240 --> 11:41:45,560
Unified memory memory architectures

16462
11:41:44,360 --> 11:41:47,756
which I thought would be you know kind

16463
11:41:45,560 --> 11:41:49,200
of useful and you might be interested in

16464
11:41:47,756 --> 11:41:51,720
but mainly what I what I want to cover

16465
11:41:49,200 --> 11:41:52,956
right now is I'm going to add to this

16466
11:41:51,720 --> 11:41:55,520
I'm going to add to this read me file as

16467
11:41:52,956 --> 11:41:57,320
well in the future um but there's

16468
11:41:55,520 --> 11:41:58,436
there's a section on dive deeper and

16469
11:41:57,320 --> 11:42:00,400
this is like if you want to take that

16470
11:41:58,436 --> 11:42:03,400
extra step and really figure out how you

16471
11:42:00,400 --> 11:42:05,200
can apply deep deep um deep

16472
11:42:03,400 --> 11:42:06,880
optimizations and advancements and

16473
11:42:05,200 --> 11:42:09,160
whatever you want to call it in in Cuda

16474
11:42:06,880 --> 11:42:10,560
and GPU programming uh especially in

16475
11:42:09,160 --> 11:42:12,840
deep learning this is this is what you

16476
11:42:10,560 --> 11:42:14,360
can do so there's this thing called

16477
11:42:12,840 --> 11:42:17,880
Quant ization which I'm going to start

16478
11:42:14,360 --> 11:42:20,880
with quantization is where you is where

16479
11:42:17,880 --> 11:42:24,560
you go from uh say

16480
11:42:20,880 --> 11:42:26,916
fp32 and you go down to say fp16 or int

16481
11:42:24,560 --> 11:42:28,680
8 you can you can actually go down you

16482
11:42:26,916 --> 11:42:30,800
can actually go from fp32 to int 8 and

16483
11:42:28,680 --> 11:42:32,916
you can still have really really good

16484
11:42:30,800 --> 11:42:35,640
performance and and quality of the of

16485
11:42:32,916 --> 11:42:37,200
the Precision on models right so there

16486
11:42:35,640 --> 11:42:39,400
are specific ways you can you can do

16487
11:42:37,200 --> 11:42:42,720
tricks around this but a lot of it has

16488
11:42:39,400 --> 11:42:44,480
to do with uh you know if your if your

16489
11:42:42,720 --> 11:42:46,480
range is is limited so if you if you can

16490
11:42:44,480 --> 11:42:50,880
hit like a maximum of say like I don't

16491
11:42:46,480 --> 11:42:52,116
know 10 and a minimum of - 10 then you

16492
11:42:50,880 --> 11:42:53,956
don't actually have to worry about a lot

16493
11:42:52,116 --> 11:42:56,000
of those exponent values right if your

16494
11:42:53,956 --> 11:42:57,436
weights are initialized and your

16495
11:42:56,000 --> 11:42:59,200
training is stable and nothing's going

16496
11:42:57,436 --> 11:43:01,320
to go like above or below 10 you don't

16497
11:42:59,200 --> 11:43:02,360
have to worry about it um you could

16498
11:43:01,320 --> 11:43:03,520
literally just cap that as your

16499
11:43:02,360 --> 11:43:06,520
Precision right and that'll be the

16500
11:43:03,520 --> 11:43:09,360
maximum it can go um those will be like

16501
11:43:06,520 --> 11:43:11,276
the the more sparse values right uh so

16502
11:43:09,360 --> 11:43:12,720
quantization is pretty much just the art

16503
11:43:11,276 --> 11:43:14,640
of doing that which is like taking

16504
11:43:12,720 --> 11:43:16,160
numbers that are really high precision

16505
11:43:14,640 --> 11:43:17,880
and then moving them down to lower

16506
11:43:16,160 --> 11:43:19,436
Precision doing really really fast

16507
11:43:17,880 --> 11:43:21,916
operations with those because I can tell

16508
11:43:19,436 --> 11:43:25,240
you for a fact int 8 is a lot faster

16509
11:43:21,916 --> 11:43:27,400
than fp32 like not just by four times

16510
11:43:25,240 --> 11:43:30,080
it's by quite a lot um and we saw that

16511
11:43:27,400 --> 11:43:32,756
in the kuo versus kubos LT section where

16512
11:43:30,080 --> 11:43:35,360
we compared 32-bit versus 16bit and then

16513
11:43:32,756 --> 11:43:36,756
performance was pretty substantial so

16514
11:43:35,360 --> 11:43:38,000
you can imagine what in8 would be

16515
11:43:36,756 --> 11:43:40,756
because it's just integers there's no

16516
11:43:38,000 --> 11:43:43,400
floating Point numbers to to worry about

16517
11:43:40,756 --> 11:43:46,436
there's no decimal places right just

16518
11:43:43,400 --> 11:43:49,080
inate so quantization is pretty cool

16519
11:43:46,436 --> 11:43:52,240
it's used a lot in current models like

16520
11:43:49,080 --> 11:43:54,840
uh you know say gp4 or like llama uh

16521
11:43:52,240 --> 11:43:57,000
llama 405b if you've heard of that one

16522
11:43:54,840 --> 11:43:59,720
uh like a lot of these actually use uh

16523
11:43:57,000 --> 11:44:02,680
quantization right so most likely like

16524
11:43:59,720 --> 11:44:04,436
bf16 or like fp8 or something like that

16525
11:44:02,680 --> 11:44:06,640
some of them even use float 4 which is

16526
11:44:04,436 --> 11:44:08,200
cool um then there's tensor cores which

16527
11:44:06,640 --> 11:44:09,756
I talked about already but I can't I

16528
11:44:08,200 --> 11:44:11,880
can't leave it out tensor cores are

16529
11:44:09,756 --> 11:44:13,360
great um I'm just not covering it in

16530
11:44:11,880 --> 11:44:15,956
this because this is kind kind of like

16531
11:44:13,360 --> 11:44:17,680
an intro it's kind of an intro course so

16532
11:44:15,956 --> 11:44:20,520
I triy to like pack as much as possible

16533
11:44:17,680 --> 11:44:22,596
into a into a certain amount of hours

16534
11:44:20,520 --> 11:44:24,160
that you could you know uh digest and

16535
11:44:22,596 --> 11:44:26,800
then if you want to continue with that

16536
11:44:24,160 --> 11:44:30,116
there's obviously tensor course too um

16537
11:44:26,800 --> 11:44:32,360
sparcity is a cool one so sparsity is

16538
11:44:30,116 --> 11:44:33,640
you can think of sparcity as um if I

16539
11:44:32,360 --> 11:44:35,320
have like an

16540
11:44:33,640 --> 11:44:41,436
array

16541
11:44:35,320 --> 11:44:43,756
um say like it would be like 0 0 0 0o um

16542
11:44:41,436 --> 11:44:48,436
like7

16543
11:44:43,756 --> 11:44:51,560
0 0 z0 0 and then say over here we'd

16544
11:44:48,436 --> 11:44:53,520
have like a like a six right this is

16545
11:44:51,560 --> 11:44:57,756
this is what sparse means so there's a

16546
11:44:53,520 --> 11:44:59,640
bunch of zeros and there's an occasional

16547
11:44:57,756 --> 11:45:01,956
like very big number that represents a

16548
11:44:59,640 --> 11:45:03,680
lot right based on its position maybe

16549
11:45:01,956 --> 11:45:07,276
based on its position relative to other

16550
11:45:03,680 --> 11:45:09,200
numbers um but the idea here is that you

16551
11:45:07,276 --> 11:45:10,956
can actually store these in much smaller

16552
11:45:09,200 --> 11:45:13,276
memory so it's a it's more of a memory

16553
11:45:10,956 --> 11:45:15,200
and compute thing more than just like is

16554
11:45:13,276 --> 11:45:17,880
this like what quality do you get from

16555
11:45:15,200 --> 11:45:20,880
this it's really performance so what we

16556
11:45:17,880 --> 11:45:22,276
can actually do is we can say um you

16557
11:45:20,880 --> 11:45:23,800
know we're going to have two matrices

16558
11:45:22,276 --> 11:45:26,480
one with the values and one with the

16559
11:45:23,800 --> 11:45:30,160
coordinates so we go

16560
11:45:26,480 --> 11:45:36,400
-7 and six and then this other Matrix

16561
11:45:30,160 --> 11:45:42,596
would be um you know 0 1 2 3 4 so this

16562
11:45:36,400 --> 11:45:45,160
would be um 4 and then 5 6 7 8 9 10 11

16563
11:45:42,596 --> 11:45:48,916
12 12 13 14 15

16564
11:45:45,160 --> 11:45:52,000
16 right and so you would end up storing

16565
11:45:48,916 --> 11:45:54,360
only four integers instead of instead of

16566
11:45:52,000 --> 11:45:56,360
16 integers and you reduce everything by

16567
11:45:54,360 --> 11:45:58,640
a lot now imagine this when you scale up

16568
11:45:56,360 --> 11:46:00,480
to you know 2D or 3D structures you're

16569
11:45:58,640 --> 11:46:02,000
saving like orders of magnitude of

16570
11:46:00,480 --> 11:46:04,116
memory and it can be really really

16571
11:46:02,000 --> 11:46:05,916
efficient right so this is something to

16572
11:46:04,116 --> 11:46:07,276
consider when you're Divi uh when you're

16573
11:46:05,916 --> 11:46:09,956
you know designing highly performant

16574
11:46:07,276 --> 11:46:12,116
neural networks is um can we capitalize

16575
11:46:09,956 --> 11:46:13,720
on things like sparsity right that might

16576
11:46:12,116 --> 11:46:15,480
be encouraged by the you know the people

16577
11:46:13,720 --> 11:46:17,080
who are writing the neural net outside

16578
11:46:15,480 --> 11:46:19,400
so when they're just writing like the P

16579
11:46:17,080 --> 11:46:20,596
torch architecture if it favors sparsity

16580
11:46:19,400 --> 11:46:22,840
if it does really well with that and

16581
11:46:20,596 --> 11:46:24,200
that's what it runs on then this is

16582
11:46:22,840 --> 11:46:25,956
really good for you this makes your job

16583
11:46:24,200 --> 11:46:28,240
easy um but sparity is just a

16584
11:46:25,956 --> 11:46:29,640
performance hack uh you you know take it

16585
11:46:28,240 --> 11:46:32,200
when you can

16586
11:46:29,640 --> 11:46:34,400
right then there's this book Cuda by

16587
11:46:32,200 --> 11:46:38,520
example

16588
11:46:34,400 --> 11:46:40,160
so um this it's literally just a book in

16589
11:46:38,520 --> 11:46:42,436
a general purpose GPU programming I

16590
11:46:40,160 --> 11:46:45,160
found this off of a Google search so

16591
11:46:42,436 --> 11:46:47,276
it's just like one of those Edo websites

16592
11:46:45,160 --> 11:46:50,640
and uh yeah it has it has a bunch of

16593
11:46:47,276 --> 11:46:52,720
things in it so like CPUs rise of GPU

16594
11:46:50,640 --> 11:46:57,160
Computing right a lot of what I covered

16595
11:46:52,720 --> 11:46:57,160
um so like what is the c to architecture

16596
11:46:58,080 --> 11:47:02,800
um pretty much a lot of a lot of what I

16597
11:47:01,200 --> 11:47:04,400
said or a lot of a lot of the a lot of

16598
11:47:02,800 --> 11:47:06,116
the important parts in here are

16599
11:47:04,400 --> 11:47:07,596
compressed down into the course right so

16600
11:47:06,116 --> 11:47:10,436
obviously not all of it is and I didn't

16601
11:47:07,596 --> 11:47:14,040
I haven't read this book either um 300

16602
11:47:10,436 --> 11:47:16,040
pages so I haven't read this book book

16603
11:47:14,040 --> 11:47:17,916
but a lot of what you're going to find

16604
11:47:16,040 --> 11:47:20,800
in here is going to be uh compressed

16605
11:47:17,916 --> 11:47:20,800
down into this

16606
11:47:21,360 --> 11:47:26,520
course

16607
11:47:23,560 --> 11:47:29,436
now there's this other article by Simon

16608
11:47:26,520 --> 11:47:30,916
the guy who works at anthropic on data

16609
11:47:29,436 --> 11:47:33,276
parallel distributed training of deep

16610
11:47:30,916 --> 11:47:35,200
learning models so that other that other

16611
11:47:33,276 --> 11:47:39,116
chapter where we were talking about

16612
11:47:35,200 --> 11:47:41,240
getting um getting uh big big algorithms

16613
11:47:39,116 --> 11:47:44,436
to train across multiple

16614
11:47:41,240 --> 11:47:46,680
instances this

16615
11:47:44,436 --> 11:47:48,520
this is a good example of it so

16616
11:47:46,680 --> 11:47:50,596
distributed training is a big problem

16617
11:47:48,520 --> 11:47:52,720
right now is getting like data centers

16618
11:47:50,596 --> 11:47:54,520
into one compact place there is research

16619
11:47:52,720 --> 11:47:56,320
around it and helping reduce that you

16620
11:47:54,520 --> 11:48:00,956
know dist distributed

16621
11:47:56,320 --> 11:48:03,200
aspect but when you have you

16622
11:48:00,956 --> 11:48:04,520
have when you have a massive data center

16623
11:48:03,200 --> 11:48:06,756
of a bunch of models and you have to get

16624
11:48:04,520 --> 11:48:08,040
them to talk to a bunch of bunch of gpus

16625
11:48:06,756 --> 11:48:09,640
sorry and you have to get them mod talk

16626
11:48:08,040 --> 11:48:12,276
to each other a certain way it's hard

16627
11:48:09,640 --> 11:48:13,680
right so this kind of goes into that um

16628
11:48:12,276 --> 11:48:15,596
I'm not going to go through this entire

16629
11:48:13,680 --> 11:48:17,756
thing but this does go through more

16630
11:48:15,596 --> 11:48:21,040
performance optimizations things like

16631
11:48:17,756 --> 11:48:24,240
all reduce which are used for um the

16632
11:48:21,040 --> 11:48:26,360
actual uh optimization process so you'll

16633
11:48:24,240 --> 11:48:28,596
see like an atom W all reduce or

16634
11:48:26,360 --> 11:48:31,680
something um there's

16635
11:48:28,596 --> 11:48:33,840
a yeah it's there there is a lot to

16636
11:48:31,680 --> 11:48:35,436
consider here but I don't even have a

16637
11:48:33,840 --> 11:48:36,640
cluster to train this on so I can't

16638
11:48:35,436 --> 11:48:38,756
really teach this

16639
11:48:36,640 --> 11:48:41,520
part

16640
11:48:38,756 --> 11:48:43,360
um we go back there's a few projects

16641
11:48:41,520 --> 11:48:46,360
that I found was that were really cool

16642
11:48:43,360 --> 11:48:50,400
one of them was mnus Cuda or CNN sorry I

16643
11:48:46,360 --> 11:48:54,160
did memis Cuda which was this and then

16644
11:48:50,400 --> 11:48:56,480
this is the actual qnn uh qnn and kublos

16645
11:48:54,160 --> 11:48:58,640
for training on the nness data set this

16646
11:48:56,480 --> 11:49:01,276
uses I believe

16647
11:48:58,640 --> 11:49:04,040
convolutions so if we like were to go

16648
11:49:01,276 --> 11:49:06,400
into yeah see it's like a V Visual

16649
11:49:04,040 --> 11:49:10,116
Studio code project or whatever so this

16650
11:49:06,400 --> 11:49:13,436
might be easier if you're on Windows but

16651
11:49:10,116 --> 11:49:18,200
like if you go into for example the the

16652
11:49:13,436 --> 11:49:18,200
network C++ file

16653
11:49:19,480 --> 11:49:24,116
um yeah I'm not going to dig through

16654
11:49:22,680 --> 11:49:26,956
this but this is a cool little project

16655
11:49:24,116 --> 11:49:28,720
that I found um you know feel free to do

16656
11:49:26,956 --> 11:49:30,200
whatever you want with it but it would

16657
11:49:28,720 --> 11:49:33,720
it came up in the GitHub search results

16658
11:49:30,200 --> 11:49:36,916
when I searched for mist Cuda so do what

16659
11:49:33,720 --> 11:49:39,160
you will with that

16660
11:49:36,916 --> 11:49:40,200
um I'm not going to go Cuda mode right

16661
11:49:39,160 --> 11:49:45,040
now that's I'm going to save the best

16662
11:49:40,200 --> 11:49:48,160
for last microad Cuda is very similar to

16663
11:49:45,040 --> 11:49:49,360
um microad by kpoy so this is something

16664
11:49:48,160 --> 11:49:51,880
I touched on earlier and this is

16665
11:49:49,360 --> 11:49:54,000
something you should review heavily for

16666
11:49:51,880 --> 11:49:57,080
understanding uh how how things like

16667
11:49:54,000 --> 11:50:00,200
back propagation work so it's pretty

16668
11:49:57,080 --> 11:50:02,560
much like a like a like a pie torch

16669
11:50:00,200 --> 11:50:04,160
autograd but very very small so if we

16670
11:50:02,560 --> 11:50:07,160
actually go into the files for it go

16671
11:50:04,160 --> 11:50:10,240
into the microgr files itself there's an

16672
11:50:07,160 --> 11:50:12,320
engine for it so the values there's like

16673
11:50:10,240 --> 11:50:13,916
a like a value thing for it so what like

16674
11:50:12,320 --> 11:50:16,160
the op operations you can do like a like

16675
11:50:13,916 --> 11:50:17,436
a power so when you go double asteris

16676
11:50:16,160 --> 11:50:20,756
it's going to call this underscore

16677
11:50:17,436 --> 11:50:22,200
uncore power as a method right um then

16678
11:50:20,756 --> 11:50:24,400
the add is just like same thing you have

16679
11:50:22,200 --> 11:50:27,240
the plus and that's going to all the the

16680
11:50:24,400 --> 11:50:29,756
add method then outside of engine you

16681
11:50:27,240 --> 11:50:31,320
have the actual neural net. py which is

16682
11:50:29,756 --> 11:50:32,956
like brings up all the abstraction of

16683
11:50:31,320 --> 11:50:34,680
like going from neurons to layers so you

16684
11:50:32,956 --> 11:50:36,200
have like a single neuron with a set of

16685
11:50:34,680 --> 11:50:37,756
Weights in it that taken you know all

16686
11:50:36,200 --> 11:50:39,680
the different X values and then dot

16687
11:50:37,756 --> 11:50:43,360
product and then output one that's a

16688
11:50:39,680 --> 11:50:44,956
neuron right so you does like a do

16689
11:50:43,360 --> 11:50:47,756
product there we can see that very

16690
11:50:44,956 --> 11:50:50,956
clearly and then there's uh you know

16691
11:50:47,756 --> 11:50:54,720
like a layer where it does a bunch of

16692
11:50:50,956 --> 11:50:57,000
neurons um and then it will and then

16693
11:50:54,720 --> 11:50:59,800
it'll just like a layer of neurons just

16694
11:50:57,000 --> 11:51:01,720
like that right a bunch of lons stacked

16695
11:50:59,800 --> 11:51:03,200
on top of each other and then the MLP

16696
11:51:01,720 --> 11:51:05,240
which is like that layer but there's

16697
11:51:03,200 --> 11:51:09,080
multiple of

16698
11:51:05,240 --> 11:51:11,800
those um and then micro Cuda is just

16699
11:51:09,080 --> 11:51:15,276
that but implemented in Cuda right there

16700
11:51:11,800 --> 11:51:17,000
there had to be one so uh yeah feel free

16701
11:51:15,276 --> 11:51:18,880
to like have fun with this and

16702
11:51:17,000 --> 11:51:20,400
everything it's it's supposed to be

16703
11:51:18,880 --> 11:51:23,400
faster so you can kind of just

16704
11:51:20,400 --> 11:51:25,680
understand things on a level of uh

16705
11:51:23,400 --> 11:51:28,320
compute unified device

16706
11:51:25,680 --> 11:51:30,596
architecture there's you like operations

16707
11:51:28,320 --> 11:51:33,520
all the Cuda operations so move to gpus

16708
11:51:30,596 --> 11:51:34,756
is like Malik and mem copy um you know

16709
11:51:33,520 --> 11:51:37,880
it's very simple interface you can

16710
11:51:34,756 --> 11:51:40,200
imagine pytorch being similar to this um

16711
11:51:37,880 --> 11:51:43,000
probably more performance optimal

16712
11:51:40,200 --> 11:51:45,000
but um you don't want want to do like a

16713
11:51:43,000 --> 11:51:46,916
Cuda M Copy and a malic every time you

16714
11:51:45,000 --> 11:51:49,080
want to move something or use a piece of

16715
11:51:46,916 --> 11:51:52,000
data you know you have the naive matal

16716
11:51:49,080 --> 11:51:55,040
kernel of course um the tanh kernel

16717
11:51:52,000 --> 11:51:57,520
right all this uh but yeah so bunch of

16718
11:51:55,040 --> 11:51:58,840
cool projects people are doing and then

16719
11:51:57,520 --> 11:52:01,916
there's this other interesting one I

16720
11:51:58,840 --> 11:52:05,360
found second second best one uh GPU

16721
11:52:01,916 --> 11:52:07,940
puzzles so you can use the qy library so

16722
11:52:05,360 --> 11:52:09,436
go qy

16723
11:52:07,940 --> 11:52:13,000
[Music]

16724
11:52:09,436 --> 11:52:14,880
python so qy open source GPU accelerated

16725
11:52:13,000 --> 11:52:17,400
Computing with python so it's

16726
11:52:14,880 --> 11:52:19,400
essentially Cuda but you get to use it

16727
11:52:17,400 --> 11:52:23,480
through a python interface which is

16728
11:52:19,400 --> 11:52:26,480
awesome um we go to the GitHub for

16729
11:52:23,480 --> 11:52:26,480
this

16730
11:52:27,200 --> 11:52:31,560
Cy

16731
11:52:29,360 --> 11:52:33,956
right there's a bunch of cool stuff on

16732
11:52:31,560 --> 11:52:35,596
this you know you just import it and

16733
11:52:33,956 --> 11:52:38,240
then you can you can make like shapes

16734
11:52:35,596 --> 11:52:41,080
and stuff and do stuff with that um

16735
11:52:38,240 --> 11:52:43,000
similar to something like p t or nump

16736
11:52:41,080 --> 11:52:44,916
right but

16737
11:52:43,000 --> 11:52:47,956
yeah so these GPU puzzles are just like

16738
11:52:44,916 --> 11:52:50,080
going through um you know solving like

16739
11:52:47,956 --> 11:52:52,040
essentially the logic problems where we

16740
11:52:50,080 --> 11:52:53,956
had a krel solve an issue for us but

16741
11:52:52,040 --> 11:52:55,880
doing that for a bunch of different

16742
11:52:53,956 --> 11:52:57,320
examples right so instead of just matrix

16743
11:52:55,880 --> 11:52:58,956
multiplication there's like a lot of

16744
11:52:57,320 --> 11:53:01,560
other things in here which you might

16745
11:52:58,956 --> 11:53:04,560
find fun to

16746
11:53:01,560 --> 11:53:04,560
practice

16747
11:53:04,720 --> 11:53:11,640
um and then the last one which I decided

16748
11:53:07,400 --> 11:53:13,276
to save for last is Cuda mode so they

16749
11:53:11,640 --> 11:53:16,480
have a they have a g Hub they have a

16750
11:53:13,276 --> 11:53:20,080
YouTube channel they have a Discord

16751
11:53:16,480 --> 11:53:21,916
server and pretty much a bunch of this

16752
11:53:20,080 --> 11:53:24,400
is it like actually contains a lot of

16753
11:53:21,916 --> 11:53:26,560
material and Beyond what I covered in my

16754
11:53:24,400 --> 11:53:29,400
course um this one was more to be like

16755
11:53:26,560 --> 11:53:31,480
video assistive but the community behind

16756
11:53:29,400 --> 11:53:33,000
Cuda mode is amazing they have really

16757
11:53:31,480 --> 11:53:35,276
really good engineers and researchers

16758
11:53:33,000 --> 11:53:36,840
here um just like building cool stuff

16759
11:53:35,276 --> 11:53:40,080
constantly people being super active in

16760
11:53:36,840 --> 11:53:41,480
the community it's a great place so uh

16761
11:53:40,080 --> 11:53:43,640
this is something I'd absolutely

16762
11:53:41,480 --> 11:53:45,916
recommend you check out

16763
11:53:43,640 --> 11:53:47,880
um and uh yeah there's there's a lot of

16764
11:53:45,916 --> 11:53:50,080
chapters like you see like flash

16765
11:53:47,880 --> 11:53:55,436
attention right they have everything

16766
11:53:50,080 --> 11:53:59,880
cutless Triton um fused kernels data

16767
11:53:55,436 --> 11:54:02,276
processing um tensor cores right so a

16768
11:53:59,880 --> 11:54:04,080
bunch of cool things um I'd recommend

16769
11:54:02,276 --> 11:54:07,596
that you join their Discord server you

16770
11:54:04,080 --> 11:54:09,596
can find that where's their Discord

16771
11:54:07,596 --> 11:54:14,320
server

16772
11:54:09,596 --> 11:54:14,320
here uh yeah bunch of

16773
11:54:15,400 --> 11:54:19,400
essentially bunch of cool uh bunch of

16774
11:54:18,000 --> 11:54:21,720
cool groups and everything it's like

16775
11:54:19,400 --> 11:54:25,560
beginner

16776
11:54:21,720 --> 11:54:27,916
section right Super Active like today

16777
11:54:25,560 --> 11:54:31,520
the last message was the last message

16778
11:54:27,916 --> 11:54:34,080
was like not even what like a few hours

16779
11:54:31,520 --> 11:54:36,200
ago and that's just one channel right so

16780
11:54:34,080 --> 11:54:37,880
you go down here the last message was

16781
11:54:36,200 --> 11:54:41,276
like 1 hour

16782
11:54:37,880 --> 11:54:43,080
ago so if you enjoyed this course um you

16783
11:54:41,276 --> 11:54:45,480
can totally find me on other platforms

16784
11:54:43,080 --> 11:54:47,756
you can find me on YouTube you can find

16785
11:54:45,480 --> 11:54:49,560
me on x/ Twitter you can find me on

16786
11:54:47,756 --> 11:54:51,720
Discord I have a Discord server full of

16787
11:54:49,560 --> 11:54:53,160
a lot of people uh you know there's Cuda

16788
11:54:51,720 --> 11:54:54,680
mode as well but I also have a server

16789
11:54:53,160 --> 11:54:56,116
with a bunch of people and we you know

16790
11:54:54,680 --> 11:54:59,720
like to learn stuff and collaborate and

16791
11:54:56,116 --> 11:55:01,480
all that um yeah find me on YouTube find

16792
11:54:59,720 --> 11:55:03,360
me on LinkedIn find me on X find me on

16793
11:55:01,480 --> 11:55:04,680
Discord those are all going to be either

16794
11:55:03,360 --> 11:55:06,040
links in the description or if they're

16795
11:55:04,680 --> 11:55:09,756
not in the description they'll be in the

16796
11:55:06,040 --> 11:55:12,720
GitHub repo um in the description below

16797
11:55:09,756 --> 11:55:12,720
thank you for watching

